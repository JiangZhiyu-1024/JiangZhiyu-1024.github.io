<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/post/</link>
        <description>Recent content in Posts on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Tue, 05 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>安全与隐私机器学习：技术与应用的综述</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/</link>
        <pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/leaves-4574860_1280.jpg" alt="Featured image of post 安全与隐私机器学习：技术与应用的综述" /&gt;&lt;h1 id=&#34;安全与隐私机器学习技术与应用的综述&#34;&gt;☕安全与隐私机器学习：技术与应用的综述
&lt;/h1&gt;&lt;h2 id=&#34;abstract&#34;&gt;🥛Abstract
&lt;/h2&gt;&lt;p&gt;机器学习（ML）中的隐私违规可能导致歧视和身份盗窃等严重后果。随着近年来用于训练模型的敏感数据越来越多，保护隐私的机器学习方法的必要性变得更加重要。本综述研究全面回顾了隐私保护机器学习（PPML）的最新方法，如安全多方计算、同态加密和差分隐私。我们还评估了PPML的缺点，包括可扩展性、计算效率以及隐私与实用性之间的权衡。最后，我们识别了PPML中的开放问题和未来研究方向，包括新兴趋势、挑战和机会。本文旨在为对PPML领域感兴趣的研究人员和从业者提供有价值的资源。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;🍵Introduction
&lt;/h2&gt;&lt;p&gt;在当今的生活中，计算能力几乎与我们生活的每个方面都有互动。这种对计算机的高度依赖可能导致严重的隐私违规。计算机使用的主要趋势之一是机器学习（ML）。根据（Burkov, 2019），机器学习是“一个研究领域，专注于设计和开发能够从数据中学习和进行预测的算法，而不需要显式编程。换句话说，它涉及开发计算模型，这些模型可以根据从数据中获得的反馈自动改进其在特定任务上的表现。”机器学习正被用于解决医疗、金融等多个领域的许多生活问题。这项技术使用包含特征的数据集来构建所需的知识，以便得到正确的答案。有一种说法认为，训练阶段使用的数据记录越多，测试阶段的准确性就越高（Hey, Tansley, &amp;amp; Tolle, 2009）。从这个角度来看，科学家们开始请求与他们研究领域相关的数据，以便将其应用于机器学习模型。如果以医疗保健为例，根据健康保险流通与问责法案（HIPAA）规定，这些信息应通过建立国家标准来保护个人健康信息的隐私和安全，规范受保护健康信息（PHI）的使用和披露。 当这些数据被共享给科学家用于实验时，许多对策被考虑在内，例如隐藏个人身份信息（PII），然而，主要问题是：这些对策是否足以保护个人身份信息和受保护健康信息？&lt;/p&gt;
&lt;p&gt;有多位研究人员思考过这个问题；（Kim, Kim, &amp;amp; Kim, 2017）发现，在韩国存在通过居民注册号（RRN）去匿名化医疗数据的可能性，RRN是每位韩国居民的独特标识符。作者指出，利用公开可获取的信息，RRN可以被高精度地重新识别。此外，（Sweeney, 2002）声称，个人的出生日期、性别和邮政编码这三项信息可以用于唯一识别美国87%的人口。仅仅使用数据匿名化技术来保护个人身份信息是不够的。从这个角度来看，隐私保护机器学习（PPML）领域应运而生。&lt;/p&gt;
&lt;p&gt;隐私保护机器学习（PPML）是一种开发机器学习算法和模型的技术，旨在在推理和训练阶段保护敏感数据的隐私。在传统的机器学习中，模型通常使用已收集、整合并用于多种应用的数据进行训练。然而，当使用敏感数据（如财务或个人健康记录）时，这种方法可能会引发隐私问题。PPML通过创建允许在不危及基础数据隐私的情况下部署和训练机器学习模型的方法来解决这个问题。&lt;/p&gt;
&lt;p&gt;隐私保护机器学习（PPML）方法大致可以分为两类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;差分隐私。这种方法通过向数据中引入随机噪声来隐蔽用户身份并阻止私人信息泄露。&lt;/li&gt;
&lt;li&gt;安全多方计算（SMC）。多个参与方可以在不泄露任何私人数据的情况下共同训练机器学习模型。数据以一种允许每个参与方对数据进行计算而不直接访问数据的方式进行交换和加密。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在这项工作中，将讨论PPML的一个新颖概述。这项综述可以帮助读者找到研究空白，并提出与该领域相关的未来工作。本文的结构将如下：第二部分将介绍有助于理解将要讨论的主题的背景术语；第三部分将讨论用于PPML的技术；第四部分将讨论与该主题相关的最新论文；第五部分将讨论该主题的挑战；最后第六部分是结论。&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;🍾Background
&lt;/h2&gt;&lt;p&gt;在本节中，将提到所需的基础概念。本节中的术语对于理解我们研究的主题概念将是有用的。&lt;/p&gt;
&lt;h3 id=&#34;机器学习模型&#34;&gt;🍷机器学习模型。
&lt;/h3&gt;&lt;p&gt;机器学习（ML）被视为人工智能（AI）领域的一部分。它旨在构建一种方法，帮助机器在许多问题中做出决策。机器学习算法可以分为两个主要领域：监督学习和无监督学习算法。这两类之间的主要区别在于是否有来自所研究领域的专家提供的标签。如果算法在学习过程中需要带有标签的数据集，那么该算法被视为监督学习。一般来说，机器学习算法可以检测所用数据集特征中的模式，并基于这些模式对新案例进行预测。&lt;/p&gt;
&lt;h3 id=&#34;隐私保护机器学习ppml中的加密&#34;&gt;🍸隐私保护机器学习（PPML）中的加密。
&lt;/h3&gt;&lt;p&gt;通过促进私密数据的安全交换、存储和处理，密码学在隐私保护机器学习（PPML）中至关重要。为加密数据并对其进行计算而不向未经授权的方披露其内容，使用了同态加密、安全多方计算和秘密共享等密码学技术。同态加密允许对加密数据进行计算，而无需先解密，从而确保在计算过程中不会泄露敏感信息。安全多方计算允许多个参与方共同计算一个函数，而不向彼此披露数据。秘密共享将数据拆分成多个部分，并以一种方式分发到不同的参与方中，只有授权的参与方可以访问和重构原始数据。这些密码学技术使得开发能够保护敏感数据隐私的PPML算法成为可能，同时仍然能够提取有用的见解和知识。图1展示了同态加密在机器学习过程中的概述（Olzak, 2022）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241104204332300.png&#34;
	width=&#34;1049&#34;
	height=&#34;612&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241104204332300_hu1550586218636423810.png 480w, https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241104204332300_hu13796497834075906721.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241104204332300&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;171&#34;
		data-flex-basis=&#34;411px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;差分隐私&#34;&gt;🍹差分隐私
&lt;/h3&gt;&lt;p&gt;差分隐私提供了数据分析中隐私保证的精确规范。它确保在分析结果中不会泄露任何数据集参与者的私人信息。&lt;/p&gt;
&lt;p&gt;“隐私损失”和“隐私预算”的概念可以用来解释差分隐私的思想。当一个人的数据被添加到或从数据集中删除时，机制的输出不会发生显著改变，则该机制被称为具有差分隐私。数学上，随机算法或机制被称为满足ε-差分隐私，如果对于任何一对由于包含或排除单个个体数据而有所不同的数据集D和D&amp;rsquo;，以及对于任何可能的输出集合S，以下不等式成立：
&lt;/p&gt;
$$
Pr [M(D) ∈ S] ≤ exp(ε) Pr [M(D′) ∈ S]
$$&lt;p&gt;
在这个方程中，Pr [M(D) ∈ S] 表示应用于数据集 D 的机制 M 输出结果在集合 S 中的概率。ε 参数控制提供的隐私保护程度，其中较小的 ε 值意味着更强的隐私保证。&lt;/p&gt;
&lt;p&gt;为了用一个例子说明这一点，考虑一个公司希望发布其员工平均收入的汇总统计数据，同时保护个人隐私的场景。通过应用差分隐私，公司可以以受控的方式向真实的平均收入添加随机噪声。&lt;/p&gt;
&lt;p&gt;假设员工的真实平均收入为50,000土耳其里拉。如果没有差分隐私，攻击者可能通过将发布的平均收入与外部信息进行比较，来确定某位员工的薪水。然而，通过应用差分隐私，公司向平均收入添加了随机噪声。例如，报告的平均收入可能是50,200土耳其里拉或49,800土耳其里拉，且概率相等。这种额外的随机性确保了攻击者无法根据发布的统计数据可靠地推断任何个别员工的确切收入。&lt;/p&gt;
&lt;p&gt;通过仔细控制添加的噪声量（由 ε 参数确定），差分隐私实现了个人隐私保护与提供准确汇总信息之间的平衡。较小的 ε 值提供更强的隐私保证，但可能引入更多噪声，从而降低发布数据的准确性，而较大的 ε 值可能提供较少的隐私保护，但会产生更准确的结果。如需了解有关差分隐私的更多信息，请参阅 (Dwork, 2006)。&lt;/p&gt;
&lt;h3 id=&#34;机器学习的隐私保护技术&#34;&gt;🍺机器学习的隐私保护技术
&lt;/h3&gt;&lt;p&gt;鉴于机器学习中使用的模型和算法通常依赖于大量敏感数据，隐私在隐私保护机器学习（PPML）中是一个关键挑战。PPML的目标是创建能够从敏感数据中学习的机器学习模型，而不泄露提供数据的人员的任何敏感信息。在处理应保密的机密信息时，例如财务或个人健康数据，这一点尤其重要。为了确保隐私，机器学习中通常采用多种隐私保护策略。其中一些机制包括数据匿名化、加密和访问限制。数据匿名化是一种在使用数据进行机器学习之前去除个人身份信息的过程。去除姓名、地址和其他可能用于识别特定个人的数据就是一种实现方式。加密则是在数据中进行编码，只有获得适当权限的人才能访问。通过访问控制措施限制谁可以访问数据以及如何访问。这些控制措施对于确保私人信息得到保护以及在整个机器学习过程中保持敏感数据的隐私至关重要。&lt;/p&gt;
&lt;p&gt;除了这些工具，隐私保护机器学习（PPML）还需要在模型推理和训练过程中对数据处理和共享协议进行仔细设计。这些协议的设计方式对防止敏感数据泄露或不当使用至关重要。为了防止重新识别个人，可能会采用差分隐私技术，例如通过对数据进行增强。多个参与方可以通过安全多方计算，在不互相披露个人信息的情况下，共同计算一个函数。数据被分成多个片段，并通过秘密共享在多个参与方之间传输，这样只有授权方才能访问并重新组装原始数据。这些加密方法使得可以创建能够保护敏感数据隐私的机器学习算法，同时仍然能够提取有价值的信息。为了保持对算法的信任，并确保个人的私密信息不被泄露，机器学习中的隐私保护至关重要。&lt;/p&gt;
&lt;h2 id=&#34;ppml中的相关工作&#34;&gt;🥃PPML中的相关工作
&lt;/h2&gt;&lt;p&gt;在本节中，将讨论学术领域在PPML（隐私保护机器学习）方面的研究工作。我们选择了该领域的重要研究论文，旨在研究它们如何解决研究问题。&lt;/p&gt;
&lt;h4 id=&#34;al-rubaie-2019&#34;&gt;🥤Al-Rubaie, 2019
&lt;/h4&gt;&lt;p&gt;本文可以被视为开始研究PPML主题的良好资源。作者提供了关于机器学习中隐私面临的威胁和挑战的概述，以及在机器学习模型的训练和推理阶段保护隐私的解决方案和技术。&lt;/p&gt;
&lt;p&gt;首先，他们讨论了在机器学习模型中应用额外对策以增强隐私的重要性。他们展示了几种可能的未经授权访问或滥用共享数据的场景。根据他们的分类，机器学习过程中可以涉及三方：数据拥有者、处理者以及接收结果的人。当这些参与者属于同一方或同一人时，能够实现最高的安全性，然而并非所有情况都是如此。他们提到了五种隐私泄露的威胁等级，这些威胁包括：私密数据的明文存储、重构攻击、模型反演攻击、成员推断攻击和再识别攻击。&lt;/p&gt;
&lt;p&gt;在私密数据的明文存储中，数据在转移到计算部分时以明文形式存储在存储设备中，这可能会影响隐私，尤其是在数据存储被攻破或遭遇不忠诚员工的内部威胁时。专家建议仅发送必要的特征，而不是发送整个数据行，但这种解决方案会导致第二种威胁——重构攻击。获取提取特征的知识或元数据有助于重建原始数据，从而可能导致隐私泄露。&lt;/p&gt;
&lt;p&gt;在模型反演攻击中，攻击者通过使用机器学习模型的输出，生成与用于构建该模型的特征向量相似的向量。这些攻击利用了作为响应返回的置信度数据。知道某个成员或参与者的数据是否被用于训练阶段，会导致第四种威胁——成员推断攻击。为保护参与者在构建数据集过程中的隐私，提出了一种解决方案，即删除与个人身份信息（PII）相关的任何特征或列。然而，根据第五种威胁——再识别攻击，攻击者可以通过将收集到的数据集与其他数据集结合，来恢复这些被遗漏的数据。&lt;/p&gt;
&lt;p&gt;接着，研究讨论了几种保护机器学习隐私的方法和解决方案，包括联邦学习、同态加密、安全多方计算和差分隐私。每种方法都进行了详细阐述，包括其基本原理、优点以及潜在应用。&lt;/p&gt;
&lt;p&gt;研究还涵盖了PPML中的问题和权衡，以及处理敏感数据时出现的伦理和法律问题。这些问题包括对模型准确性、计算时间和通信开销的影响。&lt;/p&gt;
&lt;p&gt;该论文提供了对PPML领域最新进展的全面且易于理解的总结，并强调了该领域需要进一步研究和发展的必要性，以应对机器学习中隐私保护不断变化的风险和需求。&lt;/p&gt;
&lt;h4 id=&#34;liu-guo-lam--zhao-2022&#34;&gt;🧃Liu, Guo, Lam, &amp;amp; Zhao, 2022
&lt;/h4&gt;&lt;p&gt;本文的作者提出了一种可扩展的隐私保护方案，该方案能够容忍任何参与者在任何时候的掉线。他们在提案中使用了同态加密伪随机生成器（HPRG）和沙密尔秘密共享方案。该提议的解决方案通过避免从客户端构造用于生成掩码的种子，减少了通信开销。此外，在客户端掉线的情况下，也无需像他们在比较中使用的基于SecAgg的方案那样向服务器发送额外的沙密尔共享数据。他们声称，提出的方案在抗掉线恢复能力上比其他解决方案更强。&lt;/p&gt;
&lt;p&gt;在该方案的安全性分析中，提到了四个理论及其证明。其中两个理论讨论了系统在半诚实方的保护级别。在他们的定义中，半诚实参与者“不会偏离协议，但会试图推测诚实方的信息。” 第一个定理涉及排除服务器端的半诚实参与者。对于所有的 U, t, k，其中 |C| &amp;lt; t, $x_U$, U1, U2, U3 和 C，其中 C ⊆ U 且 U3 ⊆ U2 ⊆ U1，存在一个概率多项式时间（PPT）模拟器 SIM，使得：&lt;br&gt;
&lt;/p&gt;
$$
SIM^{U,t,k}_C (x_U, U_1, U_2, U_3) ≡ REAL^{U,t,k}_C (x_U, U_1, U_2, U_3)
$$&lt;p&gt;
第二个定理则关注包括服务器在内的半诚实参与者的安全性。对于所有的 U, t, k，其中 |C{S}| &amp;lt; t, $x_U$, U1, U2, U3 和 C，其中 C ⊆ U ∪ S 且 U3 ⊆ U2 ⊆ U1 ⊆ U，存在一个概率多项式时间（PPT）模拟器 SIM，使得：
&lt;/p&gt;
$$
SIM^{U,t,k}_C (x_U, U_1, U_2, U_3) ≡ REAL^{U,t,k}_C (x_U, U_1, U_2, U_3)
$$&lt;p&gt;
另外两个理论讨论了防御活跃恶意客户端的安全性。为提供所需的安全性，提出了一种基于 HPRG 的安全聚合协议。第三个定理关注恶意客户端和诚实服务器。对于所有的 U, t, k，其中 |C| &amp;lt; t, $x_{U/C}$ 和 C，使用算法 MC，其中 C ⊆ U，所提出的协议是一个计算 FHSecAgg 的安全协议。最后一个定理涉及包括服务器在内的恶意客户端的安全性。&lt;/p&gt;
&lt;p&gt;为了理解这些理论，我们需要了解其中术语的含义。$U$ 表示一组具有本地训练模型 $x_U$ 的客户端。$S$ 表示模型的服务器端。这两种理论中的等式表示两边分布上的恒等关系。关于所提出协议的概述，私有输入可以总结为每个客户端的本地训练模型或梯度，它们可以表示为向量、用于初始化秘密密钥的密钥、以及签名的秘密密钥。在服务器端，存储了用于验证秘密通道和签名的密钥。另一方面，协议的公共输入包括客户端的数量、丢失客户端的阈值和签名过程的公钥。本地训练模型的聚合结果可以视为协议的输出。图 2 显示了协议的步骤，并对每个步骤进行了简要描述。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105101503786.png&#34;
	width=&#34;1068&#34;
	height=&#34;666&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105101503786_hu12456287249950315205.png 480w, https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105101503786_hu10669958393845248634.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241105101503786&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;文章中进行了若干测试，证明了他们所提到的假设。&lt;/p&gt;
&lt;h4 id=&#34;gupta--singh-2022&#34;&gt;🧉Gupta &amp;amp; Singh, 2022
&lt;/h4&gt;&lt;p&gt;在本文中，作者提出了一种基于云的PPML模型，结合了差分隐私和机器学习模型。该模型明确了不可信方之间的通信协议。进行的实验包括朴素贝叶斯（NB）分类器，并应用于多个数据集，最终达到了95%的准确率，超过了已有文献中的结果。图3展示了所提模型，该模型被称为基于数据和分类服务的差分隐私保护机器学习模型（DA-PMLM）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102127794.png&#34;
	width=&#34;781&#34;
	height=&#34;713&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102127794_hu3910818257146447941.png 480w, https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102127794_hu6649347863904803145.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241105102127794&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;109&#34;
		data-flex-basis=&#34;262px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;从图3中，可以看到该架构中有四个主要参与者：数据所有者（DOid），即云客户端，是数据的创建者，并愿意将其共享到云端；分类器所有者（CO），是负责向云服务提供商（CSP）提供分类服务的一方，CSP 进一步向DOid提供服务；最后是请求用户（RUid），即从CSP请求所有者数据的实体。数据所有者在共享其私人数据之前，通过差分隐私向数据中注入噪声。&lt;/p&gt;
&lt;p&gt;他们证明了以下理论：“在所提出的模型中，分类模型的隐私保护机制满足ε-差分隐私的并行组合。”&lt;/p&gt;
&lt;p&gt;图4展示了所提模型的分类和数据流。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102353362.png&#34;
	width=&#34;953&#34;
	height=&#34;461&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102353362_hu16699203993426064.png 480w, https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102353362_hu6502330861095835153.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241105102353362&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;206&#34;
		data-flex-basis=&#34;496px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在接收到带有噪声的数据 $D_{N1}^1, D_{N2}^2, \dots, D_{Nn}^n$ 来自相应的数据所有者（DOs）后，云服务提供商（CSP）通过应用标准化函数（使用Z-score标准化公式）对其进行预处理。得到的值可以用符号 $D_{\hat{N_i}}^i$ 表示。与任何机器学习过程类似，给定的数据集被划分为训练部分和测试部分。之后，进行分类模型（CM）的训练和测试，在实验中使用了朴素贝叶斯（NB）分类器，以进行评估测量。&lt;/p&gt;
&lt;h4 id=&#34;lee-et-al-2022&#34;&gt;🧊Lee, et al., 2022
&lt;/h4&gt;&lt;p&gt;本文的作者声称，当前的PPML解决方案仅限于非标准的机器学习模型。这些解决方案在实际数据集上未能证明取得良好的结果。此外，所使用的激活函数从算术角度来看较为简单，并且替换了非算术激活函数。它们还避免使用自助法（bootstrapping）。当前的解决方案中也无法实现大量层数，他们的提议包括使用标准的ResNet-20模型与RNS-CKKS全同态加密（FHE）结合自助法进行实现。为了验证他们的模型，他们将其应用于一个标准化的数据集——CIFAR-10。所得结果与使用原始ResNet-20模型且未加密时的结果非常接近，最终的准确率约为92.43%。&lt;/p&gt;
&lt;h2 id=&#34;ppml中的挑战&#34;&gt;🍻PPML中的挑战
&lt;/h2&gt;&lt;p&gt;PPML面临着若干挑战，这些挑战阻碍了其在实际应用中的广泛采用。主要的挑战之一是隐私与效用之间的权衡。像差分隐私和安全多方计算这样的PPML技术通常会引入噪声或通信开销，这可能显著影响机器学习模型的准确性和效率。另一个挑战是不同PPML技术之间缺乏标准化和互操作性，使得比较和结合不同方法的结果变得困难。此外，PPML需要专门的专业知识和资源，如加密学和安全计算的知识，这些可能并不容易为所有组织所拥有。最后，PPML还涉及法律和伦理问题，如合规性和透明度，这些问题必须仔细处理，以确保敏感数据的负责任和可信赖的使用。&lt;/p&gt;
&lt;h2 id=&#34;结论与未来工作&#34;&gt;🥂结论与未来工作
&lt;/h2&gt;&lt;p&gt;敏感数据的隐私必须得到保护，而隐私保护机器学习（PPML）这一研究领域正迅速发展。随着机器学习在医疗、银行和社交媒体等行业的广泛应用，隐私保护系统的重要性愈加突出。为了确保人们的敏感信息不被泄露，能够在尊重隐私的前提下支持机器学习的隐私保护算法和协议至关重要。该领域的研究进展涉及信息理论、机器学习和密码学等计算机科学多个分支，依赖于跨学科的合作。PPML是一个具有挑战性且复杂的问题，但它是实现机器学习带来益处的前提，而不牺牲个人隐私。在本次综述中，我们概述了PPML的主要难题和未来的研究方向，并回顾了PPML中使用的主要方法和技术。为了克服该领域中的重大问题，我们希望本综述能够成为对PPML感兴趣的研究人员和实践者的重要参考资源。未来的研究工作将包括在真实数据上的PPML研究，并与传统机器学习模型的表现进行比较。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>差分隐私：研究结果综述</title>
        <link>https://JiangZhiyu-1024.github.io/p/differential-privacy-a-survey-of-results/</link>
        <pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/differential-privacy-a-survey-of-results/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/differential-privacy-a-survey-of-results/astronomy-1867616_1920.jpg" alt="Featured image of post 差分隐私：研究结果综述" /&gt;&lt;h1 id=&#34;differential-privacy-a-survey-of-results&#34;&gt;Differential Privacy: A Survey of Results
&lt;/h1&gt;&lt;p&gt;差分隐私：研究结果综述&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要。&lt;/strong&gt; 在过去的五年中，一种新的隐私保护数据分析方法取得了成果。这种方法与统计学、数据库、理论和密码学领域的许多相关文献（但并非全部）不同之处在于，它定义了一个形式化的、普遍适用的隐私保障，并且所提出的数据分析技术被严格证明满足这一保障。出现的关键隐私保障是差分隐私。粗略来说，这确保了（几乎可以量化地）通过加入一个统计数据库而不会承担风险。在本次综述中，我们回顾了差分隐私的定义以及实现它的两种基本技术。然后，我们展示了这些技术的一些有趣应用，提出了三个具体任务的算法和关于差分隐私学习的三个一般结果。&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;🍦1. Introduction
&lt;/h2&gt;&lt;p&gt;隐私保护的数据分析也被称为统计披露控制、推断控制、隐私保护数据挖掘和私密数据分析。我们的主要动机场景是统计数据库。统计量是从样本中计算出的数量。假设一个受信任且值得信赖的管理者从大量受访者（样本）中收集敏感信息，目的是学习（并向公众发布）关于潜在群体的统计事实。问题在于如何在不侵犯个别受访者隐私的情况下发布统计信息。有两种设置：在非交互式设置中，管理者计算并发布一些统计信息，数据不会再被进一步使用。隐私问题可能会影响管理者发布的精确答案，甚至影响发布的统计量的集合。请注意，由于数据不会再被使用，管理者可以在统计信息发布后销毁数据（和自己）。在交互式设置中，管理者处于用户和数据库之间。用户提出的查询和/或对这些查询的响应，可能会被管理者修改，以保护受访者的隐私。数据不能被销毁，管理者必须在数据库的整个生命周期中保持在场。当然，任何交互式解决方案都能产生一个非交互式解决方案，前提是查询在此之前已知：管理者可以模拟一个互动，其中这些已知查询被提出，并发布结果记录。关于这个问题的文献丰富，主要来自统计学界（见，例如，[10, 14, 27, 28, 29, 38, 40, 26, 39]以及有关表格数据的受控发布、列联表和单元抑制的文献），也来自计算机科学的各种分支，如算法、数据库理论和密码学，例如在[3, 4, 21, 22, 23, 33, 34, 41, 48]，[1, 24, 25, 31]，以及[6, 9, 11, 12, 13, 18, 7, 19]中；有关1989年前领域总结的文献，见调查[2]。本次综述关于差分隐私。粗略来说，差分隐私确保删除或添加单个数据库项不会（实质性地）影响任何分析的结果。因此，加入数据库不会产生风险，提供了一种在分布信息可能导致泄露的情况下进行严格数学处理的方法。我们将首先描述三种针对特定、不相关的数据分析任务的差分隐私算法。然后，我们将提出三条关于当需要保护个别数据项隐私时的计算学习的一般结果。这在学习理论文献中通常不是一个关注点，标志着一条新研究方向的出现。&lt;/p&gt;
&lt;h2 id=&#34;2differential-privacy&#34;&gt;🍧2.Differential Privacy
&lt;/h2&gt;&lt;p&gt;在后续中，随机化函数 K 是管理者在发布信息时应用的算法。因此，输入是数据集，输出是发布的信息或记录。我们不需要区分交互式和非交互式设置。可以将数据库视为一组行。如果数据库 D1 和 D2 仅在一个元素上有所不同，我们称其为 D1 和 D2 的关系：一个是另一个的真子集，而较大的数据库仅包含一行附加的记录。&lt;/p&gt;
&lt;p&gt;定义 1. 随机化函数 K 具有 ε-differential privacy，当且仅当对于所有在最多一个元素上有所不同的数据集 D1 和 D2，以及所有 S ⊆ Range(K)，以下不等式成立：
&lt;/p&gt;
$$
\Pr[K(D_1) \in S] \leq \exp(ε) \times \Pr[K(D_2) \in S]
$$&lt;p&gt;
所涉及的概率是基于 K 的抛硬币过程。&lt;/p&gt;
&lt;p&gt;满足此定义的机制 K 解决了任何参与者对个人信息泄露的担忧：即使参与者从数据集中删除了她的数据，输出（因此输出的后果）也不会变得显著更可能或更不可能。例如，如果数据库被保险提供者查询，以决定是否为某个特定个体投保，那么该个体数据在数据库中的存在或缺失不会显著影响她获得保险的机会。因此，差分隐私是一种全面的保障。这也是一种非常强的保障，因为它是关于机制行为的统计特性，因此独立于对手/用户的计算能力和附加信息。差分隐私并不是隐私的绝对保障。事实上，Dwork 和 Naor 已经证明，任何具有非平凡效用的统计数据库都会妥协隐私的自然定义。然而，在一个已决定某些数据库的好处大于其成本的社会中，差分隐私确保参与这些社会有益的数据库仅会承担有限的额外风险。&lt;/p&gt;
&lt;p&gt;定义 1 中的参数是公开的。这个参数的选择基本上是一个社会性的问题，超出了本文的讨论范围。也就是说，我们倾向于将其视为，比如，0.01、0.1，或者在某些情况下，ln 2 或 ln 3。如果某个不良事件发生的概率非常小，可能可以接受将其增加 2 或 3 倍；而如果这个概率已经被认为接近不可接受，那么增加 $e^{0.01} ≈ 1.01$ 可能是可以容忍的，而增加 e，甚至仅仅是 $e^{0.1}$，可能就是不可接受的。&lt;/p&gt;
&lt;p&gt;定义 1 讨论了机制 K 的行为，并独立于对手或用户可能拥有的任何附加知识。因此，满足该定义的机制在保护数据库中个别行的隐私时，即使对手知道数据库中的其他所有行，也能做到这一点。&lt;/p&gt;
&lt;p&gt;定义 1 也可以扩展到群体隐私（以及个体对数据库贡献多于一行的情况）。一组 c 个参与者可能担心他们的集体数据泄露信息，即使单个参与者的数据不会。根据这个定义，我们可以将任何概率的扩展限制在最多 exp(c)，对于小的 c，这可能是可以容忍的。当然，统计数据库的目的是披露有关大型群体的汇总信息（同时保护个体隐私），因此我们应该预期随着群体规模的增加，隐私界限会逐渐减弱。&lt;/p&gt;
&lt;h2 id=&#34;3achieving-differential-privacy-in-statistical-databases&#34;&gt;🍨3.Achieving Differential Privacy in Statistical Databases
&lt;/h2&gt;&lt;p&gt;在统计数据库中实现差分隐私&lt;/p&gt;
&lt;p&gt;我们将介绍一种由 Dwork、McSherry、Nissim 和 Smith 提出的交互机制 K，适用于连续值查询的情况。在本节中，查询是一个将数据库映射到（向量形式的）实数的函数。例如，查询“计数 P”统计数据库中具有属性 P 的行数。当查询为函数 f，而数据库为 X 时，真实答案是值 f(X)。机制 K 向真实答案添加适当选择的随机噪声，以生成我们所称的响应。通过用真实答案的嘈杂版本进行回应来保护隐私的想法并不新鲜，但这种方法非常微妙。例如，如果噪声是关于原点对称的，并且同样的问题被多次询问，则响应可能会被平均，从而抵消噪声。我们必须考虑这些因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义 2.&lt;/strong&gt; 对于 f : D → $R^k$，f 的敏感性为:
&lt;/p&gt;
$$
Δf = \underset{\text{ D1, D2}}{\max} \| f(D_1) - f(D_2) \|_1
$$&lt;p&gt;
对于所有D1和D2，它们最多只在一个元素上不同。&lt;/p&gt;
&lt;p&gt;特别地，当 k = 1 时，函数 f 的敏感性是两个仅在一个元素上不同的数据库所可能取的值之间的最大差异。对于许多类型的查询，Δf 会非常小。特别是，上述讨论的简单计数查询（‘有多少行具有属性 P?’）的 Δf = 1。我们的技术在 Δf 较小时效果最佳——引入最少的噪声。请注意，敏感性仅是函数的属性，与数据库无关。敏感性本质上捕捉了需要由管理员生成的加性噪声隐藏的差异程度（即在两个仅在一个元素上不同的数据库上，f 的值之间的差异）。&lt;/p&gt;
&lt;p&gt;标准差为$\sqrt2Δf /ε$的缩放对称指数分布，记作$\text{Lap}(\Delta f / \epsilon)$ ，其在 \(x\) 处的质量与 $\exp(-|x|(\epsilon/\Delta f))$成正比。更准确地说，令$b = \Delta f/\epsilon$ 。概率密度函数为&lt;/p&gt;
$$
p(x) = \frac{1}{2b} \exp\left(-\frac{|x|}{b}\right)
$$&lt;p&gt;累积分布函数为&lt;/p&gt;
$$
D(x) = \frac{1}{2}\left(1 + \text{sgn}(x)\left(1 - \exp\left(\frac{|x|}{b}\right)\right)\right) 
$$&lt;p&gt;在查询函数 \(f\) 上，隐私机制 \(K\) 的响应为&lt;/p&gt;
$$
f(X) + (\text{Lap}(\Delta f/\epsilon))^k
$$&lt;p&gt;即独立地向 $f(x)$的每个$k$ 个分量添加分布为 $\text{Lap}(\Delta f / \epsilon)$的噪声。注意，减少已知的公共参数 $\epsilon$ 会使 $\text{Lap}(\Delta f / \epsilon)$曲线变平，从而产生更大的期望噪声幅度。当 $\epsilon$ 固定时，具有高敏感性的函数 f 会产生更平坦的曲线，同样导致更高的期望噪声幅度。&lt;/p&gt;
&lt;p&gt;为了简化，考虑 k = 1 的情况。证明 \(K\) 在单个查询函数$f$上产生$\epsilon$差分隐私是直接的。考虑任何子集$S \subseteq \text{Range}(K)$，令 $D_1, D_2$为任何在最多一个元素上不同的数据库对。当数据库为$D_1$时，任何$r \in \text{Range}(K)$ 处的概率质量与
&lt;/p&gt;
$$
\exp(-|f(D_1) - r|\frac{\Delta f}{\epsilon})
$$&lt;p&gt;成正比，$D_2$亦然。应用三角不等式，我们得到的比值最多为&lt;/p&gt;
$$
\exp\left(-|f(D_1) - f(D_2)|\frac{\Delta f}{\epsilon}\right)
$$&lt;p&gt;根据敏感性的定义，$|f(D_1) - f(D_2)| \leq \Delta f$，因此比值被限制为&lt;/p&gt;
$$
\exp(-\epsilon)
$$&lt;p&gt;从而实现了$\epsilon$差分隐私。显而易见，通过对每个查询运行 $K$，使噪声分布为&lt;/p&gt;
$$
\text{Lap}\left(\sum_{i} \Delta f_i/ \epsilon\right)
$$&lt;p&gt;可以为任何（自适应选择的）查询序列 $f_1, \ldots, f_d$实现 $\epsilon$差分隐私。换句话说，每个答案的质量随着查询的敏感性总和而下降。有趣的是，有时可以做到比这更好。大致来说，重要的是&lt;/p&gt;
$$
\Delta = ||(f_1(D_1), f_2(D_1), \ldots, f_d(D_1)) - (f_1(D_2), f_2(D_2), \ldots, f_d(D_2))||_1 
$$&lt;p&gt;的最大可能值。由于查询的潜在自适应选择，陈述的精确表述需要一些谨慎。有关全面的处理，请参见 [19]。我们在此对非自适应情况声明定理，视查询序列$f_1, f_2, \ldots, f_d$（具有各自的元数 $k_1, \ldots, k_d$）为一个单一的 $k = \sum_{i=1}^{d} k_i$元查询 $f$，并回顾对于任意$k$的定义 2。&lt;/p&gt;
&lt;p&gt;定理 1 ([19])：对于 $f : D \to \mathbb{R}^k$，机制 $K_f$ 向每个输出项独立添加服从分布 $\text{Lap}\left(\frac{\Delta f}{\epsilon}\right)$ 的噪声，具有 $\epsilon$-差分隐私。&lt;/p&gt;
&lt;p&gt;机制 $K$ 的上述描述在对不敏感查询时具有出色的准确性。特别地，确保差分隐私所需的噪声仅依赖于函数的敏感性和参数 $\epsilon$。这两者都与数据库的内容和行数无关。因此，如果数据库非常大，由差分隐私机制引入的许多典型查询的错误相对较小。我们可以将 $K$ 看作分析师与数据之间的差分隐私保护接口。这提示了一种隐私保护数据分析的一般方法：寻找需要少量不敏感查询的算法。参见例如 [7, 8, 32]。事实上，甚至计数查询也极其强大，允许对许多标准数据挖掘任务进行准确和差分隐私计算，包括主成分分析、$k$-均值聚类、分离超平面的感知机学习以及生成 ID3 决策树 [7]，以及（邻近的）半空间学习 [8]（见下面的第 4.3 节）。在定理 1 的许多应用中，尤其引人关注的是直方图查询类。直方图查询是将数据库行的域任意划分为不相交的“单元”，其真实答案是描述每个单元中数据库行数的计数集。虽然具有 $k$ 个单元的直方图查询可以视为 $k$ 个独立的计数查询，但添加或删除单个数据库行最多只能影响整个 $k$ 元组计数的一个位置（即与添加（删除）行的单元对应的计数）；此外，该单元的计数最多受到 1 的影响，因此根据定义 2，每个直方图查询的敏感性为 1。许多数据分析实际上就是直方图；因此，令人鼓舞的是，复杂的直方图并不需要每个单元中有大方差，而是需要非常小的方差。&lt;/p&gt;
&lt;h3 id=&#34;31-当噪声毫无意义时&#34;&gt;🍩3.1 当噪声毫无意义时
&lt;/h3&gt;&lt;p&gt;在某些任务中，添加噪声毫无意义。例如，函数 $f$ 可能将数据库映射到字符串、策略或树。在最近的一篇论文中，McSherry 和 Talwar 解决了在保持 $\epsilon$-差分隐私的同时优化此类函数输出的问题 [35]。假设策展人持有一个数据库 $X$，目标是生成一个对象 $y$。简而言之，他们的指数机制工作如下：假设存在一个效用函数 $u(X, y)$，用于衡量在数据库为 $X$ 时输出 $y$ 的质量。例如，如果数据库保存了个人在拍卖中为数字商品所评估的价值，那么当价格设为 $y$ 时，$u(X, y)$ 可能是这些评估的收入。拍卖是噪声毫无意义的一个很好的例子，因为稍微定得高一点的价格可能会阻止许多投标者购买。McSherry 和 Talwar 的指数机制以与 $ \exp(- u(X, y)/2)$ 成比例的概率输出 $y$。这确保了 $\Delta u$-差分隐私，或在 $\Delta u \leq 1$ 时的 $\epsilon$-差分隐私。这里，$\Delta u$ 的定义与之前略有不同；它是通过改变单行数据而引起的 $u$ 值的最大可能变化（与添加或删除一行数据不同；这两者的差异最多为二的倍数）；参见 [35]。通过这种方法，McSherry 和 Talwar 得到近似真实的拍卖，并实现几乎最优的售价。粗略来说，这表明参与者不能通过虚报其评估值来大幅降低其支付的价格。有趣的是，他们展示了差分隐私的简单组合可以用于获取拍卖，在这些拍卖中，任何 $c$ 个代理的合作组都不能通过提交除真实评估以外的出价显著提高其效用。这类似于上述备注 1 的情况，在该情况下，组合被用来为 $c$ 个个体的群体获取隐私。&lt;/p&gt;
&lt;h2 id=&#34;4-特定任务的算法&#34;&gt;🍪4 特定任务的算法
&lt;/h2&gt;&lt;p&gt;在本节中，我们描述了针对三个不相关任务的差分隐私算法。&lt;/p&gt;
&lt;h3 id=&#34;41-统计数据推断&#34;&gt;🎂4.1 统计数据推断
&lt;/h3&gt;&lt;p&gt;本节的结果来自 Dwork 和 Nissim [18]。考虑一个场景，其中数据库中的每个元素由一组 $k$ 个布尔属性 $\alpha_1, \ldots, \alpha_k$ 描述，且行是从某个底层分布中独立采样得到的，分布在 ${0, 1}^k$ 上。设 $1 \leq t \leq \frac{k}{2}$ 为整数。这里的目标是利用关于任何属性值组合的发生率的信息，以学习任意两个属性值组合的发生率。尽管我们使用“查询”一词，但这些查询在此之前都是已知的，且机制将是非交互式的。从大约 $\binom{k}{2}$ 个发布的信息中，可以计算所有 $\binom{k}{2} \cdot 2^2$ 个小项的发生率的近似值。这将允许数据分析师近似计算所有 $2^{\binom{k}{2}}$ 个长度为 2 的二元小项的概率，前提是初始近似值足够准确。我们将概率与发生率等同，因此概率空间是在数据库中的行上。固定任意一组属性。这些属性值的所有可能设置的发生率可以通过具有 $2$ 个单元的直方图来描述，直方图的敏感性为 $1$，因此我们将处理一个整体敏感性与 $O(k)$ 成比例的查询序列（实际上，它会比这个差一个因子 $t$，下面会讨论）。设 $\alpha$ 和 $\beta$ 为属性。如果在概率上 $\alpha$ 蕴含 $\beta$，则表示在给定 $\alpha$ 的情况下 $\beta$ 的条件概率超过 $\beta$ 的无条件概率。测量概率中的蕴含能力对数据挖掘至关重要。注意，由于 $\Pr[\beta]$ 可以通过计数查询简单地估计，因此测量概率中的蕴含问题归结为获得 $\Pr[\beta | \alpha]$ 的良好估计。此外，一旦我们可以估计 $\Pr[\beta | \alpha]$、$\Pr[\beta]$ 和 $\Pr[\alpha]$，就可以使用贝叶斯规则和德摩根定律来确定任何布尔函数的统计数据。例如，$\Pr[\alpha \land \beta] = \Pr[\alpha] \Pr[\beta | \alpha]$，因此如果我们对两个乘数的估计在加性 $\eta$ 内，我们就得到了乘积的估计，其准确度在 $3\eta$ 内。作为朝向非交互式解决方案的第一步，考虑交互式情况，假设我们对 $\Pr[\alpha]$ 和 $\Pr[\beta]$ 有良好的估计。确定 $\Pr[\beta | \alpha]$ 的关键是找到一个 $\alpha$ 的重集，即一个集合 $q \subseteq [n]$，使得 $\alpha$ 的发生率至少比预期高出一个标准差，然后确定该重集上 $\beta$ 的发生率是否高于 $\beta$ 的整体发生率。更具体地，可以测试该条件发生率是否高于给定阈值，然后使用二分搜索找到“正确”的阈值。找到重集很容易，因为随机选择的 $[n]$ 的子集在超过预期的 $\alpha$ 的发生率至少一个标准差的概率是常数。为了“模拟”交互式情况，策展人选择一些随机子集，并为每个子集发布（带噪声的）关于该子集中 $\alpha$ 和 $\beta$ 的发生率的估计。以高概率（取决于 $t$），至少有一个子集对 $\alpha$ 是重的。将所有部分结合起来：对于 $t$ 个随机子集，策展人发布所有 $m = \binom{k}{2}$ 个字面联结的发生率的良好近似值。具体而言，我们要求以至少 $1 - \frac{\delta}{m^2}$ 的概率计算的条件概率 $\Pr[\alpha | \beta]$ 的准确度在 $\frac{\eta}{3m^2}$ 内，其中 $\alpha$ 和 $\beta$ 现在是字面的最小项。这确保了以至少 $1 - \delta$ 的概率，所有计算的条件概率在 $\frac{\eta}{3m^2}$ 内准确，因此所有长度为 $2$ 的最小项的估计概率在 $\frac{\eta}{m^2}$ 内准确。数字 $t$ 相当大，并且依赖于许多因素，包括差分隐私参数以及 $\eta$、$\delta$、$k$ 和 $t$。文献 [18] 的分析表明，当 $\eta$ 和 $\delta$ 是常数时，这种方法将查询数量从 $O\left(\binom{k}{2}\right)$（每个 2 元组的一个直方图）减少到 $O(24 k^2 \log k)$。注意有趣的权衡：我们需要依赖于 $m^2$ 的准确度以避免进行 $m^2$ 次查询。当数据库足够大时，这种权衡是可以实现的。&lt;/p&gt;
&lt;h3 id=&#34;42--contingency-表的发布&#34;&gt;🍰4.2  contingency 表的发布
&lt;/h3&gt;&lt;p&gt;本节的结果来自 Barak、Chaudhuri、Dwork、Kale、McSherry 和 Talwar [5]。 contingency 表是一个计数表。在人口普查或其他调查的背景下，我们将个人的数据视为数据库中的一行。我们不假设行是相互独立的。目前，每一行由 $k$ 位描述 $k$ 个二元属性 $a_1, \ldots, a_k$ 的值。严格来说，contingency 表是一个 $\mathbb{R}^{2^k}$ 中的向量，描述对于 $k$ 个属性的每种设置，数据库中具有该属性值设置的行数。换句话说，它是一个具有 $2^k$ 个单元的直方图。通常情况下，contingency 表本身不会被发布，因为当 $k$ 较大时，它可能会很稀疏。相反，对于不同的属性子集，数据策展人会发布 contingency 表在每个子集上的投影，即每个可能设置的计数，这些较小的计数表称为边际，每个边际由属性的子集命名。由 $j$ 个属性（$j \leq k$）命名的边际称为 $j$-way 边际。数据策展人通常会发布许多低阶边际集合，以揭示许多不同且可能重叠的属性集合之间的关联。由于 contingency 表是一个直方图，我们可以向 contingency 表的每个单元添加与 $-1$ 成比例的独立生成的噪声，以获得一个差分隐私的（非整数且不一定非负的）表。我们稍后将讨论完整性和非负性的问题。目前，我们只需注意，从这个带噪声的表中可以直接计算任何所需的边际，并且不同边际之间的一致性是显而易见的。然而，这种方法的一个缺点是，尽管 contingency 表的每个单元中的噪声相对较小，但计算出的边际中的噪声可能较大。例如，描述属性 $a_1$ 的 1-way 表的方差为 $2^{k-1} - 2$。我们认为这不可接受，尤其是当 $n \gg 2^k$ 时。边际也是直方图。第二种方法，在低阶边际的（常见）情况下噪声较小，但不提供边际之间的一致性，其工作原理如下。设 $C$ 为要发布的边际集合。我们可以考虑一个函数 $f$，当应用于数据库时，生成所需的边际。现在，应用定理 1，选择这个 $f$（对每个表中的每个单元独立添加噪声），其敏感性 $\Delta f = |C|$。当 $n$（数据库中的行数）与 $|C|/\epsilon$ 相比很大时，这也会产生良好的准确性。因此，如果独立随机化每个（表中每个）单元导致的小表与表之间的不一致不成问题，并且用户能接受偶尔出现负值和通常非整数的单元计数，那么我们就可以完成这个过程。我们对这些隐私增强技术的产物——不一致性、负值和非整数——没有哲学或数学上的反对，但在实践中，它们可能会造成问题。例如，单元计数可能作为其他程序（可能是现成的程序）的输入，而这些程序预期正整数，从而导致类型不匹配。不一致性，更不用说负值，可能会让普通用户感到困惑，例如美国 FactFinder 网站的普通用户。接下来，我们概述 Barak 等人的主要工作步骤 [5]。&lt;/p&gt;
&lt;p&gt;移动到傅里叶域。当添加噪声时，两个自然的解决方案出现了：向源表的条目添加噪声（这是我们的第一个提议；当 $k$ 较大时，准确性较差），或向报告的边际添加噪声（我们的第二个提议；不一致性受到影响）。第三种方法是将数据转换到傅里叶域。这只是一个基底的变化。如果我们计算所有 $2^k$ 的傅里叶系数，我们将获得整个一致性表的非冗余编码。如果我们对傅里叶系数进行扰动，然后再转换回 contingency 表域，我们将得到一个（不同的，可能是非整数的，可能是负的）contingency 表，其“距离”（例如，$l_2$ 距离）与原始表的差距由扰动的大小决定。转到傅里叶域的优点是，如果只需要一组边际 $C$，那么我们不需要完整的傅里叶系数集。例如，如果 $C$ 是所有 3-way 边际的集合，那么我们只需要权重最多为 3 的傅里叶系数，数量为 $\binom{k}{3} + \binom{k}{2} + k + 1$。这将转化为一个噪声更小的边际集合。用于计算边际 $C$ 的傅里叶系数形成了数据集的一个模型，捕获了从边际集 $C$ 中可以学习的所有信息。按照定理 1 的指示对这些系数添加噪声，然后再转换回 contingency 表域，得到一个生成合成数据集的程序，确保差分隐私，同时在很大程度上（且可测量地）捕获模型中的信息。这是一个生成具有可证明差分隐私的合成数据的具体方法示例。傅里叶系数准确地描述了边际所需的信息。通过准确测量所需的信息，Barak 等人使用 [19] 的技术添加尽可能少的噪声。此外，傅里叶基底由于根据属性值集的自然分解而特别有吸引力。通过注意到计算低阶边际时不需要额外的傅里叶系数，并且使用更少的噪声系数，可以在给定边际的子边际（即，低阶边际）上施加比定理 4 中更紧的界限，从而通过减少方差提高准确性。使用线性规划和舍入。Barak 等人 [5] 使用线性规划来获得一个非负的但可能是非整数的数据集，具有（几乎）给定的傅里叶系数，然后对结果进行舍入以获得整数解。有趣的是，从线性程序获得的边际与噪声测量的边际之间的“距离”并不比原始数据的真实边际更远（在 [5] 中被精确定义）。因此，通过一致性的施加所引入的额外误差不超过隐私机制本身所引入的误差。&lt;/p&gt;
&lt;h4 id=&#34;符号和预备知识&#34;&gt;🧁符号和预备知识。
&lt;/h4&gt;&lt;p&gt;回想一下，设 $k$ 表示（布尔）属性的数量，我们可以将数据集视为一个向量 $x \in \mathbb{R}^{2^k}$，以属性元组为索引。对于每个 $\alpha \in {0, 1}^k$，数量 $x_\alpha$ 是具有该属性设置的数据元素的数量。我们令 $n = |x|_1$ 表示数据集中元组或行的总数。对于任意 $\alpha \in {0, 1}^k$，我们使用 $|\alpha|_1$ 表示非零位置的数量。我们写作 $\beta \preceq \alpha$，表示 $\alpha, \beta \in {0, 1}^k$，如果 $\alpha$ 中的每个零位置在 $\beta$ 中也是零。&lt;/p&gt;
&lt;h4 id=&#34;边际算子&#34;&gt;🥧边际算子
&lt;/h4&gt;&lt;p&gt;Barak 等人将一组边际的计算描述为对 contingency 表向量 $x$ 应用边际算子的结果。算子 $C_\alpha : \mathbb{R}^{2^k} \to \mathbb{R}^{2^{|\alpha|&lt;em&gt;1}}$，对于 $\alpha \in {0, 1}^k$，将 contingency 表映射到在 $\alpha$ 中正设置的属性的边际（这些属性有 $2^{|\alpha|&lt;em&gt;1}$ 种可能的设置）。为了简化记号，$C&lt;/em&gt;\alpha x$ 仅在那些满足 $\beta \preceq \alpha$ 的位置定义：对于任何 $\beta \preceq \alpha$，$C&lt;/em&gt;\alpha x$ 在位置 $\beta$ 的结果是 $x$ 中与 $\beta$ 在由 $\alpha$ 描述的坐标上相同的坐标的总和：
&lt;/p&gt;
$$
(C_\alpha(x))_\beta = \sum_{\gamma : \gamma \land \alpha = \beta} x_\gamma \tag{3}
$$&lt;p&gt;注意，算子 $C_\alpha$ 对于所有 $\alpha$ 是线性的。&lt;/p&gt;
&lt;p&gt;定理 2. ${f_\alpha}$ 形式构成了 $\mathbb{R}^{2^k}$ 的正交归一基。因此，可以将任何边际写成与相关的傅里叶系数的小和：&lt;/p&gt;
$$
C_\beta x = \sum_{\alpha \preceq \beta} \langle f_\alpha, x \rangle C_\beta f_\alpha. \tag{4}
$$&lt;p&gt;系数 $\langle f_\alpha, x \rangle$ 是从 $x$ 计算 $C_\beta x$ 的必要和充分数据。&lt;/p&gt;
&lt;p&gt;定理 3 ([5]). 设 $B \subseteq {0, 1}^k$ 描述一组傅里叶基向量。释放集合 $\phi_\beta = \langle f_\beta, x \rangle + \text{Lap}(|B|/2^{k/2})$ 对于 $\beta \in B$ 保护 $\epsilon$-差分隐私。&lt;/p&gt;
&lt;p&gt;证明：每个元组对每个输出坐标贡献正好是 ±$1/2^{k/2}$，因此该 $|B|$ 个输出的 $L_1$ 灵敏度至多为 $|B|/2^{k/2}$。根据定理 1，添加标准差为 $|B|/2^{k/2}$ 的对称指数噪声给出 $\epsilon$-差分隐私。&lt;/p&gt;
&lt;p&gt;备注：为了理解规模，我们可以通过随机添加或删除 $|B|2^/$ 个数据集中的个体来实现对每个坐标的类似扰动，这个数量可能远小于 $n$。&lt;/p&gt;
&lt;p&gt;将步骤整合在一起。为了计算一组边际分布 A，我们需要所有在 A 的向下闭包下的 Fourier 系数 fβ。边际分布（A ⊆ {0, 1}^k，D）的步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;令 B 为 A 在向下闭包下的集合。&lt;/li&gt;
&lt;li&gt;对于 β ∈ B，计算 $$ \phi_\beta = \langle f_\beta, D \rangle + \text{Lap}\left(\frac{|B|}{2^{k/2}}\right) $$。&lt;/li&gt;
&lt;li&gt;在以下线性规划中求解 wα，并四舍五入到最接近的整数权重 w&amp;rsquo;α：
&lt;ul&gt;
&lt;li&gt;最小化 $$ b $$&lt;/li&gt;
&lt;li&gt;约束条件：
&lt;ul&gt;
&lt;li&gt;$$ w_\alpha \geq 0 \quad \forall \alpha $$&lt;/li&gt;
&lt;li&gt;$$ \phi_\beta - \sum_\alpha w_\alpha f_\beta^\alpha \leq b \quad \forall \beta \in B $$&lt;/li&gt;
&lt;li&gt;$$ \phi_\beta - \sum_\alpha w_\alpha f_\beta^\alpha \geq -b \quad \forall \beta \in B $$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;使用列联表 w&amp;rsquo;α，计算并返回边际分布 A。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;定理 4 ([5])。使用边际分布 $\text{Marginals}(A) $的记号，以概率  $1 - \delta $，对于所有  $\alpha \in A$ ，有&lt;/p&gt;
$$
\|C_\alpha x - C_\alpha w&#39;\|_1 \leq 2\|\alpha\|_1 \frac{2|B| \log(|B|/\delta)}{ + |B|}. \quad (5)
$$&lt;p&gt;当 \( k \) 较大时，线性规划的时间复杂度是 $O(2^k) $。当 \( k \) 很大时，这并不令人满意。然而，令人惊讶的是，通过在回到数据域之前向第一个 Fourier 系数添加相对较小的量，可以实现非负性（但不是整数性）。不需要线性规划，并且引入的误差非常小。因此，如果在 $O(2^k)$ 上的多项式时间开销不可接受，而可以接受非整数性，那么该方法效果很好。我们注意到，在该工作的初步实现中，非整数性并不是问题，因为计数总是被转换为百分比。&lt;/p&gt;
&lt;h3 id=&#34;43-学习邻近半空间&#34;&gt;🍫4.3 学习（邻近）半空间
&lt;/h3&gt;&lt;p&gt;我们以一个例子结束本节，该例子受到学习理论中问题的启发，出现在Blum、Ligett和Roth即将发表的论文中[8]。其目标是提供一种对半空间查询的非交互式解决方案。从高层次来看，他们的方法是发布信息，以（近似）回答一大组“规范”查询，保证对于任何（可能是非规范的）该类型的查询，都存在一个“邻近”的规范查询。因此，数据分析师可以获得与感兴趣的查询在某种意义上接近的答案。在[8]中，查询是$\mathbb{R}^d$中的半空间查询，具体定义如下。在整个这一节中，我们采用[8]中的假设，即数据库点已缩放到单位球体上。&lt;/p&gt;
&lt;p&gt;定义 3。给定一个数据库$D \subset \mathbb{R}^d$ 和单位长度向量 $y \in \mathbb{R}^d$，半空间查询$H_y$ 定义为&lt;/p&gt;
$$
H_y(D) = \frac{|\{x \in D : \sum_{i=1}^{d} x_i \cdot y_i \geq 0\}|}{|D|}.
$$&lt;p&gt;注意，半空间查询可以通过两个计数查询来估计：“$|D|$  是多少？”和“$|{x \in D : \sum_{i=1}^{d} x_i \cdot y_i \geq 0}|$是多少？”因此，半空间查询的灵敏度最多为 2。半空间查询$H_{y_1}  和  H_{y_2}$ 之间的距离定义为它们之间角度的正弦，记作$\sin(y_1, y_2) $ 。考虑到这一点，Blum、Ligett 和 Roth 的算法确保了以下效用概念：&lt;/p&gt;
&lt;p&gt;定义 4 ([8])。数据库机制 \( A \) 对于某个度量 \( d \) 下的查询类 \( C \) 是 )-有用的，如果以概率$1 - \delta$，对于每个$Q \in C$和每个数据库 \( D \)，都有&lt;/p&gt;
$$
|Q(A(D)) - Q&#39;(D)| \leq \epsilon
$$&lt;p&gt;对于某个$Q&amp;rsquo; \in C$ 满足$d(Q, Q&amp;rsquo;) \leq \gamma$。注意，接近的是查询，而不是（不一定是）它们的答案。给定一个半空间查询$H_{y_1} $ ，下面的算法将输出一个值 \( v \)，使得&lt;/p&gt;
$$
|v - H_{y_2}(D)| &lt; \epsilon
$$&lt;p&gt;对于某个与$H_{y_1}$ $\gamma$ -接近的$H_{y_2}$。等价地，算法任意计数或不计数满足$\cos(x, y_1) \leq \gamma$的点$ x \in D$ 。Blum 等人指出，$\gamma$的作用类似于机器学习中的边际概念，并且即使$H_{y_1}$ 和 $H_{y_2}$是$\gamma$-接近的，也并不意味着查询$H_{y_1}(D)$和$H_{y_2}(D)$ 的真实答案是接近的，除非大多数数据点位于$H_{y_1}$和 $H_{y_2}$ 的$\gamma$  边际之外。&lt;/p&gt;
&lt;p&gt;定义 5 ([8])。一个半空间查询$H_y$是$b$ -离散化的，如果对于每个$i \in [d]$，$ y_i$ 可以用 \( b \) 位来表示。令$C_b$ 为所有 \( b \)-离散化半空间在$\mathbb{R}^d$  中的集合。&lt;/p&gt;
&lt;p&gt;考虑一个特定的 \( k &lt; d \) 维子空间，该子空间由一个随机的 $d \times k$矩阵 \( M \) 定义，其元素独立且均匀地从 \(\{-1, 1\}\) 中选择。考虑投影&lt;/p&gt;
$$
P_M(x) = \frac{1}{\sqrt{k}} x \cdot M,
$$&lt;p&gt;它将数据库点投影到子空间，并将它们重新缩放到单位球体。对于半空间查询$H_y$ ，投影$P_M(H_y)$ 只是由投影$P_M(y)$ 定义的 \( k \) 维半空间查询。关键的事实是，对于随机选择的 \( M \)，将数据库点 \( x \) 和半空间查询指定符 \( y \) 投影到一起，极不可能显著改变它们之间的角度：&lt;/p&gt;
&lt;p&gt;定理 5（约翰逊-林登斯特劳斯定理）。考虑将点 \( x \) 和半空间$H_y$ 投影到由投影矩阵 \( M \) 定义的随机 \( k \) 维子空间中。则有&lt;/p&gt;
$$
\Pr\left[| \cos(x, H_y) - \cos(P_M(x), H_{P_M(y)})| \geq \frac{\gamma}{4}\right] \leq 2e^{-\left(\frac{\gamma}{16}\right)^2 - \left(\frac{\gamma}{16}\right)^3} \frac{k}{4}.
$$&lt;p&gt;子空间的维度 \( k \) 被选择为使得投影一个点和一个半空间的角度变化超过$ \frac{\gamma}{4}$的概率最多为$\frac{1}{4}$ 。这导致&lt;/p&gt;
$$
k \geq \frac{4 \ln(8/\delta)}{\left(\frac{\gamma}{16}\right)^2 - \left(\frac{\gamma}{16}\right)^3}.
$$&lt;p&gt;因此，查询$H_y$ \(  \) 的答案可以通过对投影半空间查询的隐私保护估计来估计，整体准确性可以通过选择 \( m \) 个投影矩阵来提高； \( x \) 和 \( y \) 之间的角度将通过 \( m \) 组投影结果的中位数来估计。当然，如果目标只是响应少量半空间查询，经过投影过程就没有意义，更不用说进行多次投影了。但文献[8]的目标更为雄心勃勃：为（非离散化的）半空间查询提供一个$(\epsilon, \delta, \gamma)$-有用的非交互机制；这正是低维度发挥作用的地方。算法选择 \( m \) 个投影矩阵，其中 \( m \) 取决于离散化参数 \( b \)、维度 \( d \) 和失败概率 $\delta$（更具体地说，$m \in O(\ln(1/\delta) + \ln(bd))$。对于每个随机子空间（由投影矩阵 \( M \) 定义），算法选择一个“标准”半空间的网$N_M$ （由子空间中的标准向量定义），使得对于每个向量$y \in \mathbb{R}^k$ ，都有一个邻近的标准向量，具体而言，距离（诱导的正弦）最多为$\frac{3}{4}\gamma$ 。所需的标准向量数量为$O\left(\frac{1}{\gamma^{k-1}}\right)$。对于每个标准向量，策展人发布投影半空间查询的隐私保护估计。该机制是非交互式的，策展人将不再发挥进一步的作用。为了处理任意查询 \( y \)，分析师从一个空的多重集合开始。对于每个 \( m \) 个投影 \( M \)，分析师找到与$P_M(y)$最近的向量$\hat{y} \in N_M $，并将该半空间查询的答案添加到多重集合中。算法输出这些 \( m \) 个值的中位数。&lt;/p&gt;
&lt;p&gt;定理 6 ([8])。设&lt;/p&gt;
$$
n \geq \log(1/\delta) + \log m + (k - 1) \log(1/\gamma) + m O\left(\frac{1}{\gamma}\right) k - 1 2\alpha.
$$&lt;p&gt;则上述算法在保持 &lt;/p&gt;
$$ \alpha $$&lt;p&gt;-差分隐私的同时，是 &lt;/p&gt;
$$ (\epsilon, \gamma, \delta) $$&lt;p&gt;-有用的，适用于大小为 &lt;/p&gt;
$$ n $$&lt;p&gt; 的数据库。该算法的运行时间为多项式级别，具体为&lt;/p&gt;
$$
\text{poly}(\log(1/\delta), 1/\epsilon, 1/\alpha, b, d)
$$&lt;p&gt;对于常数 &lt;/p&gt;
$$ \gamma $$&lt;p&gt;。&lt;/p&gt;
&lt;h2 id=&#34;5-一般学习理论结果&#34;&gt;🍬5 一般学习理论结果
&lt;/h2&gt;&lt;p&gt;我们简要概述三个关于在交互模型中可以私密学习的常规结果。首先，我们介绍Blum、Dwork、McSherry和Nissim的结果，表明在统计查询学习模型中可学习的任何内容也可以通过交互方式有效地私密学习[7]。然后，我们转向忽略计算问题的结果，表明McSherry和Talwar的指数机制[35]可以用于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;私密学习任何可PAC学习的内容[32]；&lt;/li&gt;
&lt;li&gt;为任何具有多项式VC维度的函数类 $$ C $$ 生成一个差分隐私的“合成数据库”，该数据库对 $$ C $$ 中的任何查询给出“良好”的答案[8]。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在这种情况下使用指数机制的工作归功于Kasiviswanathan、Lee、Nissim、Raskhodnikova和Smith[32]。&lt;/p&gt;
&lt;h3 id=&#34;51-模拟统计查询模型&#34;&gt;🍭5.1 模拟统计查询模型
&lt;/h3&gt;&lt;p&gt;统计查询（SQ）模型由Kearns在[10]中提出，是一个用于考察在从基础分布独立抽样中执行的统计算法的框架。在该框架中，算法指定谓词 &lt;/p&gt;
$$ f_1, \ldots, f_k $$&lt;p&gt; 及其对应的准确度 &lt;/p&gt;
$$ \tau_1, \ldots, \tau_k $$&lt;p&gt;，并返回在 &lt;/p&gt;
$$ 1 \leq i \leq k $$&lt;p&gt; 的情况下，满足 &lt;/p&gt;
$$ f_i $$&lt;p&gt; 的样本的期望比例，误差为 &lt;/p&gt;
$$ \tau_i $$&lt;p&gt;。从概念上讲，该框架建模抽取足够数量的样本，以便观察到的满足每个 &lt;/p&gt;
$$ f_i $$&lt;p&gt; 的样本计数是实际期望的良好估计。统计查询模型在计算学习理论社区中最为常用，其目标通常是学习一个（在本例中为布尔）概念，即数据上的谓词，达到一定的准确度。形式上，如果算法 &lt;/p&gt;
$$ \delta $$&lt;p&gt; 学习一个概念 &lt;/p&gt;
$$ c $$&lt;p&gt;，则它生成的谓词在潜在分布下的误分类概率至多为 &lt;/p&gt;
$$ 1 - \delta $$&lt;p&gt;。&lt;/p&gt;
&lt;p&gt;Blum、Dwork、McSherry和Nissim表明，任何可以在统计查询模型中学习的概念都可以使用等效算法在所谓的“SuLQ”数据库上私密学习[7]；该证明并不复杂。利用本调查中呈现的稍有不同的技术重述这些结果，使得该结果的论证更加简单。假设我们在SQ模型中有一个算法，该算法进行 &lt;/p&gt;
$$ k $$&lt;p&gt; 次统计查询 &lt;/p&gt;
$$ f_1, \ldots, f_k $$&lt;p&gt;，并令 &lt;/p&gt;
$$ \tau = \min\{\tau_1, \ldots, \tau_k\} $$&lt;p&gt; 为SQ算法中所需的最小容忍度。假设 &lt;/p&gt;
$$ n $$&lt;p&gt;，数据库的大小是已知的。在这种情况下，我们可以通过询问与 &lt;/p&gt;
$$ f_i $$&lt;p&gt; 对应的谓词/计数查询（称为 &lt;/p&gt;
$$ p_i $$&lt;p&gt;）来计算 &lt;/p&gt;
$$ f_i $$&lt;p&gt; 的答案，并将结果除以 &lt;/p&gt;
$$ n $$&lt;p&gt;。因此，我们处理的查询序列的灵敏度至多为 &lt;/p&gt;
$$ k $$&lt;p&gt;。设 &lt;/p&gt;
$$ b = k/\epsilon $$&lt;p&gt;。将 &lt;/p&gt;
$$ \tau = \rho/n $$&lt;p&gt;，其中 &lt;/p&gt;
$$ \rho $$&lt;p&gt; 稍后确定，这样当对真实答案添加的噪声在计数查询 &lt;/p&gt;
$$ p_i $$&lt;p&gt; 时，其大小被限制在 &lt;/p&gt;
$$ \rho $$&lt;p&gt; 以内，统计查询的响应就在容忍度 &lt;/p&gt;
$$ \tau $$&lt;p&gt; 之内。我们希望找到 &lt;/p&gt;
$$ \rho $$&lt;p&gt;，使得当噪声根据 &lt;/p&gt;
$$ \text{Lap}(k/\epsilon) $$&lt;p&gt; 生成时，响应的噪声大小至少为 &lt;/p&gt;
$$ \rho $$&lt;p&gt; 的概率被限制为 &lt;/p&gt;
$$ \delta/k $$&lt;p&gt;。由于拉普拉斯分布是对称的，因此只需找到 &lt;/p&gt;
$$ x &lt; 0 $$&lt;p&gt; 使得在 &lt;/p&gt;
$$ x $$&lt;p&gt; 处的累积分布函数被限制为 &lt;/p&gt;
$$ \delta/2k $$&lt;p&gt;：&lt;/p&gt;
$$
\frac{1}{2} e^{-|x|/b} &lt; \frac{\delta}{2k} 
$$&lt;p&gt;通过简单计算，当 &lt;/p&gt;
$$ |x| &gt; b \ln(k/\delta) $$&lt;p&gt; 时，这成立，即当 &lt;/p&gt;
$$ |x| &gt; (k/\epsilon) \ln(k/\delta) $$&lt;p&gt;。因此，我们设 &lt;/p&gt;
$$ \rho &gt; (k/\epsilon) \ln(k/\delta) $$&lt;p&gt;。只要 &lt;/p&gt;
$$ \rho/n &lt; \tau $$&lt;p&gt;，或者更确切地说，只要 &lt;/p&gt;
$$ n &gt; \rho/\tau $$&lt;p&gt;，我们就可以模拟SQ算法。该分析仅考虑由 &lt;/p&gt;
$$ K $$&lt;p&gt; 引入的噪声；即，它假设&lt;/p&gt;
$$
\frac{1}{n} \sum_{i=1}^{n} f_j(d_i) = \Pr_{x \in D}[f_j(x)], \quad 1 \leq j \leq k, 
$$&lt;p&gt;其中 &lt;/p&gt;
$$ D $$&lt;p&gt; 是示例上的分布。以上结果同样适用，当我们假设数据库中的行是根据 &lt;/p&gt;
$$ D $$&lt;p&gt; 以独立同分布的方式抽取时，利用众所周知的事实，确保对于所有 &lt;/p&gt;
$$ f_j $$&lt;p&gt;（&lt;/p&gt;
$$ 1 \leq j \leq k $$&lt;p&gt;）同时满足容忍度 &lt;/p&gt;
$$ \tau $$&lt;p&gt; 的概率至少为 &lt;/p&gt;
$$ 1 - \delta $$&lt;p&gt; 的条件是 &lt;/p&gt;
$$ n &gt; \tau^{-2} \log(k/\delta) $$&lt;p&gt;。通过在所有地方将 &lt;/p&gt;
$$ \tau $$&lt;p&gt; 替换为 &lt;/p&gt;
$$ \tau/2 $$&lt;p&gt; 并找到 &lt;/p&gt;
$$ n $$&lt;p&gt; 的最大下界，可以处理这两种类型的错误。&lt;/p&gt;
&lt;h3 id=&#34;52-私密pac学习&#34;&gt;🍮5.2 私密PAC学习
&lt;/h3&gt;&lt;p&gt;本节的结果源于Kasiviswanathan、Lee、Nissim、Raskhodnikova和Smith [32]。我们首先对“概率近似正确”（PAC）学习的概念进行非正式和不完整的回顾，这是Valiant提出的一个概念[47]。考虑一个概念 &lt;/p&gt;
$$ t: X \rightarrow Y $$&lt;p&gt;，它将从域 &lt;/p&gt;
$$ X $$&lt;p&gt; 中取出的每个示例分配一个来自范围 &lt;/p&gt;
$$ Y $$&lt;p&gt; 的标签。与前一节相似，学习算法会从目标概念的分布 &lt;/p&gt;
$$ D $$&lt;p&gt; 中获取带标签的示例；目标是从指定的假设类中生成一个假设 &lt;/p&gt;
$$ h: X \rightarrow Y $$&lt;p&gt;，使得错误率较小，定义为：&lt;/p&gt;
$$
\text{error}(h) = \Pr_{x \in R^D}[t(x) = h(x)].
$$&lt;p&gt;概念类是概念的集合。学习理论研究哪些类型的概念类是可学习的。设 &lt;/p&gt;
$$ \alpha $$&lt;p&gt; 和 &lt;/p&gt;
$$ \beta $$&lt;p&gt; 表示两个错误界限，如果目标概念属于 &lt;/p&gt;
$$ C $$&lt;p&gt;，则目标是最小化错误，或至少确保以概率 &lt;/p&gt;
$$ 1 - \beta $$&lt;p&gt; 错误被限制在 &lt;/p&gt;
$$ \alpha $$&lt;p&gt; 之内。这是传统PAC学习的设定。如果目标概念不必属于 &lt;/p&gt;
$$ C $$&lt;p&gt;，则目标是产生一个假设，该假设在直观上与 &lt;/p&gt;
$$ C $$&lt;p&gt; 中的任何概念几乎同样有效：设 &lt;/p&gt;
$$ \text{OPT} = \min_{c \in C}\{\text{error}(c)\} $$&lt;p&gt;，我们希望&lt;/p&gt;
$$
\Pr[\text{error}(h) \leq \text{OPT} + \alpha] \geq 1 - \beta,
$$&lt;p&gt;其中概率是关于学习者看到的样本和学习者的随机性的。这个过程称为不可知学习。&lt;/p&gt;
&lt;p&gt;根据[32]的定义，我们将用其二进制编码的长度 &lt;/p&gt;
$$ d $$&lt;p&gt; 索引概念类、域和范围。对于目标概念 &lt;/p&gt;
$$ t: X_d \rightarrow Y_d $$&lt;p&gt; 和在 &lt;/p&gt;
$$ X_d $$&lt;p&gt; 上的分布 &lt;/p&gt;
$$ D $$&lt;p&gt;，令 &lt;/p&gt;
$$ Z \in D^n $$&lt;p&gt; 为一个数据库，其中包含从 &lt;/p&gt;
$$ D $$&lt;p&gt; 独立抽取的 &lt;/p&gt;
$$ n $$&lt;p&gt; 个带标签的样本。即，&lt;/p&gt;
$$ Z $$&lt;p&gt; 包含 &lt;/p&gt;
$$ n $$&lt;p&gt; 对 &lt;/p&gt;
$$ (x_i, y_i) $$&lt;p&gt;，其中 &lt;/p&gt;
$$ y_i = t(x_i) $$&lt;p&gt;，&lt;/p&gt;
$$ 1 \leq i \leq n $$&lt;p&gt;。数据分析师的目标是不可知地学习假设类 &lt;/p&gt;
$$ C $$&lt;p&gt;；策展人的目标是确保 &lt;/p&gt;
$$ \epsilon $$&lt;p&gt;-差分隐私。&lt;/p&gt;
&lt;p&gt;定理 7 ([32])。每个概念类 &lt;/p&gt;
$$ C $$&lt;p&gt; 都是 &lt;/p&gt;
$$ \epsilon $$&lt;p&gt;-差分隐私地不可知学习的，使用的假设类为 &lt;/p&gt;
$$ H = C $$&lt;p&gt;，其中 &lt;/p&gt;
$$ n \in O(\log |C|d + \log(1/\beta) \cdot \max\{1/\alpha, 1/\alpha^2\}) $$&lt;p&gt;。学习者可能不是高效的。&lt;/p&gt;
&lt;p&gt;该定理是通过McSherry和Talwar的指数机制证明的，效用函数为&lt;/p&gt;
$$
u(Z, h) = -| \{ i : t(x_i) = h(x_i) \} | 
$$&lt;p&gt;对于 &lt;/p&gt;
$$ Z \in (X \times Y)^n $$&lt;p&gt; 和 &lt;/p&gt;
$$ h \in H_d $$&lt;p&gt;。注意，&lt;/p&gt;
$$ u $$&lt;p&gt; 的敏感度为1，因为数据库中任何元素的改变最多只能导致一个错误分类的变化。该（低效）算法以与 &lt;/p&gt;
$$ \exp(u(Z, c)/2) $$&lt;p&gt; 成正比的概率输出 &lt;/p&gt;
$$ c \in H_d $$&lt;p&gt;。隐私性源于指数机制的性质和 &lt;/p&gt;
$$ u $$&lt;p&gt; 的小敏感度。准确性（以高概率低错误）稍微难以论证；证明直观上源于输出的概率随着错误分类数量的增加而呈指数下降。&lt;/p&gt;
&lt;h3 id=&#34;53-差分隐私的多项式-vc-维度类查询&#34;&gt;🍯5.3 差分隐私的多项式 VC 维度类查询
&lt;/h3&gt;&lt;p&gt;我们最后一个例子来自 Blum、Ligett 和 Roth [8]，并受到上一节结果的启发。这个学习结果再次忽略了计算效率，使用了 McSherry 和 Talwar 的指数机制。这里的目标是发布一个“合成数据集”，以确保对特定类 &lt;/p&gt;
$$ C $$&lt;p&gt; 中所有查询的“合理”准确回答。读者应该对概念类的 Vapnick-Chervonenkis (VC) 维度有所了解。粗略来说，VC 维度是衡量类中概念复杂度的一种度量。&lt;/p&gt;
&lt;p&gt;定义 6（[8]）。一个数据库机制 &lt;/p&gt;
$$ A $$&lt;p&gt; 对于类 &lt;/p&gt;
$$ C $$&lt;p&gt; 是 $(\gamma, \delta)$-有用的，如果以概率 $1 - \delta$，对于每个 &lt;/p&gt;
$$ Q \in C $$&lt;p&gt; 和每个数据库 &lt;/p&gt;
$$ D $$&lt;p&gt;，对于 &lt;/p&gt;
$$ \hat{D} = A(D) $$&lt;p&gt;，有&lt;/p&gt;
$$
|\ Q(\hat{D}) - Q(D)| \leq \gamma.
$$&lt;p&gt;令 &lt;/p&gt;
$$ C $$&lt;p&gt; 为固定的查询类。给定一个大小为 &lt;/p&gt;
$$ n $$&lt;p&gt; 的数据库 &lt;/p&gt;
$$ D \in ({0, 1}^d)^n $$&lt;p&gt;，其中 &lt;/p&gt;
$$ n $$&lt;p&gt; 足够大（作为 &lt;/p&gt;
$$ C $$&lt;p&gt; 的 VC 维度以及 &lt;/p&gt;
$$ \epsilon $$&lt;p&gt; 和 &lt;/p&gt;
$$ \delta $$&lt;p&gt; 的函数），目标是生成一个合成数据集 &lt;/p&gt;
$$ \hat{D} $$&lt;p&gt;，使其对类 &lt;/p&gt;
$$ C $$&lt;p&gt; 中的查询是 $(\gamma, \delta)$-有用的，同时确保 &lt;/p&gt;
$$ \epsilon $$&lt;p&gt;-差分隐私。合成数据集将包含 &lt;/p&gt;
$$ m = O\left(\frac{\text{VC dim}(C)}{\gamma^2}\right) $$&lt;p&gt; 个 &lt;/p&gt;
$$ d $$&lt;p&gt; 元组。它是根据指数机制选择的，使用的效用函数为&lt;/p&gt;
$$
u(D, \hat{D}) = - \max_{h \in C} \left| h(D) - \frac{n}{m} h(\hat{D}) \right| .
$$&lt;p&gt;定理 8（[8]）。对于任何函数类 &lt;/p&gt;
$$ C $$&lt;p&gt;，以及任何数据库 &lt;/p&gt;
$$ D \subset \{0, 1\}^d $$&lt;p&gt;，如果 &lt;/p&gt;
$$ |D| \geq O\left( \frac{d \cdot \text{VC dim}(C)}{\gamma^3} + \frac{\log(1/\delta)}{\gamma} \right) $$&lt;p&gt;，我们可以输出一个 $(\gamma, \delta)$-有用的数据库 &lt;/p&gt;
$$ \hat{D} $$&lt;p&gt;，同时保持 &lt;/p&gt;
$$ \epsilon $$&lt;p&gt;-差分隐私。请注意，该算法不一定是高效的。Blum 等人指出，这对于 $(\gamma, \delta)$-有用性是足够的，因为所有这种大小的数据库的集合在 &lt;/p&gt;
$$ C $$&lt;p&gt; 上形成了所有可能数据库集合的 &lt;/p&gt;
$$ \gamma $$&lt;p&gt;-覆盖。可以通过考虑匹配任何查询的条目比例来解决 &lt;/p&gt;
$$ |\hat{D}| &lt; |D| $$&lt;p&gt; 的事实，即匹配任何查询的数据库条目的数量会成比例地较小。&lt;/p&gt;
&lt;h2 id=&#34;6-总结&#34;&gt;🍼6 总结
&lt;/h2&gt;&lt;p&gt;在本节中，我们探讨了隐私机制如何根据数据库上应用的查询序列的复杂性增加噪声。尽管使用高斯噪声可以在一定程度上缓解这一问题，但Dinur和Nissim [13]（参见[17, 20]）所开始的研究表明，这种增加是必不可少的。Dinur和Nissim的结果在很大程度上推动了机制K的开发以及本综述中倡导的整个交互方法。为了更精确地分析现实攻击，并更好地理解未能提供$\epsilon$-差分隐私在实践中可能意味着什么，我们需要进一步研究以改善这些结果，或者确认这种改进是不可能的，从而理解如何在除非常大、“互联网规模”的数据集之外有效利用这些技术。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>第三周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC%E4%B8%89%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</link>
        <pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC%E4%B8%89%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E7%AC%AC%E4%B8%89%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/colorful-2609978_1280.jpg" alt="Featured image of post 第三周工作总结" /&gt;&lt;h1 id=&#34;总结&#34;&gt;总结
&lt;/h1&gt;&lt;p&gt;这一周高强度阅读把书都读完了，但是没看代码，后果是有点记不住，等有需要时再去看吧。&lt;/p&gt;
&lt;p&gt;这周看了大模型内容比较多，主要是为了快点看完相关内容找ZD哥确定毕设方向，早点开始，做的好一点。&lt;/p&gt;
&lt;p&gt;周五下午找了ZD哥确定方向，看完方向人有点蒙，可能是大模型看得多，安全那方面看得少，导致我不知道这个方向在干什么，于是开始沉思，思考了半天感觉确实不会，找gpt老师询问，没有得到结果，故有些惆怅，周日又坐在实验室上了一天的软件工程实训，还好马上就结束了，太烦了。今天是周一，先把上周周报做了&lt;/p&gt;
&lt;p&gt;我又重新换了问法，问了gpt老师，这次得到了满意的结果，找到了几篇论文，这周就读这几篇。&lt;/p&gt;
&lt;p&gt;感觉这才是周报，之前的都是书的摘抄。&lt;/p&gt;
&lt;h2 id=&#34;阅读&#34;&gt;阅读
&lt;/h2&gt;&lt;p&gt;LLM预训练数据准备&lt;/p&gt;
&lt;p&gt;transformer模型架构（不知道是不是我本，这个我从不同的地方看了听了好多遍，现在才刚有点感觉）&lt;/p&gt;
&lt;p&gt;Instruction Tuning&lt;/p&gt;
&lt;p&gt;提示学习（实在不行，没有研究天赋就去干点轻松的(bushi)）&lt;/p&gt;
&lt;p&gt;解码与部署&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;后面就是补了两篇差分隐私的文献，读完感觉对要研究的方向还是不清楚，故又找了几篇：&lt;/p&gt;
&lt;h2 id=&#34;计划&#34;&gt;计划
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文综述了在机器学习中实现安全和隐私保护的各种方法，包括使用同态加密和安全多方计算的技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Inference with Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Inference of Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Machine Learning: A Practical Approach&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Outsourced Computation in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Differentially Private Inference in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Towards Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Model Inference: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Differential Privacy</title>
        <link>https://JiangZhiyu-1024.github.io/p/differential_privacy/</link>
        <pubDate>Sat, 02 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/differential_privacy/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/differential_privacy/cosmos-6680031_1280.jpg" alt="Featured image of post Differential Privacy" /&gt;&lt;h1 id=&#34;differential-privacy--a-primer-for-the-perplexed&#34;&gt;❤️Differential Privacy – A Primer for the Perplexed
&lt;/h1&gt;&lt;p&gt;差分隐私：给困惑者的入门指南论文笔记&lt;/p&gt;
&lt;p&gt;差分隐私是一种针对隐私保护数据分析的隐私目标定义。自其提出以来，差分隐私数据分析一直是激烈的算法和理论研究的主题。不幸的是，部分文献存在对这一定义的基本误解。在这篇简短的邀请性说明中，我们阐明并回应了这些常见错误中的最重要的几种。&lt;/p&gt;
&lt;h2 id=&#34;错误-1将定义与特定算法混淆&#34;&gt;💖错误 1：将定义与特定算法混淆
&lt;/h2&gt;&lt;h4 id=&#34;原文&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;差分隐私是一个定义。它是一个数学保证，可以通过一个发布数据集统计信息的算法来满足。许多不同的算法都可以满足这一定义。接下来，我们将假设有一个包含 n 行的数据库 X，每一行的数据属于单个个体。如果每个个体的数据被描述为集合 U 的一个元素，那么我们可以将数据集视为 U 中的一个多重集（即每个元素可以出现多次的集合）。设 D 表示可能的数据集空间。存在一个算法 M，它以数据库和（可选）查询作为输入，并返回响应。我们将这种算法称为机制。我们将在后面陈述差分隐私的定义。不过，现在我们注意到，如果一个随机机制 M 在所有数据库上具有相同的输出分布，那么它的输出就不会泄露任何关于其输入的信息。实际上，这等同于以下陈述，其中 D 是所有可能数据库的集合，Q 是所有可能查询的集合：&lt;/p&gt;
&lt;p&gt;对于所有的数据库 X 和 X′ 属于 D，对于所有的查询 q 属于 Q，以及对于所有的 S⊆Range(M)
&lt;/p&gt;
$$
\frac{\Pr[M(X, q) \in S]}{\Pr[M(X&#39;, q) \in S]} = e^0 = 1
$$&lt;p&gt;
在分子和分母中，概率空间都是基于机制所做的随机选择。需要注意的是，确定性机制如果忽略其输入，也可以满足这一属性。从方程式1可以推导出，如果产生了输出y，这并不会透露关于数据库的信息。方程式1是绝对的：如果一个算法具有这个属性，那么无论观察者——即看到输出y的数据分析师知道或能访问什么，这个属性都成立。如果观察者已经知道数据库是X，或者数据库包含了本文作者的医疗数据，甚至观察者知道数据库中所有镰状细胞状态位的模2总和，这些都不能改变机制的行为保证。与满足方程式1的机制进行交互不会泄露关于数据库的任何进一步信息。&lt;/p&gt;
&lt;h4 id=&#34;解读&#34;&gt;解读
&lt;/h4&gt;&lt;p&gt;作者引入了“机制”的概念，指的是一个接受数据库和查询作为输入的算法，并返回结果。重要的是，作者提到如果一个随机机制在所有数据库上都有相同的输出分布，那么该机制不会泄露关于输入的任何信息，这是一种非常强的隐私保护保证。这为后续对差分隐私定义的详细阐述奠定了基础，并强调了实现隐私保护的机制设计的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;符号解释&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;X 和 X′：代表两个可能的数据库。&lt;/li&gt;
&lt;li&gt;D：所有可能的数据库的集合。&lt;/li&gt;
&lt;li&gt;q：表示某个查询。&lt;/li&gt;
&lt;li&gt;S：表示机制 M 的输出范围中的一个子集。&lt;/li&gt;
&lt;li&gt;Pr⁡[M(X,q)∈S]：表示在数据库 X 上执行查询 q 时，输出结果落在集合 S 中的概率。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;条件意义&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该条件表明，对于任意两个数据库 X 和 X′，它们的输出落在集合 S 中的概率比是 e0=1，这意味着两者的概率是相等的。&lt;/li&gt;
&lt;li&gt;这个特性确保了任何一个个体的存在或缺失对查询结果的影响是不可察觉的，从而有效保护个体的隐私。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;原文-1&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;这是香农（Shannon）在加密上下文中提出的完美保密的定义。这不是一个算法，而是对随机算法概率分布的条件；任何满足这一条件的算法在信息论意义上都不会泄露关于数据的信息。两个数据库X和X′之间的对称差（symmetric difference），记作X⊕X′，是指出现在X或X′中但不在它们交集中的元素或行的集合。例如，如果X′是X的一个子集，且X中仅包含一行不在X′中，那么对称差的基数为1。（由于X和X′可以是多重集合，对称差实际上要复杂一些：如果一个项在一个数据库中比另一个数据库多出现k次，那么它在对称差中出现k次）。我们现在准备阐述差分隐私的定义。它的形式与方程1完全相同，只是现在我们允许释放一些信息：&lt;/p&gt;
&lt;p&gt;定义1：一个机制满足ε-差分隐私，如果对于每对数据库X和X′，以及每个子集S ⊆ Range(M)，和每个查询q ∈ Q（如果使用可选的“查询”参数）：
&lt;/p&gt;
$$
\frac{\Pr[M(X, q) \in S]}{\Pr[M(X&#39;, q) \in S]} = e^{ε|X Θ X&#39;|}
$$&lt;p&gt;
在分子和分母中，概率空间是基于机制所做的随机选择。&lt;/p&gt;
&lt;p&gt;这个定义与完美保密的定义（方程1）有一个显著的不同：在概率比率的界限中，e的指数是ε|XX′|而不是0。&lt;/p&gt;
&lt;p&gt;这里的符号长这样，但我敲不出来，不知道这是啥东西：&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/differential_privacy/image-20241102103704588.png&#34;
	width=&#34;129&#34;
	height=&#34;32&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/differential_privacy/image-20241102103704588_hu15975875244688783122.png 480w, https://JiangZhiyu-1024.github.io/p/differential_privacy/image-20241102103704588_hu16610540167326729282.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241102103704588&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;403&#34;
		data-flex-basis=&#34;967px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;如果一个算法满足定义1，那么这一点不受观察者（看到输出y的数据分析师）所知道或能够访问的内容的影响。如果观察者已经知道数据库是X，或者数据库包含了本文作者的医疗数据，甚至如果观察者知道数据库中所有镰形细胞状态位的模2和，这都不会改变机制的行为保证。对这个定义的一个常见解释是，无论观察者（或攻击者）事先知道什么，差分隐私算法的输出几乎不会指示任何特定用户的数据是否被包含在数据集中。我们稍后会再讨论这一点。现在我们总结：这个定义不是一个算法，而是许多不同算法所满足的条件。需要注意的是，以这些术语来表述隐私，作为可以通过多种方式满足的要求，提供了一个框架，使得人们可以研究算法，比较它们的隐私保证，并理解它们对隐私的联合影响。我们相信，这是隐私科学方法中的必要一步。”&lt;/p&gt;
&lt;h4 id=&#34;解读-1&#34;&gt;解读
&lt;/h4&gt;&lt;p&gt;这段话讨论了差分隐私的定义与完美保密之间的差异，强调了差分隐私算法在不同背景下的行为保证，即使观察者掌握了一些先验知识，也无法推断出特定用户数据的包含情况。作者指出，这种定义不仅可以适用于多种算法，还提供了一个研究框架，使得对隐私保护的算法进行比较和理解成为可能。这种方法被认为是科学研究隐私的必要步骤，显示了隐私保护的复杂性和重要性。&lt;/p&gt;
&lt;h2 id=&#34;错误2混淆输出与生成它们的过程&#34;&gt;💗错误2：混淆输出与生成它们的过程
&lt;/h2&gt;&lt;h4 id=&#34;原文-2&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;考虑著名的1位一次性密码机制，它提供完美的保密性。该随机机制的输入是一个位b ∈ {0, 1}。该机制选择一个新的随机位p ∈R {0, 1}，并输出b + p mod 2。”&lt;/p&gt;
&lt;p&gt;声明1：1位一次性密码机制提供完美的保密性。证明：证明是显而易见的，因为该机制满足方程1。具体而言：人数n为1，数据库的空间是D = {0, 1}；没有‘查询’参数，机制的范围是{0, 1}。通过逐一检查所有可能性，我们看到1/2 = Pr[M(0) = 1] = Pr[M(0) = 0] = Pr[M(1) = 1] = Pr[M(1) = 0]。因此，
&lt;/p&gt;
$$
\forall X, X&#39; \in D, \forall q \in Q, \forall S \subseteq \text{Range}(M) :\frac{\Pr[M(X, q) \in S]}{\Pr[M(X&#39;, q) \in S]} = 1
$$&lt;p&gt;
请注意，尽管机制实际上是使用其输入进行计算的，因此在某种正式的操作意义上它并不忽略其输入，但效果是相同的：输出分布是相同的，与数据库无关。如上所述，这意味着机制的输出不提供关于数据库的任何信息。这在以下四种情况下都是正确的：输出为0且数据库为0，输出为1且数据库为0，输出为0且数据库为1，或者输出为1且数据库为1。在所有四种情况下，输出对数据库没有任何可学习的信息。确保这一点的是生成输出的过程。因此，我们不说“安全”或“保护隐私”的输出；相反，我们认为这些输出是根据保护隐私的过程生成的。1位一次性密码是一种提供完美隐私的过程。所有完美私密的机制都是差分隐私的，但差分隐私机制的类别中包含一些不会提供完美隐私的机制；相反，它们提供关于数据库的一些非平凡信息，这在我们希望进行保护隐私的数据分析时是合理的。随机响应就是这样一种机制[War65]。考虑以下1位随机响应机制。与1位一次性密码机制一样，可能的数据库集是D = {0, 1}。设X ∈ {0, 1}为实际数据库。然而，该机制的步骤如下：如果p = 1，则翻转第二个硬币c ∈R {0, 1}，并公布结果。如果p = 0，则发布X。&lt;/p&gt;
&lt;p&gt;声明2：1位随机响应机制是(ln 3)/2-差分隐私的。&lt;/p&gt;
&lt;p&gt;证明：固定d ∈ {0, 1}。没有损失的一般性，设X = d，X′ = 1 − d。在输入X时，以概率3/4输出d。另一方面，在输入X′时，以概率1/4输出d。两者的比率是3。对称差为2，得出((ln 3)/2)-差分隐私。请注意，对于给定数据库并不存在“好输出”或“坏输出”的概念。尽管如此，发布的值确实给出了两个1位数据库哪个更可能的一点线索，但随机性提供了不确定性。与往常一样，我们指出，1位随机响应机制保持其差分隐私属性，独立于任何看到输出的人对数据库或其与其他数据库的连接所知的内容。&lt;/p&gt;
&lt;h4 id=&#34;解读-2&#34;&gt;解读
&lt;/h4&gt;&lt;p&gt;这段文字首先声称1位随机响应机制具有特定的差分隐私级别，即(ln 3)/2。接下来，作者通过一个简单的证明来支持这一声明，设定输入数据库X和X&amp;rsquo;，并计算在这两种情况下输出d的概率。通过比较输出概率，可以得出比率为3，这意味着在一定条件下，一个数据库比另一个数据库更可能生成特定输出。这里提到的对称差反映了两个数据库之间的差异。作者进一步解释，尽管随机响应机制的输出可能提供了关于数据库的信息，但并不存在绝对的“好”或“坏”输出，随机性引入了不确定性，从而保护了隐私。此外，强调了该机制的差分隐私属性与观察者所知的信息无关，确保了其隐私保护的强度。&lt;/p&gt;
&lt;h2 id=&#34;错误3将特定差分隐私算法的低效用与差分隐私本身的失败混淆&#34;&gt;💟错误3：将特定差分隐私算法的低效用与差分隐私本身的失败混淆。
&lt;/h2&gt;&lt;p&gt;低效用指的是某个具体的差分隐私算法在提供隐私保护的同时，可能导致输出结果的质量或信息量下降。然而，这并不意味着差分隐私本身是一种失败的概念或方法。相反，某个特定算法的效用差可能源于设计、实现或数据特性等因素，而不是差分隐私的理论基础。&lt;/p&gt;
&lt;h4 id=&#34;原文-3&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;正如我们所指出的，1位完美一次性密码是一种差分隐私算法。更具体地说，对于所有ε ≥ 0，1位一次性密码算法是ε-差分隐私的。它也不提供关于数据库的信息。这并不意味着‘差分隐私不能起作用’或没有差分隐私算法能够产生有用的输出。实际上，我们也看到1位随机响应是(ln 3)/2-差分隐私的。从效用的角度来看，它是一个比一次性密码更好的(ln 3)/2-差分隐私算法。特别是，众所周知，假设随机响应调查中的所有n个受访者都正确遵循指示，人们可以从他们的回答中推导出承认特定行为的受访者数量的估计，其预期的不准确性（或扭曲）在√n的数量级上。实际上，存在一些(ln 3)/2-差分隐私算法，其准确性更高，即能在预期扭曲独立于n的情况下给出估计计数。当ε = (ln 3)/2时，拉普拉斯机制[DMNS06]发布的统计数据具有正确的均值和标准差√2(√3/2) = √3/2，而不是随机响应中标准差为Θ(√n)。因此，特定差分隐私算法的局限性不一定适用于所有差分隐私算法。任何声称差分隐私无法实现给定准确性目标的说法，必须附有证明，说明差分隐私——关于概率分布比率的具体要求——与该目标不一致。也就是说，必须表明没有任何差分隐私算法能够实现给定的准确性水平。这是相当棘手的，我们对于什么是可能的、什么是不可能的直觉往往是错误的。例如，自然会猜测上述提到的拉普拉斯机制基本上是在差分隐私下回答“统计”类型问题的最佳选择。然而，在需要同时计算多个统计数据的情况下，存在更复杂的算法（例如[BLR08]）表现得更好。尽管存在这些困难，还是有一些已发表的结果证明了差分隐私算法的非平凡局限性（见[GRS08，HT09]以获取特别优雅的例子）。&lt;/p&gt;
&lt;h4 id=&#34;解读-3&#34;&gt;解读
&lt;/h4&gt;&lt;p&gt;这段话探讨了差分隐私算法的有效性和局限性。作者首先确认了1位完美一次性密码是差分隐私的，并强调这种算法提供的信息量为零。接着，作者指出这并不意味着差分隐私本身无效，实际上，1位随机响应机制在效用上表现更好，且具有可接受的差分隐私级别。通过提供具体的例子，作者说明在一些情况下，差分隐私算法可以有效地估计特定行为的发生数量，并且存在一些算法可以在准确性方面表现得更好。&lt;/p&gt;
&lt;p&gt;作者进一步指出，对于声称差分隐私无法达到特定准确性目标的说法，必须提供充分的证明，表明差分隐私的定义与该目标之间的不一致性。强调了解决差分隐私算法的局限性并非易事，且我们的直觉可能会误导我们。作者还提到，即使拉普拉斯机制在某些情况下看似最佳，仍可能存在更复杂且效果更好的算法。最后，作者提到一些研究已证明了差分隐私算法的非平凡局限性，表明在这一领域还有很多需要探索的内容。&lt;/p&gt;
&lt;h2 id=&#34;差分隐私的杂项做和不做&#34;&gt;💝“差分隐私的杂项‘做’和‘不做’”
&lt;/h2&gt;&lt;p&gt;这部分内容通常会列出在实施和理解差分隐私时应遵循的一些基本原则和误区。以下是一些可能的“做”和“不做”的示例：&lt;/p&gt;
&lt;h5 id=&#34;做does&#34;&gt;做（Does）：
&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;确保输入的隐私&lt;/strong&gt;：在设计差分隐私机制时，要确保能够有效保护用户数据的隐私。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;选择合适的噪声机制&lt;/strong&gt;：使用合适的噪声添加方法（如拉普拉斯噪声或高斯噪声）来达到所需的差分隐私级别。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;进行严格的分析&lt;/strong&gt;：对算法进行严格的数学分析，以确认其满足差分隐私的要求。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;理解隐私损失&lt;/strong&gt;：在实际应用中，清楚了解不同参数（如ε）的选择对隐私保护和数据效用的影响。&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;不做does-not&#34;&gt;不做（Does Not）：
&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;不要忽视算法的效用&lt;/strong&gt;：在追求差分隐私的同时，不能完全忽略算法的效用，确保输出仍然有用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不要混淆差分隐私和其他隐私保护技术&lt;/strong&gt;：差分隐私是一个特定的框架，与其他隐私保护方法（如匿名化）有所不同。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不要低估数据特性&lt;/strong&gt;：在实施差分隐私时，忽视数据本身的特性可能会影响隐私保护效果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不要期望完美的隐私保护&lt;/strong&gt;：差分隐私提供的是概率性保证，而非绝对安全，因此要有合理的期望。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些“做”和“不做”可以帮助研究人员和工程师更好地理解和实施差分隐私，避免常见的误区和误解。&lt;/p&gt;
&lt;h4 id=&#34;原文-4&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;我们重申迄今为止的主要观点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;差分隐私是一个定义，而不是一个算法。&lt;/li&gt;
&lt;li&gt;对于任何特定的数据库，没有输出是‘好’或‘坏’的。隐私（完美隐私或差分隐私）是通过生成输出的过程获得的。&lt;/li&gt;
&lt;li&gt;某个特定的ε-差分隐私机制未能提供足够准确的答案，并不意味着差分隐私与该目标不兼容。 在最后一节中，我们将对此进行进一步阐述，并进一步解释差分隐私，解决文献中关于差分隐私所提供或不提供的若干误解。差分隐私通常是在提供给数据分析的敏感记录的背景下进行解释的。隐私保证是，当个体选择加入或退出输入数据库时，输出的分布，因此从这些输出得出的结论的分布，最多只会变化一个（通常较小的）乘法因子。因此，参与其中几乎没有任何危害；关于您、您的数据或更广泛的世界得出的任何结论，几乎与不使用您的数据时得出的结论一样可能。”&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;事实：差分隐私并不保证您认为的秘密保持秘密。差分隐私所保证的是，您参与调查不会泄露您在调查中贡献的具体信息。得出的结论可能会反映出关于您的统计信息。一个旨在发现某种疾病早期指示的健康调查可能会产生强有力甚至结论性的结果；这些结论适用于您并不证明差分隐私的违反。事实上，您甚至可能没有参与调查（再次强调，差分隐私确保无论您是否参与调查，这些结论性结果几乎以相同的概率得出）。特别是，如果调查显示特定私人属性与公共属性之间有很强的相关性，这并不构成差分隐私的违反，因为这种相关性几乎同样的概率会在有无任何受访者的情况下被观察到。我们最喜欢的寓言是：假设一个统计数据库教我们吸烟导致癌症。已知吸烟者艾丽斯受到了伤害，因为她的保险费上涨。无论艾丽斯是否在数据库中，她受到伤害的可能性基本上是相同的。艾丽斯也得到了帮助：因为这项研究，她参加了戒烟项目。差分隐私的设计旨在确保艾丽斯和其他人都有动力参与这些社会有益的研究。&lt;/p&gt;
&lt;h4 id=&#34;解读-4&#34;&gt;解读
&lt;/h4&gt;&lt;p&gt;这段文字强调了差分隐私的局限性，并澄清了一些常见误解。作者指出，尽管差分隐私保护了个人在参与调查时的具体贡献不被泄露，但这并不意味着与个人相关的秘密会保持秘密。调查得出的统计结论可能仍然会反映出个人的某些特征，即使这些特征并没有直接在调查中披露。&lt;/p&gt;
&lt;p&gt;接着，作者用一个例子来说明这一点：即使吸烟导致癌症的结论在某个数据库中被证实，个体（如艾丽斯）因其吸烟习惯而遭受的后果并不违反差分隐私，因为这一结论可以在几乎相同的概率下得出，而与个体是否参与调查无关。作者强调，差分隐私的设计目的是鼓励个人参与对社会有益的研究，尽管参与者可能面临一些与其私人生活相关的后果。&lt;/p&gt;
&lt;p&gt;整体而言，这段文字意在提醒读者，差分隐私虽能保护个体数据的具体性，但并不消除由统计结果引发的潜在影响，反而要让人们更积极地参与能够产生社会价值的研究。&lt;/p&gt;
&lt;h4 id=&#34;原文-5&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;在讨论差分隐私时，有一种观点认为差分隐私的定义在涉及多个参与者或多个数据记录时变得不再有效或合理。这种说法的误解主要源于对差分隐私如何处理多个数据库或多个记录的理解不足。&lt;/p&gt;
&lt;h4 id=&#34;解读-5&#34;&gt;解读
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;差分隐私的扩展性&lt;/strong&gt;：差分隐私的定义本质上是针对单个记录的，但它可以扩展到多个记录和多个参与者的情况。在这种情况下，算法仍然可以通过添加适当的噪声来保证输出的隐私性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;统计特性&lt;/strong&gt;：在处理多个记录时，差分隐私并不失去其意义，因为差分隐私关注的是如何在统计分析中保持个体数据的隐私。当多个人的数据被聚合时，差分隐私确保了即使某个特定个体的记录被包含或排除，输出的变化仍然受到限制。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;应用实例&lt;/strong&gt;：在实际应用中，差分隐私已经被成功地应用于大规模数据集和多个参与者的场景中，如大型调查和机器学习模型的训练。在这些情况下，差分隐私提供了一种框架，使得即使存在多个数据记录，仍然可以有效地保护个体隐私。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;误解来源&lt;/strong&gt;：认为“差分隐私对于多于一个记录没有意义”的观点，往往源于对其基本原理和统计保证的误解。实际上，差分隐私的设计目的就是为了解决在涉及多个数据记录时的隐私问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;原文-6&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;（奇怪的是）人们常常声称，由于概率的最大值为1，因此ε值大于1的情况几乎没有意义。具体选择这个值的来源并不清楚，因为exp(1) = e与概率的最大值并没有特别的关系，尽管大ε值在根本上是没有意义的这种观点是可以理解的。确实，较大的ε值会导致隐私保障的意义远不如较小的ε值。然而，它们仍然提供了数学上有意义的保证。考虑AOL发布的搜索日志事件，其中一名个人通过她的搜索记录被识别。假设在未包含她的数据的情况下，纽约时报记者准确描述西尔玛·阿诺德的私密担忧和关切的事件概率是十亿分之一。在这种情况下，形式为“允许您的数据被纳入将使您遭遇尴尬的风险增加e^12 &amp;lt; 164,000倍”的保证仍然使得披露的可能性非常小。&lt;/p&gt;
&lt;h4 id=&#34;解读-6&#34;&gt;解读：
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;对ε的理解&lt;/strong&gt;：作者指出，虽然有些人认为ε值大于1没有意义，但这并不准确。尽管较大的ε值确实会导致隐私保障的有效性降低，数学上它们仍然提供了某种保证。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;例子说明&lt;/strong&gt;：通过AOL搜索日志的例子，作者强调了大概率事件和隐私泄露之间的关系。即使在极小的概率下，允许数据被使用仍然可能导致一定程度的风险增加，然而这种增加并不一定会导致信息泄露。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;概率与风险&lt;/strong&gt;：作者指出，虽然较大的ε值可能在直观上让人觉得隐私保护效果不佳，但其数学意义仍然存在。在某些情况下，即使是大ε值的风险增加也可能非常小，意味着泄露的可能性仍然是微乎其微的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对风险的理解&lt;/strong&gt;：最后，作者提到即使风险增加的倍数看起来很大，但在具体情境中，泄露事件的概率可能依然很小，这意味着参与者仍然可以相对安全地共享数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;原文-7&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;错误：差分隐私不能工作，因为没有人能够解释如何设置ε的值。差分隐私方法的一大优点是能够保持个体在特定数据库中所遭受的累积隐私损失的量化衡量。这进一步强调了差分隐私研究的强大之处：我们很好地理解那些数据在多个独立操作数据库中的个体所遭受的累积隐私损失，即使对手可以访问所有这些数据库。此外，差分隐私研究发展出技术来显著减少在可以协调回答的情况下的隐私损失，这也是其强大之处。再者，差分隐私研究者已经确立这种协调对启用高复杂度查询至关重要，且无法替代。这种研究方向——在差分隐私中，而非其他私人数据分析方法——使我们能够通过ε来理解和减轻隐私损失。是的，这项研究尚不完整。是的，以下形式的定理似乎令人感到限制重重：&lt;/p&gt;
&lt;p&gt;如果一个个体参与了10,000个对抗性选择的数据库，并且我们希望确保她的累积隐私损失在概率至少为1 - e^(-32)的情况下被限制在e^1以内，那么每个数据库的ε值设定为1/801差分隐私是足够的。&lt;/p&gt;
&lt;p&gt;但我们还有什么其他方法可以找到理解如何放松对最坏情况对手保护的起点？我们还能如何测量这样做的影响？还有什么其他技术可以证明这样的主张？从更哲学的层面考虑，可以将其比作时间：一个人的一生中只有那么多小时，一旦这些时间耗尽，你就会死亡。（这有时比某人得知关于你私密信息的情况更糟。）然而，我们作为一个社会找到了一些方法来为个体的时间确定价值，而这一过程的基本部分就是能够量化地衡量时间。对于隐私的价值，确实需要新的思考，但我们相信，现在能够做到这一点是一件非常好的事情；每一个有意义的隐私保障都应该是量化的。&lt;/p&gt;
&lt;h4 id=&#34;解读-7&#34;&gt;解读
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;放松隐私保护的思考&lt;/strong&gt;：作者提到在理解如何放宽对最坏情况对手的保护时，探讨不同的方法和工具是必要的。这表明在设计隐私保护机制时，需要权衡保护程度和实际应用的灵活性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;量化的必要性&lt;/strong&gt;：通过量化隐私保护的效果，作者认为可以更好地理解和测量隐私保护的影响。这种量化方法有助于设计出既能有效保护隐私又能满足实际需求的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间的比喻&lt;/strong&gt;：作者通过将隐私的价值与时间的价值进行比较，强调了对隐私进行量化的复杂性。就像时间的价值在社会中得到认可并进行量化一样，隐私的价值也应该被理解和衡量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;隐私价值的新思考&lt;/strong&gt;：最后，作者提到在隐私价值方面需要新的思考方式，指出这种新思考不仅是必要的，而且是积极的。这种思考方式可以推动隐私保护的理论和实践发展，使其更加科学和有效。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>解码与部署</title>
        <link>https://JiangZhiyu-1024.github.io/p/decoderbs/</link>
        <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/decoderbs/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/decoderbs/roses-7314623_1280.jpg" alt="Featured image of post 解码与部署" /&gt;&lt;h1 id=&#34;解码与部署&#34;&gt;💐解码与部署
&lt;/h1&gt;&lt;p&gt;大语言模型是通过文本生成的方式进行工作的。在自回归架构中，模型针对输入内容逐个单词生成输出内容的文本。这个过程一般被称为 解码。&lt;/p&gt;
&lt;h2 id=&#34;解码策略&#34;&gt;🌸解码策略
&lt;/h2&gt;&lt;p&gt;大语言模型的生成方式本质上是一个概率采样过程，需要合适的解码策略来生成合适的输出内容。&lt;/p&gt;
&lt;h3 id=&#34;语言模型解码的背景知识&#34;&gt;🌹语言模型解码的背景知识
&lt;/h3&gt;&lt;p&gt;模型M每次根据当前上下文词元序列𝒖=[𝑢1,𝑢2,··· ,𝑢𝑡]建模下一个词的概率分布𝑃（即公式$O = softmax(W^LY_L)$的输出），然后根据一定的解码策略选择下一个词𝑢′，之后再将𝒖和𝑢′作为新的上下文重复上述步骤，直到生成结束词元或者达到长度上限为止。在这个流程中，解码策略将主要关注如何基于概率分布𝑃选择合适的下一个词𝑢′。自回归的解码策略并不局限于特定架构，常见的编码器-解码器、因果解码器和前缀解码器均可适用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101095949003.png&#34;
	width=&#34;825&#34;
	height=&#34;261&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101095949003_hu12975133203137575490.png 480w, https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101095949003_hu12752602229915317363.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241101095949003&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;316&#34;
		data-flex-basis=&#34;758px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;贪心搜索&#34;&gt;🌺贪心搜索
&lt;/h4&gt;&lt;p&gt;目前常见的大语言模型主要是通过语言建模任务进行预训练的。基于这种训练方式，一种直观的解码策略是贪心搜索（Greedy Search）。具体来说，贪心搜索在每个生成步骤中都选择概率最高的词元， 其可以描述为以下形式:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101171614302.png&#34;
	width=&#34;267&#34;
	height=&#34;42&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101171614302_hu7922445418515866011.png 480w, https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101171614302_hu2751309916288702486.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241101171614302&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;635&#34;
		data-flex-basis=&#34;1525px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101100407951.png&#34;
	width=&#34;782&#34;
	height=&#34;310&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101100407951_hu18145890914685294099.png 480w, https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101100407951_hu13445056324940742564.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;252&#34;
		data-flex-basis=&#34;605px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;贪心搜索示意图&lt;/center&gt;
&lt;h4 id=&#34;概率采样&#34;&gt;🌻概率采样
&lt;/h4&gt;&lt;p&gt;该方法根据模型建模的概率分布采样得到下一个词元，旨在增强生成过程中的随机性和结果的多样性：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101101405007.png&#34;
	width=&#34;510&#34;
	height=&#34;201&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101101405007_hu15109640633028076707.png 480w, https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101101405007_hu5138290837623247879.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;“Iamsleepy. I start a pot of” 语境中下一个词元的降序排列概率分布&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;253&#34;
		data-flex-basis=&#34;608px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在图中展示了下一个词元的概率分布，虽然单词“coffee”被选中的概率较高，但基于采样的策略同时也为选择其他单词（如“water”、“tea”等）留有一定的可能性，从而增加了生成文本的多样性和随机性。&lt;/p&gt;
&lt;h3 id=&#34;贪心搜索的改进&#34;&gt;🌼贪心搜索的改进
&lt;/h3&gt;&lt;p&gt;贪心搜索在每个生成步骤中均选择最高概率的词元，这可能会由于忽略在某些步骤中概率不是最高、但是整体生成概率更高的句子而造成局部最优。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;束搜索. 在解码过程中，束搜索（BeamSearch）[1]会保留前𝑛个具有最高概率的句子，并最终选取整体概率最高的生成回复。这里的𝑛被称为束大小（Beam Size）。当 𝑛 = 1，束搜索就退化为贪心搜索。如图9.3所示（𝑛=2），第一步保留了概率最高的两个词“coffee” 和 “water” 作为候选；第二步基于 “coffee” 和 “water” 均进行扩展，得到模型在两个上下文内容下的概率分布，最后选择联合概率最高的两个句子 “coffee then” 和 “coffee and” 作为候选。在下面的生成步骤中，将会继续基于这两个候选去进行扩展，每次都选择联合概率最高的两个句子。最后，当两个束的句子均生成结束后，选择整体生成概率最高的候选句子作为最终的输出。在实践中，束的数量通常设定在3到6的范围内，设置过大的束会显著增加运算开销，并可能会导致性能下降。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101101436913.png&#34;
	width=&#34;811&#34;
	height=&#34;310&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101101436913_hu7206092375177728480.png 480w, https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101101436913_hu8943785241907144639.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;束搜索示意图（𝑛=2）&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;261&#34;
		data-flex-basis=&#34;627px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;长度惩罚. 由于束搜索中需要比较不同长度候选句子的概率，往往需要引入长度惩罚（LengthPenalty）（亦称为长度归一化）技术。如果没有长度惩罚，传统的束搜索会倾向于生成较短的句子，因为每生成一个单词，都会乘以一个小于1的概率，使得句子的生成概率逐渐变小。因此，可以在生成概率的计算中引入长度惩罚，通过将句子概率除以其长度的指数幂𝛼，对于句子概率进行归一化处理，从而鼓励模型生成更长的句子。在实践中，𝛼通常设置为0.6到0.7之间的数值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重复惩罚.为了缓解贪心搜索重复生成的问题，可以使用𝑛-元惩罚（𝑛-gram Penalty）来强制避免生成重复的连续𝑛个词元，实践中𝑛通常设置为3到5之间的整数。进一步地，研究人员还提出了相对“温和”的惩罚机制来降低生成重复词元的概率，而不是“一刀切”地完全避免某些短语的生成，如出现惩罚（Presence Penalty）和频率惩罚（FrequencyPenalty）。具体地，出现惩罚在生成过程中会将已经生成词元的logits减去惩罚项𝛼来降低该词元之后生成的概率。频率惩罚相较于出现惩罚，会记录每个词元生成的数目，然后减去出现次数乘以惩罚项𝛼，因此如果一个词元生成得越多，惩罚也就越大。在实践中，𝛼的取值范围通常在0.1到1之间。这些重复惩罚方法不止适用于贪心搜索，对于随机采样也均适用。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;随机采样的改进策略&#34;&gt;🌷随机采样的改进策略
&lt;/h3&gt;&lt;p&gt;基于概率采样的方法会在整个词表中选择词元，这可能会导致生成不相干的词元。为了进一步提高生成质量，可以进一步使用一些改进的采样策略，减少具有极低概率词汇对于生成结果的影响。&lt;/p&gt;
&lt;p&gt;温度采样（TemperatureSampling）. 为了调节采样过程中的随机性，一种有效的方法是调整softmax函数中的温度系数。具体来说，公式5.11中的$l = W^LyL$ 称为logits，调整温度系数后的softmax计算式如下：
&lt;/p&gt;
$$
P(u_j | \mathbf{u}_i) = \frac{\exp(l_j / t)}{\sum_{j&#39;} \exp(l_{j&#39;} / t)}
$$&lt;p&gt;
其中，$l_{j^′ }$表示每个候选词元的logit，𝑡是温度系数。具体来说，降低温度系数𝑡会使得概率分布更加集中，从而增加了高概率词元的采样可能性，同时降低了低概率词元的采样可能；当温度系数𝑡设置为1时，该公式退化为标准的随机采样方法；而当𝑡趋近于0时，实际上等同于贪心搜索，即总是选择概率最高的词。此外，当𝑡趋近于无穷大时，温度采样会退化为均匀采样。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101102926570.png&#34;
	width=&#34;779&#34;
	height=&#34;300&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101102926570_hu4149375308625516168.png 480w, https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101102926570_hu16440094783273864492.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;温度设置为1.3、1.0和0.7时的下一个词的概率分布变化&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;259&#34;
		data-flex-basis=&#34;623px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;低资源部署策略&#34;&gt;🌱低资源部署策略
&lt;/h2&gt;&lt;p&gt;由于大模型的参数量巨大，在解码阶段需要占用大量的显存资源，因而在实际应用中的部署代价非常高。这里介绍一种常用的模型压缩方法，即模型量化（Model Quantization），来减少大模型的显存占用，从而使得能够在资源有限的环境下使用大模型.&lt;/p&gt;
&lt;h3 id=&#34;量化基础知识&#34;&gt;🌲量化基础知识
&lt;/h3&gt;&lt;p&gt;在神经网络压缩中，量化通常是指从浮点数到整数的映射过程[2]，目前比较常用的是8比特整数量化，即INT8量化。针对神经网络模型，通常有两种类型的数据需要进行量化，分别为权重量化（也称为模型参数量化）和激活（值）量化，它们都以浮点数形式进行表示与存储。&lt;/p&gt;
&lt;h4 id=&#34;量化的数学表述&#34;&gt;🌳量化的数学表述
&lt;/h4&gt;&lt;p&gt;量化的过程可以表示为一个函数，该函数将连续的输入映射到离散的输出集合。一般来说，这个过程涉及到四舍五入或截断等近似操作。下面介绍一个一般形式的量化函数：
&lt;/p&gt;
$$
x_q = R(x/S)-Z.
$$&lt;p&gt;
通过上述数学变换，量化算法将浮点数向量𝒙转化为量化值𝒙𝒒。其中，𝑆表示缩放因子，用于确定裁剪范围，𝑍表示零点因子，用于确定对称或非对称量化，𝑅(·) 表示将缩放后的浮点值映射为近似整数的取整操作。一般来说，裁剪范围对于量化性能有很大影响，通常需要根据实际数据分布进行校准，可以通过静态（离线）或动态（运行时）方式。&lt;/p&gt;
&lt;p&gt;作为上述变换的逆过程，反量化（Dequantization）对应地从量化值中恢复原始值，该过程首先加上零点因子，然后乘以缩放因子：
&lt;/p&gt;
$$
\tilde{\mathbf{x}} = \mathbf{S} \cdot (\mathbf{x}_q + \mathbf{Z})
$$&lt;p&gt;
进一步，可以定义量化误差是原始值𝒙和恢复值$\tilde{\mathbf{x}}$之间的数值差异：$Δ=∥𝒙−\tilde{\mathbf{x}}∥^2_2$。&lt;/p&gt;
&lt;h4 id=&#34;量化的策略&#34;&gt;🌴量化的策略
&lt;/h4&gt;&lt;p&gt;基于上述量化函数的定义形式，下面介绍一些对于量化函数常见的分类与实现策略。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;均匀量化和非均匀量化.根据映射函数的数值范围是否均匀，可以将量化分为两类：均匀量化和非均匀量化。均匀量化是指在量化过程中，量化函数产生的量化值之间的间距是均匀分布的。这种方法通常用于将连续的数据转换为离散的表示，以便在计算机中进行高效处理。与此不同，在非均匀量化方法中，它的量化值不一定均匀分布，可以根据输入数据的分布范围而进行调整。其中，均匀量化方法因其简单和高效的特点而在实际中被广泛使用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101110603458.png&#34;
	width=&#34;699&#34;
	height=&#34;232&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101110603458_hu12814023126450044580.png 480w, https://JiangZhiyu-1024.github.io/p/decoderbs/image-20241101110603458_hu6369628559836161934.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;对称量化和非对称量化对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;301&#34;
		data-flex-basis=&#34;723px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对称量化和非对称量化. 根据零点因子𝑍是否为零，均匀量化可以进一步分为两类：对称量化（𝑍=0）和非对称量化（𝑍≠0）。对称量化与非对称量化的一个关键区别在于整数区间零点的映射，对称量化需要确保原始输入数据中的零点（𝑥=0）在量化后仍然对应到整数区间的零点。而非对称量化则不同，根据前面的公式可以看出此时整数区间的零点对应到输入数值的𝑆·𝑍。为了方便讨论，这里以一个常见的8比特量化为例进行介绍。如图9.6(a)和图9.6(b)所示，对称量化将输入数值𝒙通过一个映射公式转换为八比特整数的表示范围内，如果是有符号整数，则该范围可以设置为[−128,127]，适用于𝒙的数值大致分布在零点两侧的情况，如果是无符号整数，则设置为[0,255]，适用于输入数值基本都分布在零点一侧的情况。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;量化粒度的选择. 量化算法通常针对一个批次的数据进行处理，其中批次的规模大小就反应了量化粒度，可以由算法设计人员进行选择。在神经网络模型中，输入数据和模型权重通常是以张量的形式进行表示与组织的。首先，如果每个张量只定义一组量化参数（即𝑆和𝑍），这称为按张量量化。为了进一步提高量化的精度，可以针对每个张量定义多组量化参数，例如可以为权重矩阵的列维度（也称为“通道”）设置特定的量化参数，称为按通道量化。还有一些研究工作采用了更细粒度的量化方案，对一个通道的数值细分为多个组，即按组的方式进行量化。在神经网络量化中，从按张量到按组，量化粒度越来越小，且使用较小的粒度通常可以提高量化的准确性，有效保持原始模型的性能。但是由于引入了更多的量化参数，在使用时会带来额外的计算开销。相反，按张量量化的粒度较粗，可能会引入较大的误差，但由于在硬件实现上更加简单，也是一种常见的量化粒度选择策略。在实践中，量化粒度需要根据具体任务和模型进行选择，应该采用可以平衡量化准确性以及额外计算开销的合适粒度。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;大模型训练后量化方法&#34;&gt;🌵大模型训练后量化方法
&lt;/h3&gt;&lt;p&gt;基于上述的量化基础知识，本部分将主要介绍大语言模型相关的量化方法。通常来说，模型量化方法可以分为两大类，即量化感知训练（Quantization-Aware Training,QAT）和训练后量化（Post-Training Quantization,PTQ）。从方法的名字可以看出，量化感知训练方法需要更新权重进而完成模型量化，而训练后量化方法则无需更新模型权重。与小型语言模型相比，在为大语言模型设计或选择量化方法时需要侧重关注两个主要因素。首先，大语言模型由大规模神经网络参数组成，在设计量化算法时需要考虑到所需要花费的计算开销。一般来说，训练后量化方法需要更少的算力开销，实践中应用更为广泛。其次，大语言模型中具有不同的数值分布特征（如激活值中存在较大的数值），这使得对于大语言模型进行低比特量化变得非常困难，特别是对于激活值。&lt;/p&gt;
&lt;h4 id=&#34;权重量化&#34;&gt;🌿权重量化
&lt;/h4&gt;&lt;p&gt;首先介绍主要面向模型权重的量化方法。其中，主流的权重量化方法通常是基于逐层量化的方法进行设计的，旨在通过最小化逐层的重构损失来优化模型的量化权重，可以刻画为：$\text{argmin}_{\mathbf{W}} | \mathbf{X} \mathbf{W} - \mathbf{X} \mathbf{W} |_2^2$，其中𝑾，𝑾 分别表示原始权重和量化后的权重，𝑿为输入。&lt;/p&gt;
&lt;p&gt;为了有效地优化该目标函数，GPTQ[3]的基本想法是在逐层量化的基础上，进一步将权重矩阵按照列维度分组（例如128个列为一组），对一个组内逐列进行量化，每列参数量化后，需要适当调整组内其他未量化的参数，以弥补当前量化造成的精度损失。在具体的实现中，GPTQ还进一步采用了特殊设计的优化方法来加速整个过程，如延迟批次更新、Cholesky重构等。GPTQ可以在3比特或4比特精度下实现对于大语言模型的有效权重量化。&lt;/p&gt;
&lt;p&gt;进一步，AWQ[4]发现在逐层和逐组权重量化过程中，对于模型性能重要的权重只占全部参数的一小部分（0.1%∼1%），并且应该更加关注那些与较大激活值维度相对应的关键权重，因为它们对应着更为重要的特征。为了加强对于这部分关键权重的关注，AWQ方法提出引入针对权重的激活感知缩放策略。具体来说，AWQ的优化目标将逐层的重构损失$| \mathbf{X} \mathbf{W} - \mathbf{X} \mathbf{W} |_2^2$修改为：$\left| \left( \text{diag}(\mathbf{s})^{-1} \cdot \mathbf{X} \right) \cdot \mathbf{Q}(\mathbf{W} \cdot \text{diag}(\mathbf{s})) - \mathbf{X} \mathbf{W} \right|_2^2$，其中𝑄 为量化函数。通过引入缩放因子𝒔，AWQ算法可以使得量化方法更为针对性地处理关键权重所对应的权重维度。&lt;/p&gt;
&lt;h2 id=&#34;模型蒸馏&#34;&gt;☘️模型蒸馏
&lt;/h2&gt;&lt;p&gt;模型蒸馏（ModelDistillation）的目标是将复杂模型（称为教师模型）包含的知识迁移到简单模型（称为学生模型）中，从而实现复杂模型的压缩。一般来说，通常会使用教师模型的输出来训练学生模型，以此来传递模型知识。以分类问题为例，教师模型和学生模型在中间每一层会输出特征表示（特指神经网络模型），在最后一层会输出针对标签集合的概率分布。模型蒸馏的核心思想是，引入额外的损失函数（称为蒸馏损失函数），训练学生模型的输出尽可能接近教师模型的输出。在实际应用中，蒸馏损失函数通常与分类损失函数（交叉熵损失函数）联合用于训练学生模型。下面首先介绍传统的知识蒸馏方法，再介绍其在大语言模型中的应用。&lt;/p&gt;
&lt;h2 id=&#34;参考文献&#34;&gt;🍀参考文献
&lt;/h2&gt;&lt;p&gt;[1]. CARNEGIE-MELLON UNIV PITTSBURGH PA DEPT OF COMPUTER SCIENCE. Speech Understanding Systems. Summary of Results of the Five-Year Research Effort at Carnegie-Mellon University. 1977.&lt;/p&gt;
&lt;p&gt;[2]. Amir Gholami et al. “A Survey of Quantization Methods for Efficient Neural Network Inference”. In: arXiv preprint arXiv:2103.13630 (2021).&lt;/p&gt;
&lt;p&gt;[3]. EliasFrantaretal.“GPTQ:AccuratePost-TrainingQuantizationforGenerativePre-trained Transformers”. In: arXiv preprint arXiv:2210.17323 (2022).&lt;/p&gt;
&lt;p&gt;[4]. Ji Lin et al. “AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration”. In: arXiv preprint arXiv:2306.00978 (2023).&lt;/p&gt;
</description>
        </item>
        <item>
        <title>提示学习</title>
        <link>https://JiangZhiyu-1024.github.io/p/prompt_learning/</link>
        <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/prompt_learning/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/prompt_learning/birch-trees-8345812_1280.jpg" alt="Featured image of post 提示学习" /&gt;&lt;h1 id=&#34;提示学习&#34;&gt;🎄提示学习
&lt;/h1&gt;&lt;p&gt;经过预训练、指令微调和人类对齐后，我们接下来讨论如何通过提示学习方法来有效地使用大语言模型解决实际任务。目前常用的方法是设计合适的提示（Prompting），通过自然语言接口与大模型进行交互。在现有研究中，任务提示的设计主要依靠人工设计和自动优化两种策略来实现。为了更好地 解决未见过的任务，一种典型的提示方法是上下文学习（In-contextLearning,ICL），它将任务描述与示例以自然语言文本形式加入到提示中。此外，思维链提示（Chain-of-Thought, CoT）作为一种增强技术，将一系列中间推理步骤加入到提示中，以增强复杂推理任务的解决效果。&lt;/p&gt;
&lt;h2 id=&#34;基础提示&#34;&gt;🎆基础提示
&lt;/h2&gt;&lt;p&gt;因为大语言模型的微调代价较高，基于自然语言的提示方法已经成为了使用大语言模型解决下游任务的主要途径。由于提示的质量在很大程度上会影响大语言模型在特定任务中的表现，因此一系列工作深入研究了通过人工设计或自动优化的方法来生成合适的任务提示。&lt;/p&gt;
&lt;h3 id=&#34;人工提示设计&#34;&gt;🎇人工提示设计
&lt;/h3&gt;&lt;p&gt;针对特定任务设计合适的任务提示，这一过程被称为“提示工程”（Prompt Engineering）。&lt;/p&gt;
&lt;h4 id=&#34;关键要素&#34;&gt;🧨关键要素
&lt;/h4&gt;&lt;p&gt;一般而言，针对大语言模型的提示设计需要考虑四个关键要素，即任务描述、输入数据、上下文信息和提示策略。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;任务描述.任务描述部分展示了大语言模型应当遵循的具体指令。一般来说，用户应该使用清晰的、具体的表达来描述任务目标。进一步，某些特定任务还需要对于输入或输出的格式进行更详细的说明，可以使用关键词或者特殊符号来强 调特殊设置以指导大语言模型更好地完成任务。&lt;/li&gt;
&lt;li&gt;输入数据. 通常情况下，用户可以直接使用自然语言描述输入数据的内容。对于特殊形式的输入数据，则需要采用合适的方法使其能够被大语言模型读取与理解。例如，对于结构化数据（如知识图谱、表格等），通常使用线性化方法将其转换为易于处理的文本序列[264]。此外，由于结构化数据具有较好的组织形式，可以使用编程代码中的数据结构进行存储表示，将结构化数据中的属性表示为数据结构中的变量。基于代码表示的结构化数据可以使用外部工具（如程序执行器）进行准确地读取。&lt;/li&gt;
&lt;li&gt;上下文信息. 除了任务描述和输入数据外，上下文信息对某些特定任务也非常重要。例如，搜索引擎可以为开放问答任务提供参考文档，可以通过将检索到的参考文档以上下文信息的形式引入提示作为大语言模型的输入。在引入外部信息时，需要对于这些信息进行合适的格式化，以加强大语言模型对它们的利用。此外，上下文学习中的任务示例数据也有助于提升大语言模型处理复杂任务的能力，大模型可以通过这些示例数据学习任务目标、输出格式以及输入和输出之间的映射关系。&lt;/li&gt;
&lt;li&gt;提示策略. 针对不同的大语言模型设计合适的提示策略对于激发模型解决特定任务的能力非常重要。在某些情况下，添加特定的前缀或后缀有助于引导大语言模型解决复杂任务。例如，使用前缀“让我们一步一步地思考”可以激发大语言模型的逐步推理能力，而使用前缀“你是这项任务（或这个领域）的专家”可以提高大语言模型在某些特定任务（或领域）中的表现。此外，对于对话式的大语言模型（例如ChatGPT），由于其使用了大量对话数据进行训练，因此更合适的做法是将提示拆分为多个子任务提示，以多轮对话的方法逐步输入给大语言模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;自动提示优化&#34;&gt;✨自动提示优化
&lt;/h3&gt;&lt;p&gt;人工设计提示虽然比较直接，但是需要耗费较多的人工成本，同时要求设计人员具有丰富的提示工程经验。此外，大语言模型对于提示设计比较敏感，人工设计的提示有时很难获得最优的效果，还可能导致任务性能的下降。需要注意的是，由于大语言模型参数量巨大，并且很多工作机制已经与传统预训练模型有着较大的差异，许多提示优化方法已经不再适用于大语言模型。&lt;/p&gt;
&lt;h4 id=&#34;离散提示优化&#34;&gt;🎉离散提示优化
&lt;/h4&gt;&lt;p&gt;离散提示通常是由一系列自然语言词元组成的完整句子表达（如“请根据提供的检索信息回答下列问题”）。然而，在离散的词元空间中进行组合搜索，不仅时间复杂度高，而且可能导致非最优的搜索结果。下面将介绍四种常见的离散提示优化方法，能够提升离散任务提示的有效性与搜索效率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于梯度的方法. 这类方法通过梯度更新技术以最大化模型的似然分数来优化离散提示的搜索过程。一种早期的代表性方法[1]使用梯度引导技术，首先将提示初始化为一系列“[MASK]”标记，然后迭代地将提示中的词元替换为词典中的其他词元，通过词元替换产生的对数似然变化来近似估计梯度，进而为提示的每个位置贪心搜索出最佳的词元。由于该方法对提示的每个位置都进行所有候选词元的替换和梯度评估，因此需要模型进行多次前向和后向计算，导致搜索过程的效率较低。为了改进搜索效率，可以将离散词元转化为连续嵌入表示（又称为“软词元”），使用梯度直接对连续嵌入参数进行优化，最后将每个连续嵌入映射为词典中最邻近的离散词元。&lt;/li&gt;
&lt;li&gt;基于强化学习的方法. 为了实现更有效的离散词元选择，另一种解决方法是将离散提示优化问题转换为强化学习问题，并使用强化学习算法进行求解。具体来说，可以将预训练语言模型作为强化学习中的策略网络并依次生成提示中的词元。在提示生成结束之后，策略网络可以获得任务特定的奖励信号，该奖励信号可通过强化学习算法用于策略网络参数的训练。在实践中，可以设计不同类型的奖励信号，比如真实标签与基于提示的预测标签是否一致、生成文本与给定条件的匹配程度。在最后的测试阶段，基于训练好的策略网络，可以采用贪心搜索策略来生成任务提示中的每个词元。&lt;/li&gt;
&lt;li&gt;基于编辑的方法. 这类方法主要关注如何通过编辑现有的提示来改进模型的性能，通常是基于模型在目标任务上的表现来判断提示的好坏。它特别适用于无法直接访问模型内部状态（如梯度）的情况，例如只能通过API调用的模型。在这类方法中，通常需要事先定义好编辑操作，然后迭代地对提示进行修改，直至达到最大迭代轮次或者模型最佳性能。提示的关键要素包括任务描述、输入数据、上下文信息和提示策略。因此，常用的提示编辑操作有修改任务描述、添加或删除上下文任务示例、调整输入到输出的标签映射器（例如可以使用“positive/negative”或者“正/负”表示二分类）等。此外，提示编辑操作也可以根据不同的场景或者需求进行设计，以适配下游具体任务。整体流程可以概述如下：基于预定义的编辑操作，在现有提示的基础上修改得到新提示，并输入至模型得到目标任务上的表现，根据表现筛选出合适的提示。由于上述过程可能需要迭代进行，可以只选择少量测试样例来评估模型表现，以减少计算开销。&lt;/li&gt;
&lt;li&gt;基于大语言模型的方法. 由于大语言模型具有通用的任务能力，因此可以将提示优化看作一个待求解的任务，进而直接使用大语言模型作为提示生成器来生成或改进提示[2,3]。基于大语言模型的自动提示生成框架将提示优化过程看作是一个由大语言模型指导的黑盒优化问题。该框架首先利用提示生成模型（用于生成提示指令的大语言模型）基于少量上下文示例生成一批候选的任务指令。随后，使用“目标模型”（用于下游测试的大语言模型）对这些候选指令在目标任务上的表现进行逐一评估。在评估过程中，可以采用模型困惑度或任务准确率作为衡量指令质量的指标。上述过程可以通过基于蒙特卡洛搜索的多轮优化策略进行扩展。在每一轮迭代中，根据模型表现对候选指令进行筛选得到高评分指令，并利用大语言模型生成与高评分指令相似的新指令，从而扩展候选指令集。迭代完成后，选择模型表现最佳的候选指令作为最终使用的提示。然而，上述方法没有充分考虑提示的整个历史改进轨迹，因此可能在提示搜索过程中陷入局部最优或者产生效果震荡，无法生成更好的提示。为了解决这一问题，可以将所有改进的历史提示及其分数纳入提示优化过程，以指导大语言模型逐步生成更好的新提示。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;连续提示优化&#34;&gt;🎊连续提示优化
&lt;/h4&gt;&lt;p&gt;与离散提示不同，连续提示由一组连续空间中的嵌入向量组成，可以根据下游任务的损失直接通过梯度更新进行优化。值得注意的是，已有连续提示优化的工作主要是基于预训练语言模型开展的，由于大语言模型参数量巨大，连续提示受到的关注较为有限。已有的连续提示优化研究通常依赖于有监督学习方法。当数据稀缺的情况下，还可以采用迁移学习方法来缓解目标任务标注数据不足的问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;监督学习方法. 这类方法将连续提示向量视为可训练的模型参数，基于下游任务数据，通过最小化交叉熵损失来优化连续提示。Prefix-tuning [4] 会在语言模型的每个Transformer 层预置一串前缀（即一组可训练的连续向量），而Prompt-tuning[5]只会在输入层加入可训练的提示向量。 通过固定语言模型的大规模参数而只微调这些连续的提示向量，可以有效节省训练所需要的参数量。然而，这些提示优化方法通常与输入无关，缺乏对于输入语义的充分考虑。&lt;/li&gt;
&lt;li&gt;迁移学习方法. 有监督学习方法通常需要充足的训练数据来学习最优的任务提示，很难在数据稀缺场景下获得较好的模型性能。为了解决这个问题，基于提示的迁移学习方法首先为若干个具有代表性的源任务学习一个所有任务共享的连续提示，然后使用该提示初始化目标任务的提示，这可以为下游任务的提示优化提供良好的初始点。然而，这种方法存在一定的局限性，它在解决目标任务的所有实例时都使用了相同提示，而即使是一个精心优化过的提示也未必适合所有的任务实例。为了解决这一问题，可以为每个源任务独自学习任务特定的连续提示（而不是所有源任务共享），在解决目标任务的实例时，可以采用注意力机制等方式学习目标实例与每个源任务提示的相关性权重系数，对若干个源任务的提示向量进行加权组合，将组合后的新提示（为连续向量形式）用于帮助模型解决当前任务实例。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;上下文学习&#34;&gt;🎋上下文学习
&lt;/h2&gt;&lt;p&gt;在GPT-3的论文[6]中，OpenAI研究团队首次提出上下文学习（In-context learning,ICL）这种特殊的提示形式。目前，上下文学习已经成为使用大语言模型解决下游任务的一种主流途径。下面将详细介绍这一提示方法。&lt;/p&gt;
&lt;h3 id=&#34;上下文学习的形式化定义&#34;&gt;🎍上下文学习的形式化定义
&lt;/h3&gt;&lt;p&gt;根据GPT-3论文中所给出的描述[6]，上下文学习使用由任务描述和（或）示例所组成的自然语言文本作为提示。图10.1展示了上下文学习的提示构建过程。首先，通过自然语言描述任务，并从任务数据集中选择一些样本作为示例。其次，根据特定的模板，将这些示例按照特定顺序组合成提示内容。最后，将测试样本添加到提示后面，整体输入到大语言模型以生成输出。基于任务描述以及示例信息，大语言模型无需显式的梯度更新即可识别和执行新的任务。&lt;/p&gt;
&lt;p&gt;形式上，我们使用𝐷𝑘={𝑓(𝑥1,𝑦1),&amp;hellip;, 𝑓(𝑥𝑘,𝑦𝑘)}来表示由𝑘个样本构成的一组示例数据，其中𝑓(𝑥𝑘,𝑦𝑘)是一个函数，负责将第𝑘个任务样本转换为自然语言提示。给定任务描述𝐼、示例𝐷𝑘以及新的输入𝑥𝑘+1，大语言模型生成答案ˆ 𝑦𝑘+1的过程可以通过下面的公式来表述：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/prompt_learning/image-20241101153946290.png&#34;
	width=&#34;652&#34;
	height=&#34;82&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/prompt_learning/image-20241101153946290_hu5461085767367129954.png 480w, https://JiangZhiyu-1024.github.io/p/prompt_learning/image-20241101153946290_hu14656965869284567427.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241101153946290&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;795&#34;
		data-flex-basis=&#34;1908px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;值得一提的是，上下文学习与指令微调之间存在着紧密的联系，因为它们都涉及将任务或样本转化为自然语言形式供大语言模型进行处理。在原始的GPT-3论文中，作者将上下文学习的提示定义为任务描述和示例的组合，这两部分均为可选。按照这个定义，如果大语言模型仅通过任务描述（即任务指令）来解决未见过的任务，也可以被看作是上下文学习的一种特例。两者的主要区别是，指令微调需要对大语言模型进行微调，而上下文学习仅通过提示的方式来调用大语言模型解决任务。此外，指令微调还可以有效提升大语言模型在执行目标任务时的上下文学习能力，尤其是在零样本场景下（即仅依赖任务描述而无需额外的示例）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/prompt_learning/image-20241101154120822.png&#34;
	width=&#34;845&#34;
	height=&#34;388&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/prompt_learning/image-20241101154120822_hu720514284260820817.png 480w, https://JiangZhiyu-1024.github.io/p/prompt_learning/image-20241101154120822_hu16401459081467296356.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;一个关于上下文学习和思维链提示的比较说明&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;217&#34;
		data-flex-basis=&#34;522px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;思维链提示&#34;&gt;🎁思维链提示
&lt;/h2&gt;&lt;p&gt;思维链提示[7,8]是一种高级提示策略，旨在增强大语言模型在各类复杂推理任务上的表现。常见的推理任务包括算术推理[9]、常识推理[9]以及符号推理[7]等多种任务。与上下文学习方法仅使用⟨输入，输出⟩二元组来构造提示不同，思维链提示进一步融合了中间的推理步骤来指导从输入到输出的推理过程。下图展示了一个思维链提示的具体例子。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/prompt_learning/image-20241101154746320.png&#34;
	width=&#34;826&#34;
	height=&#34;354&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/prompt_learning/image-20241101154746320_hu9530795340784985899.png 480w, https://JiangZhiyu-1024.github.io/p/prompt_learning/image-20241101154746320_hu6992914138589783299.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241101154746320&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;233&#34;
		data-flex-basis=&#34;560px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;思维链提示技术的演化过程&lt;/center&gt;
&lt;h2 id=&#34;参考文献&#34;&gt;🎑参考文献
&lt;/h2&gt;&lt;p&gt;[1]. Taylor Shin et al. “AutoPrompt: Eliciting Knowledge from Language Models with Auto matically Generated Prompts”. In: EMNLP. 2020.&lt;/p&gt;
&lt;p&gt;[2]. Yongchao Zhou et al. “Large Language Models are Human-Level Prompt Engineers”. In: ICLR. 2023.&lt;/p&gt;
&lt;p&gt;[3]. Chengrun Yang et al. “Large Language Models as Optimizers”. In: arXiv preprint arXiv: 2309.03409 (2023).&lt;/p&gt;
&lt;p&gt;[4]. XiangLisa Li and Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Gen eration”. In: ACL. 2021.&lt;/p&gt;
&lt;p&gt;[5]. Brian Lester, Rami Al-Rfou, and Noah Constant. “The Power of Scale for Parameter Efficient Prompt Tuning”. In: EMNLP. 2021.&lt;/p&gt;
&lt;p&gt;[6]. TomB.Brown et al. “Language Models are Few-Shot Learners”. In: NeurIPS. 2020.&lt;/p&gt;
&lt;p&gt;[7]. JasonWeietal. “Chain of Thought Prompting Elicits Reasoning in Large Language Mod els”. In: arXiv preprint arXiv:2201.11903 (2022).&lt;/p&gt;
&lt;p&gt;[8]. Zheng Chu et al. “A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future”. In: arXiv preprint arXiv:2309.15402 (2023).&lt;/p&gt;
&lt;p&gt;[9]. Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. “A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers”. In: ACL. 2020.&lt;/p&gt;
&lt;p&gt;[10]. AlonTalmoret al. “CommonsenseQA: A Question Answering Challenge Targeting Com monsense Knowledge”. In: NAACL-HLT. 2019.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>HumanAlignment</title>
        <link>https://JiangZhiyu-1024.github.io/p/humanalignment/</link>
        <pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/humanalignment/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/humanalignment/flowers-7382926_1920.jpg" alt="Featured image of post HumanAlignment" /&gt;&lt;h1 id=&#34;人类对齐&#34;&gt;🍁人类对齐
&lt;/h1&gt;&lt;p&gt;在大语言模型的学习过程中，如何确保大语言模型的行为与人类价值观、人类真实意图和社会伦理相一致成为了一个关键研究问题，通常称这一研究问题为人类对齐（HumanAlignment）。&lt;/p&gt;
&lt;h2 id=&#34;人类对齐的背景与标准&#34;&gt;🍂人类对齐的背景与标准
&lt;/h2&gt;&lt;h3 id=&#34;背景&#34;&gt;🌰背景
&lt;/h3&gt;&lt;p&gt;尽管大语言模型在下游任务中表现出优秀的性能，这些模型有时会出现错误或具有危害性的行为，例如无法正确遵循指令、生成虚假信息、以及产生有害、有误导性以及带有偏见的表达。在大语言模型的预训练和有监督微调的过程中，主要训练目标是根据上下文内容来预测下一个词元。但是，这一过程并未充分考虑人类的价值观或偏好，可能导致大语言模型从数据中学习到不符合人类期望的生成模式。为了规避这些潜在风险，研究人员提出了“人类对齐”这一关键概念，旨在保证大语言模型的行为与人类期望和价值观相一致[1,2]。&lt;/p&gt;
&lt;p&gt;与预训练和指令微调不同，人类对齐需引入全新的评估标准，如有用性、诚实性和无害性。 为了更直观地理解人类对齐对于大语言模型的重要性，例1对比了同一个语言模型在对齐前后对于相同输入的不同输出。在这个例子当中，输入的问题刻意包含了具有误导性的逻辑关系，即“土地价格”和“有污染的产业”是有直接关系的。因此，在经过人类价值观对齐之前的大语言模型会被输入中的错误逻辑所引导，产生了带有偏见的建议“农村地区更适合发展污染较严重的产业”。在经济生产中，发展有污染的产业需要综合考虑多方面的因素，不能仅仅因为土地价格更为便宜就认为适合发展相关产业。对齐前的大语言模型给出了一个错误的观点，不符合人类价值观，违背了无害性的原则。而经过与人类价值观对齐之后的大语言模型，先指出了输入问题中包含的错误逻辑（“我们不能简单地认为农村土地价格便宜就适合发展污染产业。”），并且给出了正确且合理的做法。对齐后的大语言模型的回复符合有用性和无害性，与人类价值观和偏好相符。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/humanalignment/image-20241031145634528.png&#34;
	width=&#34;770&#34;
	height=&#34;610&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/humanalignment/image-20241031145634528_hu8661545991163250632.png 480w, https://JiangZhiyu-1024.github.io/p/humanalignment/image-20241031145634528_hu13213345832109560578.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241031145634528&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;126&#34;
		data-flex-basis=&#34;302px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;例1  大语言模型（YuLan）对于相同输入在对齐前后的不同输出&lt;/center&gt;
&lt;p&gt;思考：&lt;/p&gt;
&lt;p&gt;之前我就遇到过类似的情况，在前面的文章中我也提到过，LLM似乎就是倾向于认同人类的提问（就算那是错误的），到这里明白了，原来是没有经过对齐。那么我就有新的问题了，对齐也是要训练的吧，而且得要人来准备数据，这就又是一个耗费人力的工程了。&lt;/p&gt;
&lt;h3 id=&#34;对齐标准&#34;&gt;🍠对齐标准
&lt;/h3&gt;&lt;p&gt;人类对齐是一个较为抽象的概念，难以直接进行形式化建模，关于对齐的定义和标准也存在不同的观点。这里主要围绕三个具有代表性的对齐标准展开讨论，分别是有用性（Helpfulness）、诚实性（Honesty）和无害性（Harmlessness），这三种对齐标准已被现有的大语言模型对齐研究广泛使用[1,3]。下面具体介绍这三个代表性的对齐标准。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有用性. 在实际应用中，大语言模型需要提供有用的信息，能够准确完成任务，正确理解上下文，并展现出一定的创造性与多样性。模型应尽量以简洁、高效的方式协助用户完成任务。当任务描述存在歧义或涉及背景信息时，模型应具备主动询问并获取任务相关信息的能力，同时具有一定的敏感度、洞察力和审慎态度。由于用户意图的多样性，有用性这一对齐标准仍然难以进行统一的定义与刻画，需要根据不同的用户进行确定。&lt;/li&gt;
&lt;li&gt;诚实性. 模型的输出应具备真实性和客观性，不应夸大或歪曲事实，避免产生误导性陈述，并能够应对输入的多样性和复杂性。在人机交互过程中，大语言模型应向用户提供准确内容，还应适当表达对于输出信息的不确定性程度，以避免任何形式的误导。本质上，这要求模型了解自身的能力和知识水平。与有用性和无害性相比，诚实性是一个更为客观的标准，对人类标注的依赖相对较少。&lt;/li&gt;
&lt;li&gt;无害性. 大语言模型应避免生成可能引发潜在负面影响或危害的内容。在处理敏感主题时，模型应遵循道德标准和社会价值观，从而消除冒犯性与歧视性。此外，模型需要能够检测到具有恶意目的的查询请求。当模型被诱导执行危险行为（如犯罪行为）时，应直接予以拒绝。然而，何种行为被视为有害，很大程度上取决于大语言模型的使用者、用户问题类型以及使用大语言模型的背景。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上述三种通用的对齐标准较为宽泛，因此许多研究针对性地给出了一些更为细化的对齐标准，以更全面地规范大语言模型的输出。例如，行为对齐要求人工智能系统能够做出符合人类期望的行为；在此基础上，意图对齐则进一步要求大语言模型在意图和行为上都要与人类期望保持一致，这涉及到哲学、心理学以及技术细节上的多重挑战；道德对齐要求语言模型应避免涉及非法、不道德或有害的话题，在回应中优先考虑用户安全、道德准绳和行为边界。这些对齐标准在本质上与前述三个标准是相似的，研究人员可以根据任务的特定需求进行调整。&lt;/p&gt;
&lt;p&gt;在实践中，红队攻击（RedTeaming）技术也被广泛运用，通过人工或自动 化的手段，以对抗方式探测大语言模型，诱导其生成有害输出，并据此针对性地 调整大语言模型，以避免产生此类有害输出[4]。&lt;/p&gt;
&lt;h2 id=&#34;基于人类反馈的强化学习&#34;&gt;🍊基于人类反馈的强化学习
&lt;/h2&gt;&lt;p&gt;由于对齐标准难以通过形式化的优化目标进行建模，因此研究人员提出了基于人类反馈的强化学习（Reinforcement Learning from Human Feedback,RLHF），引入人类反馈对大语言模型的行为进行指导。&lt;/p&gt;
&lt;h3 id=&#34;rlhf-概述&#34;&gt;🥮RLHF 概述
&lt;/h3&gt;&lt;p&gt;RLHF首先需要收集人类对于不同模型输出的偏好，然后使用收集到的人类反馈数据训练奖励模型，最后基于奖励模型使用强化学习算法（例如Proximal Policy Optimization,PPO[5]）微调大语言模型。这种将人类反馈纳入大语言模型训练过程的方法已成为实现人类对齐的主要技术途径之一。&lt;/p&gt;
&lt;h4 id=&#34;rlhf算法系统&#34;&gt;🎑RLHF算法系统
&lt;/h4&gt;&lt;p&gt;RLHF 算法系统主要包括三个关键组成部分：需要与人类价值观对齐的模型、基于人类反馈数据学习的奖励模型以及用于训练大语言模型的强化学习算法。&lt;/p&gt;
&lt;p&gt;具体来说，待对齐模型一般指的是经过预训练、具备一定通用能力的大语言模型。然而，这些模型并没有与人类价值观对齐，在下游任务中可能表现出不合适甚至有害的行为。&lt;/p&gt;
&lt;p&gt;奖励模型的作用是为强化学习过程提供指导信号，反映了人类对于语言模型生成文本的偏好，通常以标量值的形式呈现。奖励模型既可以采用人类偏好数据对已有的语言模型继续微调，也可以基于人类偏好数据重新训练一个新的语言模型。&lt;/p&gt;
&lt;p&gt;在训练过程中，基于奖励模型提供的反馈信号，RLHF使用特定的强化学习算法进行大语言模型的训练。目前，PPO算法[5] 是一种被广泛用于人类对齐的强化学习算法。&lt;/p&gt;
&lt;h4 id=&#34;rlhf的关键步骤&#34;&gt;🥧RLHF的关键步骤
&lt;/h4&gt;&lt;p&gt;监督微调. 为了让待对齐语言模型具有较好的指令遵循能力，通常需要收集高质量的指令数据进行监督微调。指令数据一般包括任务描述和示例输出，可以由人类标注员针对特定任务编写，也可以由大语言模型自动生成。&lt;/p&gt;
&lt;p&gt;奖励模型训练.第二步是使用人类反馈数据训练奖励模型。具体来说，首先使用语言模型针对任务指令生成一定数量的候选输出。随后，邀请标注员对于输出文本进行偏好标注，这个标注过程可以采用多种形式，其中最常用的是对候选文本进行排序标注，这样可以有效减少多个标注员之间的不一致情况。进一步，使用人工标注的偏好数据进行奖励模型的训练，使其能够建模人类偏好。&lt;/p&gt;
&lt;p&gt;强化学习训练. 在这一步骤中，语言模型对齐被转化为一个强化学习问题。具体来说，待对齐语言模型担任策略实施者的角色（称为策略模型），它接收提示作为输入并返回输出文本，其动作空间是词汇表中的所有词元，状态指的是当前已生成的词元序列。&lt;/p&gt;
&lt;h3 id=&#34;人类反馈数据的收集&#34;&gt;🔥人类反馈数据的收集
&lt;/h3&gt;&lt;p&gt;在预训练阶段，大语言模型通过语言建模目标在大规模无标注语料库上进行训练。然而，这一过程无法直接反映人类对于大语言模型输出的主观和定性偏好（本书中称为“人类反馈”）。为了实现有效的人类对齐，需要使用高质量的人类反馈数据（不是高质量人类，哈哈哈）对大语言模型进行针对性的微调。&lt;/p&gt;
&lt;h4 id=&#34;人类反馈形式&#34;&gt;🧡人类反馈形式
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;基于评分的人类反馈.最直接的标注方式是根据预设的标准邀请标注人员对于大语言模型的输出进行打分，从而作为模型输出质量的判断。&lt;/li&gt;
&lt;li&gt;基于排序的人类反馈.排序是一种比较典型的人类偏好标注形式。最简单的方式是标注人员根据自身偏好对于大语言模型的输出进行全排序。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;奖励模型的训练&#34;&gt;🏮奖励模型的训练
&lt;/h3&gt;&lt;p&gt;由于RLHF的训练过程中需要依赖大量的人类偏好数据进行学习，因此很难在训练过程中要求人类标注者实时提供偏好反馈。为此，我们需要训练一个模型来替代人类在RLHF训练过程中实时提供反馈，这个模型被称为奖励模型。&lt;/p&gt;
&lt;p&gt;思考：&lt;/p&gt;
&lt;p&gt;这里倒是解决我的疑问了，可以训练一个奖励模型干这个事。&lt;/p&gt;
&lt;h2 id=&#34;非强化学习的对齐方法&#34;&gt;🀨非强化学习的对齐方法
&lt;/h2&gt;&lt;p&gt;尽管RLHF已被证明是一种较为有效的语言模型对齐技术，但是它也存在一些局限性。首先，在RLHF的训练过程中，需要同时维护和更新多个模型，这些模型包括策略模型、奖励模型、参考模型以及评价模型。这不仅会占用大量的内存资源，而且整个算法的执行过程也相对复杂。此外，RLHF中常用的近端策略优化算法在优化过程中的稳定性欠佳，对超参数的取值较为敏感，这进一步增加了模型训练的难度和不确定性。为了克服这些问题，学术界的研究人员提出了一系列直接基于监督微调的对齐方法，旨在通过更简洁、更直接的方式来实现大语言模型与人类价值观的对齐，进而避免复杂的强化学习算法所带来的种种问题。&lt;/p&gt;
&lt;h3 id=&#34;代表性监督对齐算法dpo&#34;&gt;☕代表性监督对齐算法DPO
&lt;/h3&gt;&lt;p&gt;直接偏好优化（DirectPreference Optimization, DPO）是一种不需要强化学习的对齐算法。由于去除了复杂的强化学习算法，DPO可以通过与有监督微调相似的复杂度实现模型对齐，不再需要在训练过程中针对大语言模型进行采样，同时超参数的选择更加容易。&lt;/p&gt;
&lt;h2 id=&#34;参考文献&#34;&gt;🌆参考文献
&lt;/h2&gt;&lt;p&gt;[1]. Long Ouyang et al. “Training language models to follow instructions with human feed back”. In: arXiv preprint arXiv:2203.02155 (2022).&lt;/p&gt;
&lt;p&gt;[2]. Daniel M. Ziegler et al. “Fine-Tuning Language Models from Human Preferences”. In: arXiv preprint arXiv:1909.08593 (2019).&lt;/p&gt;
&lt;p&gt;[3]. Amelia Glaese et al. “Improving alignment of dialogue agents via targeted human judge ments”. In: arXiv preprint arXiv:2209.14375 (2022).&lt;/p&gt;
&lt;p&gt;[4]. Ethan Perez et al. “Red Teaming Language Models with Language Models”. In: EMNLP. 2022.&lt;/p&gt;
&lt;p&gt;[5]. John Schulman et al. “Proximal policy optimization algorithms”. In: arXiv preprint arXi v:1707.06347 (2017).&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Instruction Tuning</title>
        <link>https://JiangZhiyu-1024.github.io/p/instructiontuning/</link>
        <pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/instructiontuning/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/instructiontuning/apples-1776744_1280.jpg" alt="Featured image of post Instruction Tuning" /&gt;&lt;h1 id=&#34;指令微调instruction-tuning&#34;&gt;指令微调（Instruction Tuning）
&lt;/h1&gt;&lt;p&gt;指令微调（Instruction Tuning）是指使用自然语言形式的数据对预训练后的大语言模型进行参数微调，这一术语由谷歌研究员在2022年的一篇ICLR论文中正式提出[1]。在另外一些参考文献中，指令微调也被称为有监督微调（Supervised Fine-tuning）[2] 或多任务提示训练（MultitaskPromptedTraining）[3]。指令微调过程需要首先收集或构建指令化的实例，然后通过有监督的方式对大语言模型的参数进行微调。经过指令微调后，大语言模型能够展现出较强的指令遵循能力，可以通过零样本学习的方式解决多种下游任务。&lt;/p&gt;
&lt;h2 id=&#34;指令数据的构建&#34;&gt;🏔️指令数据的构建
&lt;/h2&gt;&lt;p&gt;一般来说，一个经过指令格式化的数据实例包括任务描述（也称为指令）、任务输入-任务输出以及可选的示例。&lt;/p&gt;
&lt;h3 id=&#34;基于现有的nlp任务数据集构建&#34;&gt;⛰️基于现有的NLP任务数据集构建
&lt;/h3&gt;&lt;p&gt;学术界围绕传统NLP任务（如机器翻译、文本摘要和文本分类等）发布了大量的开源数据集合，这些数据是非常重要的监督学习数据资源，可以用于指令数据集的构造。通常来说，这些NLP数据集都包括输入和输出两个主要部分。&lt;/p&gt;
&lt;p&gt;例如，在中英翻译任务中，输入是“大语言模型已经成为机器学习的一个重要研究方向”，而相应的输出则是“Large language models have become one important research direction for machine learning”。&lt;/p&gt;
&lt;p&gt;为了生成指令化的训练数据，一个非常关键的步骤就是为上述的“输入-输出”对数据添加任务描述信息，用于指导模型去理解任务目标以及相关信息。在上述的例子中，可以向中译英的翻译数据集中添加指令， 例如“请把这个中文句子翻译成英文”。通过上述操作，就可以将一个NLP任务的数据实例全部通过自然语言形式进行表达，进而数据实例可以被用于大语言模型的指令微调。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/instructiontuning/image-20241031094251824.png&#34;
	width=&#34;576&#34;
	height=&#34;447&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/instructiontuning/image-20241031094251824_hu8415555102542121778.png 480w, https://JiangZhiyu-1024.github.io/p/instructiontuning/image-20241031094251824_hu18443101324586505955.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241031094251824&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;128&#34;
		data-flex-basis=&#34;309px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;现有NLP数据集的指令格式化示意图&lt;/center&gt;
&lt;p&gt;为了更好地标注NLP指令微调数据，研究 人员开发了众包平台PromptSourcehttps://github.com/bigscience-workshop/promptsource，它能够高效地支持标注人员为不同数据集创建、共享及验证任务描述。&lt;/p&gt;
&lt;h3 id=&#34;基于日常对话数据构建&#34;&gt;🌋基于日常对话数据构建
&lt;/h3&gt;&lt;p&gt;尽管通过指令格式化已有的NLP数据集能够获得大量的指令数据实例，但是这些数据的多样性比较局限，与人类的真实需求也不能很好匹配。为此，研究人员开始使用用户在日常对话中的实际需求作为任务描述。例如，InstructGPT[2]将用户提交给OpenAI API的查询作为任务描述。由于这些用户查询源自于真实应用场景，均采用自然语言形式进行表达，因此特别适合大模型学习指令跟随能力。为了进一步增加任务的多样性，OpenAI还雇佣标注者创作更多的真实生活任务，包括开放式生成、开放式问答、头脑风暴等任务，然后由另一组标注者来回答这些问题作为输出。OpenAI最终将指令（用户真实查询或者人工标注的任务）和期望的输出（人工编写的答案）配对作为一个训练实例。但是，OpenAI没有对外开放所使用的指令数据。&lt;/p&gt;
&lt;h3 id=&#34;基于合成数据构建&#34;&gt;🗻基于合成数据构建
&lt;/h3&gt;&lt;p&gt;Self-Instruct   &amp;mdash;&amp;mdash;-&amp;gt;     Yizhong Wang et al. “Self-Instruct: Aligning Language Model with Self Generated In structions”. In: arXiv preprint arXiv:2212.10560 (2022).&lt;/p&gt;
&lt;p&gt;Evol-Instruct   &amp;mdash;&amp;mdash;-&amp;gt;     Can Xu et al. “WizardLM: Empowering Large Language Models to Follow Complex In structions”. In: arXiv preprint arXiv:2304.12244 (2023).&lt;/p&gt;
&lt;h2 id=&#34;指令微调的训练策略&#34;&gt;🏖️指令微调的训练策略
&lt;/h2&gt;&lt;h3 id=&#34;优化设置&#34;&gt;🏜️优化设置
&lt;/h3&gt;&lt;p&gt;指令微调中的优化器设置（AdamW或Adafactor）、稳定训练技巧（权重衰减和梯度裁剪）和训练技术（3D并行、ZeRO和混合精度训练）都与预训练保持阶段一致，可以完全沿用。&lt;/p&gt;
&lt;p&gt;不同之处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;目标函数. 预训练阶段通常采用语言建模损失，优化模型在每一个词元上的损失。而指令微调可以被视为一个有监督的训练过程，通常采用的目标函数为序列到序列损失，仅在输出部分计算损失，而不计算输入部分的损失。&lt;/li&gt;
&lt;li&gt;批次大小和学习率.考虑到预训练阶段已经学习到了能够展现较好性能的模型参数，指令微调阶段通常只需要使用较小的批次大小和学习率对模型进行小幅度的调整。例如InstructGPT(175B)微调的批次大小为8，学习率恒定为5.03×10−6；Alpaca (7B) 微调的批次大小为128，学习率预热到2×10−5，然后采用余弦衰减策略。&lt;/li&gt;
&lt;li&gt;多轮对话数据的高效训练.对于一个多轮对话数据，通常的训练算法是将其拆分成多个不同的对话数据进行单独训练。为了提升训练效率，可以采用特殊的掩码机制来实现多轮对话数据的高效训练。在因果解码器架构中，由于输入输出没有明显的分界，可以将所有一个对话的多轮内容一次性输入模型，通过设计损失掩码来实现仅针对每轮对话的模型输出部分进行损失计算，从而显著减少重复前缀计算的开销。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;数据组织策略&#34;&gt;🏝️数据组织策略
&lt;/h3&gt;&lt;p&gt;平衡数据分布：最常见的方法是样本比例混合策略，即把所有数据集进行合并，然后从混合数据集中等概率采样每个实例。例如，研究者建议混合使用NLP任务数据（如FLANv2）、对话数据（如ShareGPT）和合成数据（如GPT4-Alpaca），来进行大模型的指令微调。&lt;/p&gt;
&lt;p&gt;多阶段指令数据微调：首先使用大规模NLP任务指令数据对模型进行微调，然后再使用相对多样的日常对话指令和合成指令进一步微调。为了避免能力遗忘问题，可以在第二阶段中添加一些NLP指令数据。&lt;/p&gt;
&lt;p&gt;结合预训练数据与指令微调数据：为了使得微调过程更加有效和稳定，可以在指令微调期间引入预训练数据和任务，这可以看作是对于指令微调的正则化。&lt;/p&gt;
&lt;h2 id=&#34;参数高效的模型微调&#34;&gt;🏛️参数高效的模型微调
&lt;/h2&gt;&lt;p&gt;通过指令微调，大语言模型能够更好地学习遵循和执行人类指令。然而，由于大语言模型的参数量巨大， 进行全参数微调（需要较多的算力资源开销）在本节中，我们将讨论如何针对大语言模型进行参数高效微调（Parameter-efficient Fine-tuning），也称为轻量化微调 （Lightweight Fine-tuning）。在现有文献中，参数高效微调[4,5,6] 是一个重要的 研究方向，旨在减少需要训练的模型参数量，同时保证微调后的模型性能能够与全量微调的表现相媲美。下面将首先介绍常用于Transformer架构的参数高效微调方法，然后以LoRA微调方法为例介绍参数高效微调的代码实现。&lt;/p&gt;
&lt;h3 id=&#34;低秩适配微调方法&#34;&gt;🏘️低秩适配微调方法
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/instructiontuning/image-20241031105628096.png&#34;
	width=&#34;862&#34;
	height=&#34;418&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/instructiontuning/image-20241031105628096_hu12366843635982129503.png 480w, https://JiangZhiyu-1024.github.io/p/instructiontuning/image-20241031105628096_hu15626326028252868261.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241031105628096&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;206&#34;
		data-flex-basis=&#34;494px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;LoRA微调示意图&lt;/center&gt;
&lt;h4 id=&#34;lora基础&#34;&gt;🏬LoRA基础
&lt;/h4&gt;&lt;p&gt;大语言模型中包含大量的线性变换层，其中参数矩阵的维度通常很高。研究人员[6]发现模型在针对特定任务进行适配时，参数矩阵往往是过参数化（Over-parametrized）的，其存在一个较低的内在秩。为了解决这一问题，LoRA[6] 提出在预训练模型的参数矩阵上添加低秩分解矩阵来近似每层的参数更新，从而减少适配下游任务所需要训练的参数。给定一个参数矩阵𝑾，其更新过程可以一般性地表达为以下形式：
&lt;/p&gt;
$$
W = W_0 +ΔW
$$&lt;p&gt;
其中，$W_0$是原始参数矩阵，Δ𝑾是更新的梯度矩阵。LoRA的基本思想是冻结原始矩阵$W_0 ∈ R^{H \times H}$，通过低秩分解矩阵$A ∈ R^{H \times H}$和$B ∈ R^{H \times H}$来近似参数更新矩阵$Δ𝑾 = A·B^T$，其中𝑅≪𝐻是减小后的秩。在微调期间，原始的矩阵参数$W_0$不会被更新，低秩分解矩阵𝑨和𝑩则是可训练参数用于适配下游任务。在前向传播过程中，原始计算中间状态$h=W_0·x$的公式修改为：
&lt;/p&gt;
$$
h = W_0·x + A·B^T·x
$$&lt;p&gt;
在训练完成后，进一步将原始参数矩阵$W_0$和训练得到的权重𝑨和𝑩进行合并：$W = W_0 + A·B^T$，得到更新后的参数矩阵。因此，LoRA微调得到的模型在解码过程中不会增加额外的开销。&lt;/p&gt;
&lt;h4 id=&#34;lora变种&#34;&gt;🏦LoRA变种
&lt;/h4&gt;&lt;p&gt;在原始的LoRA实现中，每个低秩矩阵的低秩参数𝑅都被设置为固定且相同的数值，并且在训练过程中无法进行调整，这种设定忽略了不同的秩在微调任务中可能产生的差异化影响。因此，通过这种方式训练得到的低秩矩阵往往并非最优解。AdaLoRA[7]讨论了如何更好地进行秩的设置。它引入了一种动态低秩适应技术，在训练过程中动态调整每个参数矩阵需要训练的秩同时控制训练的参数总量。具体来说，模型在微调过程中通过损失来衡量每个参数矩阵对训练结果的重要性，重要性较高的参数矩阵被赋予比较高的秩，进而能够更好地学习到有助于任务的信息。相对而言，不太重要的参数矩阵被赋予比较低的秩，来防止过拟合并节省计算资源。尽管LoRA能够有效地节省显存，但对于参数规模达到上百亿级别的模型而言，其微调所需的成本仍然相当高昂。QLoRA[8]将原始的参数矩阵量化为4比特，而低秩参数部分仍使用16比特进行训练，在保持微调效果的同时进一步节省了显存开销。根据上一小节的分析，对于给定参数量为𝑃的模型，QLoRA微调所需要的显存由LoRA微调所需要的2𝑃进一步下降为0.5𝑃。因此通过QLoRA技术，可以在一张A6000(48GB)的GPU上微调65B的模型，接近16比特模型微调的性能。&lt;/p&gt;
&lt;h3 id=&#34;其他高效微调方法&#34;&gt;🏡其他高效微调方法
&lt;/h3&gt;&lt;p&gt;适配器微调、前缀微调、提示微调&lt;/p&gt;
&lt;h2 id=&#34;参考文献&#34;&gt;参考文献
&lt;/h2&gt;&lt;p&gt;[1]. Jason Wei et al. “Finetuned Language Models are Zero-Shot Learners”. In: ICLR. 2022.&lt;/p&gt;
&lt;p&gt;[2]. Long Ouyang et al. “Training language models to follow instructions with human feed back”. In: arXiv preprint arXiv:2203.02155 (2022).&lt;/p&gt;
&lt;p&gt;[3]. VictorSanhetal. “Multitask Prompted Training Enables Zero-Shot Task Generalization”. In: ICLR. 2022.&lt;/p&gt;
&lt;p&gt;[4]. XiangLisa Li and Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Gen eration”. In: ACL. 2021.&lt;/p&gt;
&lt;p&gt;[5]. Brian Lester, Rami Al-Rfou, and Noah Constant. “The Power of Scale for Parameter Efficient Prompt Tuning”. In: EMNLP. 2021.&lt;/p&gt;
&lt;p&gt;[6]. EdwardJ.Huetal. “LoRA:Low-RankAdaptation of Large Language Models”. In: ICLR. 2022.&lt;/p&gt;
&lt;p&gt;[7]. Qingru Zhang et al. “Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning”. In: arXiv preprint arXiv:2303.10512 (2023).&lt;/p&gt;
&lt;p&gt;[8]. TimDettmersetal.“QLoRA:EfficientFinetuningofQuantizedLLMs”.In:arXivpreprint arXiv:2305.14314 (2023).&lt;/p&gt;
</description>
        </item>
        <item>
        <title>transformer模型架构</title>
        <link>https://JiangZhiyu-1024.github.io/p/transformermodelbase/</link>
        <pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/transformermodelbase/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/transformermodelbase/house-7497002_1280.png" alt="Featured image of post transformer模型架构" /&gt;&lt;h1 id=&#34;模型架构&#34;&gt;模型架构
&lt;/h1&gt;&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391.png&#34;
	width=&#34;807&#34;
	height=&#34;689&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391_hu13532172091181392611.png 480w, https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391_hu12135867853004229579.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241029095402391&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;281px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;大语言模型架构配置表&lt;/center&gt;&gt;
&lt;h2 id=&#34;transformer-模型&#34;&gt;🍈Transformer 模型
&lt;/h2&gt;&lt;p&gt;当前主流的大语言模型都是基于Transformer模型进行设计的。Transformer是由多层的多头自注意力（Multi-headSelf-attention）模块堆叠而成的神经网络模型。原始的Transformer 模型由编码器和解码器两个部分构成，而这两个部分实际上可以独立使用，例如基于编码器架构的BERT模型[1]和解码器架构的GPT模型[2]。与BERT等早期的预训练语言模型相比，大语言模型的特点是使用了更长的向量维度、更深的层数，进而包含了更大规模的模型参数，并主要使用解码器架构，对于Transformer 本身的结构与配置改变并不大。&lt;/p&gt;
&lt;h3 id=&#34;输入编码&#34;&gt;🍉输入编码
&lt;/h3&gt;&lt;p&gt;在Transformer 模型中，输入的词元序列(𝒖 = [𝑢1,𝑢2,&amp;hellip;,𝑢𝑇]) 首先经过一个输入嵌入模块（InputEmbeddingModule）转化成词向量序列。具体来说，为了捕获词汇本身的语义信息，每个词元在输入嵌入模块中被映射成为一个可学习的、具有固定维度的词向量$v_t$ ∈$R^H$。由于Transformer的编码器结构本身无法识别序列中元素的顺序，位置编码（PositionEmbedding,PE）被引入来表示序列中的位置信息。给定一个词元$u_t$，位置编码根据其在输入中的绝对位置分配一个固定长度的嵌入向量$p_t$ ∈$R^H$。然后，每个词元对应的词向量和位置向量将直接相加，生成了最终的输入嵌入序列𝑿=[𝒙1,&amp;hellip;,𝒙𝑇]，并且被传入到后续层中：$x_t=v_t+p_t$.&lt;/p&gt;
&lt;p&gt;通过这种建模方法的表示，Transformer 模型可以利用位置编码 𝒑𝑡 建模不同词元的位置信息。由于不同词元的位置编码仅由其位置唯一决定，因此这种位置建模方式被称为绝对位置编码。尽管绝对位置编码能够一定程度上建模位置信息，然而它只能局限于建模训练样本中出现的位置，无法建模训练数据中未出现过的位置，因此极大地限制了它们处理长文本的能力。&lt;/p&gt;
&lt;h3 id=&#34;多头自注意力机制&#34;&gt;🍋多头自注意力机制
&lt;/h3&gt;&lt;p&gt;多头自注意力是Transformer模型的核心创新技术。相比于循环神经网络（Recurrent Neural Network, RNN）和卷积神经网络（ConvolutionalNeuralNetwork,CNN）等传统神经网络，多头自注意力机制能够直接建模任意距离的词元之间的交互关系。作为对比，循环神经网络迭代地利用前一个时刻的状态更新当前时刻的状态，因此在处理较长序列的时候，常常会出现梯度爆炸或者梯度消失的问题。而在卷积神经网络中，只有位于同一个卷积核的窗口中的词元可以直接进行交互，通过堆叠层数来实现远距离词元间信息的交换。&lt;/p&gt;
&lt;p&gt;多头自注意力机制通常由多个自注意力模块组成。在每个自注意力模块中，对于输入的词元序列，将其映射为相应的查询（Query,𝑸）、键（Key,𝑲）和值（Value,𝑽）三个矩阵。然后，对于每个查询，将和所有没有被掩盖的键之间计算点积。这些点积值进一步除以$\sqrt{D}$进行缩放（𝐷是键对应的向量维度），被传入到softmax函数中用于权重的计算。进一步，这些权重将作用于与键相关联的值，通过加权和的形式计算得到最终的输出。在数学上，上述过程可以表示为：
&lt;/p&gt;
$$
Q = XW^Q,
$$$$
K = XW^K,
$$$$
V = XW^V,
$$$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{D}})V.
$$&lt;p&gt;与单头注意力相比，多头注意力机制的主要区别在于它使用了𝐻组结构相同，但映射参数不同的自注意力模块。输入序列首先通过不同的权重矩阵被映射为一组查询、键和值。每组查询、键和值的映射构成一个“头”，并独立地计算自注意力的输出。最后，不同头的输出被拼接在一起，并通过一个权重矩阵$W^O$∈$R^{H \times H}$ 进行映射，产生最终的输出。如下面的公式所示：
&lt;/p&gt;
$$
head_n = Attention(XW^Q_n,XW^K_n,XW^V_n)
$$$$
MHA = Concat(head_1,...,head_N)W^O
$$&lt;p&gt;由上述内容可见，自注意力机制能够直接建模序列中任意两个位置之间的关系，进而有效捕获长程依赖关系，具有更强的序列建模能力。另一个主要的优势是，自注意力的计算过程对于基于硬件的并行优化（如GPU、TPU等）非常友好，因此能够支持大规模参数的高效优化。&lt;/p&gt;
&lt;h3 id=&#34;前馈网络层&#34;&gt;🍏前馈网络层
&lt;/h3&gt;&lt;p&gt;为了学习复杂的函数关系和特征，Transformer 模型引入了一个前馈网络层（Feed Forward Netwok, FFN），对于每个位置的隐藏状态进行非线性变换和特征提取。具体来说，给定输入𝒙，Transformer中的前馈神经网络由两个线性变换和一个非线性激活函数组成：
&lt;/p&gt;
$$
FFN(X) = σ(XW^U + b_1)W^D + b_2
$$&lt;p&gt;
其中$W^U$ ∈ $R^{H \times H}$ 和$W^D$ ∈ $R^{H \times H}$  分别是第一层和第二层的线性变换权重矩阵，$b_1$ ∈ $R^{𝐻^′}$ 和 $b_2$ ∈ $R^H$ 是偏置项，𝜎是激活函数（在原始的Transformer中，采用 ReLU 作为激活函数）。前馈网络层通过激活函数引入了非线性映射变换，提升了 模型的表达能力，从而更好地捕获复杂的交互关系。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440.png&#34;
	width=&#34;650&#34;
	height=&#34;691&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440_hu8196060707081265206.png 480w, https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440_hu16997779109624366145.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241029105634440&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;94&#34;
		data-flex-basis=&#34;225px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;Transformer 架构图&lt;/center&gt;
&lt;h3 id=&#34;编码器&#34;&gt;🍐编码器
&lt;/h3&gt;&lt;p&gt;在Transformer 模型中，编码器（Encoder）的作用是将每个输入词元都编码成一个上下文语义相关的表示向量。编码器结构由多个相同的层堆叠而成，其中每一层都包含多头自注意力模块和前馈网络模块。在注意力和前馈网络后，模型使用层归一化和残差连接来加强模型的训练稳定度。其中，残差连接（Residual Connection）将输入与该层的输出相加，实现了信息在不同层的跳跃传递，从而缓解梯度爆炸和消失的问题。而LayerNorm则对数据进行重新放缩，提升模型的训练稳定性。编码器接受经过位置编码层的词嵌入序列𝑿作为输入，通过多个堆叠的编码器层来建模上下文信息，进而对于整个输入序列进行编码表示。由于输入数据是完全可见的，编码器中的自注意力模块通常采用双向注意力，每个位置的词元表示能够有效融合上下文的语义关系。在编码器-解码器架构中，编码器的输出将作为解码器（Decoder）的输入，进行后续计算。形式化来说，第𝑙层（𝑙∈{1,&amp;hellip;,𝐿}）的编码器的数据处理过程如下所示：
&lt;/p&gt;
$$
X^′_l = LayerNorm(MHA(X_{l-1})+X_{l-1})
$$$$
X_l = LayerNorm(FFN(X^′_l)+X^′_l)
$$&lt;p&gt;其中，$X^′_l$ 和 $X_l$ 分别是该Transformer层的输入和输出，$X^′_l$是该层中输入经过多头注意力模块后的中间表示，LayerNorm表示层归一化。&lt;/p&gt;
&lt;h3 id=&#34;解码器&#34;&gt;🥥解码器
&lt;/h3&gt;&lt;p&gt;Transformer 架构中的解码器基于来自编码器编码后的最后一层的输出表示以及已经由模型生成的词元序列，执行后续的序列生成任务。与编码器不同，解码器需要引入掩码自注意力（MaskedSelf-attention）模块，用来在计算注意力分数的时候掩盖当前位置之后的词，以保证生成目标序列时不依赖于未来的信息。除了建模目标序列的内部关系，解码器还引入了与编码器相关联的多头注意力层，从而关注编码器输出的上下文信息$X_L$。同编码器类似，在每个模块之后，Transformer 解码器也采用了层归一化和残差连接。在经过解码器之后，模型会通过一个全连接层将输出映射到大小为𝑉的目标词汇表的概率分布，并基于某种解码策略生成对应的词元。在训练过程中，解码器可以通过一次前向传播，让每个词元的输出用于预测下一个词元。而在解码过程，解码器需要经过一个逐步的生成过程，将自回归地生成完整的目标序列。解码器的数据流程如下所示：
&lt;/p&gt;
$$
Y^′_l = LayerNorm(MaskedMHA(Y_{l-1})+Y_{l-1})
$$$$
Y^&#34;_l = LayerNorm(CrossMHA(Y^′_l,X_L)+Y^′_l)
$$$$
Y_l = LayerNorm(FFN(Y^&#34;_l)+Y^&#34;_l)
$$&lt;p&gt;其中，$Y_{l-1}$ 和 $Y_l$ 分别是该Transformer 层的输入和输出，$Y^′_l$ 和 $Y^&amp;quot;_l$ 是该层中输入经过掩码多头注意力MaskedMHA和交叉多头注意力CrossMHA模块后的中间表示，LayerNorm 表示层归一化。然后将最后一层的输入𝒀𝐿映射到词表的维度上：
&lt;/p&gt;
$$
O = softmax(W^LY_L)
$$&lt;p&gt;
其中，𝑶 ∈$R^{H \times V}$ 是模型最终的输出，代表下一个词在词表上的概率分布；$W^L$ ∈ $R^{H \times V}$ 是将输入表示映射到词汇表维度的参数矩阵，而$W^LY_L$是概率化前的中间值，通常被称为logits。&lt;/p&gt;
&lt;p&gt;[1]. JacobDevlin et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. In: NAACL-HLT. 2019.&lt;/p&gt;
&lt;p&gt;[2]. Alec Radford et al. “Improving language understanding by generative pre-training”. In: OpenAI Blog (2018).&lt;/p&gt;
</description>
        </item>
        <item>
        <title>LLM预训练数据准备</title>
        <link>https://JiangZhiyu-1024.github.io/p/pretraining-data-preparation-for-large-models/</link>
        <pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/pretraining-data-preparation-for-large-models/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/pretraining-data-preparation-for-large-models/road-5710320_1280.jpg" alt="Featured image of post LLM预训练数据准备" /&gt;&lt;h1 id=&#34;大模型预训练数据准备&#34;&gt;大模型预训练数据准备
&lt;/h1&gt;&lt;h2 id=&#34;数据来源&#34;&gt;✅数据来源
&lt;/h2&gt;&lt;p&gt;根据来源不同，预训练数据主要分为两种类型：通用文本数据和专用文本数据&lt;/p&gt;
&lt;h3 id=&#34;通用文本数据&#34;&gt;🔥通用文本数据
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;网页：随着互联网的普及与发展，网页的数据规模持续扩大，覆盖的内容类型也变得丰富多样。使用大规模网页文本数据进行预训练，有助于大语言模型获取多样化的语言知识，并增强其自然语言理解和生成的能力[1,2]。为了便于使用网页数据进行预训练或相关研究，相关机构已经爬取并发布了多个大规模的网页数据集，包括C4[2]、RefinedWeb[3]、CC-Stories [4] 等。然而，这些网页数据集中既包含了维基百科这种高质量文本，也不可避免地引入了广告网页等低质量文本。因此，在进行预训练之前，对网页进行筛选和处理显得尤为重要，这直接关系到最终数据的质量与预训练效果。&lt;/li&gt;
&lt;li&gt;书籍：相较于其他语料，书籍中的文本内容往往更为正式与详实，篇幅也相对较长。这些书籍文本在大语言模型的学习过程中，发挥着非常重要的作用，它们不仅能够帮助模型积累丰富的语言知识，还可以加强其长程语义关系的建模。现有的研究工作通常使用Books3和Bookcorpus2等开源书籍数据集。这些数据可以在Pile 数据集中获得[5]。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;专用文本数据&#34;&gt;💯专用文本数据
&lt;/h3&gt;&lt;p&gt;专用数据集有助于提升大语言模型解决特定下游任务的能力。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多语文本. 在预训练语料中，加入多语言的文本数据可以增强模型的多语理解与生成能力。BLOOM[6]模型和PaLM[7]模型在其预训练语料中分别使用了涵盖46种和122种语言的多语数据，进而使得这两个模型在翻译、跨语言摘要和问答等多语言任务中性能表现优异。相比于仅针对单一目标语言进行微调的模型，在多语言语料库上训练过的大语言模型能够更好地建立多语言间的语义关联，为跨语言理解与对话任务提供支持。不仅如此，多语言数据还能有效增加数据的多样性，从而有助于提升模型的综合性能。&lt;/li&gt;
&lt;li&gt;科学文本. 随着科学研究的不断发展，相关出版物的数量不断增加。为了增强大语言模型对科学知识的理解，可以将科学文本数据加入到模型的预训练语料中。通过在大规模的科学文本语料上进行预训练，大语言模型可以在自然科学以及工程技术方面建立坚实的知识基础，从而在科学问答与推理等任务上取得出色的表现[8]。构建科学文本语料的常用方法是收集arXiv论文、科学教材、数学网页等科学资源。然而，由于科学文本数据中包含数学公式、蛋白质序列等特殊符号，通常需要采用特定的分词和预处理技术，将这些不同格式的数据转化为大语言模型能够处理的统一格式。&lt;/li&gt;
&lt;li&gt;代码. 代码能力目前已经成为大语言模型备受关注的一种能力。为了提高模型的代码能力，需要在大量代码语料上进行预训练，进而提高其所生成的程序质量。这些由大语言模型编写的程序甚至可以成功通过专家设计的单元测试用例[9] 或解决具有挑战性的算法竞赛问题[10]。一般来说，常用于大语言模型预训练的代码语料有两种来源，第一种是StackExchange等编程问答社区的数据，第二种是 GitHub 等开源项目仓库。这两种来源包含了代码以及对应的注释和文档。与自然语言文本相比，代码主要以结构化的编程语言形式呈现。在代码数据上训练能够提升模型的结构化语义理解与逻辑推理能力[11]。同时，代码中的函数调用关系还有助于增强模型的工具使用与学习能力[12]。此外，将推理任务格式化为代码 可以帮助大语言模型生成更准确的结果[13,14]。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;数据预处理&#34;&gt;🗡数据预处理
&lt;/h2&gt;&lt;p&gt;当收集了丰富的文本数据之后，为了确保数据的质量和效用，还需要对数据进行预处理，从而消除低质量、冗余、无关甚可能有害的数据。&lt;/p&gt;
&lt;h3 id=&#34;质量过滤&#34;&gt;🌈质量过滤
&lt;/h3&gt;&lt;h4 id=&#34;基于启发式规则的方法p74&#34;&gt;基于启发式规则的方法（P74）
&lt;/h4&gt;&lt;p&gt;设计规则来针对地识别和剔除低质量的文本数据[6,15]。&lt;/p&gt;
&lt;p&gt;基于语种的过滤.&lt;/p&gt;
&lt;p&gt;基于简单统计指标的过滤.&lt;/p&gt;
&lt;p&gt;基于关键词的过滤.&lt;/p&gt;
&lt;h5 id=&#34;思考启发式规则过滤需要自己设计并决定使用什么规则很考验想法的合理性和这方面的经验&#34;&gt;思考：启发式规则过滤需要自己设计并决定使用什么规则，很考验想法的合理性和这方面的经验。
&lt;/h5&gt;&lt;h4 id=&#34;基于分类器的方法&#34;&gt;基于分类器的方法
&lt;/h4&gt;&lt;p&gt;训练用于判别数据质量的文本分类器，进行预训练语料的清洗。&lt;/p&gt;
&lt;p&gt;可以将维基百科等高质量数据作为正样本，同时从网页中筛选出含有不良内容或低质量数据的样本作为负样本。利用这个训练好的文本分类器，能够精准地识别和过滤低质量数据，从而显著提升整个语料库的质量。&lt;/p&gt;
&lt;p&gt;注意：基于分类器的方法也可能无意中删除一些低资源但高质量的文本，如文言文数据等，数据清洗人员需要意识到这种情况，并且建立合理的数据召回与保留机制。&lt;/p&gt;
&lt;p&gt;目前常用来实现分类器的方法包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;轻量级模型（如FastText等）&lt;/li&gt;
&lt;li&gt;可微调的预训练语言模型（如BERT、BART或者LLaMA等）&lt;/li&gt;
&lt;li&gt;闭源大语言模型API（如 GPT-4、Claude 3）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这三个方法各自具有不同的优缺点：轻量级模型效率较高，但是分类的准确率和精度可能受限于模型能力；预训练语言模型可以针对性微调， 但是分类性能的通用性和泛化性仍然有一定的限制；闭源大语言模型的能力较强， 但是无法灵活针对任务进行适配，而且用于预训练数据清洗需要花费较高的成本。 对于后两种方法来说，除了简单地进行数据过滤，还可以针对性进行数据的改写，从而使得一些整体质量还不错、但存在局部数据问题的文本仍然可以被保留下来使用。&lt;/p&gt;
&lt;p&gt;效率问题：&lt;/p&gt;
&lt;p&gt;启发式规则写得好，过滤起来快又好，分类器更精准但是慢，可以结合一下，首先利用启发式规则进行初步筛选，以快速排除不符合要求的文档，随后再采用分类器方法进一步精细过滤，确保最终筛选出的语料具有较好的文本质量。在这一过程中，还可以同时应用多种分类器，可以先使用轻量级分类器进行数据过滤，进而使用更为有效但是资源消耗更高的分类器在粗滤后的数据上再次进行选择。&lt;/p&gt;
&lt;p&gt;这看起来怎么似曾相识呢？果然计算机里面好多知识都是互通的。&lt;/p&gt;
&lt;h3 id=&#34;敏感内容过滤&#34;&gt;📚敏感内容过滤
&lt;/h3&gt;&lt;p&gt;与质量过滤类似，不同类型的数据内容往往需要采用特定的过滤规则。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;过滤有毒内容：为了精确过滤含有有毒内容的文本，可以采用基于分类器的过滤方法。Jigsaw评论数据集[16]提供了用于训练毒性分类器的数据。该数据集收集了近160K条论坛评论数据，每条评论都经过细致的标注，包括“有毒”、“严重有毒”、“有威胁”、“侮辱性”、“暴力”以及“身份仇恨”等六个类别。利用这一数据集进行训练，可以构建出高效的毒性文本分类器。通过设置合理的阈值，训练完成的分类器将能够有效识别并过滤掉含有有毒内容的信息。在进行分类阈值设置时，需要在精确度和召回率之间寻求平衡，避免过多或者过少去除候选数据。Dolma的技术报告[17]指出，使用高阈值时去除的数据会过少，语料中未过滤掉的有毒内容会导致模型在下游任务上的性能下降；而低阈值则会过滤更多的有毒内容，但同时也会造成大量数据的浪费。考虑到后续的预处理操作（如质量筛选、去重等）同样能够有效剔除有害内容，Dolma选择为分类器设定了一个相对较高的阈值（0.4），从而保留更多的候选数据。最终，Dolma在这一阶段仅过滤了CommonCrawl中30%左右的数据。&lt;/li&gt;
&lt;li&gt;过滤隐私内容：预训练文本数据大多来自互联网，其中可能包括用户生成的敏感信息或可识别的个人信息（Personally Identifiable Information, PII），如姓名、地址和电话号码等。这些信息如果不加处理，将增加隐私泄露的潜在风险。例如，在2023年11月有用户发现，反复要求ChatGPT重复某个单词可能会使其无意间泄露训练数据中的个人隐私信息，这个漏洞现在已经修复。因此，在预处理阶段，需要去除这些可识别的个人信息。一种直接且有效的方法是使用启发式方法，如关键字识别，来检测和删除这些私人信息[18]。Dolma采用了基于规则的方法来过滤数据集中的隐私内容，主要标注了三类敏感信息：邮箱地址、IP地址以及电话号码。在文本收集过程中，一旦检测到这些隐私信息，Dolma会根据其出现的频率采取不同的处理策略。具体来说，如果某个文档中的隐私信息少于五条，Dolma 会使用特定的词元（如“|||EMAIL_ADDRESS|||”、“|||PHONE_NUMBER|||” 和“|||IP_ADDRESS|||”）来替换这些信息，以保护用户的隐私。然而，如果文档中的隐私信息达到六条或更多，Dolma会选择直接删除整个文档。这是因为当文档中频繁出现隐私信息时，很可能还隐藏着其他未标注的敏感内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;数据去重&#34;&gt;📈数据去重
&lt;/h3&gt;&lt;p&gt;由于大语言模型具有较强的数据拟合与记忆能力，很容易习得训练数据中的重复模式，可能导致对于这些模式的过度学习。总体来说，去重算法的设计可以基于不同的计算粒度以及匹配方法。&lt;/p&gt;
&lt;p&gt;计算粒度.去重可以在句子级别、文档级别和数据集级别等多种粒度上进行。现有方法主要依靠单词或 n 元词组的重叠这类表层特征，来衡量文档的重叠比率，进而检测和删除包含相似 内容的重复文档。现有的数据集往往采用多阶段、多粒度的方式来实现高效的去重。首先针对数据集和文档级别进行去重，旨在去除那些具有高度相似甚至完全一致内容的文档，例如多个URL可能具有相同的网页内容，或者网页数据集和新闻数据集中包含相同的新闻文档。随后，可以进一步在句子级别实现更为精细的去重。例如，可以计算两个句子之间公共子串的长度（公共子串长度，好熟悉，算法无处不在。），当其长度过长时直接删除某一个句子。&lt;/p&gt;
&lt;p&gt;用于去重的匹配方法.在去重过程中，可以使用精确匹配算法（即每个字符完全相同）或近似匹配算法（基于某种相似性度量）[19]。对于精确匹配来说，通常使用后缀数组来匹配最小长度的完全相同子串[20]。对于近似匹配来说，可以采用局部敏感哈希（Locality-SensitiveHashing,LSH）算法，如最小哈希（MinHash） 来实现。考虑到预训练数据集合的规模非常大，实现中可以综合考虑去重效率和去重效果之间的权衡。例如，RefinedWeb在文档层面采用了开销较小的近似匹配技术来实现去重，而在句子层面则采用了精确匹配算法来确保去重的准确性。&lt;/p&gt;
&lt;h2 id=&#34;词元化&#34;&gt;⭐词元化
&lt;/h2&gt;&lt;p&gt;词元化（Tokenization）是数据预处理中的一个关键步骤，旨在将原始文本分割成模型可识别和建模的词元序列，作为大语言模型的输入数据。&lt;/p&gt;
&lt;h3 id=&#34;bpe分词&#34;&gt;🌼BPE分词
&lt;/h3&gt;&lt;p&gt;BPE算法从一组基本符号（例如字母和边界字符）开始，迭代地寻找语料库中的两个相邻词元，并将它们替换为新的词元，这一过程被称为合并。合并的选择标准是计算两个连续词元的共现频率，也就是每次迭代中，最频繁出现的一对词元会被选择与合并。合并过程将一直持续达到预定义的词表大小。&lt;/p&gt;
&lt;h4 id=&#34;bpe算法的代码如下&#34;&gt;BPE算法的代码如下
&lt;/h4&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;40
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;collectionsimportdefaultdict&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;extract_frequencies&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sequence&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;c1&#34;&gt;#给定一个字符串，计算字符串中的单词出现的频率，并返回词表（一个词到频率的映射字典）。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;token_counter&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Counter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;item&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;insequence&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;tokens&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;item&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;lt;/w&amp;gt;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;token_counter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tokens&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;token_counter&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;frequency_of_pairs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;frequencies&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#给定一个词频字典，返回一个从字符对到频率的映射字典。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;pairs_count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Counter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;token&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;frequencies&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;items&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;chars&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;token&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inrange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;pair&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;pairs_count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pair&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pairs_count&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;merge_vocab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;merge_pair&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vocab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;c1&#34;&gt;#给定一对相邻词元和一个词频字典，将相邻词元合并为新的词元，并返回新的词表。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;re_pattern&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;escape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;merge_pair&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;pattern&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;compile&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;(?&amp;lt;!\S)&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re_pattern&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;(?!\S)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;updated_tokens&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pattern&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sub&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;merge_pair&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;token&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;freq&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;token&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;freq&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;invocab&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;items&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;updated_tokens&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;encode_with_bpe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;texts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iterations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;#给定待分词的数据以及最大合并次数，返回合并后的词表。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;vocab_map&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;extract_frequencies&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;texts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iterations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;pair_freqs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;frequency_of_pairs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vocab_map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pair_freqs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;most_common_pair&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pair_freqs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_common&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;vocab_map&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;merge_vocab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_common_pair&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vocab_map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vocab_map&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;num_merges&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;bpe_pairs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode_with_bpe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_merges&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;字节级别的BPE（Byte-levelBPE,B-BPE）是BPE算法的一种拓展。它将字节视为合并操作的基本符号，从而可以实现更细粒度的分割，且解决了未登录词问题。采用这种词元化方法的代表性语言模型包括GPT-2、BART和LLaMA。具体来说，如果将所有Unicode字符都视为基本字符，那么包含所有可能基本字符的基本词表会非常庞大（例如将中文的每个汉字当作一个基本字符）。而将字节作为基本词表可以设置基本词库的大小为256，同时确保每个基本字符都包含在词汇中。例如，GPT-2的词表大小为50,257，包括256个字节的基本词元、一个特殊的文末词元以及通过50,000次合并学习到的词元。&lt;/p&gt;
&lt;h4 id=&#34;bpe算法的具体流程示例&#34;&gt;BPE算法的具体流程示例
&lt;/h4&gt;&lt;p&gt;假设语料中包含了五个英文单词： “loop”，“pool”，“loot”，“tool”，“loots”&lt;/p&gt;
&lt;p&gt;在这种情况下，BPE假设的初始词汇表即为： [“l”,“o”,“p”,“t”,“s”]&lt;/p&gt;
&lt;p&gt;在实践中，基础词汇表可以包含所有ASCII字符，也可能包含一些Unicode字符 （比如中文的汉字）。如果正在进行分词的文本中包含了训练语料库中没有的字 符，则该字符将被转换为未知词元（如“&amp;lt;UNK&amp;gt;”）。 假设单词在语料库中的频率如下： （“loop”，15），（“pool”，10），（“loot”，10），（“tool”，5），（“loots”，8）&lt;/p&gt;
&lt;p&gt;其中，出现频率最高的是“oo”，出现了48次，因此，学习到的第一条合并规则 是（“o”,“o”）→“oo”，这意味着“oo”将被添加到词汇表中，并且应用这一 合并规则到语料库的所有词汇。在这一阶段结束时，词汇和语料库如下所示： 词汇：[“l”,“o”,“p”,“t”,“s”,“oo”] 语料库：（“l” “oo” “p”，15），（“p” “oo” “l”，10），（“l” “oo” “t”， 10），（“t” “oo” “l”，5），（“l” “oo” “t” “s”，8）&lt;/p&gt;
&lt;p&gt;此时，出现频率最高的配对是（“l”，“oo”），在语料库中出现了33次，因此学习 到的第二条合并规则是（“l”，“oo”）→“loo”。将其添加到词汇表中并应用到所 有现有的单词，可以得到： 词汇：[“l”,“o”,“p”,“t”,“s”,“oo”,“loo”] 语料库：（“loo” “p”，15），（“p” “oo” “l”，10），（“loo” “t”，10），（“t” “oo” “l”， 5），（“loo” “t” “s”，8）&lt;/p&gt;
&lt;p&gt;现在，最常出现的词对是（“loo”,“t”），因此可以学习合并规则（“loo”,“t”） →“loot”，这样就得到了第一个三个字母的词元： 词汇：[“l”,“o”,“p”,“t”,“s”,“oo”,“loo”,“loot”] 语料库：（“loo” “p”，15），（“p” “oo” “l”，10），（“loot”，10），（“t” “oo” “l”， 5），（“loot” “s”，8） 可以重复上述过程，直到达到所设置的终止词汇量。&lt;/p&gt;
&lt;h3 id=&#34;wordpiece-分词&#34;&gt;🎵WordPiece 分词
&lt;/h3&gt;&lt;p&gt;WordPiece分词和BPE分词的想法非常相似，都是通过迭代合并连续的词元，但是合并的选择标准略有不同。在合并前，WordPiece分词算法会首先训练一个语言模型，并用这个语言模型对所有可能的词元对进行评分。然后，在每次合并时，它都会选择使得训练数据的似然性增加最多的词元对。&lt;/p&gt;
&lt;p&gt;谷歌并未发布WordPiece分词算法的官方实现&lt;/p&gt;
&lt;p&gt;与BPE类似，WordPiece 分词算法也是从一个小的词汇表开始，其中包括模型使用的特殊词元和初始词汇表。由于它是通过添加前缀（如BERT的##）来识别子词的，因此每个词的初始拆分都是将前缀添加到词内的所有字符上。举例来说，“word”会被拆分为：“w##o ##r ##d”。与BPE方法的另一个不同点在于，WordPiece分词算法并不选择最频繁的词对，而是使用下面的公式为每个词对计算分数：
&lt;/p&gt;
$$
得分 = \frac{\text{词对出现的频率}}{\text{第一个词出现的频率} \times \text{第二个词出现的频率}}
$$&lt;h3 id=&#34;unigram-分词&#34;&gt;🐉Unigram 分词
&lt;/h3&gt;&lt;p&gt;与BPE分词和WordPiece 分词不同，Unigram 分词方法 [21] 从语料库的一组足够大的字符串或词元初始集合开始，迭代地删除其中的词元，直到达到预期的词表大小。它假设从当前词表中删除某个词元，并计算训练语料的似然增加情况，以此来作为选择标准。这个步骤是基于一个训练好的一元语言模型来进行的。为估计一元语言模型，它采用期望最大化（Expectation–Maximization,EM）算法：在每次迭代中，首先基于旧的语言模型找到当前最优的分词方式，然后重新估计一元概率从而更新语言模型。这个过程中一般使用动态规划算法（即维特比算法，Viterbi Algorithm）来高效地找到语言模型对词汇的最优分词方式。采用这种分词方法的代表性模型包括T5和mBART。&lt;/p&gt;
&lt;h2 id=&#34;数据调度&#34;&gt;🍌数据调度
&lt;/h2&gt;&lt;p&gt;数据调度（DataScheduling）主要关注两个方面：各个数据源的混合比例以及各数据源用于训练的顺序（称为数据课程，Data Curriculum）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/pretraining-data-preparation-for-large-models/image-20241028165311963.png&#34;
	width=&#34;684&#34;
	height=&#34;252&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/pretraining-data-preparation-for-large-models/image-20241028165311963_hu14172581646839069204.png 480w, https://JiangZhiyu-1024.github.io/p/pretraining-data-preparation-for-large-models/image-20241028165311963_hu10794267152505365926.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241028165311963&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;271&#34;
		data-flex-basis=&#34;651px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;预训练大语言模型时数据调度的示意图&lt;/center&gt;
&lt;h3 id=&#34;数据混合&#34;&gt;🎈数据混合
&lt;/h3&gt;&lt;p&gt;在预训练期间，将根据混合比例从不同数据源中采样数据：数据源的权重越大，从中选择的数据就越多。进一步，可能会对每个数据源的全部数据进行上采样或下采样，以创建特定的数据混合集合作为预训练数据。&lt;/p&gt;
&lt;h3 id=&#34;数据课程&#34;&gt;🎉数据课程
&lt;/h3&gt;&lt;p&gt;除了设置有效的数据混合配比外，在训练过程中对于预训练数据的顺序进行合适的安排也比较重要。具体来说，数据课程是指按照特定的顺序安排预训练数据进行模型的训练。例如，从简单/通用的数据开始，逐渐引入更具挑战性/专业化的数据。更广泛地说，它可以指训练期间在不同阶段使用不同的数据源混合配比。为了设定合适的数据课程，一种实用方法是基于专门构建的评测基准监控大语言模型的关键能力的学习过程，然后在预训练期间动态调整数据的混合配比。&lt;/p&gt;
&lt;h2 id=&#34;参考文献&#34;&gt;🧨参考文献
&lt;/h2&gt;&lt;p&gt;[1]. Alec Radford et al. “Language models are unsupervised multitask learners”. In: OpenAI Blog (2019).&lt;/p&gt;
&lt;p&gt;[2]. Colin Raffel et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”. In: J. Mach. Learn. Res. (2020).&lt;/p&gt;
&lt;p&gt;[3]. JiantaoQiuetal. “WanJuan-CC:ASafeandHigh-QualityOpen-sourcedEnglish Webtext Dataset”. In: arXiv preprint arXiv:2402.19282 (2024).&lt;/p&gt;
&lt;p&gt;[4]. Trieu H. Trinh and Quoc V. Le. “A Simple Method for Commonsense Reasoning”. In: arXiv preprint arXiv:1806.02847 (2018).&lt;/p&gt;
&lt;p&gt;[5]. LeoGaoetal.“ThePile: An 800GBDataset of Diverse Text for Language Modeling”. In: arXiv preprint arXiv:2101.00027 (2021).&lt;/p&gt;
&lt;p&gt;[6]. Teven Le Scao et al. “BLOOM: A 176B-Parameter Open-Access Multilingual Language Model”. In: arXiv preprint arXiv:2211.05100 (2022).&lt;/p&gt;
&lt;p&gt;[7]. Aakanksha Chowdhery et al. “PaLM: Scaling Language Modeling with Pathways”. In: arXiv preprint arXiv:2204.02311 (2022).&lt;/p&gt;
&lt;p&gt;[8]. Tarek Saier, Johan Krause, and Michael Färber. “unarXive 2022: All arXiv Publications Pre-Processed for NLP, Including Structured Full-Text and Citation Network”. In: arXiv preprint arXiv:2303.14957 (2023).&lt;/p&gt;
&lt;p&gt;[9]. MarkChenetal.“EvaluatingLargeLanguageModelsTrainedonCode”.In:arXivpreprint arXiv:2107.03374 (2021).&lt;/p&gt;
&lt;p&gt;[10]. YujiaLietal.“Competition-LevelCodeGenerationwithAlphaCode”.In:Science(2022).&lt;/p&gt;
&lt;p&gt;[11]. Yingwei Ma et al. “At Which Training Stage Does Code Data Help LLMs Reasoning?” In: arXiv preprint arXiv:2309.16298 (2023).&lt;/p&gt;
&lt;p&gt;[12]. Ke Yang et al. “If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents”. In: arXiv preprint ar Xiv:2401.00812 (2024).&lt;/p&gt;
&lt;p&gt;[13]. AmanMadaan et al. “Language Models of Code are Few-Shot Commonsense Learners”. In: EMNLP. 2022.&lt;/p&gt;
&lt;p&gt;[14]. Yuhuai Wu et al. “Autoformalization with Large Language Models”. In: arXiv preprint a rXiv:2205.12615 (2022).&lt;/p&gt;
&lt;p&gt;[15].  JackW.Raeetal.“ScalingLanguageModels:Methods,Analysis&amp;amp;InsightsfromTraining Gopher”. In: arXiv preprint arXiv:2112.11446 (2021).&lt;/p&gt;
&lt;p&gt;[16]. CJ Adams et al. Toxic comment classification challenge, 2017. &lt;a class=&#34;link&#34; href=&#34;https://kaggle.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kaggle.com/&lt;/a&gt; competitions/jigsaw-toxic-comment-classification-challenge. 2017.&lt;/p&gt;
&lt;p&gt;[17]. Luca Soldaini et al. “Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research”. In: arXiv preprint arXiv:2402.00159 (2024).&lt;/p&gt;
&lt;p&gt;[18]. Hugo Laurençon et al. “The bigscience roots corpus: A 1.6 tb composite multilingual dataset”. In: Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022.&lt;/p&gt;
&lt;p&gt;[19]. Guilherme Penedo et al. “The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only”. In: arXiv preprint arXiv:2306.01116 (2023).&lt;/p&gt;
&lt;p&gt;[20]. Udi Manber and Eugene W. Myers. “Suffix Arrays: A New Method for On-Line String Searches”. In: SIAM J. Comput. (1993).&lt;/p&gt;
&lt;p&gt;[21]. Taku Kudo. “Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates”. In: ACL. 2018.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>2024.10.21至27第三周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/third-week/</link>
        <pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/third-week/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/third-week/cards-8594729_640.jpg" alt="Featured image of post 2024.10.21至27第三周工作总结" /&gt;&lt;h1 id=&#34;文献阅读&#34;&gt;文献阅读
&lt;/h1&gt;&lt;h5 id=&#34;每周都完不成上次的任务杂事太多了尽快完成&#34;&gt;每周都完不成上次的任务，杂事太多了，尽快完成：
&lt;/h5&gt;&lt;h2 id=&#34;detecting-formal-thought-disorder-by-deep-contextualized-word-representations&#34;&gt;Detecting formal thought disorder by deep contextualized word representations
&lt;/h2&gt;&lt;p&gt;竟然要收费，不看了，让gpt给讲一下&lt;/p&gt;
&lt;p&gt;这篇论文《通过深度上下文化词表示检测形式思维障碍》研究了如何使用深度学习中的上下文词嵌入模型（如ELMo）来检测精神疾病患者的形式思维障碍（Formal Thought Disorder, FTD）。FTD是一种影响语言表达和思维逻辑的精神疾病症状，常见于精神分裂症患者。&lt;/p&gt;
&lt;p&gt;论文主要利用深度上下文化词嵌入模型，如ELMo，通过生成词在句子中的嵌入表示，捕捉语境中词的多义性和丰富的语义信息。研究团队通过分析患者的语言数据，构建模型来区分正常语言与存在思维障碍的语言表达，从而实现自动化的FTD检测。&lt;/p&gt;
&lt;p&gt;研究的核心贡献是使用更复杂的语义表示（如上下文化词嵌入）来解决传统方法无法有效处理的词义歧义问题。&lt;/p&gt;
&lt;h2 id=&#34;distributed-representations-of-words-and-phrases-and-their-compositionality&#34;&gt;Distributed Representations of Words and Phrases and their Compositionality
&lt;/h2&gt;&lt;p&gt;《Distributed Representations of Words and Phrases and their Compositionality》是由Mikolov等人在2013年提出的一篇重要论文，介绍了&lt;strong&gt;Word2Vec&lt;/strong&gt;模型的改进和创新。这篇文章主要讨论了如何通过分布式词表示（distributed word representations）更好地捕捉单词和短语的语义信息，并且提出了两种核心模型：&lt;strong&gt;CBOW（Continuous Bag of Words）&lt;/strong&gt; 和  &lt;strong&gt;Skip-gram&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这篇论文的主要贡献包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Skip-gram和CBOW模型&lt;/strong&gt;：论文提出了两种用于生成词向量的架构。Skip-gram模型的目标是根据一个词预测其上下文，而CBOW模型则根据上下文词来预测目标词。这两种模型能够有效地捕捉单词之间的语义关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;层次Softmax和负采样&lt;/strong&gt;：为了提高训练大规模语料库的效率，作者引入了&lt;strong&gt;层次Softmax&lt;/strong&gt;和**负采样（negative sampling）**技术，这大幅降低了模型的计算复杂度，使得大规模训练变得可行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;词和短语的组合性&lt;/strong&gt;：论文不仅处理了单词的表示，还处理了短语的表示，通过数据驱动的方式将多词短语映射为词向量，捕捉更复杂的语义结构。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;向量运算反映语义关系&lt;/strong&gt;：Word2Vec生成的词向量能够通过简单的向量运算表达词语之间的关系。例如，向量运算 &lt;code&gt;vec(&amp;quot;King&amp;quot;) - vec(&amp;quot;Man&amp;quot;) + vec(&amp;quot;Woman&amp;quot;) ≈ vec(&amp;quot;Queen&amp;quot;)&lt;/code&gt; 说明了这种分布式表示在捕捉语义关系上的强大能力。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;llm-book&#34;&gt;LLM BOOK
&lt;/h2&gt;&lt;p&gt;找到一本书，准确的来说是一个github仓库，组织者把近些年的LLM相关的东西都整进去了，好全，写的真好，库库看吧那就&lt;/p&gt;
&lt;p&gt;先把资源放在这里：&lt;a class=&#34;link&#34; href=&#34;https://github.com/RUCAIBox/LLMSurvey/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RUCAIBox/LLMSurvey: The official GitHub page for the survey paper &amp;ldquo;A Survey of Large Language Models&amp;rdquo;.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;第一部分-背景与基础知识&#34;&gt;第一部分 背景与基础知识
&lt;/h3&gt;&lt;h4 id=&#34;语言模型的发展历程p16&#34;&gt;语言模型的发展历程（P16）
&lt;/h4&gt;&lt;p&gt;LLM大语言模型那肯定是从LM语言模型来的，语言模型的发展又经历了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;统计语言模型（StatisticalLanguageModel, SLM）&lt;/li&gt;
&lt;li&gt;神经语言模型（NeuralLanguageModel,NLM）&lt;/li&gt;
&lt;li&gt;预训练语言模型（Pre-trainedLanguageModel,PLM）&lt;/li&gt;
&lt;li&gt;大语言模型（LargeLanguageModel, LLM）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;llm的特点p19&#34;&gt;LLM的特点（P19）
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;具有较为丰富的世界知识&lt;/li&gt;
&lt;li&gt;具有较强的通用任务解决能力&lt;/li&gt;
&lt;li&gt;具有较好的复杂任务推理能力&lt;/li&gt;
&lt;li&gt;具有较强的人类指令遵循能力&lt;/li&gt;
&lt;li&gt;具有较好的人类对齐能力&lt;/li&gt;
&lt;li&gt;具有可拓展的工具使用能力&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;大语言模型对于产业应用带来了变革性的技术影响，将会催生一个基于大语言模型的应用生态系统。&lt;/p&gt;
&lt;h3 id=&#34;第二部分-预训练&#34;&gt;第二部分 预训练
&lt;/h3&gt;&lt;p&gt;大语言模型的第一个训练阶段是预训练，预训练语料的规模和质量对于提升大语言模型的能力至关重要。&lt;/p&gt;
&lt;h4 id=&#34;数据来源&#34;&gt;数据来源
&lt;/h4&gt;&lt;p&gt;通用文本数据和专用文本数据。通用文本数据涵盖了网页、书籍和对话文本等。由于通用文本数据规模较大、多样性强且易于获取，大多数大语言模型都会收集大量的通用文本数据，以增强其语言建模能力。此外，为了进一步提升大语言模型在特定专业任务上的表现，人们还将预训练语料的范围扩展至更专业的数据集，如多语数据、科学数据和代码数据等。&lt;/p&gt;
&lt;h5 id=&#34;通用文本数据&#34;&gt;通用文本数据
&lt;/h5&gt;&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026154722061.png&#34;
	width=&#34;792&#34;
	height=&#34;657&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026154722061_hu1725408825362406164.png 480w, https://JiangZhiyu-1024.github.io/p/third-week/image-20241026154722061_hu12406635621069597218.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241026154722061&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;120&#34;
		data-flex-basis=&#34;289px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;现有大语言模型预训练数据中各种数据来源的比例分布图&lt;/center&gt;&gt;
&lt;h5 id=&#34;专用文本数据&#34;&gt;专用文本数据
&lt;/h5&gt;&lt;p&gt;多语文本、 科学文本、代码。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026155259857.png&#34;
	width=&#34;831&#34;
	height=&#34;189&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026155259857_hu14579868356241669728.png 480w, https://JiangZhiyu-1024.github.io/p/third-week/image-20241026155259857_hu13508329583982414925.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241026155259857&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;439&#34;
		data-flex-basis=&#34;1055px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;典型的预训练数据预处理流程图&lt;/center&gt;&gt;
&lt;h4 id=&#34;数据预处理&#34;&gt;数据预处理
&lt;/h4&gt;&lt;p&gt;构建并使用系统化的数据处理框架（如开源库Data-Juicer）&lt;/p&gt;
&lt;p&gt;具体操作（P74）&lt;/p&gt;
&lt;h1 id=&#34;附录&#34;&gt;附录
&lt;/h1&gt;&lt;h2 id=&#34;softmax是啥&#34;&gt;softmax是啥？
&lt;/h2&gt;&lt;h4 id=&#34;softmax是一种常用的函数特别是在多分类任务中用来将一个向量中的元素转换为0到1之间的概率分布其核心作用是&#34;&gt;Softmax是一种常用的函数，特别是在&lt;strong&gt;多分类任务&lt;/strong&gt;中，用来将一个向量中的元素转换为0到1之间的概率分布。其核心作用是：
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;将实数映射为概率分布&lt;/strong&gt;：Softmax 接受一个实数向量，输出一个与输入维度相同的向量，其中每个元素的值表示该元素作为某个类别的概率，且这些概率的和为1。这样就可以根据这些概率来进行分类预测。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;公式&lt;/strong&gt;：Softmax 函数的定义如下：
&lt;/p&gt;
$$
   \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
   $$&lt;p&gt;
其中，\( z_i \) 是输入向量中的某个元素，\( e^{z_i} \) 是该元素的指数值，分母是所有输入元素的指数值的和。这个函数将每个 \( z_i \) 映射为一个在 \( (0,1) \) 之间的概率，并且所有输出的概率之和为1。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;作用机制&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;指数放大效果&lt;/strong&gt;：Softmax 函数通过对输入值取指数（\( e^x \)），会将较大的输入值放大更多，而较小的值则被进一步压缩，突出更大的概率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;归一化&lt;/strong&gt;：Softmax 保证所有输出的概率之和为1，符合概率的定义。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;应用场景&lt;/strong&gt;：在&lt;strong&gt;神经网络的最后一层&lt;/strong&gt;，特别是分类任务中，Softmax 常用于多类分类问题，比如在自然语言处理（NLP）中的词分类、图像识别中的对象分类等。它将网络的输出转化为易于理解的概率值，帮助模型决定输入属于哪个类别。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;举例&#34;&gt;举例
&lt;/h4&gt;&lt;p&gt;假设我们有一个模型输出三个值 \( [2.0, 1.0, 0.1] \)，通过 Softmax 转换后得到的结果是大约 \( [0.71, 0.26, 0.03] \)，其中最大的值0.71表示模型认为输入属于第一个类别的概率最大。&lt;/p&gt;
&lt;p&gt;简言之，Softmax 是一种将输出转换为概率分布的工具，特别适合多分类问题的场景。&lt;/p&gt;
&lt;h2 id=&#34;看了半天论文感觉没有一个时间线很难受不清楚那些论文之间的关系问问gpt&#34;&gt;看了半天论文，感觉没有一个时间线很难受，不清楚那些论文之间的关系，问问gpt：
&lt;/h2&gt;&lt;h3 id=&#34;基础概念与理论&#34;&gt;&lt;strong&gt;基础概念与理论&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;先掌握大模型背后的基本理论，这些知识是理解后续论文的基础：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语言模型的基础：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;经典论文：《Attention Is All You Need》（Transformer）&lt;/li&gt;
&lt;li&gt;相关概念：自注意力机制、序列到序列模型、词嵌入（如Word2Vec、GloVe）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自回归模型与自编码器：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;GPT 系列（GPT, GPT-2, GPT-3）的原理和应用&lt;/li&gt;
&lt;li&gt;BERT 及其衍生模型的预训练与微调方式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;扩展学习与生成式任务：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》&lt;/li&gt;
&lt;li&gt;生成式预训练模型的设计与任务应用（如文本生成、机器翻译）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;模型的训练与优化&#34;&gt;&lt;strong&gt;模型的训练与优化&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;学习如何高效地训练大模型，并且了解模型的优化技术：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;微调与参数高效训练：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Efficient Fine-Tuning Techniques for Large Models: Adapter, LoRA 等&lt;/li&gt;
&lt;li&gt;探索模型压缩技术（知识蒸馏、量化、剪枝等）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对比学习与自监督学习：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;《SimCLR》、《MoCo》 等代表性论文，理解对比学习在大模型中的应用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;模型对齐与安全&#34;&gt;&lt;strong&gt;模型对齐与安全&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;在大模型发展过程中，模型对齐（alignment）和伦理问题变得越来越重要：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型对齐与价值观嵌入：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;《InstructGPT: Training language models to follow instructions with human feedback》&lt;/li&gt;
&lt;li&gt;RLHF（强化学习通过人类反馈）等对齐策略&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型安全与公平性：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;探讨模型偏见问题以及在实际应用中的安全隐患&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;视觉-语言模型与多模态模型&#34;&gt;&lt;strong&gt;视觉-语言模型与多模态模型&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;学习大模型在跨模态任务中的应用，比如视觉-语言任务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CLIP（Contrastive Language-Image Pretraining）&lt;/strong&gt;：理解图像和文本共同表征的训练方法&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DALL-E、Stable Diffusion&lt;/strong&gt;：生成式视觉任务的模型设计与应用&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;视觉语言对话系统&lt;/strong&gt;：如《Visual ChatGPT》&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;前沿研究与新趋势&#34;&gt;&lt;strong&gt;前沿研究与新趋势&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;在深入理解基本模型后，可以关注大模型领域的前沿研究方向：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;大模型与知识图谱的结合&lt;/strong&gt;：如何将知识图谱融入语言模型&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;领域自适应模型&lt;/strong&gt;：如何通过特定领域数据进行模型的进一步微调&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;检索增强生成（RAG）&lt;/strong&gt;：如《RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;实战应用与开源框架&#34;&gt;&lt;strong&gt;实战应用与开源框架&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;阅读开源项目中的技术文档，参与开源项目（如Hugging Face的模型库）&lt;/li&gt;
&lt;li&gt;学习如何应用大模型解决实际问题，并尝试训练和微调自己的模型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样，你可以从基础概念、模型训练到前沿研究和实战应用，逐步深入，既能掌握理论又能结合实践。这个过程可以帮助你筛选出重要的论文和研究方向。&lt;/p&gt;
&lt;h2 id=&#34;对齐是什么意思&#34;&gt;对齐是什么意思？
&lt;/h2&gt;&lt;p&gt;在大型语言模型（LLM）中，“对齐”通常指的是模型的输出与人类意图或价值观的匹配程度。具体来说，它涉及确保模型在生成文本时能够产生符合人类期望的、有益的和安全的内容。对齐的目标是减少模型生成有害、误导或不准确的信息。&lt;/p&gt;
&lt;p&gt;对齐可以通过多种方法实现，包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练数据的选择和清洗&lt;/strong&gt;：使用高质量的数据集进行训练，确保模型接触到的内容符合人类的价值观和期望。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;强化学习&lt;/strong&gt;：利用人类反馈来微调模型，使其在生成内容时更符合人类的意图。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;安全性和伦理性评估&lt;/strong&gt;：在模型发布之前，对其输出进行评估，以识别和修正潜在的偏见或有害内容。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过这些方法，可以提高模型在特定任务或应用场景中的可靠性和安全性。&lt;/p&gt;
&lt;h2 id=&#34;arxiv-论文是什么&#34;&gt;arXiv 论文是什么？
&lt;/h2&gt;&lt;p&gt;arXiv 是一个开放获取的电子预印本存档，主要用于科学研究领域，特别是在物理学、数学、计算机科学、统计学、金融学、生物学和其他相关学科。研究者可以在 arXiv 上免费上传和分享他们的研究论文，通常是在正式出版之前的版本。&lt;/p&gt;
&lt;p&gt;arXiv 的特点包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;快速发布&lt;/strong&gt;：研究者可以迅速分享他们的研究成果，而不必等待同行评审和正式出版的过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;开放获取&lt;/strong&gt;：任何人都可以访问和下载论文，无需订阅或支付费用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;广泛的学科覆盖&lt;/strong&gt;：涵盖多个领域，促进跨学科的交流和合作。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;社区反馈&lt;/strong&gt;：研究者可以在正式发表之前收到来自社区的反馈，帮助他们改进论文。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;arXiv 的论文通常以“预印本”的形式存在，因此它们可能尚未经过同行评审。这意味着在引用这些论文时，研究者需要谨慎，了解其可能的局限性。&lt;/p&gt;
&lt;h2 id=&#34;在arxiv上发还未出版的论文不怕被别人剽窃成果吗&#34;&gt;在arXiv上发还未出版的论文不怕被别人剽窃成果吗？
&lt;/h2&gt;&lt;p&gt;在 arXiv 上发布未出版的论文确实存在被剽窃的风险，但研究者通常认为在 arXiv 上共享研究成果是有益的，原因如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;时间戳和公开记录&lt;/strong&gt;：一旦论文被上传到 arXiv，上传的时间戳可以作为知识产权的证明。这在某种程度上可以保护研究者的原创性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;学术社区的支持&lt;/strong&gt;：arXiv 是一个活跃的学术社区，研究者通常会关注相关领域的论文，并且对剽窃行为持有负面态度。学术界普遍重视诚信，剽窃通常会受到严厉的谴责。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;早期反馈&lt;/strong&gt;：通过发布预印本，研究者可以获得来自同行的反馈，从而改进自己的工作。这也有助于在正式出版之前建立声誉。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;增加曝光率&lt;/strong&gt;：在 arXiv 上发布可以提高研究的可见性，吸引潜在合作者或资助者。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;通用人工智能系统是什么&#34;&gt;通用人工智能系统是什么？
&lt;/h2&gt;&lt;p&gt;通用人工智能系统（AGI, Artificial General Intelligence）指的是一种能够理解、学习和应用知识，完成任何人类所能完成的智力任务的人工智能系统。与当前的狭义人工智能（Narrow AI）相比，后者在特定任务或领域表现出色（如图像识别、自然语言处理等），AGI 的目标是实现一种更广泛的智能，具备跨领域的学习和适应能力。&lt;/p&gt;
&lt;p&gt;AGI 的特点包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;自主学习&lt;/strong&gt;：能够从有限的数据中学习并推理，具备自我改进的能力。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多任务处理&lt;/strong&gt;：能够在多个不同的领域和任务中表现出色，而不局限于特定的应用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;常识推理&lt;/strong&gt;：具备理解和应用常识的能力，能够处理复杂的、不确定的环境。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;灵活性和适应性&lt;/strong&gt;：能够适应新情况和挑战，灵活地应对不同的环境和需求。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;AGI 的发展仍然是一个前沿研究领域，目前尚未实现。许多研究者在探索如何构建这样的系统，同时也讨论其潜在的伦理和社会影响，例如对就业、隐私和安全的影响。&lt;/p&gt;
&lt;h1 id=&#34;思考&#34;&gt;思考
&lt;/h1&gt;&lt;p&gt;随着看的东西多了，和使用chatgpt，我隐隐感觉到，由于chatgpt的语言模型主要是通过预测下一个最可能的词来生成文本，这一过程称为自回归生成（autoregressive generation）。导致gpt经常会顺着你的话去说，即有更小的概论去反对你的话，尽量让你说的话看起来正确，当然，如果你说的是：“太阳从西边升起”，那么他会义无反顾的反驳你。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>记第一次创建我的个人博客</title>
        <link>https://JiangZhiyu-1024.github.io/p/firstblog/</link>
        <pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/firstblog/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/firstblog/liuying5.jpg" alt="Featured image of post 记第一次创建我的个人博客" /&gt;&lt;h1 id=&#34;第一次成功创建个人博客&#34;&gt;第一次成功创建个人博客
&lt;/h1&gt;&lt;h2 id=&#34;回忆&#34;&gt;回忆
&lt;/h2&gt;&lt;p&gt;想当年其实我自己创建过，跟着一个教程做的，但当时才大一，很多东西都不懂，哼哧哼哧做了半天，最后不知道哪里不行，没有成功，于是放弃了，改用CSDN写，但现在我又有时间了，保研结束，该拾掇拾掇自己了，我觉得个人博客就像自己的一个小花园，写文章就好像在里面种花，精心打理后很有成就感，所以这次花了一天时间重新建起了我的花园，圆了大一时的梦，顺便做个记录，好记性不如烂笔头，忘了回来查查。&lt;/p&gt;
&lt;h2 id=&#34;资料&#34;&gt;资料
&lt;/h2&gt;&lt;p&gt;这次是跟着B站一个up主建立的博客，讲的非常清楚，把他的视频链接放在这里。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1bovfeaEtQ?vd_source=9cfc0ca3d84e1ed0064763e32a183f15&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.bilibili.com/video/BV1bovfeaEtQ?vd_source=9cfc0ca3d84e1ed0064763e32a183f15&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;记录&#34;&gt;记录
&lt;/h2&gt;&lt;h3 id=&#34;文章存放&#34;&gt;文章存放
&lt;/h3&gt;&lt;p&gt;content/post放的是文章&lt;/p&gt;
&lt;h3 id=&#34;文章名&#34;&gt;文章名
&lt;/h3&gt;&lt;p&gt;文章都叫index.md&lt;/p&gt;
&lt;h3 id=&#34;日期问题&#34;&gt;日期问题
&lt;/h3&gt;&lt;p&gt;每个文章前面要有插入一个yaml块，里面放一些信息，日期这里一开始有一个非常搞心态的问题，因为我发现我随便测试日期，显示的日期都是10-10，09-09这种，日期和月份一样，后来检查了半天才发现，视频中up主在修改配置文件hugo.yaml时，把日期格式修改为了2006-01-02，而我顺手就写了个2006-01-01，我发现他把我这个格式理解成了月份和日期要一样，于是产生了那个令人哭笑不得的现象。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/firstblog/image-20241025210633071.png&#34;
	width=&#34;712&#34;
	height=&#34;148&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/firstblog/image-20241025210633071_hu7103448187948757607.png 480w, https://JiangZhiyu-1024.github.io/p/firstblog/image-20241025210633071_hu13902960389822929032.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241025210633071&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;481&#34;
		data-flex-basis=&#34;1154px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;深刻理解配置文件&#34;&gt;深刻理解配置文件
&lt;/h3&gt;&lt;p&gt;今天才知道仓库的actions存放的是一些自动执行的脚本，一开始新建文章的开头配置文件没有写好，导致脚本运行一直出错，现在改好之后，看不到错的记录了，一片绿很养眼&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/firstblog/image-20241025210613894.png&#34;
	width=&#34;1920&#34;
	height=&#34;851&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/firstblog/image-20241025210613894_hu2810913375413829170.png 480w, https://JiangZhiyu-1024.github.io/p/firstblog/image-20241025210613894_hu13167408314589472085.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241025210613894&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;225&#34;
		data-flex-basis=&#34;541px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;slug&#34;&gt;slug
&lt;/h3&gt;&lt;p&gt;这个头配置文件里的slug: firstblog是给链接起个名字，我这个内容是从我另一个博客复制过来的，于是他俩冲突了，显示的内容都杂在一起了，坏了，刚交上去就出bug，正好记上&lt;/p&gt;
&lt;h2 id=&#34;收获&#34;&gt;收获
&lt;/h2&gt;&lt;p&gt;对github仓库加深了理解&lt;/p&gt;
&lt;p&gt;对一些配置文件的存在有了新的认识&lt;/p&gt;
&lt;h2 id=&#34;致谢&#34;&gt;致谢
&lt;/h2&gt;&lt;p&gt;感谢up主：Letere-莱特雷&lt;/p&gt;
&lt;p&gt;对，因为对这个博主的英文名字很感兴趣，于是给自己起了一个名字Zion Blaze,我感觉very古德，比我的损友老马起的foolish fishy强一w倍，明天联系我的律师起诉他。&lt;/p&gt;
&lt;p&gt;感谢chatgpt同学，今天依旧稳定发力，明天开始继续研究LLM，争取有朝一日让gpt同学感谢感谢我。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>2024.10.14至20第二周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/second-week/</link>
        <pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/second-week/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/second-week/leaves-6729056_640.jpg" alt="Featured image of post 2024.10.14至20第二周工作总结" /&gt;&lt;h2 id=&#34;文献阅读&#34;&gt;文献阅读
&lt;/h2&gt;&lt;p&gt;看了知乎的科普文章，先从经典的论文开始看：&lt;/p&gt;
&lt;h3 id=&#34;attention-is-all-you-need&#34;&gt;Attention Is All You Need
&lt;/h3&gt;&lt;p&gt;《Attention is All You Need》是由Vaswani等人在2017年提出的一篇重要论文，首次引入了Transformer模型。以下是其主要内容的总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;：传统的序列到序列模型（如RNN和LSTM）在处理长序列时存在瓶颈。Transformer模型通过自注意力机制克服了这些限制，使得模型可以并行处理数据，提升了训练效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型架构&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder-Decoder结构&lt;/strong&gt;：Transformer由两个主要部分组成：编码器和解码器。编码器将输入序列转化为上下文向量，解码器则根据上下文生成输出序列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自注意力机制&lt;/strong&gt;：每个输入的表示都通过自注意力机制与其他输入进行交互，从而捕捉序列中的依赖关系。每个位置的表示通过加权求和得到，权重由输入的相似度计算得到。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多头注意力&lt;/strong&gt;：模型使用多个注意力头并行计算不同的表示，能够更全面地捕捉信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;位置编码&lt;/strong&gt;：由于Transformer没有循环结构，论文引入了位置编码，提供序列中词语的位置信息，以保留词序信息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练与优化&lt;/strong&gt;：Transformer使用了残差连接和层归一化，以提高训练的稳定性和收敛速度。论文中还介绍了基于Adam优化器的训练过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;：Transformer在机器翻译任务上取得了显著的性能提升，并且在其他NLP任务中也展示了强大的通用性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;影响&lt;/strong&gt;：该论文奠定了现代NLP研究的基础，推动了基于Transformer的预训练语言模型（如BERT、GPT等）的发展。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;差分隐私深度学习deep-learning-with-differential-privacy&#34;&gt;差分隐私深度学习(Deep Learning with Differential Privacy)
&lt;/h3&gt;&lt;p&gt;《Deep Learning with Differential Privacy》是由Martin Abadi等人在2016年提出的一篇重要论文，探讨了在深度学习中实现差分隐私的策略。以下是其主要内容的总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;差分隐私&lt;/strong&gt;：一种保护用户数据隐私的技术，旨在确保模型在训练时无法推断出任何单个用户的敏感信息。即使攻击者知道模型和数据集的其余部分，也无法有效地获取关于某个特定用户的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度学习与隐私问题&lt;/strong&gt;：深度学习模型通常依赖大量数据，这使得保护用户隐私变得尤为重要。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型与训练&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;噪声注入&lt;/strong&gt;：通过在模型的梯度更新中添加噪声，可以达到差分隐私的效果。噪声的大小与隐私预算（ε）相关，预算越小，隐私保护越强，但模型的性能可能会受到影响。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Accounting&lt;/strong&gt;：论文提出了一种计算隐私损失的框架，确保在每次训练步骤中监控和控制隐私损失。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算法设计&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DP-SGD（Differentially Private Stochastic Gradient Descent）&lt;/strong&gt;：论文提出了一种改进的随机梯度下降算法，结合了梯度裁剪和噪声注入，以实现差分隐私。这种方法确保在训练过程中，个别数据对模型的影响是被限制的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文通过实验展示了DP-SGD在图像分类任务上的有效性，同时比较了不同噪声级别对模型性能的影响，证明了在保持合理准确性的同时实现差分隐私是可行的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;应用与展望&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文强调了在各类机器学习应用中（如医疗、金融等）保护隐私的重要性，提出将差分隐私技术与深度学习结合是一个有效的解决方案，并鼓励未来的研究在这一领域的进一步探索。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;感觉偏向数学一点，全是公式和证明，好难看懂。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;差分隐私顾名思义就是用来防范差分攻击的
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;加入噪声，改变原来的概率分布
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;hr&gt;
&lt;h3 id=&#34;deep-reinforcement-learning-from-human-preferences&#34;&gt;Deep Reinforcement Learning from Human Preferences
&lt;/h3&gt;&lt;p&gt;《Deep Reinforcement Learning from Human Preferences》是由Christiano等人在2017年提出的一篇重要论文，探讨了如何利用人类偏好来训练强化学习模型。以下是其主要内容的总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;传统的强化学习方法通常依赖于奖励信号来指导学习，但设计有效的奖励函数在复杂任务中往往困难且耗时。&lt;/li&gt;
&lt;li&gt;人类可以提供更直观的偏好信息，这为训练强化学习代理提供了一种新途径。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;方法概述&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;人类偏好收集&lt;/strong&gt;：论文提出了一种方法，通过收集人类对不同行为的偏好来生成奖励信号。人类评估多个策略的表现，从中选择更好的策略，以此来建立奖励模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励模型&lt;/strong&gt;：利用收集到的人类偏好数据，训练一个深度神经网络来预测代理在给定状态下的奖励。这种奖励模型可以用来指导强化学习算法。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练过程&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;深度强化学习&lt;/strong&gt;：在获得奖励模型后，使用深度强化学习算法（如Proximal Policy Optimization，PPO）来训练代理。代理通过与环境交互，不断优化其策略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;迭代学习&lt;/strong&gt;：通过不断收集新的偏好数据并更新奖励模型，实现迭代学习，使得代理能够逐步提升性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文在多个复杂任务（如Atari游戏和模拟环境）中展示了该方法的有效性，代理能够通过学习人类的偏好，超越基于手动设计的奖励函数的表现。&lt;/li&gt;
&lt;li&gt;结果表明，利用人类偏好进行训练能够加速学习过程，并提升代理的最终性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;意义与展望&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该方法展示了人类偏好在强化学习中的潜力，提出了一种新的思路来解决奖励设计问题。&lt;/li&gt;
&lt;li&gt;论文强调了未来研究中结合人类反馈和偏好的重要性，特别是在涉及复杂决策和安全性问题的领域。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;附录&#34;&gt;附录
&lt;/h2&gt;&lt;h3 id=&#34;残差连接residual-connection和层归一化layer-normalization是什么&#34;&gt;残差连接（residual connection）和层归一化（layer normalization）是什么？
&lt;/h3&gt;&lt;p&gt;残差连接（Residual Connection）和 层归一化（Layer Normalization）是深度学习中常用的两种技术，主要用于提高神经网络的训练效率和性能。下面是它们的定义和作用：&lt;/p&gt;
&lt;h4 id=&#34;残差连接residual-connection&#34;&gt;残差连接（Residual Connection）
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：残差连接是一种将输入直接与输出相加的结构，通常在深度神经网络中使用。公式表示为：
&lt;/p&gt;
$$
\text{Output} = \text{Layer}(x) + x
$$&lt;p&gt;
其中 \(x\) 是输入，\(\text{Layer}(x)\) 是通过某个层（如卷积层或全连接层）处理后的输出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;缓解梯度消失问题&lt;/strong&gt;：在非常深的网络中，梯度可能会随着层数增加而消失，导致网络难以训练。残差连接提供了一条捷径，使得梯度可以更容易地通过网络传播。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加速收敛&lt;/strong&gt;：通过引入直接路径，残差连接有助于提高网络的收敛速度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提高模型性能&lt;/strong&gt;：它允许模型学习恒等映射，这使得训练更深的网络成为可能，提升了模型的表现。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;层归一化layer-normalization&#34;&gt;层归一化（Layer Normalization）
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：层归一化是一种对网络中每一层的激活进行归一化的技术，它通过计算每个样本的均值和标准差来调整激活值。层归一化的公式为：
&lt;/p&gt;
$$
\text{LayerNorm}(x) = \frac{x - \mu}{\sigma + \epsilon} \cdot \gamma + \beta
$$&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(x\) 是输入。&lt;/li&gt;
&lt;li&gt;\(\mu\) 是输入的均值。&lt;/li&gt;
&lt;li&gt;\(\sigma\) 是输入的标准差。&lt;/li&gt;
&lt;li&gt;\(\epsilon\) 是一个小常数，避免除以零。&lt;/li&gt;
&lt;li&gt;\(\gamma\) 和 \(\beta\) 是可学习的参数，用于缩放和平移归一化的结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;减少内部协变量偏移&lt;/strong&gt;：层归一化帮助减少模型训练过程中每层输入的变化，使得网络在训练时更加稳定。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提高训练速度&lt;/strong&gt;：通过标准化输入，层归一化可以使得训练过程更加平滑，从而加快收敛速度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;适用于变长序列&lt;/strong&gt;：与批量归一化（Batch Normalization）不同，层归一化可以直接应用于变长的输入序列，适合于RNN和Transformer等模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;残差连接&lt;/strong&gt;主要通过提供捷径来缓解深层网络中的梯度消失问题，并加速训练过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;层归一化&lt;/strong&gt;则通过标准化激活值来提高训练的稳定性和速度。这两者结合使用，能够显著提升深度学习模型的表现。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>2024.10.7至13第一周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/first-week/</link>
        <pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/first-week/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/first-week/banlan.jpg" alt="Featured image of post 2024.10.7至13第一周工作总结" /&gt;&lt;h1 id=&#34;工作总结&#34;&gt;&lt;strong&gt;工作总结&lt;/strong&gt;：
&lt;/h1&gt;&lt;h2 id=&#34;文献阅读&#34;&gt;文献阅读
&lt;/h2&gt;&lt;h3 id=&#34;综述类&#34;&gt;综述类：
&lt;/h3&gt;&lt;p&gt;2023年国内大模型发展综述与趋势研判_赵子忠&lt;/p&gt;
&lt;p&gt;AIGC大模型测评综述：使能技术、安全隐患和应对_许志伟&lt;/p&gt;
&lt;p&gt;AI大模型发展综述_张乾君&lt;/p&gt;
&lt;h3 id=&#34;深入&#34;&gt;深入：
&lt;/h3&gt;&lt;hr&gt;
&lt;p&gt;《A Neural Probabilistic Language Model》是Yoshua Bengio等人在2003年发表的一篇重要论文，提出了一种基于神经网络的语言模型方法。这篇论文的主要目标是通过神经网络来改进传统的语言模型。&lt;/p&gt;
&lt;p&gt;传统的语言模型的任务是估计一个词序列的概率：
P(w1,w2,…,wT)
在这种情况下，模型需要根据上下文词来预测当前词的概率。传统的做法一般是通过统计方法（如n元语法模型）来估计这些概率，但这类方法有一个很大的问题：它们无法处理高维数据，尤其是在词汇量非常大的情况下，计算复杂度极高，且泛化能力有限。&lt;/p&gt;
&lt;p&gt;Bengio等人提出了一种新的神经网络语言模型，目的是克服这个问题。这个模型的核心思想是使用一个多层感知器来学习词的分布式表示（即词向量），并基于这些词向量来估计概率。具体过程大致如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;词嵌入&lt;/strong&gt;：每个词通过一个嵌入矩阵被映射为一个连续向量，这种向量的维度比词汇表要小得多。这一步的目的是将离散的词映射到一个连续的低维空间，减少计算复杂度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;神经网络建模&lt;/strong&gt;：利用一个前馈神经网络（通常是多层感知器），根据前面的若干个词的词向量，来预测下一个词的条件概率。通过这个网络，模型可以学习到词与词之间的相似性和上下文关系。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练&lt;/strong&gt;：模型通过最大化训练数据的似然估计来进行训练。具体来说，模型会调整嵌入矩阵和网络的参数，使得预测的概率尽可能接近真实的词序列概率。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这篇论文的创新之处在于提出了“词向量”的概念，并展示了使用神经网络来处理语言建模问题的潜力。这为后来的深度学习在自然语言处理（NLP）中的应用奠定了基础。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;附录&#34;&gt;附录
&lt;/h1&gt;&lt;h2 id=&#34;什么是语言模型&#34;&gt;什么是语言模型
&lt;/h2&gt;&lt;p&gt;语言模型（Language Model, LM）是自然语言处理中的一个核心概念，其主要任务是对一个词序列的概率分布进行建模。简单来说，语言模型的目标是估计一段文本或句子出现的概率，或者根据前面的词来预测下一个词。&lt;/p&gt;
&lt;h3 id=&#34;语言模型的定义&#34;&gt;语言模型的定义
&lt;/h3&gt;&lt;p&gt;给定一个词序列
&lt;/p&gt;
$$
P( w_1, w_2, \dots, w_T )
$$&lt;p&gt;
，语言模型的目标是计算这个序列的联合概率：
&lt;/p&gt;
$$
P(w_1, w_2, \dots, w_T)
$$&lt;p&gt;
这通常可以分解为条件概率的乘积：
&lt;/p&gt;
$$
P(w_1, w_2, \dots, w_T) = P(w_1) \cdot P(w_2 | w_1) \cdot P(w_3 | w_1, w_2) \cdot \dots \cdot P(w_T | w_1, w_2, \dots, w_{T-1})
$$&lt;p&gt;
这意味着每个词的概率取决于它前面的词。一个好的语言模型能够通过学习语料库中的模式，准确地捕捉这些条件概率。&lt;/p&gt;
&lt;h3 id=&#34;语言模型的用途&#34;&gt;语言模型的用途
&lt;/h3&gt;&lt;p&gt;语言模型有广泛的应用场景，包括但不限于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;文本生成&lt;/strong&gt;：语言模型可以用来自动生成自然语言文本。例如，给定一部分句子，模型可以预测并生成合理的下一个词，从而逐步生成完整的句子或段落。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;机器翻译&lt;/strong&gt;：在机器翻译中，语言模型帮助翻译系统生成符合目标语言语法的翻译结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;语音识别&lt;/strong&gt;：语言模型能够帮助语音识别系统，在可能的识别结果中选择最合适的词序列。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;拼写纠正&lt;/strong&gt;：在文本输入或搜索引擎中，语言模型可以用于纠正拼写错误，提供更符合语言习惯的结果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;传统语言模型&#34;&gt;传统语言模型
&lt;/h3&gt;&lt;p&gt;在神经网络语言模型之前，最常用的语言模型是&lt;strong&gt;n元语法模型（n-gram Model）&lt;/strong&gt;。n元语法模型通过假设一个词的概率只与它前面的n-1个词有关来简化问题。举个例子，三元语法模型（trigram model）认为：
&lt;/p&gt;
$$
P(w_T | w_1, w_2, \dots, w_{T-1}) \approx P(w_T | w_{T-2}, w_{T-1})
$$&lt;p&gt;这种简化降低了计算复杂度，但也限制了模型的表达能力，因为它只能捕捉到短距离的上下文信息，无法充分考虑长距离的依赖关系。&lt;/p&gt;
&lt;h3 id=&#34;神经网络语言模型&#34;&gt;神经网络语言模型
&lt;/h3&gt;&lt;p&gt;传统语言模型的一个主要问题是词汇量增大时，模型的计算复杂度随之增加。为了解决这个问题，Yoshua Bengio等人在《A Neural Probabilistic Language Model》论文中提出了使用神经网络进行语言建模的想法。这种神经网络模型通过将词表示为&lt;strong&gt;词向量（word embeddings）&lt;/strong&gt;，并通过深度学习模型来捕捉更复杂的语境和词间关系，从而解决了传统语言模型的许多不足之处。&lt;/p&gt;
&lt;p&gt;总之，语言模型的核心任务就是对自然语言的概率进行建模，并通过学习语言中的统计模式来预测文本。&lt;/p&gt;
&lt;h2 id=&#34;什么是纬度灾难&#34;&gt;什么是纬度灾难？
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;维度灾难&lt;/strong&gt;（Curse of Dimensionality）是指在处理高维数据时，随着数据维度增加，问题的复杂性呈指数级增长，导致很多传统方法在高维空间中变得低效或不可行的现象。&lt;/p&gt;
&lt;p&gt;其影响包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;数据稀疏性&lt;/strong&gt;：在高维空间中，数据点变得非常稀疏，因为数据的总量相对于空间的体积变得极其小，导致模型难以从有限的数据中捕捉到模式和关系。简单来说，随着维度的增加，需要的样本量也随之爆炸性增长，否则模型可能会过拟合或无法泛化。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;计算复杂度增加&lt;/strong&gt;：在高维空间中，计算距离、相似性等操作的复杂度大大增加，使得很多算法在高维情况下效率低下。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型的泛化能力下降&lt;/strong&gt;：由于数据点过于稀疏，模型很难学习到有用的特征，从而可能导致泛化能力下降。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;具体来说，维度灾难对机器学习、统计学、最优化等领域带来了极大的挑战，尤其是当模型需要处理很多特征或变量时。为应对这个问题，研究人员提出了许多方法，如&lt;strong&gt;降维技术&lt;/strong&gt;（如主成分分析PCA）和&lt;strong&gt;分布式表示&lt;/strong&gt;（如词嵌入），以减少有效维度或通过分布式方法来表示复杂的数据结构。&lt;/p&gt;
&lt;h2 id=&#34;有概率为什么要引入似然的概念他们有什么不同&#34;&gt;有概率为什么要引入似然的概念，他们有什么不同？
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;概率&lt;/strong&gt;和&lt;strong&gt;似然&lt;/strong&gt;虽然看起来相似，但在统计学中它们有不同的定义和用途：&lt;/p&gt;
&lt;h3 id=&#34;概率probability&#34;&gt;&lt;strong&gt;概率（Probability）&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;概率是用来描述随机事件发生的可能性。在给定模型参数的情况下，概率告诉我们某个事件或观测结果发生的机会有多大。&lt;/p&gt;
&lt;p&gt;假设我们有一个随机变量 \( X \)，它取某个值 \( x \) 的概率可以表示为 \( P(X = x|\theta) \)，其中 \( \theta \) 是模型的参数。例如，如果我们有一个硬币掷出的模型，参数 \( \theta \) 可以是硬币正面朝上的概率（比如 \( \theta = 0.5 \)）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：给定模型的参数 \( \theta \)，事件 \( X = x \) 发生的概率是多少？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;似然likelihood&#34;&gt;&lt;strong&gt;似然（Likelihood）&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;似然是从&lt;strong&gt;反方向&lt;/strong&gt;来考虑的。它描述的是&lt;strong&gt;在已知观测数据的前提下，模型参数的可能性&lt;/strong&gt;。具体来说，似然是给定观测数据后，模型参数如何使观测数据变得最可能的一个度量。&lt;/p&gt;
&lt;p&gt;假设我们已经观察到数据 \( X = x \)，现在我们想知道，在不同的模型参数 \( \theta \) 下，这个数据出现的可能性有多大。似然可以表示为 \( L(\theta|X = x) \)，或者更直观地写作 \( P(X = x|\theta) \)，虽然数学表达式相同，但意义不同——在这里我们关注的是参数 \( \theta \) 的值使观测到的数据最可能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：给定观测到的数据 \( X = x \)，模型参数 \( \theta \) 有多大可能是正确的？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;概率和似然的区别&#34;&gt;概率和似然的区别
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;概率&lt;/strong&gt;：我们知道参数 \( \theta \)，希望知道某个事件 \( X \) 发生的概率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题：&lt;strong&gt;给定参数，事件的概率是多少？&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;似然&lt;/strong&gt;：我们知道事件（观测数据），希望推断出最有可能的参数 \( \theta \)。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题：&lt;strong&gt;给定观测数据，哪个参数最可能是正确的？&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;例子&#34;&gt;例子
&lt;/h3&gt;&lt;h4 id=&#34;抛硬币的例子&#34;&gt;抛硬币的例子
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;概率&lt;/strong&gt;：假设你有一枚硬币，已知它是公平的，即 \( \theta = 0.5 \)，那么掷硬币得到正面的概率是 \( P(\text{正面}) = 0.5 \)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;似然&lt;/strong&gt;：现在，你掷硬币10次，得到8次正面和2次反面。你不知道硬币的公平性（即不知道 \( \theta \) 的值）。你想根据这次实验结果估计硬币正面朝上的概率 \( \theta \)。此时，你需要用&lt;strong&gt;似然&lt;/strong&gt;来衡量在不同 \( \theta \) 值下，观测结果（8次正面，2次反面）出现的可能性有多大，从而找到最有可能的 \( \theta \)。&lt;/p&gt;
&lt;p&gt;似然函数 \( L(\theta|X) \) 可能在某个 \( \theta \) 值处达到最大值，这个 \( \theta \) 就是最能解释观测数据的参数值。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;概率&lt;/strong&gt;用于给定模型参数时预测事件的发生可能性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;似然&lt;/strong&gt;用于在已知观测数据时，推断哪个参数最能解释这些数据。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
