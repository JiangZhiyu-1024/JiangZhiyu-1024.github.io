[{"content":"大模型基础知识 代码来源：https://github.com/openai/gpt-2\nGPT2源代码提供了以下几个文件：\nencoder.py\ngenerate_unconditional_samples.py\ninteractive_conditional_samples.py\nmodel.py\nsample.py\nmodel 1 2 3 4 5 6 7 8 def default_hparams(): return HParams( n_vocab=0, # 词汇表大小（通常是词汇表中包含的词语或符号数量） n_ctx=1024, # 输入序列的最大长度（例如，一个输入文本序列的最大 token 数量） n_embd=768, # 嵌入层的维度大小（每个词或符号在嵌入空间中的表示长度） n_head=12, # 多头自注意力机制中的头数（即注意力头的数量） n_layer=12, # Transformer 模型中的层数（即堆叠的 transformer 层数） ) 词汇表 词汇表（Vocabulary）是自然语言处理（NLP）模型中用于将语言转换为数字表示的一组词或符号的集合。在深度学习模型中，词汇表用于将文本数据中的每个单词或符号映射为一个唯一的数字（通常是一个整数），以便计算机能够理解和处理。\n词汇表的作用： 映射文本到数字：由于机器学习模型无法直接处理文本数据，必须将文本中的每个词或符号转换为数字。词汇表就是这个映射的核心工具。例如，对于句子“我爱自然语言处理”，我们需要将“我”、“爱”、“自然”、“语言”、“处理”这些词转化为模型可以理解的数字。 限制模型输入的规模：在实际操作中，词汇表的大小有限。通常，我们不会为每个可能的字词创建一个索引，而是根据训练数据中出现的词汇频率来限制词汇表的大小。这样，词汇表的规模可以控制模型的输入维度，避免过大的计算和内存开销。 处理未知词汇：词汇表通常会为所有未出现在训练数据中的词汇或符号保留一个特殊的“未知词”标记（如 [UNK]）。当模型遇到这些未见过的词时，会用这个特殊标记来代替。 词汇表的组成： 词：如果你使用的是基于单词的词汇表，那么词汇表中的元素就是单词。例如，假设你有一个很小的词汇表：\n1 2 3 4 5 css 复制代码 [\u0026#39;我\u0026#39;, \u0026#39;爱\u0026#39;, \u0026#39;自然\u0026#39;, \u0026#39;语言\u0026#39;, \u0026#39;处理\u0026#39;] 子词（Subword）：对于一些较为复杂的语言模型，尤其是像 GPT-2 和 BERT 这样的模型，它们通常使用 子词 作为词汇表的基本单元。子词是一个比单词更小的语言单位，它可以是词的一部分或者是一个常见的词根、前缀、后缀等。例如，词汇表中可能包含 ##ing、un 等子词单元，而不是整个单词 running 或 undo。这有助于模型更好地处理未见过的词汇，并减少词汇表的大小。\n特殊标记：除了实际的词或子词，词汇表通常还会包含一些特殊标记：\n[PAD]：填充标记，用于将输入文本填充到相同的长度。 [UNK]：未知标记，用于处理那些在训练集中没有出现过的词。 [CLS]：分类标记，通常用于标记输入的开始（特别是在 BERT 等模型中）。 [SEP]：分隔标记，通常用于分隔不同句子或文本段落（在 BERT 等模型中也有使用）。 向量空间（Vector Space） 向量空间是一个数学概念，指的是所有向量的集合，可以在其中进行加法和标量乘法等操作。在机器学习和自然语言处理中，我们通常将每个词、子词或符号表示为一个高维空间中的向量。例如，一个简单的二维向量空间可能包含了这样的一组向量：\n(1, 0) (0, 1) (1, 1) 在 NLP 中，这些向量并没有具体的几何含义，而是用来表示不同的语义特征。每个向量的元素（即各维度的值）代表了某种抽象的语义属性。\n词向量空间意味着每个词都被映射为一个向量，且这些向量之间有一定的关系。例如，\u0026ldquo;king\u0026rdquo; 和 \u0026ldquo;queen\u0026rdquo; 可能在向量空间中非常接近，因为它们的语义相似，尽管它们的拼写不同。嵌入的目标就是将语言中的词语、句子或其他语言单位映射到这样一个向量空间中。\n嵌入（Embedding） **嵌入（Embedding）**是将一个离散的词汇项（如单词、子词）转换为连续的向量表示的过程。这些向量通常具有固定的维度（例如 300 维、768 维等），并且能够捕捉该词的语义信息。\n换句话说，嵌入是对词汇的数字表示，它通过将每个词或符号映射到一个向量空间中，使得模型能够理解和处理语言中的复杂结构。\n例如，假设你有以下三个词汇：\n\u0026ldquo;猫\u0026rdquo;（cat） \u0026ldquo;狗\u0026rdquo;（dog） \u0026ldquo;汽车\u0026rdquo;（car） 通过嵌入，这些词语会被映射为固定长度的向量，例如：\n\u0026ldquo;猫\u0026rdquo; → [0.45, 0.87, 0.34, ...] \u0026ldquo;狗\u0026rdquo; → [0.42, 0.89, 0.31, ...] \u0026ldquo;汽车\u0026rdquo; → [0.21, 0.08, 0.76, ...] 这些向量的维度（例如 300 维或 768 维）反映了嵌入空间的复杂度。模型通过训练来优化这些向量，使得语义相似的词在嵌入空间中距离更近，语义不同的词距离较远。\n向量（Vector） 向量是在数学和计算机科学中用来表示方向和大小的对象。每个词或符号被嵌入为一个向量，这个向量通常是一个浮动的数字数组。向量的每个元素都表示一个特定的特征维度。\n在 NLP 中，词的向量通常有以下几个特点：\n稠密表示：每个词的向量是一个低维的稠密向量，不像传统的“one-hot”编码（仅使用0和1来表示词汇）。稠密向量包含了丰富的语义信息。 语义关系：通过训练，词向量可以捕捉到词汇之间的语义关系。例如，“king” 和 “queen” 可能有相似的向量，因为它们共享许多语义特征。 例如，假设我们训练了一个模型，得到以下的词向量：\n\u0026ldquo;苹果\u0026rdquo; → [0.5, -0.2, 0.1, 0.7] \u0026ldquo;香蕉\u0026rdquo; → [0.4, -0.1, 0.2, 0.6] 这些向量的数值表示了每个词的特征和语义信息。在训练过程中，模型会通过调整这些向量，使得语义相似的词的向量在空间中彼此接近。\n为什么要使用嵌入？ 捕捉语义相似性：通过嵌入，模型可以理解哪些词是相似的，哪些是不同的。比如“猫”和“狗”虽然拼写不同，但它们都是“动物”，因此它们的向量会很接近。 降维：将词语从“one-hot”编码的高维空间（例如，假设有 10,000 个不同的词，one-hot 向量就是 10,000 维的）映射到一个低维空间（如 300 维）。这样可以减少计算的复杂度和内存消耗。 增强模型理解：通过将词转换为向量，模型可以更加有效地理解文本中的语义、句法关系和上下文信息。这对于复杂的任务（如文本分类、情感分析、翻译等）非常重要。 1 2 3 4 5 def shape_list(x): \u0026#34;\u0026#34;\u0026#34;Deal with dynamic shape in tensorflow cleanly.\u0026#34;\u0026#34;\u0026#34; static = x.shape.as_list() # 获取静态形状 dynamic = tf.shape(x) # 获取动态形状 return [dynamic[i] if s is None else s for i, s in enumerate(static)] # 返回合并后的形状 static = x.shape.as_list()：\nx.shape 是 TensorFlow 张量 x 的静态形状。 as_list() 将这个形状从 TensorShape 对象转换为 Python 列表。静态形状的值如果是已知的，会直接作为数值存在。如果某个维度的值是 None（表示该维度在静态形状中不确定），那么会是 None。 例如，如果 x 的形状是 [None, 32, 32, 3]，则 static 会是 [None, 32, 32, 3]。 dynamic = tf.shape(x)：\ntf.shape(x) 返回张量 x 的动态形状，它是一个 TensorFlow 张量，表示运行时该张量的形状。与静态形状不同，动态形状会根据实际运行时的输入数据进行计算。 返回值是一个形状为 [d0, d1, ..., dn] 的张量（d0，d1 等是张量的各个维度的动态大小）。 return [dynamic[i] if s is None else s for i, s in enumerate(static)]：\n这个列表推导式的目的是将静态形状和动态形状合并成一个形状列表。如果某个静态形状的维度是 None（即在静态形状中未指定），那么就使用对应的动态形状。 enumerate(static) 会遍历静态形状中的每个维度，如果该维度是 None，就从动态形状中取对应的维度值；如果该维度不是 None，则保留静态形状中的值。 例如：\n假设 static = [None, 32, 32, 3]（第一维度 None，表示它是动态的）和 dynamic = [64, 32, 32, 3]（运行时该张量的实际形状是 [64, 32, 32, 3]）。 那么，函数的返回值将是 [64, 32, 32, 3]。 为什么需要这个函数？\n在 TensorFlow 中，许多操作要求处理的张量形状是已知的（静态形状），但有些情况下形状在执行时才会动态确定，特别是在处理输入数据的批量大小、图像尺寸等变量时。shape_list 函数就是用来统一处理这两种情况的，使得无论是在静态还是动态形状下，代码都能够正常运行而不会出错。\n举个例子： 假设我们有一个张量 x，它的形状是 [None, 64, 64, 3]（批量大小未知）：\n1 2 3 4 5 6 import tensorflow as tf x = tf.placeholder(tf.float32, shape=[None, 64, 64, 3]) # 动态批量大小 shape = shape_list(x) print(shape) # 输出形状 在这个例子中，如果你传递一个大小为 [32, 64, 64, 3] 的输入，shape_list 函数会返回 [32, 64, 64, 3]，即将 None 替换为实际的批量大小 32。\n1 2 3 4 def softmax(x, axis=-1): x = x - tf.reduce_max(x, axis=axis, keepdims=True) ex = tf.exp(x) return ex / tf.reduce_sum(ex, axis=axis, keepdims=True) 这个函数实现了 Softmax 操作，将输入 x 转换为概率分布，常用于多分类模型的输出层。通过减去最大值避免指数计算中的数值溢出，确保计算稳定性。\n1 2 def gelu(x): return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3)))) GELU 是一种平滑的、概率性的激活函数，能够更自然地通过高斯分布近似非线性激活。相比于 ReLU，GELU 在训练时通常能更好地保留梯度，并且能够减少死神经元的问题。\n代码中使用了 tf.tanh 和 tf.pow（TensorFlow 的函数）与 NumPy 的常量进行计算，确保了在 TensorFlow 中的高效计算。\nLayer Normalization 是一种在深度学习中常用的技术，旨在对每个样本的特征进行归一化，使得特征的均值为0，方差为1，从而加速训练并提高模型的性能。\n代码解释：\n1 2 3 4 5 6 def norm(x, scope, *, axis=-1, epsilon=1e-5): \u0026#34;\u0026#34;\u0026#34;Normalize to mean = 0, std = 1, then do a diagonal affine transform.\u0026#34;\u0026#34;\u0026#34; with tf.variable_scope(scope): n_state = x.shape[-1].value g = tf.get_variable(\u0026#39;g\u0026#39;, [n_state], initializer=tf.constant_initializer(1)) b = tf.get_variable(\u0026#39;b\u0026#39;, [n_state], initializer=tf.constant_initializer(0)) x: 输入张量，通常是经过激活函数后的某一层的输出。 scope: 变量作用域，用于创建和管理变量。 axis=-1: 归一化时的维度，通常在 Layer Normalization 中是最后一维（即特征维度）。axis=-1 表示归一化最后一维，通常是一个样本的所有特征。 epsilon=1e-5: 为了避免在除法中出现除零错误，epsilon 是一个很小的常数（默认值为 1e−51e-51e−5）。 n_state = x.shape[-1].value: 获取输入张量 x 在最后一维的大小，代表特征数目。 接下来，定义了两个变量 g 和 b，分别用于对归一化后的结果进行线性变换：\ng 是缩放因子（通常称为 Gamma）。 b 是平移因子（通常称为 Beta）。 1 2 g = tf.get_variable(\u0026#39;g\u0026#39;, [n_state], initializer=tf.constant_initializer(1)) b = tf.get_variable(\u0026#39;b\u0026#39;, [n_state], initializer=tf.constant_initializer(0)) g 和 b 都是与输入 x 的最后一维特征数目相同的向量。g 初始化为 1，b 初始化为 0。 归一化步骤： 1 2 u = tf.reduce_mean(x, axis=axis, keepdims=True) s = tf.reduce_mean(tf.square(x - u), axis=axis, keepdims=True) u = tf.reduce_mean(x, axis=axis, keepdims=True): 计算输入 x 在指定 axis 维度上的均值（平均值）。这里，axis 默认为 -1，即沿着最后一维计算均值。 s = tf.reduce_mean(tf.square(x - u), axis=axis, keepdims=True): 计算输入 x 与均值 u 的差的平方的平均值，即方差。keepdims=True 保证输出维度与输入相同，便于后续操作。 1 x = (x - u) * tf.rsqrt(s + epsilon) x = (x - u) * tf.rsqrt(s + epsilon) 对输入\n1 x 进行标准化（即归一化处理），使得均值为 0，方差为 1。\nx - u：减去均值，使得每个元素的均值为 0。 tf.rsqrt(s + epsilon)：计算标准差的倒数并进行缩放，rsqrt 是 1 / sqrt(x)。epsilon 用于防止除 0 错误。 1 2 x = x * g + b return x x = x \\* g + b: 最后，对标准化后的 x 进行线性变换，使用参数 g（缩放因子）和 b（平移因子）。这种变换使得归一化的结果可以恢复一定的表达能力，类似于 Batch Normalization 中的 affine 变换。 return x: 返回归一化并经过变换的结果。 1 2 3 4 def split_states(x, n): \u0026#34;\u0026#34;\u0026#34;Reshape the last dimension of x into [n, x.shape[-1]/n].\u0026#34;\u0026#34;\u0026#34; *start, m = shape_list(x) return tf.reshape(x, start + [n, m//n]) 这个函数 split_states 的作用是将输入张量 x 的最后一维拆分成两个部分，其中一个部分的大小是 n，另一个部分的大小是 m // n（m 是输入张量的最后一维大小）。\n\\*start, m = shape_list(x)：\nshape_list(x) 会返回张量 x 的形状（作为一个列表）。*start 会将除了最后一维之外的所有维度（即张量的前几维）存入 start 变量中，m 则保存张量最后一维的大小。 例如，如果 x 的形状是 [batch_size, time_steps, features]，那么 start 将保存 [batch_size, time_steps]，m 将保存 features。 tf.reshape(x, start + [n, m // n])：\nstart 是 x 的前几个维度的列表，n 是传入的参数，表示你想将最后一维拆分成 n 部分，m // n 是每一部分的大小。 tf.reshape 函数将输入张量 x 重新调整形状，形状变为 [start, n, m // n]。 例如，如果 x 的形状是 [batch_size, time_steps, features]，并且传入 n = 2，那么新的形状将是 [batch_size, time_steps, 2, features // 2]。 1 2 3 4 def merge_states(x): \u0026#34;\u0026#34;\u0026#34;Smash the last two dimensions of x into a single dimension.\u0026#34;\u0026#34;\u0026#34; *start, a, b = shape_list(x) return tf.reshape(x, start + [a*b]) 这个函数 merge_states 的作用是将输入张量 x 的最后两个维度合并成一个维度。也就是说，它通过将最后两个维度相乘，将它们“压扁”为一个新的维度。\n\\*start, a, b = shape_list(x):\nshape_list(x) 返回张量 x 的形状，作为一个列表。 *start 会将 x 的所有维度（除了最后两个维度）存入 start，a 和 b 分别保存 x 的倒数第二维和最后一维的大小。 举个例子：\n假设 x 的形状是 [batch_size, time_steps, features, channels]，那么 start = [batch_size, time_steps, features]，a = features，b = channels。 tf.reshape(x, start + [a \\* b]):\ntf.reshape(x, start + [a * b]) 会将 x 重新调整形状，新的形状由 start（原先的所有维度，除了最后两个）和 a * b（将最后两个维度相乘的结果）组成。 a * b 是将原本的最后两个维度合并成一个新的维度。 这个 conv1d 函数实现了一个 1D 卷积 操作。虽然它的名字和传统的 1D 卷积类似，但它实际上通过矩阵乘法和适当的形状变换来模拟 1D 卷积的操作。以下是代码逐行解释：\n代码解释： 1 2 3 4 5 6 7 def conv1d(x, scope, nf, *, w_init_stdev=0.02): with tf.variable_scope(scope): *start, nx = shape_list(x) w = tf.get_variable(\u0026#39;w\u0026#39;, [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev)) b = tf.get_variable(\u0026#39;b\u0026#39;, [nf], initializer=tf.constant_initializer(0)) c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf])) + b, start + [nf]) return c \\*start, nx = shape_list(x): shape_list(x) 返回张量 x 的形状，作为一个列表。*start 会将除了最后一维之外的所有维度（即张量的前几维）存入 start，nx 保存 x 的最后一维大小。 比如，如果 x 的形状是 [batch_size, length, in_channels]，那么 start 将是 [batch_size, length]，nx 就是 in_channels。 w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev)): 创建一个卷积核（权重） w，其形状为 [1, nx, nf]。这里的 1 是卷积核的大小（即卷积操作是 1D 的），nx 是输入的最后一维大小（通常是输入通道数），nf 是卷积操作后输出通道的数量。 initializer=tf.random_normal_initializer(stddev=w_init_stdev) 初始化权重值为从正态分布中采样，标准差为 w_init_stdev，默认为 0.02。 b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0)): 创建偏置 b，其形状为 [nf]，即与输出通道数相同，初始化为零。 c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf])) + b, start + [nf]): tf.reshape(x, [-1, nx]): 将输入张量 x 重塑为形状 [-1, nx]，即将所有前面的维度展平，保留最后一维 nx。这一步相当于将输入的多维张量展平成二维矩阵，每一行代表一个数据样本的输入通道。 tf.reshape(w, [-1, nf]): 将卷积核 w 重塑为形状 [-1, nf]，这意味着卷积核的权重被展平为一个矩阵，其中每个输入通道的权重被分配到输出通道。 tf.matmul(...): 执行矩阵乘法。此处是将展平后的输入张量 x 与展平后的卷积核 w 相乘，类似于卷积操作中的加权求和过程。 + b: 将偏置加到结果中。 tf.reshape(..., start + [nf]): 将矩阵乘法后的结果再重塑回原来的形状，保留除了最后一维以外的所有维度，最后一维变为 nf，即输出通道数。 return c: 返回卷积操作后的结果 c，它的形状是 [batch_size, length, nf]，即输出的批次大小、长度和输出通道数。 总结： 这个 conv1d 函数实现的是一种通过矩阵乘法模拟 1D 卷积操作的方式。传统的 1D 卷积通过滑动窗口对输入进行局部加权求和，而这个实现通过将输入展平成矩阵后与卷积核进行矩阵乘法，从而实现类似的效果。\n主要特点： 权重形状：[1, nx, nf]，表示卷积核的大小为 1xnx，即卷积操作是对输入的每个位置进行加权求和，输出通道数为 nf。 矩阵乘法实现卷积：通过 tf.matmul 将输入张量和卷积核做矩阵乘法，而不是传统的滑动窗口卷积。 展平操作：输入张量和卷积核都通过 tf.reshape 进行了展平处理，以便进行矩阵乘法。 这种方法虽然在实现上与传统的卷积不同，但效果是相同的，并且可以通过矩阵乘法的方式提高计算效率或适应某些特定的任务需求。\n这段代码定义了一个函数 attention_mask，用于生成注意力掩码（attention mask）\n1 2 3 4 5 6 7 8 9 def attention_mask(nd, ns, *, dtype): \u0026#34;\u0026#34;\u0026#34;1\u0026#39;s in the lower triangle, counting from the lower right corner. Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn\u0026#39;t produce garbage on TPUs. \u0026#34;\u0026#34;\u0026#34; i = tf.range(nd)[:,None] j = tf.range(ns) m = i \u0026gt;= j - ns + nd return tf.cast(m, dtype) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def attn(x, scope, n_state, *, past, hparams): assert x.shape.ndims == 3 # Should be [batch, sequence, features] assert n_state % hparams.n_head == 0 if past is not None: assert past.shape.ndims == 5 # Should be [batch, 2, heads, sequence, features], where 2 is [k, v] def split_heads(x): # From [batch, sequence, features] to [batch, heads, sequence, features] return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3]) def merge_heads(x): # Reverse of split_heads return merge_states(tf.transpose(x, [0, 2, 1, 3])) def mask_attn_weights(w): # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst. _, _, nd, ns = shape_list(w) b = attention_mask(nd, ns, dtype=w.dtype) b = tf.reshape(b, [1, 1, nd, ns]) w = w*b - tf.cast(1e10, w.dtype)*(1-b) return w def multihead_attn(q, k, v): # q, k, v have shape [batch, heads, sequence, features] w = tf.matmul(q, k, transpose_b=True) w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype)) w = mask_attn_weights(w) w = softmax(w) a = tf.matmul(w, v) return a with tf.variable_scope(scope): c = conv1d(x, \u0026#39;c_attn\u0026#39;, n_state*3) q, k, v = map(split_heads, tf.split(c, 3, axis=2)) present = tf.stack([k, v], axis=1) if past is not None: pk, pv = tf.unstack(past, axis=1) k = tf.concat([pk, k], axis=-2) v = tf.concat([pv, v], axis=-2) a = multihead_attn(q, k, v) a = merge_heads(a) a = conv1d(a, \u0026#39;c_proj\u0026#39;, n_state) return a, present ","date":"2025-01-10T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E8%A1%A5%E4%B9%A0gpt2%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/tree-838666_1280_hu8399293750494639000.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E8%A1%A5%E4%B9%A0gpt2%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/","title":"大模型基础知识补习\u0026GPT2代码分析"},{"content":"Language Models are Unsupervised Multitask Learners Abstract 自然语言处理任务，如问答、机器翻译、阅读理解和摘要，通常通过在特定任务数据集上进行监督学习来解决。我们展示了当语言模型在一个名为 WebText 的新数据集上进行训练时，它们开始在没有任何显式监督的情况下学习这些任务。该数据集包含了数百万个网页。当条件是一个文档加上问题时，语言模型生成的答案在 CoQA 数据集上的 F1 分数达到了 55，匹配或超越了 4 个基线系统中的 3 个，而没有使用超过 127,000 个训练示例。语言模型的能力对零样本任务转移的成功至关重要，并且增加其能力会以对数线性方式提高各个任务的性能。我们的最大模型 GPT-2 是一个拥有 15 亿参数的 Transformer，在零样本设置下，在测试的 8 个语言建模数据集中的 7 个上达到了最先进的结果，但仍然没有完全拟合 WebText。来自该模型的样本反映了这些改进，并且包含了连贯的段落文本。这些发现表明，通过自然发生的示范学习执行任务的语言处理系统建设，可能是一条有前景的道路。\nIntroduction 机器学习系统现在在它们所训练的任务上表现优异（预期中），通过结合大规模数据集、高容量模型和监督学习（Krizhevsky et al., 2012）（Sutskever et al., 2014）（Amodei et al., 2016）。然而，这些系统脆弱且对数据分布（Recht et al., 2018）和任务规范（Kirkpatrick et al., 2017）的细微变化敏感。当前的系统更适合被描述为狭窄的专家，而不是有能力的通才。我们希望朝着更加通用的系统发展，这些系统能够执行多个任务——最终无需手动为每个任务创建和标注训练数据集。\n创建机器学习系统的主流方法是收集一个训练示例数据集，展示期望任务的正确行为，训练系统模仿这些行为，然后在独立同分布（IID）的持出样本上测试其性能。这种方法在狭窄专家任务上取得了良好进展。但在处理可能输入的多样性和变化时，像图像字幕生成模型（Lake et al., 2017）、阅读理解系统（Jia \u0026amp; Liang, 2017）和图像分类器（Alcorn et al., 2018）经常表现出不稳定的行为，突显了这种方法的一些局限性。我们怀疑，单任务训练在单一领域数据集上的普遍存在，是当前系统缺乏泛化能力的主要原因。要在现有架构下取得对鲁棒系统的进展，可能需要在广泛的领域和任务上进行训练和性能评估。最近，提出了一些基准测试，如 GLUE（Wang et al., 2018）和 decaNLP（McCann et al., 2018），开始研究这一问题。多任务学习（Caruana, 1997）是一个有前景的框架，可以提高总体性能。然而，多任务训练在自然语言处理中的应用仍处于初期阶段。最近的工作报告了适度的性能提升（Yogatama et al., 2019），迄今为止最雄心勃勃的两项工作分别在 10 和 17 对（数据集，目标）上进行了训练（McCann et al., 2018）（Bowman et al., 2018）。从元学习的角度来看，每对（数据集，目标）是从数据集和目标的分布中抽取的一个训练示例。当前的机器学习系统需要数百到数千个示例来归纳出能良好泛化的函数。这表明，多任务训练可能需要与当前方法一样多的有效训练对才能实现其潜力。继续以当前技术通过蛮力方式扩展数据集的创建和目标设计，达到可能要求的规模，将是非常困难的。这促使我们探索用于进行多任务学习的额外设置。\n方法 我们方法的核心是语言建模。语言建模通常被视为从一组示例（x1, x2, \u0026hellip;, xn）中进行无监督的分布估计，每个示例由变长的符号序列（s1, s2, \u0026hellip;, sn）组成。由于语言具有自然的顺序性，通常将符号的联合概率分解为条件概率的乘积（Jelinek \u0026amp; Mercer, 1980）（Bengio et al., 2003）：\n$p(x) = \\prod_{i=1}^{n} p(s_n | s_1, \u0026hellip;, s_{n-1})$\n这种方法使得从 p(x)p(x) 及其任何条件形式 p(sn−k,\u0026hellip;,sn∣s1,\u0026hellip;,sn−k−1)p(s_{n-k}, \u0026hellip;, s_n | s_1, \u0026hellip;, s_{n-k-1}) 进行可操作的采样和估计成为可能。近年来，能够计算这些条件概率的模型表现出了显著的改进，例如像 Transformer（Vaswani et al., 2017）这样的自注意力架构。\n这种方法使得从 p(x)p(x) 及其任何条件形式 p(sn−k,\u0026hellip;,sn∣s1,\u0026hellip;,sn−k−1)p(s_{n-k}, \u0026hellip;, s_n | s_1, \u0026hellip;, s_{n-k-1}) 进行可操作的采样和估计成为可能。近年来，能够计算这些条件概率的模型表现出了显著的改进，例如像 Transformer（Vaswani et al., 2017）这样的自注意力架构。\n学习执行单一任务可以在概率框架中表示为估计条件分布 p(output∣input)p(\\text{output} | \\text{input})。由于一个通用系统应该能够执行许多不同的任务，即使是针对相同的输入，它也应该不仅仅依赖于输入，还应依赖于将要执行的任务。换句话说，它应该建模 p(output∣input,task)p(\\text{output} | \\text{input}, \\text{task})。在多任务学习和元学习设置中，这种任务条件化已被不同方式地形式化。任务条件化通常在架构层面实现，例如（Kaiser et al., 2017）中的任务特定编码器和解码器，或者在算法层面，如 MAML（Finn et al., 2017）中的内外循环优化框架。但正如 McCann et al.（2018）所示，语言提供了一种灵活的方式来指定任务、输入和输出，所有这些都可以作为符号序列。例如，翻译训练示例可以写为序列（translate to french, english text, french text）。同样，阅读理解训练示例可以写为（answer the question, document, question, answer）。McCann et al.（2018）证明，使用这种格式的示例，可以训练一个单一模型 MQAN 来推理并执行许多不同的任务。\n语言建模原则上也能够学习 McCann et al.（2018）中的任务，而无需明确监督哪些符号是需要预测的输出。由于监督目标与无监督目标相同，只是评估在序列的一个子集上，因此无监督目标的全局最小值也是监督目标的全局最小值。在这个略带玩具性质的设置中，讨论过的关于将密度估计作为原则性训练目标的问题（Sutskever et al., 2015）被绕开了。问题转变为我们是否能够在实践中将无监督目标优化到收敛。初步实验确认，足够大的语言模型能够在这种玩具式的设置中执行多任务学习，但学习速度远远慢于显式监督的方法。\n尽管从上述明确设置到“自然语言环境”中的混乱局面是一个巨大的跳跃，Weston（2016）在对话的背景下提出了开发能够直接从自然语言学习的系统的需求，并展示了一个概念验证——通过使用教师输出的前向预测，在没有奖励信号的情况下学习一个问答任务。虽然对话是一种有吸引力的方法，但我们担心它过于限制性。互联网包含了大量的信息，这些信息是被动可用的，无需互动交流。我们的猜测是，拥有足够容量的语言模型将开始学习推断和执行在自然语言序列中展示的任务，以便更好地预测它们，无论这些任务是如何获取的。如果语言模型能够做到这一点，它实际上就会执行无监督的多任务学习。我们通过分析语言模型在零样本设置下在各种任务上的表现来测试这一假设是否成立。\n2.1. 训练数据集 大多数先前的工作都在单一文本领域上训练语言模型，例如新闻文章（Jozefowicz et al., 2016）、维基百科（Merity et al., 2016）或小说书籍（Kiros et al., 2015）。我们的方法激励构建尽可能大且多样化的数据集，以收集尽可能多领域和背景中的自然语言任务示范。\n一个有前景的、多样且几乎无限的文本来源是网络抓取数据，如 Common Crawl。尽管这些档案比当前的语言建模数据集大几个数量级，但它们存在显著的数据质量问题。Trinh \u0026amp; Le（2018）在他们的常识推理工作中使用了 Common Crawl，但指出有大量文档“其内容大多难以理解”。我们在使用 Common Crawl 进行初步实验时也观察到了类似的数据问题。Trinh \u0026amp; Le（2018）取得的最佳结果是使用了 Common Crawl 的一个小子样本，仅包含最接近其目标数据集——Winograd Schema Challenge 的文档。虽然这是在特定任务上提高性能的务实方法，但我们希望避免事先对将要执行的任务做出假设。\n因此，我们创建了一个新的网页抓取，强调文档质量。为此，我们只抓取了那些经过人工策划/筛选的网页。手动筛选整个网页抓取将非常昂贵，因此作为起点，我们抓取了来自社交媒体平台 Reddit 的所有外链，这些链接至少获得了 3 个 karma 点数。这可以被视为一个启发式指标，表明其他用户是否认为该链接有趣、具有教育意义，或者仅仅是好笑。\n最终生成的数据集 WebText 包含了这些 4500万个链接的文本子集。为了从 HTML 响应中提取文本，我们使用了 Dragnet（Peters \u0026amp; Lecocq, 2013）和 Newspaper1 内容提取器的组合。本文中所有呈现的结果均使用了 WebText 的初步版本，该版本不包含 2017年12月之后创建的链接，并且在去重和一些基于启发式的清理后，包含了略多于 800万个文档，总计 40 GB 的文本。我们从 WebText 中移除了所有维基百科文档，因为维基百科是其他数据集的常见数据来源，可能会由于训练数据与测试评估任务的重叠而复杂化分析。\n2.2. 输入表示 一个通用的语言模型（LM）应该能够计算任何字符串的概率（并且生成它）。当前的大规模语言模型包括诸如小写化、分词和处理词汇外的符号等预处理步骤，这些步骤限制了模型可以表示的字符串空间。虽然将 Unicode 字符串处理为 UTF-8 字节序列，如 Gillick et al.（2015）等工作所示，优雅地满足了这一要求，但当前的字节级语言模型在像 One Billion Word Benchmark（Al-Rfou et al., 2018）这样的庞大数据集上，与基于词汇的语言模型相比并不具有竞争力。我们在尝试在 WebText 上训练标准字节级语言模型时也观察到了类似的性能差距。\n字节对编码（BPE）（Sennrich et al., 2015）是一种在字符级和词汇级语言建模之间的实际折衷方法，能够有效地在词汇级输入（针对常见符号序列）和字符级输入（针对不常见符号序列）之间进行插值。尽管其名为字节对编码，参考的 BPE 实现通常操作的是 Unicode 代码点，而不是字节序列。这些实现需要包括所有 Unicode 符号的完整空间，以便对所有 Unicode 字符串进行建模。这将导致一个超过 130,000 的基础词汇表，尚未添加任何多符号标记。这相比于 BPE 通常使用的 32,000 到 64,000 词汇量，显得过于庞大。相比之下，字节级版本的 BPE 只需要一个大小为 256 的基础词汇表。然而，直接将 BPE 应用于字节序列会导致不理想的合并，因为 BPE 使用基于贪心频率的启发式方法来构建标记词汇表。我们观察到，BPE 会将许多常见单词的不同版本（如 dog）合并，因为它们会有很多变体，如 dog, dog!, dog? 等。这样会导致有限词汇槽和模型容量的次优分配。\n为了避免这种情况，我们防止 BPE 在任何字节序列的字符类别之间进行合并。我们为空格添加了一个例外，这显著提高了压缩效率，同时仅在跨多个词汇标记的词语上产生最小的碎片化。\n这种输入表示方法使我们能够将词汇级语言模型的经验优势与字节级方法的通用性相结合。由于我们的方法可以为任何 Unicode 字符串分配概率，这使得我们能够在任何数据集上评估我们的语言模型，而不受预处理、分词或词汇表大小的影响。\n2.3. 模型 我们为语言模型使用基于 Transformer（Vaswani et al., 2017）架构的设计。该模型大致遵循了 OpenAI GPT 模型（Radford et al., 2018）的细节，但进行了几处修改。层归一化（Ba et al., 2016）被移到了每个子模块的输入处，类似于预激活残差网络（He et al., 2016），并在最终的自注意力块之后增加了一个额外的层归一化。我们使用了修改过的初始化方法，考虑到模型深度对残差路径的累积影响。在初始化时，我们通过 1/√N 的因子缩放残差层的权重，其中 N 是残差层的数量。词汇表扩展到 50,257。我们还将上下文窗口大小从 512 扩展到 1024 个标记，并使用了更大的批次大小，设为 512。\n3. 实验 我们训练并基准测试了四个大约按对数均匀间隔大小分布的语言模型（LM）。各架构的概况总结在表 2 中。最小的模型相当于原始的 GPT，第二小的模型与 BERT（Devlin et al., 2018）中的最大模型相当。我们的最大模型，我们称之为 GPT-2，参数量比 GPT 大了一个数量级。每个模型的学习率是手动调整的，以在 WebText 的 5% 留出样本上获得最佳的困惑度（perplexity）。所有模型在 WebText 上仍然存在欠拟合问题，且在更多训练时间下，留出样本的困惑度尚未显著改进。\n3.1. 语言建模 作为零-shot 任务迁移的初步步骤，我们有兴趣了解 WebText 语言模型在其主要训练任务——语言建模上的零-shot 域迁移表现。由于我们的模型是在字节级别上操作，并且不需要有损的预处理或分词处理，我们可以在任何语言模型基准上评估它。语言建模数据集的结果通常以一种数量来报告，这个数量是每个标准化预测单元（通常是字符、字节或单词）的平均负对数概率的缩放或指数化版本。我们通过计算 WebText 语言模型的对数概率并除以标准单元的数量来评估相同的数量。\n对于许多数据集来说，WebText 语言模型在测试时会显著超出分布，需要预测经过严格标准化的文本、分词伪影（如断开的标点符号和缩写）、洗牌后的句子，甚至是字符串 ，后者在 WebText 中极为罕见——在 40 亿字节的文本中仅出现 26 次。我们在表 3 中报告了使用可逆的反分词器（去除尽可能多的分词/预处理伪影）所得到的主要结果。由于这些反分词器是可逆的，我们仍然可以计算数据集的对数概率，它们可以被视为一种简单的领域适应方法。我们观察到，使用这些反分词器时，GPT-2 的困惑度提高了 2.5 到 5。\nWebText 语言模型在不同领域和数据集之间迁移良好，在 8 个数据集中有 7 个数据集的零-shot 设置中提升了当前的最佳表现。在如 Penn Treebank 和 WikiText-2 等小型数据集上，取得了显著的改进，这些数据集的训练标记数仅为 100 万到 200 万个。在衡量长期依赖性的测量数据集（如 LAMBADA（Paperno et al., 2016）和儿童图书测试（Hill et al., 2015））上，也观察到了显著的改进。然而，在 One Billion Word Benchmark（Chelba et al., 2013）上，我们的模型仍显著逊色于先前的工作。这可能是由于 One Billion Word 数据集既是最大的数据集，又具有一些最具破坏性的预处理——1BW 的句子级洗牌去除了所有的长距离结构。\n3.2. 儿童图书测试 (Children’s Book Test, CBT) 儿童图书测试（CBT）（Hill et al., 2015）旨在考察语言模型在不同类别的词汇上的表现：命名实体、名词、动词和介词。与其报告困惑度作为评估指标，CBT 通过自动构建的完形填空测试（cloze test）来报告准确度，任务是预测在 10 个可能的选择中，哪个是正确的缺失词。按照原论文中提出的语言模型方法，我们计算每个选择的概率，并在给定该选择的条件下计算句子的其余部分的概率，最后预测具有最高概率的选项。正如图 2 所示，随着模型大小的增加，性能稳步提升，并缩小了与人类表现之间的大部分差距。数据重叠分析显示，CBT 测试集中有一本书《丛林之书》（The Jungle Book），其作者为鲁德亚德·吉卜林（Rudyard Kipling），它出现在 WebText 数据集中，因此我们报告在验证集上的结果，该验证集与 WebText 数据集没有显著重叠。GPT-2 在常见名词类别上取得了新的最先进成绩：93.3%，在命名实体类别上取得了 89.1%。我们还应用了反分词器，去除 CBT 中的 PTB 风格分词伪影。\n3.3. LAMBADA LAMBADA 数据集（Paperno et al., 2016）测试系统建模文本中长程依赖关系的能力。任务是预测句子的最后一个词，这些句子至少需要 50 个标记的上下文才能让人类成功预测。GPT-2 将困惑度从 99.8（Grave et al., 2016）降低到 8.6，并将语言模型在该测试中的准确度从 19%（Dehghani et al., 2018）提高到 52.66%。对 GPT-2 错误的分析表明，大多数预测是句子的有效延续，但不是有效的最后词。这表明语言模型没有利用“该词必须是句子的最后一个词”这一附加约束。通过添加一个停用词过滤器来作为这种约束的近似方法，准确率进一步提高到 63.24%，在这一任务上提升了 4%的最先进水平。之前的最先进结果（Hoang et al., 2018）使用了不同的受限预测设置，其中模型的输出仅限于上下文中出现的单词。对于 GPT-2 来说，这种限制反而是有害的，因为 19% 的答案并不在上下文中。我们使用了没有预处理的数据集版本。\n3.4. Winograd Schema Challenge Winograd Schema Challenge（Levesque et al., 2012）旨在通过测量系统解决文本中歧义的能力，评估其常识推理能力。最近，Trinh \u0026amp; Le（2018）通过使用语言模型（LMs）显著提高了这一挑战的表现，通过更高的概率预测歧义的解决方案。我们遵循他们的问题表述，并通过完整和部分评分技术可视化我们模型的表现（见图 3）。GPT-2 将最先进的准确率提高了 7%，达到了 70.70%。该数据集相对较小，仅有 273 个示例，因此我们建议阅读 Trichelair 等人（2018）的工作，以帮助理解这一结果。\n3.5. 阅读理解 对话式问答数据集（CoQA）（Reddy et al., 2018）包含来自 7 个不同领域的文档，并配有问题提问者与回答者之间关于文档的自然语言对话。CoQA 测试了阅读理解能力，以及模型回答依赖于对话历史的问题（例如，“为什么？”）的能力。\n在给定文档、相关对话历史以及最终标记 A: 的条件下，从 GPT-2 进行贪婪解码时，在开发集上达到了 55 的 F1 分数。这一成绩与 4 个基准系统中的 3 个匹配或超过，而这些基准系统使用了 127,000 多个手动收集的问答对进行训练。当前最先进的监督方法（基于 BERT 的系统）（Devlin et al., 2018）接近人类 89 F1 的表现。虽然 GPT-2 在没有任何监督训练的情况下取得的表现令人兴奋，但对其答案和错误的部分检查表明，GPT-2 经常使用简单的基于检索的启发式方法，例如在回答 “谁？” 问题时，返回文档中的某个名字。\n3.6. 摘要生成 我们测试了 GPT-2 在 CNN 和 Daily Mail 数据集（Nallapati et al., 2016）上的摘要生成能力。为了引导摘要生成行为，我们在文章后添加了文本 \u0026ldquo;TL;DR:\u0026quot;，然后使用 Top-k 随机采样（Fan et al., 2018）生成 100 个词，k 设置为 2，这可以减少重复，并鼓励生成比贪婪解码更抽象的摘要。我们使用这 100 个词中的前三个句子作为摘要。虽然从定性上看，生成的文本类似于摘要（见表 14），但它们通常聚焦于文章中的最近内容，或者混淆了具体细节，比如涉及多少辆车的事故，或者一个标志是出现在帽子上还是衬衫上。在常用的 ROUGE 1、2、L 指标上，生成的摘要只有在接近经典神经基准的表现时，才刚刚超越了从文章中随机选择 3 个句子的表现。当任务提示被移除时，GPT-2 在综合指标上的表现下降了 6.4 个点，这证明了通过自然语言在语言模型中引发特定任务行为的能力。\n3.7. 翻译 我们测试了 GPT-2 是否已经学会了如何将一种语言翻译成另一种语言。为了帮助模型推断这是一个期望的任务，我们将语言模型条件化在一组示例对的上下文中，格式为 \u0026ldquo;英语句子 = 法语句子\u0026rdquo;，然后在最后的提示 \u0026ldquo;英语句子 = \u0026quot; 后，从模型中进行贪婪解码并使用第一个生成的句子作为翻译。在 WMT-14 英法翻译测试集上，GPT-2 获得了 5 BLEU，略逊色于先前无监督词汇翻译研究（Conneau et al., 2017b）中通过双语词典推断的逐词替换的翻译结果。在 WMT-14 法英翻译测试集上，GPT-2 能够利用其非常强大的英语语言模型，表现显著更好，达到了 11.5 BLEU。这超过了几种无监督机器翻译基准（Artetxe et al., 2017；Lample et al., 2017），但仍然远远低于当前最好的无监督机器翻译方法（Artetxe et al., 2019）取得的 33.5 BLEU。这个任务的表现让我们感到意外，因为我们在 WebText 的过滤步骤中故意移除了非英语网页。为了确认这一点，我们对 WebText 进行了字节级语言检测，结果只检测到 10MB 的法语数据，这大约是先前无监督机器翻译研究中常见的单语法语语料库的 500 倍小。\n3.8. 问答 测试语言模型中包含了哪些信息的一个潜在方法是评估它生成事实性问题正确答案的频率。之前在神经系统中展示的这种行为，如《神经对话模型》（Vinyals \u0026amp; Le, 2015），由于缺乏高质量的评估数据集，结果仅限于定性的展示。最近推出的自然语言问题数据集（Kwiatkowski et al., 2019）是一个很有前景的资源，可以更定量地测试这一点。与翻译类似，语言模型的上下文是通过示例问答对进行初始化，帮助模型推断数据集的简短回答风格。\n在使用常见的精确匹配指标对问答进行评估时，GPT-2 正确回答了 4.1% 的问题，该指标通常用于诸如 SQUAD 之类的阅读理解数据集。作为对比，最小的模型的准确率不超过 1.0%，它仅仅返回每个问题类型（如谁、什么、哪里等）的最常见答案，这是一个极其简单的基线。GPT-2 正确回答了 5.3 倍更多的问题，这表明模型容量在神经系统在此类任务上的糟糕表现中起到了重要作用。\nGPT-2 对其生成的答案分配的概率已经很好地进行了校准，GPT-2 在最有信心的 1% 问题上的准确率为 63.1%。GPT-2 在开发集问题上生成的 30 个最有信心的答案显示在表 5 中。尽管如此，GPT-2 的表现仍然远远不及开放领域问答系统的 30% 到 50% 的表现范围，这些系统将信息检索与提取式文档问答相结合（Alberti et al., 2019）。\n4. 泛化与记忆 最近在计算机视觉领域的研究表明，常见的图像数据集包含大量几乎重复的图像。例如，CIFAR-10 的训练集和测试集之间有 3.3% 的重叠（Barz \u0026amp; Denzler, 2019）。这导致了机器学习系统泛化性能的过度报告。随着数据集规模的增加，这一问题变得越来越可能，提示 WebText 可能也会出现类似的现象。因此，分析测试数据中有多少也出现在训练数据中是很重要的。\n为了研究这个问题，我们创建了包含 WebText 训练集标记的 8-gram 的布隆过滤器。为了提高召回率，字符串被规范化为只包含小写字母数字单词，并使用单个空格作为分隔符。布隆过滤器的构建方式使得假阳性率的上限为 $1 \\times 10^{-8}$。我们通过生成 100 万个字符串来进一步验证假阳性率，其中没有一个被过滤器找到。\n这些布隆过滤器使我们能够计算给定数据集中的 8-gram 有多少也出现在 WebText 训练集中。表 6 显示了常见语言模型基准测试的测试集的重叠分析。常见语言模型数据集的测试集与 WebText 训练集之间的重叠率在 1% 到 6% 之间，平均重叠率为 3.2%。令人有些惊讶的是，许多数据集与它们自己训练集的重叠较大，平均重叠率为 5.9%。\n我们的方法优化了召回率，尽管手动检查重叠时发现了许多常见的短语，但也有许多较长的匹配是由于数据重复。这并不是 WebText 独有的现象。例如，我们发现 WikiText-103 的测试集中有一篇文章也出现在训练集中。由于测试集中只有 60 篇文章，因此至少有 1.6% 的重叠。更令人担忧的是，根据我们的程序，1BW 的测试集与其自身训练集的重叠率接近 13.2%。\n在 Winograd Schema 挑战中，我们发现只有 10 个 schema 与 WebText 训练集有 8-gram 重叠。其中 2 个是虚假的匹配。在剩下的 8 个 schema 中，只有 1 个 schema 出现在会泄露答案的上下文中。\n在 CoQA 中，约有 15% 的新闻领域文档已经出现在 WebText 中，模型在这些文档上表现好约 3 F1。CoQA 的开发集指标报告了 5 个不同领域的平均表现，我们由于不同领域之间的重叠，测得约 0.5-1.0 F1 的增益。然而，由于 CoQA 发布的时间晚于 WebText 中链接的截止日期，因此 WebText 中并没有实际的训练问题或答案。\n在 LAMBADA 中，平均重叠率为 1.2%。GPT-2 在具有超过 15% 重叠的样本上表现好约 2 个 perplexity。当排除所有具有任何重叠的样本时，重新计算指标，结果从 8.6 perplexity 改变为 8.7，并且准确率从 63.2% 降至 62.9%。这一非常小的整体结果变化可能是由于只有 1/200 的样本具有显著重叠。\n总体而言，我们的分析表明，WebText 训练数据与特定评估数据集之间的数据重叠对报告的结果提供了一个小但一致的益处。然而，对于大多数数据集，我们没有注意到比标准训练集和测试集之间已有的重叠更大的重叠，如表 6 所示。理解和量化高度相似的文本如何影响性能是一个重要的研究问题。更好的去重技术，如可扩展的模糊匹配，也可能有助于更好地回答这些问题。现在，我们建议在创建新的 NLP 数据集的训练和测试集时，使用基于 n-gram 重叠的去重作为一个重要的验证步骤和理性检查。\n另一种可能的确定 WebText LMs 性能是否归因于记忆化的方法是检查它们在自己的 held-out 集上的表现。如图 4 所示，WebText 的训练集和测试集上的表现相似，且随着模型大小的增加表现一起改善。这表明，即使是 GPT-2，在 WebText 上仍然在许多方面存在欠拟合。\nGPT-2 还能够写关于会说话的独角兽的新闻文章。一个示例见表 13。\n5.相关工作 本研究的一个重要部分是衡量在更大数据集上训练的大型语言模型的性能。这与 Jozefowicz 等人（2016）的工作类似，他们在 1 亿单词基准测试上扩展了基于 RNN 的语言模型。Bajgar 等人（2016）也通过创建一个更大的训练数据集，结合了 Project Gutenberg 的内容，以补充标准训练数据集，从而提高了在儿童书籍测试（Children’s Book Test）中的结果。Hestness 等人（2017）对各种深度学习模型的性能如何随模型容量和数据集大小的变化进行的深入分析也具有启发性。我们的实验虽然在任务间噪音较大，但也表明类似的趋势在目标的子任务上依然成立，并延续至超过 10 亿参数的模型范围。\n在生成模型中，已有研究记录了有趣的学习功能，例如在 RNN 语言模型中，单元执行行宽跟踪和引号/评论检测（Karpathy 等人，2015）。对我们工作的启发性观察来自 Liu 等人（2018）的发现，即训练生成 Wikipedia 文章的模型也学会了在不同语言之间翻译名字。\n此前的研究还探索了替代方法，用于过滤和构建大型网页文本语料库，如 iWeb 语料库（Davies，2018）。此外，针对语言任务的预训练方法也有大量研究。除了介绍中提到的那些，GloVe（Pennington 等人，2014）扩展了词向量表示学习到整个 Common Crawl 数据集。深度表示学习在文本中的影响力早期工作之一是 Skip-thought Vectors（Kiros 等人，2015）。McCann 等人（2017）探索了基于机器翻译模型得到的表示的使用，Howard \u0026amp; Ruder（2018）则改进了（Dai \u0026amp; Le，2015）提出的基于 RNN 的微调方法。Conneau 等人（2017a）研究了自然语言推理模型学习的表示的迁移性能，Subramanian 等人（2018）则探索了大规模多任务训练。\nRamachandran 等人（2016）证明了 seq2seq 模型通过使用预训练语言模型作为编码器和解码器初始化时，会获得显著的性能提升。更近期的工作表明，当微调用于复杂的生成任务，如闲聊对话和基于对话的问题回答系统时，语言模型的预训练也是有帮助的（Wolf 等人，2019；Dinan 等人，2018）。\n6.讨论 许多研究致力于学习（Hill 等人，2016）、理解（Levy \u0026amp; Goldberg，2014）以及批判性地评估（Wieting \u0026amp; Kiela，2019）监督和无监督预训练方法的表示。我们的结果表明，无监督任务学习是一个值得探索的额外有前景的研究领域。这些发现可能有助于解释预训练技术在下游 NLP 任务中广泛成功的原因，因为我们展示了在极限情况下，这些预训练技术之一开始直接学习执行任务，而无需监督适应或修改。\n在阅读理解方面，GPT-2 在零-shot 设置下的表现与监督基准模型相当。然而，在其他任务（如摘要生成）上，尽管它在定性上能够执行任务，但根据定量指标，其表现仍然只是初步的。尽管这一研究结果具有启示性，但在实际应用方面，GPT-2 的零-shot 性能仍然远未达到可用水平。\n我们已经研究了 WebText LMs 在许多经典 NLP 任务上的零-shot 性能，但仍有许多额外的任务可以进行评估。无疑，在许多实际任务中，GPT-2 的表现仍然不比随机更好。即使是在我们评估的常见任务（如问答和翻译）上，语言模型只有在具备足够的容量时，才会开始超越简单的基准模型。\n虽然零-shot 性能为 GPT-2 在许多任务中的潜在表现建立了基准，但目前尚不清楚在微调之后其性能的上限在哪里。在某些任务上，GPT-2 完全抽象化的输出与基于抽取式指针网络（Vinyals 等人，2015）的输出显著不同，后者目前在许多问答和阅读理解数据集上是最先进的。鉴于 GPT 微调的先前成功，我们计划在 decaNLP 和 GLUE 等基准测试上进一步研究微调，尤其是因为尚不清楚额外的微调训练数据和 GPT-2 的容量是否足以克服 BERT（Devlin 等人，2018）所展示的单向表示的低效性。\n7.结论 当一个大型语言模型在足够大且多样化的数据集上训练时，它能够在许多领域和数据集上表现良好。GPT-2 在8个测试的语言建模数据集中的7个上达到了最先进的零-shot表现。该模型在零-shot设置下能够执行的任务的多样性表明，经过训练以最大化足够多样化文本语料库的似然的高容量模型，开始学会在无需显式监督的情况下执行大量令人惊讶的任务。\n","date":"2025-01-08T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/language-models-are-unsupervised-multitask-learners/tree-3072431_1280_hu12245760729682699742.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/language-models-are-unsupervised-multitask-learners/","title":"Language Models are Unsupervised Multitask Learners"},{"content":"调研论文速览2 A Practical and Privacy-Preserving Framework for Real-World Large Language Model Services 《一种实际且隐私保护的框架用于现实世界的大语言模型服务》\n大语言模型（LLMs）在文本理解和生成方面展现了卓越的能力，越来越多地被应用于各个领域以提升生产力。然而，由于训练和维护这些模型的高成本，再加上一些LLM是专有的，个人往往依赖于LLM公司提供的在线人工智能即服务（AIaaS）。这种商业模式带来了显著的隐私风险，因为服务提供商可能会利用用户的痕迹模式和行为数据。本文提出了一种实际且隐私保护的框架，通过防止服务提供商将请求与提交者个人关联，确保用户的匿名性。我们的框架基于部分盲签名，保证用户请求的不可链接性。此外，我们还提出了两种针对基于订阅和基于API的服务模型的策略，确保既保护用户的隐私，又维护服务提供商的利益。该框架设计与现有LLM系统无缝集成，因为它不需要修改底层架构。实验结果表明，我们的框架几乎不增加计算和通信开销，使其成为现实应用中可行的解决方案。\n引言 训练架构和数据的快速进展使得大语言模型（LLMs）在内容分析和生成方面表现出色。鉴于这些能力，LLM已广泛应用于各个领域，以提高生产力和效率，包括医学[1]、机器人技术[2, 3]和教育[4]。然而，训练和维护这些模型的高成本（例如，LLaMa-3是在15万亿多语言标记上进行预训练的[5]），以及一些LLM无法公开访问的事实，为个人和小企业开发和维护自己的LLM带来了重大挑战。因此，许多专注于LLM的公司开始提供在线LLM服务，这通常被称为人工智能即服务（AIaaS）（例如，OpenAI的ChatGPT、Google的Gemini）。尽管AIaaS带来了好处，但依赖这些服务引发了严重的隐私问题，特别是用户信息可能被曝光。服务提供商可能会记录与用户请求相关的信息，如请求者的身份、请求时间和响应。这些信息可能被用来追溯查询到个人用户，从而损害用户的隐私。因此，LLM服务中迫切需要保证匿名性，这与其他在线服务中的隐私保护需求类似。\n在LLM服务中的隐私保护方法中，一种常见的做法是加密用户请求数据，使得服务器无法解释请求的内容。常用的技术包括同态加密（HE）[6, 7, 8]、多方计算（MPC）[9, 10]和秘密共享（SS）[11]。然而，这些方法通常需要大量资源，带来显著的计算需求和通信开销，同时还需要修改现有架构以支持加密。此外，虽然这些技术可以防止服务提供商了解查询的内容，但请求和响应的时间等侧信道信息仍然可能无意中泄露用户的活动模式。\n为了解决这些问题，一个实用的安全框架应满足几个关键属性，以保证LLM服务中的用户匿名性。首先，该框架应防止服务提供商将请求与单个用户关联，从而保护匿名性。其次，它应具有足够的通用性，能够与现有LLM架构兼容，确保实用性并与其他系统模块无缝集成。本文提出了一个满足这些要求的框架。我们的框架将现有的LLM服务视为黑盒，并作为一个附加层操作，对底层系统完全透明，即无需对现有基础设施进行更改。此外，该框架采用盲签名来确保请求的完全匿名性，防止服务提供商追溯请求的来源。具体而言，系统采用部分盲签名和定制策略来解决LLM服务中两种常见的商业模式。对于订阅模式，基于部分盲签名的方案通过限制订阅期，允许用户在指定时间内进行无限次请求。相比之下，基于API的模式采用另一种基于部分盲签名的方案，结合请求级别的计费策略，限制用户的请求次数或资源（例如，预定义的请求中发送的令牌数量）。\n本框架的贡献如下：\n该框架提供了一种实用的解决方案，在保持LLM服务中用户匿名性的同时，适应现有的商业模式。 提议的系统可以无缝集成到当前系统中，几乎不增加计算和通信开销。 框架展示了部分盲签名与定制策略在实际场景中的应用，展示了它在其他在线服务中的扩展潜力。 提供了框架的实验结果，衡量了系统中部分盲签名的性能。这些结果为部分盲签名相关的计算和通信成本提供了有价值的见解。 相关工作 A. 语言模型 语言模型（LMs）是预测给定上下文中后续或缺失标记概率的统计模型。大语言模型（LLMs）是预训练语言模型（PLMs）的一类，利用前所未有的规模进行机器学习[12]。LLM与其他PLM的主要区别在于其庞大的训练数据和模型规模，这使得它们具备了突现能力——这些能力是通过扩展模型规模而产生的行为[13]。这些模型在内容分析和生成方面展现了卓越的性能，通常能够在各种任务中实现类人表现[14]。LLM已广泛应用于多个领域，包括教育、金融和医疗保健[15]。此外，一种名为“提示工程”的技术应运而生，用于优化LLM在特定任务中的输出[16, 17]。\nLLM的开发涉及三个主要阶段：预训练、微调和推理。\n预训练：在预训练阶段，LLM在大规模数据集上进行训练，以获取一般知识。这些数据集通常包含来自不同来源的内容，如博客、文章和书籍。例如，2024年8月的CommonCrawl数据集包含了23亿网页。GPT-3是在包括过滤后的CommonCrawl数据和维基百科在内的数据集上进行预训练的，数据量约为3000亿个标记[18]。类似地，LLaMA-3的预训练数据集包含了15万亿个多语言标记[5]。预训练过程计算密集且耗时。例如，训练LLaMA-3.1 70B模型需要估计700万小时的H100-80GB硬件计算时间[4]。 微调：预训练完成后，LLM会针对特定任务进行微调。用于微调的数据集通常较小，并包含领域特定的知识。在这个阶段，通常会采用人类反馈的强化学习（RLHF）来优化模型的性能，正如GPT-4的训练过程中所采用的那样[19]。 推理：一旦完成预训练和微调，LLM就可以用于推理。然而，部署LLM面临着显著的存储和计算挑战，因为它们的架构复杂，模型规模庞大，通常包含数十亿个参数。因此，对于个人用户来说，在本地运行LLM通常是不切实际的。为了解决这些挑战，许多公司提供了AI即服务（AIaaS）的LLM服务，例如OpenAI的ChatGPT和Google的Gemini。 B. LLM推理阶段中的隐私保护 在当前的商业模式中，LLM公司通常部署训练好的模型，并通过在线服务将推理阶段暴露给用户。这些服务允许用户提交输入数据，系统基于这些数据返回LLM生成的推理结果。然而，这种方法可能会暴露用户潜在的隐私风险。为了利用LLM进行文本分析或生成，用户通常需要在请求中提供信息，其中可能包括敏感数据。\n为了解决这些问题，许多研究工作专注于增强LLM推理过程中的用户隐私保护。**同态加密（HE）**是其中一种方法，它允许对加密数据进行计算。通过将HE集成到LLM中，用户可以提交加密的输入数据进行推理，只有最终结果需要解密。像[7]和[20]这样的研究提出了基于HE的协议，以高效地执行矩阵乘法——这一在变换器模型中常见的操作，从而实现对加密数据的计算。另一种方法是[6]提出的加密友好的近似技术，用于优化变换器中的组件，以减少处理加密数据时的计算开销。\n另一种密码学技术是多方计算（MPC），在这种方法中，多方共同计算部分结果，且没有任何一方可以访问完整的输入或结果。最终的输出通过结合每一方的部分结果来重建。[21]提出了一种协议，确保在包括模型所有者、MPC服务器和用户的三方系统中，任何单一的MPC服务器都无法独立恢复用户的输入数据。\n**差分隐私（DP）**是另一种保护用户隐私的著名技术，确保无法从数据中推断出单个用户的信息。[22]将DP应用于扰动LLM输出，而[23]提出了DP-forward技术，在前向传播过程中扰动嵌入矩阵，以确保训练和推理阶段的安全性。尽管这些方法提供了不同程度的安全性，但它们通常会引入大量的计算和通信开销，导致在实际场景中的推理变得不切实际。例如，在一个安全的三方协议下，生成一个64个单词的句子可能需要长达20分钟的时间[21]。此外，某些侧信道漏洞，如用户请求的时机和数据的大小，仍然可能被服务提供商利用。\n盲签名 盲签名方案最早由Chaum [24]提出，是一种交互式密码协议，允许签名者在不查看消息内容的情况下对其进行认证。这保证了请求者的匿名性，同时保持签名消息的完整性。该方案的一个变体，称为部分盲签名，随后被提出，允许请求者和签名者共同预先商定部分消息内容。这种变体在实际场景中尤为有用，例如在需要共享信息（如过期日期或发行时间戳）时 [25]。盲签名已广泛应用于确保用户匿名的系统中。Chaum的开创性工作——不可追踪的数字现金——使用盲签名来追踪双重支付，同时确保交易无法追溯到花费者的身份 [26]。此外，盲签名还被集成到许多电子投票系统中，用以保护选民的身份并维护投票过程的机密性 [27]。类似地，[28]提议将盲签名集成到比特币协议中，以使交易接收者与其公钥解关联，从而增强隐私性。\nBOLT: Privacy-Preserving, Accurate and Efficient Inference for Transformers 摘要——变压器（transformers）的出现带来了传统机器学习任务的重大进展。然而，它们的广泛应用引发了对推理过程中敏感信息泄露的潜在担忧。现有的基于安全多方计算（MPC）的方法在应用于变压器模型时面临局限性，主要由于模型规模庞大以及资源密集型的矩阵乘法运算。本文提出了BOLT，一个用于变压器模型的隐私保护推理框架，支持高效的矩阵乘法和非线性计算。结合我们创新的机器学习优化方法，BOLT将通信成本降低了10.91倍。我们在不同数据集上的评估表明，BOLT在保持与浮点模型相当的准确性的同时，在各种网络环境下实现了比现有最先进系统快4.8到9.5倍的推理速度。\n引言 变压器模型（Transformer models）近年来已成为一项颠覆性技术。ChatGPT [1] 使得基于变压器的语言模型的强大功能对所有人都可获取。与传统的监督学习和任务特定学习相比，大型变压器模型在大量未标注的文本数据上进行训练，并且直接适用于多种应用，如翻译、内容生成或问答。例如，它们可以在医疗领域用于通过分析电子健康记录、医学文献和临床笔记来识别模式和风险因素，从而显著推动诊断、治疗和药物发现 [66]，[5]，[62]。变压器是一种通用的神经网络架构，其特点是使用注意力机制 [71]。注意力机制能够有效地捕捉标记之间的关系，建模上下文信息并捕获输入标记序列中的长程依赖性。然而，尽管这些强大的模型具有优异的性能，但也伴随着隐私等方面的风险 [46]，[69]。ChatGPT 是机器学习即服务（MLaaS）的一个例子，其中服务器（OpenAI）托管一个专有模型，用户将数据输入模型并返回预测结果。然而，MLaaS 设置会引发隐私问题：用户要么必须将私密数据上传到公司的服务器，要么服务器需要将专有模型存储在用户的边缘设备上。ChatGPT 操作的就是前者设置。因此，用户数据隐私的严重问题被提出，甚至导致意大利暂时禁用 ChatGPT [47]，[55]，[45]。虽然隐私权与有效的数据分析似乎存在冲突，但我们可以采用一种称为安全多方计算（MPC）的密码学方法，在不影响功能的情况下保障数据和模型的隐私。从高层次看，MPC 允许 n 个参与方 p1, …, pn，具有相应的输入 x1, …, xn，通过计算公共函数 f(x1, …, xn) 的输出，而无需向其他方泄露各自的 xi。每个方的数据在整个计算过程中都有效“加密”。在计算结束时，每个方只会知道最终结果，而无法获得任何额外的信息。虽然 MPC 可以用于通用计算，但由于加密原语的开销，它通常会带来大量的计算和通信成本。近期的研究集中在开发针对特定工作负载的优化解决方案，以提高卷积神经网络（CNN）的私密评估和训练效率 [49]，[50]，[25]，[41]，[60]，[4]。然而，保护变压器模型的隐私尤其具有挑战性，原因有以下几点：\n变压器的规模远大于卷积神经网络，包含数亿到数十亿个参数 [17]，[52]，[56]，[57]。 先前关于私密神经网络的研究提出了优化的私密矩阵-向量乘法协议，但变压器需要进行大规模的矩阵-矩阵乘法。 安全地评估变压器中的成千上万个输入以进行复杂的非线性计算是非常昂贵的。 因此，需要开发新的协议来保护变压器推理的隐私。根据我们所知，Iron [28] 是目前最先进的系统，研究了如何在标准变压器推理过程中完全保护数据隐私。然而，由于其显著的性能开销，它在提供实际可行的解决方案方面存在不足。例如，我们对 Iron 的实现要求 280.99 GB 的通信，用时 216 分钟完成 BERT-base 模型（110M 参数 [17]）在 WAN 设置（100 Mbps, 80 ms）下的端到端推理。随着系统和硬件的持续发展 [63]，[64]，[36]，[37]，[68]，我们认为，减少网络通信至关重要，因为计算可以加速和并行化。BOLT 中的线性和非线性协议以及机器学习优化方法显著减少了通信开销，同时保持了计算效率，标志着隐私保护变压器推理的实用性取得了重要进展。\n我们的贡献 我们提出了 BOLT，一个新颖的隐私保护推理系统，用于变压器模型，它通过减少通信开销来解决上述挑战，从而提高了端到端的运行时性能。BOLT 保证了客户端输入数据的机密性，同时保护了服务提供商的知识产权——其模型。BOLT 集成了加密改进、非线性函数的准确高效近似以及从机器学习角度出发的算法增强。\n通信优化和计算高效的线性操作 在 BOLT 中，我们面临的第一个挑战是变压器模型中的线性操作要复杂得多，且具有较高的乘法深度。许多过去的安全推理协议仅将同态加密（HE）用于明文-密文的矩阵-向量乘法，这些操作通常出现在卷积神经网络中。Iron 通过使用 HE 和 MPC 的混合来解决这个问题，但这种方法导致了较高的通信成本。相反，BOLT 的加密改进仅使用 HE 来有效节省通信成本。我们在 HE 中开发了计算高效、低深度的算法。我们的第一个思路是对密文-明文矩阵-矩阵乘法进行替代性解释，这样可以实现优化的密文打包。先前的工作 [35]，[28] 浪费了一部分通信，因为它们让一些密文槽“空着”。然而，单纯改变打包方式并不够，因为这仍然需要大量计算开销较大的旋转操作来执行变压器模型中的矩阵乘法。我们提出了第二个优化方案来解决这个问题：我们将婴儿步-巨人步策略 [27]，[7] 应用于我们的矩阵乘法算法，以减少输入密文旋转的次数，而是将旋转操作应用于部分求和的中间结果。这使得 BERT-base 模型的密文旋转次数减少了 2.33 倍到 9.33 倍。最后，我们设计了高效且低乘法深度的密文-密文矩阵乘法算法，以提高 HE 中的计算效率。\n准确且高效的非线性操作 对于非线性操作，准确和高效的协议也是至关重要的。Iron 表明，超过 75% 的总运行时间都花费在非线性层上，这是因为将明文非线性公式直接转换为 MPC 协议时，涉及的复杂计算非常昂贵。这些计算包括三角函数和指数函数，这些在 MPC 中本身就已经非常昂贵 [59]。为了解决这个问题，我们为 GELU 和 Tanh 引入了两种高精度的多项式近似，分别是 4 阶和 5 阶多项式，并且优化了 Softmax 程序。考虑到即使是四次或五次乘法，在输入维度较大的情况下（例如，BERT-base 中 12 层的每个 GELU 函数的维度为 128 × 3072），这些操作也可能非常昂贵，我们基于 [51] 提出了一个额外的优化，这个优化对于安全计算领域具有独立意义：一种多项式预处理技术，允许在已知多项式系数的情况下，将评估阶数为 n 的多项式（霍纳法则）的乘法次数减少到大约 ⌈ n / 2 ⌉ 次。\n机器学习优化 我们还提出了机器学习优化，以提高效率和准确性。具体而言，我们开发了基于注意力得分的“无关词消除”。这些得分衡量了输入标记之间的相关性，作为标记重要性的度量。然后，我们应用无关的比托尼克排序（Oblivious Bitonic Sort）来丢弃贡献较小的标记，从而显著减少输入大小。此外，我们还利用安全计算感知的微调来弥合安全计算和浮动点计算之间的差距，进一步提高准确性。\n结合我们所有的优化，BOLT 在不同的网络设置下比 Iron 减少了 10.91 倍的通信，并且总运行时间提高了 4.8 到 9.5 倍。我们认为这是向实用的隐私保护变压器推理迈出的重要一步。我们的代码已开源，您可以在 https://github.com/Clive2312/BOLT 找到。\nCurl: Private LLMs through Wavelet-Encoded Look-Up Tables Curl: 通过小波编码查找表实现私有化大语言模型（LLMs）\n最近，变压器模型的进展已经彻底改变了机器学习，成为大规模语言模型（LLMs）的核心。然而，将这些系统集成到日常应用中时，暴露客户查询给模型所有者引发了隐私问题。安全多方计算（MPC）允许各方在保持敏感用户输入和专有模型私密的情况下评估机器学习应用程序。由于MPC本身的成本，最近的研究提出了针对特定模型的优化，但这些优化妨碍了机器学习研究人员的广泛采用。CrypTen（NeurIPS’21）旨在通过常见的机器学习抽象（如张量和模块化神经网络）暴露MPC原语来解决这个问题。不幸的是，CrypTen和许多其他MPC框架依赖于非线性函数的多项式近似，导致较高的误差和通信复杂性。\n本文介绍了Curl，这是一个易于使用的MPC框架，通过查找表来评估非线性函数，从而获得更好的近似，并显著减少了回合和通信开销。Curl暴露了与CrypTen类似的编程模型，并且通过张量实现高度并行化。Curl的核心依赖于离散小波变换，以减少查找表的大小而不牺牲准确性，从而在非线性函数（如对数和倒数）的评估中，相较于CrypTen实现了最多19倍的回合和通信减少。我们在包括BERT、GPT-2和GPT Neo在内的多种LLM上评估了Curl，并与一些最先进的相关工作进行了比较，如Iron（NeurIPS’22）和Bolt（S\u0026amp;P’24），在通信和延迟方面至少减少了1.9倍。\n最后，我们通过在独立模型中证明广泛使用的概率截断协议的安全性，解决了关于其安全性的长期争议。此问题具有独立意义，因为许多相关工作依赖于这种截断样式。\n引言 像GPT-2、GPT-4 [1]、BERT [19] 和 LLaMA [66] 这样的“大型语言模型”（LLMs）已经成为展示人工智能能力的典范。LLM帮助个人和企业完成日常任务，从机器翻译 [5]、文本生成 [31] 到问答系统 [56] 等等。为了生成类人响应，LLM已在大量数据上进行训练，并通过与用户的互动不断学习。然而，随着LLM日益融入人们的生活，隐私问题成为了一个关键关注点，因为个人经常分享敏感信息，包括姓名、地址、信用卡号码，甚至是财务和医疗信息 [60]。更进一步，个性化AI的发展也在推动这种趋势，OpenAI为ChatGPT启用了记忆功能 [28]。为了使一个基于LLM的助手能够理解个人偏好、习惯和工作流程，并提供量身定制的帮助，它需要访问大量个人数据。随着这些个性化数据的保留并用于持续改进LLM，数据泄露或未经授权访问的可能性对个人隐私构成了巨大的风险。\n隐私增强技术（PETs），如多方计算（MPC） [27,75]，为隐私保护机器学习（PPML）用例提供了大量可能性。在最常见的设置中，服务器拥有一个专有模型，并旨在将其作为服务提供给客户，以便客户可以在不暴露任何数据给模型的情况下使用该模型 [13,45,46,37,32]。目标是使客户仅获得推理结果，而不会获取有关模型的任何信息，同时模型提供者不会了解客户的输入。另一个常见的PPML用例涉及多个不信任方共同安全地训练一个模型，使用他们的敏感数据，但彼此之间不暴露任何数据 [52,65,44,73,39]。一些MPC系统利用线性秘密共享方案，因为它们允许以最小的通信开销执行线性操作 [17,24]。然而，非线性操作（如平方根、对数）通常会带来更大的挑战，并需要专门的技术，如多项式近似 [45]、查找表评估 [69] 或其他特定协议的解决方案 [8,16]。此外，结合领域知识可以显著提高这些协议的效率和效果，通过调整参数更好地适应特定的使用场景 [35,47,57]。\n相关工作 聚焦于函数秘密共享 最近，几项安全计算研究通过利用函数秘密共享（FSS） [3,69,64,55,54] 取得了进展。Pika [69] 扩展了 [3] 的前期工作，展示了一种通过FSS安全评估查找表（LUTs）的新方法。尽管它在像MNIST和CIFAR-10这样的流行数据集上展现了有效性，但其扩展性仍然面临挑战。正如Grotto [64] 所指出的，对于大型LUT，其计算成本可能由于需要大量的分布式点函数（DPF）评估而使得协议不可行。与之相比，Curl通过集成离散小波变换（DWT）技术，避免了对DPF评估的需求，从而解决了这一挑战。此外，我们的技术还能通过减少LUT的大小，降低计算和通信的复杂性，从而使基于FSS的框架受益。\n另一方面，Grotto [64] 提出了利用自定义样条和DPF来高效处理部分函数的创新协议。然而，与Sigma [32]等替代方案相比，它在计算和通信开销方面仍面临挑战。Orca [39] 展示了GPU加速在FSS协议中的潜力，特别是为卷积神经网络（CNNs）量身定制的协议。然而，由于其依赖于复杂的非线性操作，Orca在面向其他架构（如变压器模型）时的适用性受到质疑 [39]。Sigma [32] 在Orca的基础上进行构建，并通过适用于小范围的协议，依赖最小大小的LUT来区别于其他方案。尽管其通过自定义协议和小尺寸的LUT展现了效率，但它仍然需要大量的计算和通信资源（例如1TB的RAM和大约9Gbps的通信链路）。此外，它使用确定性截断，尽管声称提高了安全性，但与概率截断方法相比，其速度仍显不足。不幸的是，所有基于FSS的工作都集中在两方设置中，因为当参与方超过两方时，FSS变得不切实际。\n聚焦于预处理 由Pika [69] 发起的这一研究方向，集中在带有额外经销商方的两方设置。在这种情况下，预处理阶段需要经销商准备并分发一个大小为O(n)的元素。在假设经销商不会与任何一方串通的前提下，减少对这种假设的依赖是理想的。这促使了无需经销商的协议的发展。值得注意的是，一些先前的工作已经试图在两方设置中实现这一点，使得双方能够独立生成所需的预处理材料 [38,18,6]。\n一次性真值表（OTTT）协议 [38] 使用布尔电路表示每个可能输入的表，导致相对于比特大小的指数级复杂性。OP-LUT协议 [18] 尝试增强OTTT预处理阶段，但仅在小比特大小下取得改进，通信成本依然是指数级的。SP-LUT协议 [18] 显著增强了预处理阶段，但修改了在线阶段，导致在线阶段在比特大小上的通信成本呈指数级增长。FLUTE [6] 相比于之前的工作有所进步，但在设置阶段仍然需要O(2n)的通信。因此，寻找一个具有亚指数级通信复杂度的预处理阶段，同时保持在线阶段的高效性，仍然是一个未解决的挑战。\n聚焦于全同态加密 全同态加密（FHE）方案通过使用查找表（LUT）显著提高了性能并开启了新的应用场景。这一概念最初由Ducas和Micciancio [22] 提出，他们设计了一种方法，使用LUT在FHE中评估任意二进制门。基于这一基础，Chillotti等人 [10] 通过一组分层多路复用器实现了任意函数的评估，从而发展出了Torus FHE（TFHE）方案。尽管他们开发了一种快速的自引导机制，能够在大约10毫秒内在CPU上执行，但其采用受限。这主要是因为多路复用器的控制输入需要新的密文，这意味着无法对其进行先前计算——而且这种方法要求将程序表示为确定性自动机。\nTFHE的进一步改进出现在[11]中，提出了可编程自引导（PBS）技术，允许高效和通用的LUT评估。为了增强适应性，HELM [29] 在PBS技术的基础上进行了扩展，并引入了一个框架，能够自动将Verilog硬件描述语言（HDL）转换为加密电路。HELM有三种操作模式。第一种模式专门处理二进制门。第二种模式使用安全的LUT评估来处理整数。第三种混合模式与二进制电路一起工作，并在返回二进制领域之前，使用LUT评估进行整数的安全处理。然而，HELM的限制在于它只能处理非常低精度的LUT。这一限制的原因在于，将整数转换为多个比特需要多个n对1的LUT。\n最近，Ripple [30] 提出了基于离散小波变换（DWT）理论的压缩技术，用于减小LUT的大小，从而加速PBS技术在平滑函数上的应用。\n聚焦于安全LLM推理 由于变压器相比传统神经网络具有更高的复杂性，安全LLM推理框架最近才开始受到关注。确保安全推理的技术包括一些著名的实现，例如THE-X [9] 和Iron [35]，它们是最早使用同态加密进行矩阵乘法和非线性函数计算的框架。随后，几项研究探讨了多方计算（MPC）技术在私有变压器推理系统中的应用 [47,21]。MPCFormer [47] 利用CrypTen [45] MPC引擎，而Puma [21] 则使用SecretFlow-SPU [51] MPC引擎来评估MPC在LLM推理中的适用性。MPCFormer引入了一种蒸馏过程，强模型训练弱模型以提高准确性，从而弥补使用小（2阶）非线性函数近似的局限性。然而，MPCFormer面临着近似精度损失的问题，需要对模型进行微调。另一方面，Puma使用GeLU激活函数的分段近似，这需要进行多达6阶的比较和多项式计算。\n最近的研究越来越多地关注混合解决方案，将特定操作的最佳技术结合起来 [57,39]。Bolt [57] 开发了一种整合全同态加密和MPC的解决方案。与Puma [21] 类似，Bolt使用分段近似，需要进行比较，这增加了轮次复杂度和通信开销。为了提高效率，Bolt采用了单词消除技术，同时保持准确性。此外，Bolt通过增加多项式的阶数来增强MPCFormer的多项式近似，并通过Motzkin多项式预处理程序减轻相应的效率损失。与此不同，Curl通过使用优化的查找表协议避免了比较和多项式近似，从而减少了通常由多项式近似带来的轮次和通信复杂度，同时保持了准确性。\n专注于安全截断 机器学习任务使用浮点数来构建精确的模型[34]。然而，广泛认为，使用浮点数会导致多方计算（MPC）在效率方面变得不切实际。为此，大多数关于机器学习在MPC中的研究采用了定点表示，而不是浮点表示，这需要一个截断协议来保持恒定的精度。在现有的截断方法中——确定性、最接近整数和概率性——确定性和概率性方法（见第3节）在安全计算环境中应用最为广泛[8,48]。此外，许多研究倾向于使用概率性截断，因为它具有较低的通信成本，从而提高了性能，例如[53,52,59,13,14,46,44,63]。尽管如此，一些最新的研究表达了对概率性截断的担忧，并选择了更加资源密集的协议来实现确定性截断[48,33,32]。关于概率性截断的担忧包括安全性和四舍五入误差。\n正确性误差 在讨论上述两个问题之前，我们回顾一下，由Catrina和Hoogh提出的广泛使用且廉价的概率性截断协议在域上具有正确性误差，误差概率为 $2^{-(n - |x|)}$，其中 $|x|$ 是截断输入 $x$ 的最大位数。这就要求使用显著更大的环，并设置 $n \\geq |x| + \\kappa$，其中 $\\kappa$ 是统计安全参数。Damgård 等人[15]提出了一个概率性协议，正确性误差为零，但它需要一个非常规的位分解协议。随后的研究[13,25]优化了[8,15]，并展示了一个常规回合的概率性截断协议，其通信和计算成本与Catrina和Hoogh[8]相匹配，但没有正确性误差，并且允许 $n \\geq |x| + 1$。CrypTen[45] 提出了一个受[70]启发的协议，用于公有值除法，可以简化为一个带有非零正确性误差的概率性截断。\n安全性 最近，Li等人[48]发现了[8]中概率性截断安全性证明的一个安全漏洞，揭示了该协议泄露的信息与理想的概率性截断功能之间的差异。值得注意的是，同样的问题也出现在[25]中的 $n$-最优概率性截断中。这导致了最近一些论文，如[32,32]，因对安全性问题的担忧而放弃了廉价的概率性截断协议。在本文中，我们展示了[25]中的 $n$-最优概率性截断协议（以及[8]中的原始概率性截断）安全地实现了我们定义的自然理想概率性截断功能。这使我们能够在不妥协安全性的情况下，利用概率性截断的效率优势（见第3节）。\n四舍五入误差 LLAMA论文[33]最近建议使用确定性截断，声称它能提高推理精度。然而，这一主张并没有得到实验证据的支持，而早期的研究表明，概率性截断可能会导致精度下降的结果，也未通过实验证明[63,13]。有趣的是，Gupta等人[34]证明，使用16位定点数并具有12位精度和概率性截断的计算，能够实现与32位浮点数几乎等效的精度。相反，在这样的条件下，截断到最接近整数无法有效地训练。此外，尽管这不是工作的主要焦点，Keller和Sun[43]提供了实验证据，表明在神经网络中，概率性截断与截断到最接近整数之间的差异很小。实际上，在28个不同的实验中，13个显示概率性截断具有更好的准确性，13个显示截断到最接近整数更优，2个实验的准确性相同。\n我们的贡献 在本文中，我们介绍了Curl，一个用户友好的多方计算（MPC）框架，旨在通过查找表（LUT）高效地评估非线性函数。Curl解决了安全协议中三个至关重要的方面：效率、准确性和安全性。Curl采用了一种新颖的LUT压缩技术，利用离散小波变换（DWT）实现高效的近似，并将误差降到最小。这有效地减小了原始LUT的大小，同时保持了非线性函数的准确性，从而超越了传统的多项式分段近似方法。通过最小化通信成本，Curl显著提高了端到端的运行时性能，并支持包括GeLU、SiLU、高斯误差函数、Sigmoid和双曲正切在内的广泛激活函数。此外，Curl将[57,32]中的GeLU协议中使用的分段近似方法推广到任何有界的奇偶函数，适用于任何收敛到分段多项式的函数。Curl的核心技术扩展到LLM（大语言模型）使用的复杂非线性函数，确保了客户输入数据的隐私，同时保护了服务提供商的知识产权，例如其模型。\n我们在多个LLM模型上评估了Curl，包括BERT（tiny、base、large）、GPT-2和GPT-Neo，涵盖了CPU和GPU后端。我们的评估显示，Curl在技术上超越了现有的最先进框架，达到了至少减少6.5倍的回合数和1.9倍的通信开销。最后，Curl通过引入自然理想功能、相应协议并提供仿真安全性证明，解决了关于高效概率性截断的安全性问题。许多依赖这种截断方式的工作已经被[48]证明存在安全漏洞，因为它们无法在真实-理想范式下进行仿真。我们的结果具有独立意义，因为我们的证明解决了这一争议，并恢复了人们对安全概率性截断协议的信心。\n我们总结了以下贡献：\n基于DWT压缩的创新框架，在低回合数和通信复杂度的前提下实现高精度。将其应用于LLM推理时，与现有方法相比，Curl实现了至少6.5倍的回合复杂度降低和1.9倍的通信降低。 Curl基于用户友好的CrypTen[45]框架，提供了高度的灵活性和较低的采用门槛，便于开发者和研究人员使用，从而促进了安全计算技术的普及。作为概念验证，我们实现了多种LLM模型和多个非线性函数，均可在CPU和GPU上运行。 我们引入了一种新颖的自然理想功能，用于高效的概率性截断，并证明了Escudero等人[25]的安全性。该结果具有独立意义。 East: Efficient and Accurate Secure Transformer Framework for Inference East: 高效且准确的安全Transformer推理框架\n摘要 Transformer因其强大的优势，已成功应用于实际场景，如ChatGPT。然而，在服务过程中，用户的输入会泄露给模型提供者。随着人们对隐私的关注，隐私保护的Transformer推理服务需求日益增加。非线性函数的安全协议在隐私保护的Transformer推理中至关重要，但这一领域的研究仍不充分。因此，设计实用的安全协议以处理非线性函数，尽管困难，却对模型性能具有重要意义。本文提出了一个名为East的框架，旨在实现高效且准确的安全Transformer推理。首先，我们提出了一种新的不可知分段多项式评估算法，并将其应用于激活函数，相较于现有方法，显著降低了GELU的运行时和通信开销，分别提升了1.5倍和2.5倍。其次，我们精心设计了softmax和层归一化的安全协议，确保准确地保持所需功能。第三，我们详细进行了若干优化，以提高整体效率。我们将East应用于BERT，并且结果表明，推理精度与明文推理一致，且无需微调。与Iron相比，我们在通信成本上降低了约1.8倍，运行时减少了约1.2倍。\nTHE-X[11] 是首个探索使用加密技术进行隐私保护Transformer推理（PPTi）的工作。在THE-X[11]中，GELU被替换为ReLU，且不支持tanh。THE-X[11]使用近似方法处理softmax和层归一化（LN），将Transformer转换为支持完全同态加密（HE）操作的函数。THE-X[11]存在几个问题：（1）THE-X[11]缺乏对服务器隐私的保护，因为在计算ReLU时，THE-X[11]将中间结果泄露给客户端，这可能导致服务器隐私泄露。（2）与明文推理相比，THE-X[11]的准确性平均下降超过1%。（3）在使用THE-X[11]时，原始模型需要转换为近似形式，这使得原始模型参数无法使用，并需要额外的模型重新训练。一些后续工作（[12][13]，[14]，[15]）通过采用更有效的近似方法并使用MPC确保隐私，解决了前两个问题。这些工作集中在Transformer的MPC友好近似上，而忽略了第三个问题。事实上，在明文训练模型时，MPC友好的Transformer可能会限制其灵活性。类似的通过改变模型架构加速计算的工作也在[16]中使用，该工作在文本生成任务中进行隐私保护。最近的工作Iron[17]提供了解决所有三个问题的方案。\nIron[17]保留了原始模型架构，并基于同态加密（HE）和秘密共享（SS）构建了多个隐私保护的Transformer协议。在Iron[17]中，矩阵乘法通过优化自Cheetah[9]的HE实现来构建，而非线性函数的安全协议则通过基于查找表（LUT）和SS的原语从SIRNN[18]中设计。最近，另一项工作[19]直接使用加密电路（GC）来处理非线性函数，这与Iron[17]不同。另一项工作[20]为高开销函数设计了高质量的近似，并获得了比[12]更好的结果，但[20]使用了安全三方计算（3PC）技术，这与2PC适用的场景不同。在上述工作中，据我们所知，Iron[17]是目前最先进的2PC PPTi工作。Iron[17]的障碍在于基于LUT的非线性函数带来了相当大的开销。在Iron[17]中，非线性函数的运行时和通信分别占总开销的约80%和87%。因此，设计既高效又安全的非线性函数协议，同时保持模型架构，是一个挑战。\n我们的贡献\n在本文中，我们设计了East，一个高效且准确的安全Transformer推理框架。本文的贡献如下：\n提出了用于激活函数（如GELU和tanh）的通信高效协议，采用我们新的隐式近似函数评估方法。与NFGen[21]相比，我们的协议在运行时和通信方面分别提高了约1.5倍和2.6倍。 通过提出的转换方法和误差限制确定方法，仔细设计了softmax和LN的协议。与Iron[17]相比，我们的协议在softmax的通信开销上减少了约1.3倍，在LN的运行时上减少了约1.2倍。 进行了多项优化以提升整体性能，包括线性层合并、截断优化、打包通信处理以及填充和掩蔽处理。 我们使用East进行了BERT[22]推理，结果表明推理准确性与明文推理一致。与Iron[17]相比，我们在非线性函数的运行时上减少了约1.2倍，通信开销减少了约1.8倍。\nEfficient Privacy-Preserving KAN Inference Using Homomorphic Encryption 使用同态加密进行高效的隐私保护KAN推理\n最近提出的Kolmogorov-Arnold网络（KANs）提供了更强的可解释性和更高的模型表现力。然而，KANs在推理过程中也面临隐私泄露的挑战。同态加密（HE）促进了深度学习模型的隐私保护推理，使资源有限的用户能够在确保数据安全的情况下受益于深度学习服务。然而，KANs的复杂结构，尤其是包含像SiLU激活函数和B样条函数等非线性元素，使现有的隐私保护推理技术无法满足需求。为了解决这一问题，我们提出了一种专为KANs量身定制的高效且准确的隐私保护推理方案。我们的方法引入了一种任务特定的多项式近似方法，用于SiLU激活函数，动态调整近似范围以确保在实际数据集上获得高精度。此外，我们还开发了一种高效的方法，用于在HE域内计算B样条函数，利用重复打包、懒惰组合和比较函数等技术。我们在符号公式评估和图像分类任务中评估了我们的隐私保护KAN推理方案的有效性。实验结果表明，我们的模型在多个数据集上实现了与明文KANs相当的准确性，并且优于明文MLPs。此外，在CIFAR-10数据集上，我们的推理延迟比原始方法快了超过7倍。\nENCRYPTION-FRIENDLY LLM ARCHITECTURE 适合加密的LLM架构\n大型语言模型（LLMs）根据用户互动提供个性化回应，但这一应用场景引发了严重的隐私问题。同态加密（HE）是一种支持加密状态下算术运算的加密协议，并为隐私保护机器学习（PPML）提供了潜在的解决方案。然而，变压器模型的计算强度给将HE应用于LLMs带来了挑战。在这项工作中，我们提出了一种修改后的HE友好型变压器架构，重点关注个性化（私密）微调后的推理。通过利用LoRA微调和高斯核，我们在保持与明文模型相当的性能的同时，实现了显著的计算加速——微调加速为6.94倍，推理加速为2.3倍。我们的研究结果为在数据保护至关重要的领域提供隐私保护LLM服务提供了可行的概念验证。\n引言\n大型语言模型（LLMs）的出现，如BERT系列（Devlin等，2019；Liu等，2019；Sanh等，2019；Lan等，2020；Clark等，2020；He等，2021）、GPT系列（Radford，2018；Radford等，2019；Tom B. Brown等，2020；OpenAI，2023）和ChatGPT（OpenAI，2024），开启了自然语言处理（NLP）和人工智能（AI）的新时代。LLMs的众多能力中，最受关注之一是它们能够根据用户交互提供个性化响应，特别是在微调的应用下。然而，这种应用场景引发了关于用户隐私的严重担忧。对此，GDPR（欧洲联盟，2016）和CCPA（加利福尼亚州，2018）等法规已进行修订。在意大利，ChatGPT甚至一度被暂时禁用（McCallum，2023），包括苹果和三星在内的多家大型企业也限制了其在公司内部的使用（Mok，2023）。\n隐私保护机器学习（PPML）指的是在保护数据隐私的同时使用机器学习的方法。PPML的技术包括安全多方计算（MPC）（Yao，1982）、差分隐私（Dwork，2006）和同态加密（HE）（Rivest等，1978；Gentry，2009）。其中，只有MPC和HE基于加密假设提供了可证明的安全性。MPC利用方之间的通信，但这些通信使得加速和并行化神经网络的重计算变得具有挑战性。相比之下，HE支持在加密状态下进行算术计算，无需通信。自Gentry（2009）开创性工作以来，已开发出多个HE方案（Brakerski，2012；Brakerski等，2014；Ducas和Micciancio，2015；Chillotti等，2016；Cheon等，2017）。特别是，CKKS（Cheon等，2017）方案对于并行评估大规模实数（而非整数）数据特别高效，且在PPML文献中得到广泛应用（Han等，2019；Lee等，2022a；b；c；2023b）。\n理论上，同态加密（HE）为解决LLMs相关的隐私问题提供了优雅的解决方案。然而，尽管在HE操作的理论和实现上取得了显著的近期进展，保护LLMs的HE仍然面临挑战，主要是由于它们的计算规模。变压器模型（Vaswani等，2017）以大量矩阵乘法和各种非多项式操作为特征，直接将这些操作适应到HE中会导致显著的计算时间增加和精度损失。\n在这项工作中，我们提出了一种修改后的HE友好型变压器架构，重点关注个性化（私密）微调后的推理。我们指出，之前关于同态加密变压器的研究通常忽视了微调，因为它的复杂性。我们的方法有两个主要的算法组件：LoRA（Hu等，2022）微调和使用高斯核（GK）替代注意力机制中的softmax。我们展示了LoRA和GK能够在HE下显著加速加密变压器计算，同时保持与明文模型相当的性能水平。使用CKKS方案对加密数据进行的修改后BERT模型的实验结果证明了其安全处理自然语言数据的能力。我们的研究结果为在数据保护至关重要的领域提供隐私保护LLM服务提供了希望。\n先前的研究\n基于变压器的语言模型和LoRA。自从注意力机制的出现（Vaswani等，2017）以来，变压器已成为语言模型的标准。基于变压器的语言模型有三种类型：仅编码器模型，包括BERT系列（Devlin等，2019；Liu等，2019；Sanh等，2019；Lan等，2020；Clark等，2020；He等，2021），输出可用于下游任务的输入嵌入；编码器-解码器模型，使用原始变压器架构，如MarianMT（Junczys-Dowmunt等，2019）、T5（Raffel等，2020）、BART（Lewis等，2020）、mBART（Liu等，2020）和mT5（Xue等，2021），用于翻译、摘要等任务；仅解码器模型，包括GPT系列（Radford，2018；Radford等，2019；Tom B. Brown等，2020；OpenAI，2023）和Llama系列（Touvron等，2023a；b；Dubey等，2024），生成对用户查询的回答。这些大型语言模型（LLMs）遵循扩展法则（Kaplan等，2020），因此LLMs的规模趋向于不断增长。因此，这些模型需要大量的内存容量进行推理和微调。为了解决这个问题，LoRA（Hu等，2022）主要用于微调预训练的LLM。通过冻结所有其他权重，LoRA适配器被添加到模型的重要层，如微调期间的注意力层。使用LoRA，可以微调LLMs，仅更新不到1%的所有参数。\n使用HE的隐私保护变压器。许多研究者探索了利用HE为变压器模型提供隐私保护的算法。主要有两种场景：交互式，结合了安全多方计算（MPC）与HE，和非交互式，仅依赖HE。在交互式场景中，可以通过方之间的通信来减少加密计算时间。THE-X（Chen等，2022）提出了第一个BERT-tiny推理协议，引入了HE友好的工作流程和通过通信进行的非多项式评估。随后的研究（Hao等，2022；Li等，2023；Akimoto等，2023；Dong等，2023；Pang等，2024）提高了计算时间并降低了通信成本。然而，随着模型规模的增加，交互式方法可能在大规模通信中遇到困难，并且它们还要求数据所有者在计算过程中保持在线。为了解决这些问题，非交互式研究正在进行。Zimerman等（2024）提出了第一个HE友好的变压器模型，替换了原始变压器结构，最小化了逼近域，并获得了经过更改结构的预训练权重。使用这些权重，他们进行了BERT的非交互式推理。NEXUS（Zhang等，2024a）进一步提出了一种非交互式BERT推理方法，在不重新训练的情况下，使用多项式逼近处理非多项式操作。最近，Park等（2024）提出了Powerformer，像我们的方法一样，提出将注意力中的softmax替换为他们的BRP-max函数，以实现同态推理。\n微调在提升变压器模型和提供更个性化的响应中起着至关重要的作用。然而，先前的工作主要集中在安全推理上，因其涉及的显著计算复杂性，尤其是在非交互式设置中，微调这一挑战大多被忽视。只有少数尝试，如Lee等（2022b）和HETAL（Lee等，2023b）的研究，探索了微调，专注于分类头，而忽略了其他关键组件，如注意力层和前馈网络。\n贡献\n我们提出了一种同态加密（HE）友好的变压器架构，重点关注个性化（私密）微调后的推理。我们解决了HE变压器模型的两个计算瓶颈：（i）通过使用LoRA，我们避免了大规模的密文-密文矩阵乘法（CCMMs），（ii）我们使用更简单的高斯核（GK）替代了softmax层，而softmax层在HE下的计算通常具有挑战性。在对HE加密的BERT风格变压器进行实验时，我们展示了微调加速了6.94倍，推理加速了2.3倍。\nFaster CryptoNets: Leveraging Sparsity for Real-World Encrypted Inference 更快的 CryptoNets：利用稀疏性实现现实世界中的加密推理\n摘要—同态加密使得可以在数据保持加密状态的情况下进行任意计算。这一隐私保护特性对机器学习具有吸引力，但由于加密方案的巨大开销，计算时间通常较长。我们提出了Faster CryptoNets，这是一种利用神经网络进行高效加密推理的方法。我们开发了一种剪枝和量化方法，利用底层加密系统中的稀疏表示来加速推理。我们推导出了流行激活函数的最佳近似方法，能够实现最大稀疏编码并最小化近似误差。我们还展示了如何利用隐私安全的训练技术，通过迁移学习和差分隐私来减少加密推理在现实数据集上的开销。实验结果表明，我们的方法在保持竞争性准确度的同时，较以前的方法实现了显著的加速。该研究提高了使用同态加密保护用户隐私的深度学习系统的可行性。\n引言 随着基于云的机器学习服务的普及，确保敏感医疗记录、财务数据以及进入第三方管道的其他信息的机密性变得尤为重要。传统的机器学习算法需要访问原始数据，这可能带来潜在的安全和隐私风险。在某些领域，如医疗行业，法规可能会禁止使用外部预测服务，特别是在技术无法提供必要的隐私保障时。本文针对加密推理任务，以实现安全的机器学习服务。我们假设第三方服务提供商已经拥有一个训练好的模型，这在“机器学习即服务”范式中是常见的做法。利用加密技术，像研究医院或欺诈检测公司这样的组织可以为用户提供预测服务，同时确保所有相关方的安全保障。我们遵循前人的方法[29]，[68]，采用同态加密（HE）将训练好的机器学习模型转换为支持HE的模型。同态加密[56]允许机器学习模型在加密数据上执行计算。根据设计，输出的预测结果也是加密的，这防止了输入或输出信息泄露给模型的宿主。如图1所示，模型既不解密数据，也不需要私钥[12]。然而，有几个挑战阻碍了加密机器学习的广泛应用。一个主要瓶颈是计算复杂性。对于普通网络，推理通常在毫秒级别完成，而加密网络则需要每个样本数分钟或数小时[29]，[38]。此外，同态加密的算术运算集较小，无法使用现代激活函数[15]，因此需要使用较为简单且性能较低的激活函数。\nHETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption 摘要 迁移学习是一种公认的标准方法，用于通过向在大规模数据集上预训练的模型添加和微调新的分类层，从而高效地训练解决数据稀缺问题的机器学习模型。尽管许多之前的研究提出了使用同态加密解决机器学习即服务环境中迁移学习的数据隐私问题，但它们大多仅专注于加密推理。本文提出了HETAL，一种基于同态加密的迁移学习算法，旨在通过使用CKKS同态加密方案加密客户数据来保护客户在训练任务中的隐私。HETAL是首个严格提供加密训练的实际方案，采用基于验证的早期停止，并实现了与非加密训练相同的准确度。我们提出了一种高效的加密矩阵乘法算法，其速度比先前的方法快1.8到323倍，并且还提出了一种精度更高、覆盖面更广的softmax近似算法。对五个知名基准数据集的实验结果表明，总训练时间为567至3442秒，均低于一个小时。\nIron: Private Inference on Transformers Iron: Transformer模型上的私密推理\n我们开启了在客户端-服务器环境中对基于Transformer的模型进行私密推理的研究，其中客户端拥有私密输入，服务器持有专有模型。我们的主要贡献是提供了若干种新的安全协议，用于矩阵乘法和复杂的非线性函数（如Softmax、GELU激活函数和LayerNorm），这些都是Transformer模型中的关键组成部分。具体来说，我们首先提出了一种基于同态加密的定制协议，用于矩阵乘法，关键依赖于一种新颖的紧凑打包技术。该设计在最有效的工作下，实现了√m×的通信量减少（m是输出矩阵的行数）。其次，我们通过整合先进的底层协议和专门的优化，设计了三个非线性函数的高效协议。与现有的最先进协议相比，我们的方法减少了大约一半的通信和计算开销。此外，所有协议都具有数值精度，能够保持明文模型的准确性。这些技术共同使我们能够实现Iron，一个高效的基于Transformer的私密推理框架。对多个现实数据集和模型进行的实验表明，Iron在通信和运行时上比现有方法分别减少了3∼14倍和3∼11倍。\n引言 基于Transformer的模型[1–5]由于其强大的表示能力，在自然语言处理（NLP）和计算机视觉（CV）领域取得了巨大的成功。作为一种新型的神经网络架构，Transformer[1]主要利用自注意力机制来计算表示，而不依赖于序列对齐的递归或卷积。基于此的工作，许多Transformer变体，如NLP中的BERT[2]和GPT[3]，以及CV中的ViT[4]和Swin Transformer[5]，在许多现实任务中达到了最先进的性能。Transformer及其他大模型的成功推动了新兴的推理服务和应用[6, 7]。特别是，服务提供商基于Transformer训练复杂的模型，并将其部署为付费推理服务，例如机器翻译和问答服务。客户端提交输入样本并获得所需的响应。然而，当前的推理系统面临严重的隐私问题[8]。一方面，客户端需要将机密输入发送给服务提供商，如果提供商不可信，可能会泄露客户端的数据隐私；另一方面，提供商不愿将专有的Transformer模型分发给客户端，因为模型的构建需要大量数据和计算资源[9]。因此，尽管Transformer在性能上取得了前所未有的突破，但在隐私约束下仍存在一定的差距，这也激励了我们对私密Transformer推理的研究。\n私密推理旨在保护服务器的模型权重不被客户端泄露，同时保证服务器无法获取客户端私密输入的任何信息。近年来，使用安全的2方计算（2PC）技术[10–14]，已在传统神经网络（如卷积神经网络）上实现私密推理。然而，由于结构上的根本差异，私密Transformer推理带来了若干新的挑战。首先，基于Transformer的模型使用大量的高维矩阵乘法，而不是以往研究中广泛使用的矩阵-向量乘法。尽管我们可以直接将先前的矩阵-向量乘法协议扩展到我们的环境，但不幸的是，即使是最有效的设计[14]，由于大量密文的交互，通信开销也非常大。其次，Transformer模型在每个模块中使用复杂的数学函数，如Softmax、GELU激活函数[15]和LayerNorm，而不是像ReLU和Maxpool这样的加密友好型非线性函数。现有方法要么使用精度受损的高阶多项式近似[16, 11]，要么仅支持特定场景下的有限数学函数[17]。更糟糕的是，这些方法计算量大，且通常需要大量的通信（更多相关工作请参见附录A.5）。为了在隐私敏感场景中促进基于Transformer的推理服务的广泛应用，设计高效的协议来处理上述复杂操作至关重要。\n本文设计了Iron，一个高效的混合加密框架，用于私密Transformer推理，确保不会泄露服务器模型权重或客户端输入的任何敏感信息。Iron为Transformer中的复杂操作提供了若干新的专门化协议，以缓解性能开销。具体而言，我们首先提出了一种定制的基于同态加密的矩阵乘法协议。我们的创新点在于，通过设计一种紧凑的打包方法，将更多的明文输入打包到单个密文中，同时保持矩阵乘法的功能。与在Cheetah中实现的最有效的矩阵-向量乘法解决方案[14]相比，我们在通信开销上实现了√m×的改进（m是输出矩阵的行数），这对于各种Transformer模型而言，约减少了8倍。其次，我们精心设计了针对Softmax、GELU和LayerNorm的高效协议。这些协议基于SIRNN[17]，这是用于递归神经网络的私密推理的最先进加密框架，并进行了若干定制优化，如减少Softmax中的指数运算开销，并简化GELU和LayerNorm的计算。这些优化在三个非线性函数上实现了1.3∼1.8倍的运行时减少和1.4∼1.8倍的通信减少。此外，这些协议在数值上具有精确性，能够保持明文模型的准确性。我们还为所设计的协议提供了正式的安全性证明，以展示其安全保障。\n基于上述高效组件，我们实现了私密Transformer推理框架Iron，并在GLUE基准测试[18]上对多种BERT架构[2]（包括BERT-Tiny、BERT-Medium、BERT-Base和BERT-Large）进行了端到端实验。值得注意的是，由于这些模型共享非常相似的架构和相同的操作，Iron可以轻松扩展到其他基于Transformer的模型（如ViT）。实验结果表明，Iron在四个BERT模型上相较于SIRNN实现了3∼14倍的通信减少和3∼11倍的运行时减少。此外，与通用的最先进框架MP-SPDZ[16]相比，Iron在通信和计算效率上分别提升了两个数量级。\n与此同时，另有一项并行工作[19]提出了一个名为THE-X的基于同态加密的隐私保护Transformer推理方法。下面，我们将说明在协议设计和安全性方面的几个重要区别。 （1）协议设计。我们的工作旨在为Transformer模型的复杂操作设计新的高效协议，而THE-X则通过替换为加密友好的操作来实现这一目标。例如，THE-X将GELU替换为更简单的ReLU操作，将Softmax替换为ReLU与多项式的组合。 （2）安全性。我们的工作比THE-X提供了更严格的隐私保护。具体而言，我们的工作使用同态加密和秘密共享技术，隐藏所有层的私密信息（包括中间结果）。这种严格的隐私保证符合最近的最先进私密推理工作[14, 17]。然而，在THE-X中，每个非线性层的输入会泄露给客户端，这在实际应用中可能导致严重的隐私泄露[13]。因此，我们的工作可以用来增强THE-X的安全性。\nPOWER-SOFTMAX: TOWARDS SECURE LLM INFERENCE OVER ENCRYPTED DATA POWER-SOFTMAX：面向加密数据的安全LLM推理\n摘要 现代加密方法，如同态加密（HE），用于实现隐私保护的大型语言模型（LLMs）要求LLMs具有多项式形式。然而，由于Transformer模型包含非多项式组件，如Softmax和层归一化，形成这种表示形式具有挑战性。以往的方法要么直接用高阶多项式来近似预训练模型，这在同态加密下效率较低，要么在训练前将非多项式组件替换为更容易近似的原语，例如将Softmax替换为逐点注意力。后者方法可能会带来可扩展性问题。我们提出了一种新的同态加密友好的自注意力变体，它提供了一种稳定的形式用于训练，并且易于用多项式进行近似，从而实现安全推理。我们的工作引入了第一个具有32层和超过十亿参数的多项式LLM，超越了以往模型的规模十倍以上。所得到的模型展示了与相同大小的标准Transformer相当的推理能力和上下文学习（ICL）能力，标志着该领域的一项突破。最后，我们提供了加密数据上每个计算的详细延迟分解，为进一步优化铺平了道路，并探讨了依赖于我们HE友好变体的Transformer与标准Transformer之间的归纳偏差差异。我们的代码附在补充材料中。\n引言 隐私保护机器学习（PPML）解决方案，特别是隐私保护的大型语言模型（LLMs）Yan et al.（2024）；Yao et al.（2024），旨在为用户数据、模型拥有者或两者提供机密性保证。实现这一目标的一个显著加密原语是同态加密（HE），它允许在加密数据上执行计算，而不会向（可能不可信的）计算环境泄露任何信息。此外，它还支持非交互式计算，从而提高了这些解决方案的可用性。\n然而，现代的同态加密方案，如CKKS Cheon et al.（2017），面临着仅支持对加密数据执行多项式计算的重大挑战。这一限制使得在HE环境中部署深度学习模型变得复杂，特别是对于依赖非多项式函数（如自注意力中的Softmax）的LLM。为了克服这一问题，现有方法已经将这些非多项式操作通过诸如唯一多项式近似Lee et al.（2021）或微调程序Baruch et al.（2022）等技术转化为多项式形式。虽然这些方法已经使得前馈网络（FFNs）、卷积神经网络（CNNs）Baruch et al.（2023）；Lee et al.（2022），以及小型Transformer Zimerman et al.（2024）；Zhang et al.（2024b）能够在HE上执行，但它们通常会面临稳定性和敏感性问题Zhou et al.（2019）；Goyal et al.（2020），从而阻碍了有效的扩展。\n我们采取了不同的方法。我们并不是修改现有的Transformer模型以适应HE的约束，而是通过CKKS约束的视角重新审视Transformer架构的核心设计原则Vaswani et al.（2017）。具体来说，我们提出：是否存在适用于HE的操作符，可以复制自注意力的关键设计原则？我们找到了肯定的答案，通过引入基于幂运算的自注意力变体，使其更容易用多项式表示。具有这一变体的模型在多个基准测试中保持了与基于Softmax的Transformer相当的性能，并保持了自注意力的核心设计特性。我们还提出了包括长度无关的近似或改进数值稳定性的变体。整体机制提供了一种比以往方法更适合HE且更高效的Transformer解决方案，使我们的方法能够高效扩展到具有32层和14亿参数的LLM。\n我们的主要贡献：（i）我们提出了一种专门针对HE环境的HE友好的自注意力变体。该变体最大限度地减少了非多项式操作的使用，同时保持了注意力机制的核心原则。此外，我们通过引入数值稳定的训练方法和用于推理的长度无关的计算策略，扩展了这一方法。因此，我们的模型能够实现大规模的安全推理，并且比现有方法更高效。（ii）我们利用这一技术开发了RoBERTa的多项式变体，以及首个多项式LLM，具有推理和上下文学习（ICL）能力，并且是迄今为止训练的最大多项式模型，包含32层Transformer和大约十亿个参数。（iii）我们提供了早期的消融研究和加密数据上的延迟分析，为进一步改进铺平了道路。\nPowerformer: Efficient Privacy-Preserving Transformer with Batch Rectifier-Power Max Function and Optimized Homomorphic Attention \u0026ldquo;Powerformer: 高效隐私保护变换器，采用批量整流器-幂最大函数和优化同态注意力\u0026rdquo;\n摘要 我们提出了一种高效的非交互式隐私保护变换器推理架构，称为Powerformer。由于softmax是一个非代数运算，之前的研究尝试修改它以使其适应同态加密（HE），但这些方法由于多次引导（bootstrapping）而导致准确度下降或执行时间延长。我们提出用一种基于ReLU的新函数——批量整流器-幂最大（Batch Rectifier-Power max, BRPmax）函数替代softmax，且无需任何不稳定的近似方法，该方法在BERT-Large模型中超越了原始BERT的性能，同时需要的层数更少，从而只需一次引导即可运行。我们还提出了一种专门针对注意力模块的矩阵乘法算法，相比现有的最先进方法，减少了35%到91%的密钥切换次数。我们设计了清晰的端到端基于HE的私有变换器模型实现，我们在BERT-tiny模型上使用RNS-CKKS实现的Powerformer，在单线程CPU上运行时需时503秒，据我们所知，这是首个使用HE的端到端非交互式变换器实现。\n引言 变换器（Transformer）模型是近年来最突出的技术之一，基于变换器的语言模型ChatGPT通过最大化大型语言模型的潜力，推动了传统人工智能能力的边界，实现了与人类的自然互动，带来了重大创新。然而，ChatGPT目前采用的是机器学习即服务（MLaaS）模型，其中客户端将数据发送到服务器，服务器使用其模型进行推理并将结果返回给客户端。这个过程将客户端的数据暴露给服务器，产生了显著的隐私问题。由于这些问题，注重安全的公司对使用ChatGPT持谨慎态度，担心企业数据的潜在泄露，这最终限制了强大的基于变换器的模型的充分利用。为了解决MLaaS环境中的隐私问题，利用同态加密（HE）对加密数据进行推理的隐私保护机器学习（PPML）得到了积极研究 [3, 7, 13, 16, 19, 27]。随着对变换器模型关注的增加，关于变换器安全推理的研究也日益增多 [4, 6, 9, 21, 23, 28, 29]。PPML研究分为交互式PPML（通过客户端与服务器之间的通信进行推理）和非交互式PPML（服务器在没有通信的情况下对加密数据进行推理）。\n交互式PPML [6, 9, 13, 21, 23, 24, 27] 主要采用使用同态加密（HE）和多方计算（MPC）的方法，这些方法具有相对较快的执行时间。然而，当针对像变换器这样的复杂模型时，这种方法可能会导致通信成本超过几十GB，这在带宽不足的环境中可能成为限制因素。与此相对，非交互式PPML 仅涉及服务器对客户端加密数据进行计算，无需任何通信，客户端只需发送加密数据并接收加密后的最终结果。由于不需要在线通信的优势，许多研究采用了非交互式方法 [3, 7, 10, 16, 19, 28, 29]。本文重点讨论非交互式隐私保护变换器推理。RNS-CKKS [5] 是非交互式变换器推理的良好选择，因为它具有快速执行大量实数运算的优势。在RNS-CKKS上实现高效的非交互式隐私保护变换器推理的两个主要挑战是softmax操作和注意力所需的矩阵乘法。基于HE的变换器研究中最关键的问题是，目前仍没有完整解决基于HE的变换器模型在实践中稳定运行的清晰端到端实现的论文。\nSoftmax函数 Softmax函数可以通过以下方程计算，其中减去$x_{\\text{max}}$是为了数值稳定性 [15]： $$\ry_i = \\frac{\\exp(x_i - x_{\\text{max}})}{\\sum_{j=0}^{m-1} \\exp(x_j - x_{\\text{max}})}\r$$ 在同态加密（HE）中应用softmax函数的两种最常见方法是：用多项式函数替代softmax函数，或准确地近似softmax函数本身。第一种方法是用多项式替代softmax函数并对模型进行微调，可能改善基于HE的实现中的时间性能，但一个关键问题是，当多项式未能有效替代神经网络中的softmax时，可能会导致准确度下降。例如，Zimerman等人提出的softmax替代方法 [29] 显著降低了准确度，如第2.1节中的实验结果所示。第二种方法则专注于提供快速且准确的近似。例如，最近的研究，如 [28]，使用泰勒级数和Goldschmidt多项式近似方法来近似softmax函数。由于指数函数的高不稳定性，在RNS-CKKS上进行准确计算是困难的。因此，为了稳定性，使用$x_{\\text{max}}$进行减法变得不可避免，这进一步需要大量的比较操作，导致在安全推理过程中显著的计算开销。\n矩阵乘法 所有涉及基于同态加密（HE）私有推理的论文都未完全呈现适用于整个变换器网络的端到端矩阵乘法方法。例如，本研究领域的先前研究包括NEXUS [28]和Zimerman等人的研究 [29]。NEXUS提出了一种用于计算多头注意力中查询矩阵（Qi）、键矩阵（Ki）和值矩阵（Vi）的密文-明文乘法算法，以及用于计算$Q_iK_i^T$的密文-密文乘法。然而，后续乘法的同态算法，如与Vi的乘法或输出权重矩阵$W_O$的乘法，在NEXUS中并未提及。此外，NEXUS采用了多种打包方法，如按组件打包、按行打包、按列打包和按对角线打包，但没有详细说明推理过程中打包结构如何变化，以支持非交互式计算。上述打包方法的定义在B.2节中提供。另一方面，Zimerman等人的研究 [29]没有提供详细的实现细节，如同态矩阵乘法算法，也没有提供在加密状态下的实现代码，使得很难复现他们的非交互式实现并分析其效率。因此，未来的基于HE的变换器实现研究必须提出专门适用于HE私有变换器推理的矩阵乘法方法，并提供该方法的端到端实现源代码。\n尽管没有完全解决变换器网络的HE实现问题，但在HE-MPC混合实现中提出了几种适用于注意力模块的矩阵乘法方法。然而，这些矩阵乘法的计算量并未得到优化，或者由于非紧凑实现，产生了大量的中间密文，从而导致了显著的引导操作量。例如，BOLT [23]提出了一种利用列主打包的同态矩阵乘法算法，尽管明文-密文乘法需要2√d个密钥切换，效率相对较高，但密文-密文矩阵乘法仍然需要多达$d \\log d + 2d$个密钥切换，相比于Jiang等人提出的著名同态矩阵乘法方法 [10]，这是一个效率较低的实现。另一方面，NEXUS [28]中使用的按行和按列打包方法效率较低，因为它们留下了许多未使用的槽位，我们的分析表明，使用这些打包方法需要大量的引导操作。此外，撇开引导操作的数量不谈，使用NEXUS提出的矩阵乘法算法实现多头注意力时，至少需要6d²次密钥切换。对于我们所针对的变换器模型（d = 128），这会导致过多的密钥切换。虽然Jiang等人提出了一种高效的通用密文-密文矩阵乘法算法，但该方法需要为注意力模块（具有复杂的矩阵操作结构）进行专门化。\n我们的贡献 我们提出了一种高效的端到端私有基于同态加密（HE）的变换器实现，称为PowerFormer，通过解决基于HE实现的两个重要研究问题：\n我们引入了一组新的softmax替代函数——批量整流器-幂最大（Batch Rectifier-Power max，BRPmax）函数集，超越了现有的softmax替代方法。这一创新方法通过我们新的训练方法实现了高数值稳定性、与原始BERT模型几乎相同的分类准确性，以及高效的推理运行时性能。 我们提出了一种针对多头注意力优化的矩阵乘法操作，呈现了一种端到端实现方法，最小化了多头注意力中的密钥切换操作次数和引导操作次数。 批量整流器-幂最大（BRPmax）方法 我们提出将softmax函数近似为$ \\text{ReLU}(x + c)^p / R_d $，其中$c$、$p$和$R_d$是常数，且$c$、$p$是固定的。通过采用批量归一化中的批量方法，运行中的分母$R_d$在训练过程中被学习，并在推理过程中作为常数处理，从而通过乘以常数$1/R_d$来在加密数据上执行操作。ReLU函数通过Lee等人提出的方法 [17, 18] 被精确近似。我们通过数值实验展示了Powered ReLU（整流器-幂）函数有效地替代了指数函数。我们还展示了蒸馏学习中的批量方法解决了动态缩放问题，这是PPML中的常见挑战。\n优化同态注意力 我们进一步优化了Jiang等人 [10] 提出的矩阵-矩阵乘法算法。对于明文-密文矩阵乘法，我们将密文-密文矩阵乘法算法中的多个明文-密文乘法步骤集成到一个单一的baby-step giant-step求和中，与 [10] 相比，密钥切换次数减少了85%到91%。我们还提出了一种针对查询矩阵和键矩阵乘法优化的分块矩阵乘法算法，密钥切换次数比 [10] 减少了35%。\n端到端基于HE的变换器实现 我们提出了首个端到端非交互式安全变换器推理架构——Powerformer。该架构通过使用多项式近似替代非线性操作（如softmax、GELU和层归一化），同时利用我们高效的矩阵乘法算法。尽管我们的重点是BERT-tiny模型，但该架构可以轻松扩展到其他模型，如BERT-medium和BERT-base。我们已使用RNS-CKKS库Lattigo [1] 完成了Powerformer架构的端到端实现。与仅提供加密数据上执行的单个组件结果的NEXUS不同，我们实现了整个过程并展示了完整的结果。我们的实现仅在单个CPU线程上运行了503秒，比基于单线程性能的NEXUS快至少39倍。鉴于这是单线程实现，我们预计在GPU或硬件加速的情况下，运行时间将减少数十到数百倍 [12, 14]。\nPrivformer: Privacy-preserving Transformer with MPC \u0026ldquo;Privformer: 基于多方计算的隐私保护Transformer\u0026rdquo;\n摘要——Transformer 是一种处理序列数据的深度学习架构。Transformer 在多个序列数据分析任务中达到了最先进的水平，其变体，如 BERT 和 GPT-3，已成为解决自然语言处理（NLP）一般任务的事实标准。本文提出了一种三方多方计算（MPC）协议，用于在诚实多数设置下进行 Transformer 的安全推理。当使用现有构建块实现 Transformer 的 MPC 协议时，注意力层是最耗时的部分。注意力机制是 Transformer 的核心组件，能够捕捉和利用输入序列中元素之间的复杂依赖关系。注意力机制调用指数函数 O(S²) 次，这在使用现有 MPC 原语实现 Transformer 时成为主要瓶颈。为了解决这个问题，我们采用了 Performer [11]，这是 Transformer 的一种变体，其中调用指数函数的 sigmoid 函数被 ReLU 函数替代，ReLU 是一种更适合 MPC 的非线性函数。此外，通过引入基于核的注意力矩阵近似方法，并使用随机正交矩阵，我们展示了注意力层可以通过 O(S) 次 ReLU 函数调用来处理。我们通过三方 MPC 对 Transformer 进行端到端实现，研究了该方法的效率。实验评估表明，对于一个输出序列长度为 64 的翻译任务，整个计算过程在局域网环境下大约需要 19 分钟。\n引言——Transformer [49] 在机器学习中广泛应用于自然语言处理（NLP）任务，如机器翻译 [51]、文本摘要 [33]、问答 [8] 等。最初，Transformer 被应用于需要利用输入序列中元素间复杂依赖关系的 NLP 任务（例如，依存句法分析用于确定语法结构）。随后，Transformer 也被应用于视觉问答（VQA）[10]、图像分类 [18]、图像生成 [56] 和蛋白质折叠预测（AlphaFold 2）[24] 等任务。有关 Transformer 及其变体的更多信息，请参见 [48]。在大规模通用数据集（例如，通用文本语料库）上训练的 Transformer 被称为预训练模型（如 BERT、GPT-3），并且已知能够处理多种任务。将预训练模型应用于特定任务时，通常通过迁移学习使用专门为该任务准备的小量训练数据对模型进行微调。通过结合预训练和迁移学习，即使在合理的计算时间内，使用少量的训练数据也能实现高精度的识别性能。\n使用基于 Transformer 的模型的一大难题是其庞大的模型规模。例如，GPT-3 是一个生成句子的语言模型，包含大约 1750 亿个参数。因此，在使用基于 Transformer 的模型时，通常将模型部署在云服务器上，而不是本地计算资源上。使用时，信息流遵循 API 模式；用户将输入数据发送到服务器，服务器使用模型进行处理，并仅将结果返回给用户。从用户的角度来看，数据隐私是云端模型部署中的主要问题之一。在 API 模式下，用户必须将所有数据提交到云服务器。例如，考虑将文件进行云端机器翻译。如果文件内容高度机密，将整个文件发送到云服务器是不可接受的。此外，借助最先进的机器翻译技术，实时口译对话已成为可能。然而，由于同样的原因，当需要保护机密性时，基于云的 API 模式并不现实。隐私问题可以通过将整个模型分发给用户并在用户端执行所有计算来解决。然而，近期的最先进的基于 Transformer 的模型规模极大，需要 GPU 进行处理。在用户端准备如此大规模的计算环境也是不现实的。此外，模型通常需要经过大量的努力和成本进行特定任务的微调。这样的模型是宝贵的信息资产，将这些模型分发给用户可能是不可接受的。\n在过去十年中，已经开发了多种用于处理序列数据的架构，如循环神经网络（RNN）、LSTM 和 Seq2Seq。目前，预训练的 Transformer 变体，如 BERT（Bidirectional Encoder Representations from the Transformers）[17] 和 GPT（Generative Pre-trained Transformers）-3 [6]，被认为是许多 NLP 任务的最先进技术，并被广泛作为事实标准使用。考虑到上述情况，本文旨在开发一种三方多方计算（MPC）协议，用于在诚实多数设置下对 Transformer 进行安全推理，使得能够通过 API 使用部署在云服务器上的 Transformer，同时保护模型和用户的隐私。\n相关工作——在过去五年中，深度学习推理和训练中的隐私保护取得了显著进展。大多数研究依赖于一种或多种隐私保护计算技术，如同态加密 [46]、[22]、秘密共享 [32]、[44]、[13] 以及它们的组合 [45]、[23]、[35]。卷积神经网络（CNN）的隐私保护推理和训练已被深入研究，并通过 CrypTFlow [26]、CrypTFlow 2 [43]、CryptGPU [47]、[39] 等展示了对大规模 ImageNet 数据进行隐私保护计算是可行的。综述论文 [43] 总结了深度学习隐私保护计算技术的最新进展。隐私保护计算技术在序列分析（例如自然语言处理、语音识别、时间序列分析）中的应用，仍然少于在图像识别中的应用。然而，已有一些研究探索了深度神经网络在序列分析中的隐私保护计算，如循环神经网络（RNN）、LSTM、Seq2Seq 和基于 Transformer 的模型。SIRNN [42] 提出了一个二方协议，用于处理序列数据的 RNN 推理。他们引入了高效的通信协议，用于处理通过多方计算（MPC）计算成本高昂的非线性函数，如指数函数、sigmoid 函数、tanh 函数和平方根逆函数，采用查找表和混合位宽的方法。该协议结合了秘密共享和盲传输，保证了半诚实安全性。冯等人提出了使用加法和乘法秘密共享的 n 方协议，用于计算在神经网络中常用的典型非线性函数 sigmoid 函数和 tanh 函数 [19]。使用这些技术，冯等人提出了 PrivLSTM 和 PrivSeq2Seq，分别在半诚实对抗模型中安全地计算 LSTM 和 Seq2Seq 的推理。王等人提出了一种 n 方 MPC 协议，使用加法和二进制秘密共享，在半诚实对抗模型中实现 Transformer 的近似推理 [53]。该工作集中于 Transformer 计算中软最大（softmax）函数评估所占的主导地位；他们提出通过使用 Linformer [52] 和 Nystromformer [55]，这些方法可以近似计算 Transformer，从而减少 softmax 函数的评估时间。该工作的目标和方法与我们的工作相似，但在以下两点上存在显著差异。\n首先，Nystromformer 仅考虑了 Transformer 的编码器部分，而未考虑涉及遮罩注意力的解码器部分。遮罩注意力是处理复杂任务（如机器翻译和句子生成）所必需的组件。正如我们将在 5.2 节中详细讨论的那样，在应用多方计算（MPC）时，特别需要考虑如何将遮罩矩阵应用到注意力矩阵中。其次，该框架假设使用半诚实对手模型，并不支持对抗恶意对手的安全性。我们的协议是在诚实多数设置下工作的。陈等人提出了一种协议 THE-X，用于使用同态加密进行 Transformer 的安全推理 [9]。陈等人通过微调引入了 Transformer 的多项式激活函数近似。给定一个预训练的 Transformer 模型，非多项式函数（如高斯误差线性单元（GELU）、softmax 和层归一化）被多项式替代，整个模型进行了微调，以提高其推理性能。他们的实验评估评估了预测性能，但未报告计算时间和通信带宽。李等人还提出了基于同态（CKKS）加密的隐私保护嵌入，并通过对 BERT 嵌入的加密进行文本分类来展示其性能 [30]。该框架支持 CKKS 加密方案的高效 GPU 实现；然而，只有下游分类（逻辑回归）部分通过同态加密得到保护，整个序列编码部分则没有加密保护。除了 Transformer 的隐私保护计算外，还有一些关于模型隐私和输入隐私的研究。Lang 等人和 Coavoux 等人研究了 Transformer 编码器获得的潜在表示信息泄露的风险，并提出了使用对抗训练的防御方法 [12]，[29]。Lu 等人揭示了使用在联邦学习过程中收集的梯度信息来反转 Transformer 梯度的风险 [34]。Qu 等人研究了相同的私人输入泄露风险，并提出了使用差分隐私的对策 [41]。这些研究的目标与我们的工作是正交的，且这些研究采用了不同的安全模型。\n我们的贡献\nTransformer 编码器层对某些任务（如分类）非常有用。然而，当 Transformer 被要求作为生成模型工作时（例如机器翻译和句子生成），涉及遮罩注意力的解码器部分是必不可少的。在本研究中，我们开发了用于全 Transformer 模型推理的多方计算（MPC）协议，包括编码器层和解码器层。本工作的贡献总结如下：\n我们设计了一个 3 方 MPC 协议，用于在诚实多数设置下对 Transformer 进行安全推理。诚实多数设置下的 MPC 协议由 Araki 等人 [3] 和 Furukawa 等人 [20] 首先提出，并随后应用于 ABY3 [16]、SecureNN [50] 和 FALCON [31] 中的机器学习模型的推理和训练。FALCON 引入的 MPC 协议实现了隐私保护计算，这些计算通常用于深度神经网络，如矩阵乘法、修正线性单元（ReLU）、ReLU 的导数（DReLU）和批量归一化。其中一些协议是通过结合 ABY3 和 SecureNN 中使用的技术设计的。我们的协议也在很大程度上依赖于这些工作的协议。\n我们的技术贡献有三个方面：\n一个用于计算注意力的 MPC 协议，ReLU 注意力，使用 O(S) 次 ReLU 调用，且在 O(1) 轮中完成； 两个用于遮罩注意力的 MPC 协议：一个在 O(1) 轮中需要 O(S²) 时间，另一个在 O(S) 轮中实现 O(S) 时间复杂度； 一个新的用于平方根逆运算的 MPC 协议。 首先，我们提出了一个高效的 MPC 协议——ReLU 注意力，用于计算注意力矩阵，这是 Transformer 编码器层中的核心组件。实现注意力层可以通过简单地结合现有的协议来实现，但这种简单的组合会因以下原因而导致效率低下。通过初步研究，我们揭示了在注意力层中，sigmoid 函数中的指数运算是整个计算的瓶颈（见第 4 节）。更精确地说，给定长度为 S 的输入序列，在 MPC 中，指数运算会在注意力层中调用 O(S²) 次。为了解决这一问题，我们引入了两种技术：（1）我们将注意力层中的 sigmoid 函数替换为 ReLU 函数（ReLU 注意力，见第 5 节），（2）我们使用 Performer [11] 对 Transformer 进行近似，Performer 是注意力的基于核的泛化方法。由于 ReLU 函数对 MPC 更友好，替换 sigmoid 为 ReLU 提高了计算效率。此外，Performer 使用随机正交矩阵对 ReLU 注意力进行近似，从而仅需 O(S) 次 ReLU 调用来处理注意力层。\n其次，我们通过实验研究了两种 MPC 协议处理遮罩注意力层的效率，遮罩注意力层是 Transformer 解码器层中的另一个核心组件。与用于（非遮罩）ReLU 注意力的 MPC 协议不同，处理遮罩注意力的 MPC 即使在近似的情况下，也需要 O(S²) 次 ReLU 调用（见第 5.2.1 节）。这是因为遮罩矩阵需要应用于 S × S 的注意力矩阵，无论是否进行近似。一个解决方法是按顺序应用遮罩到注意力向量，这样 MPC 就不需要处理整个注意力矩阵。通过这种修改，我们可以将 ReLU 的调用次数减少到 O(S) 次；但是，由于顺序遮罩，轮次复杂度增加到 O(S)（见第 5.2.2 节）。考虑到这一轮次和通信复杂度的差异，这两种协议的优越性取决于通信环境。通过实验评估，我们揭示了这些策略在遮罩注意力中的计算效率。\n第三，我们提出了一种新的用于平方根逆运算的 MPC 协议。平方根逆运算包含在批量归一化层或层归一化层中，通常不仅出现在 Transformer 中，也出现在各种神经网络中，并且已经在 SIRNN 和 FALCON 中进行了研究。据我们所知，只有 FALCON [31] 在诚实多数设置下研究了平方根逆运算的 MPC。一些协议的限制是，它们通过近似估算 log2 b，其中 b 是输入，并以明文形式在各方之间共享，这可能泄漏关于私有输入序列的信息（信号的方差）。我们提出了一种用于平方根逆运算的 MPC 协议，该协议不会泄漏任何中间值，并且在诚实多数设置下是安全的。\n本文的结构安排如下：第二节介绍了我们提出的方法所需的基本构件；第三节定义了我们的问题和威胁模型；第四节讨论了通过组合现有 MPC 协议来实现 Transformer 时的问题；第五节提出了 ReLU 注意力和遮罩 ReLU 注意力的 MPC 协议；第六节介绍了平方根逆运算的 MPC 协议；第七节我们构建了 Privformer，这是一个端到端的 MPC 协议，用于计算 Transformer，使用了所提出的构件；第八节讨论了所提出协议的安全性；第九节通过实验展示了这些协议的效率；第十节总结了我们的工作。\nReLU’s Revival: On the Entropic Overload in Normalization-Free Large Language Models ReLU的复兴：关于无归一化大语言模型中的熵过载问题\nLayerNorm是现代大语言模型（LLM）中的一个关键组件，能够稳定训练并确保优化过程的顺利进行。然而，它在机制可解释性、异常特征抑制、信号传递的真实性以及私密推理的计算和通信复杂性方面带来了显著的挑战。本研究探索了无归一化解码器-only LLM中理想的激活函数。与传统上在基于Transformer的模型中偏好使用GELU不同，我们的实验证据表明了一个相反的趋势：在无LayerNorm的模型中，ReLU显著优于GELU，带来了8.2%的困惑度改进。我们发现GELU存在一个关键问题，即早期层会经历熵过载，导致注意力头的表征能力未得到充分利用。这表明，像GELU这样平滑的激活函数并不适合无LayerNorm架构，而ReLU的几何属性——在输入空间中的专业化和类内选择性——有助于在没有LayerNorm的情况下改善学习动态和信息保留。本研究为优化Transformer架构提供了关键见解，特别是在LayerNorm带来显著挑战的情况下。代码和实现可在relu-revival-normfree获得。\n引言 动机与挑战 LayerNorm [1] 是现代大语言模型（LLM）成功的关键架构组件之一，通过对层内特征的输入进行归一化来稳定训练过程。此外，它在增强模型的非线性表征能力方面也起到了重要作用 [2-5]。尽管LayerNorm具有诸多优点，但它在特定场景下会带来一些实际挑战，具体包括：\n私密推理（PI）：PI协议使得在加密数据上进行推理成为可能，且不暴露输入数据，从而确保数据隐私并保护模型权重 [6-14]。混合PI协议在处理LayerNorm中的逆平方根计算时遇到困难，这使得它成为仅次于GELU的第二大开销操作，贡献了22%的总延迟和通信开销 [11]。此外，纯同态加密（HE）PI需要对LayerNorm进行多项式近似，这在具有广泛方差范围的情况下非常具有挑战性 [8]。 机制可解释性：LayerNorm增加了残差流的复杂性，使得分析和理解Transformer模型内部工作变得更加困难 [15]，限制了LLM在要求透明性和可解释性的应用中的适用性。 低精度训练：LayerNorm中的可训练参数与异常特征的放大相关，这对LLM量化训练提出挑战，因为它加剧了数值不稳定性并降低了低精度训练下的性能 [16-19]。 信号传播：LayerNorm已被证明对信号传播产生负面影响 [20]。 这些挑战突显了需要无LayerNorm架构的必要性，这样可以保留Transformer的优点，同时避免其缺点。然而，这一转变也带来了新的考虑，尤其是在选择前馈网络（FFN）激活函数时。此前的研究 [20-22] 探讨了设计无归一化LLM的各种架构启发式方法，但去除归一化层对FFN激活函数选择的影响仍未得到充分探索。 研究洞察与影响 本研究超越了以往的工作，深入探讨了无归一化LLM中激活函数的设计选择，提供了关于这些选择如何影响学习动态、内部表征和整体模型性能的新见解。我们的研究揭示了几个关键发现：\nReLU在无LayerNorm模型中优于GELU：与传统做法相反，我们表明在无LayerNorm的模型中，使用ReLU作为FFN激活函数的模型在困惑度上显著优于使用GELU的模型，困惑度提高了8.2%（见图2和表2）。\n具有可学习负斜率的学习动态\n：为了进一步探索，我们在泄漏ReLU激活函数中实验了可学习的负斜率，并使用了两种配置：\n层级配置：每一层有一个独立的可学习斜率。最初，较浅的层学习正斜率，而较深的层学习负斜率。但随着训练的进行，所有层的斜率都逐渐收敛到接近零的值（见图3a）。 全局配置：所有层共享一个可学习的斜率。该斜率最初为正，然后逐渐收敛到接近零（见图3b）。这些结果表明，无LayerNorm模型本质上更倾向于使用具有零负斜率的ReLU类激活函数。 GELU激活中的熵过载：为了深入分析，我们检查了按头的熵值，发现使用GELU激活的无归一化模型中，早期层经历了熵过载，注意力头的大部分达到了接近最大熵水平，这表明注意力头的表征能力未得到充分利用。\n贡献 我们的主要贡献如下：\n我们通过对无归一化解码器-only模型中激活函数的深入分析，研究了从头开始训练时的学习动态。 我们从香农熵的角度探索了不同激活函数在基准模型和无归一化模型中对注意力得分分布的影响，为无LayerNorm模型架构设计提供了有价值的见解。 我们在GPT-2和Pythia [23] 模型上进行了各种上下文大小（128和256）的实验，训练数据来自CodeParrot [24]，并使用了21亿训练token。 Secure Transformer Inference Made Non-interactive 使得变换器推理安全且非交互式\n随着ChatGPT的普及，安全变换器推理已成为一个重要的研究课题。现有的解决方案通常是交互式的，涉及大量的通信负载以及客户端和服务器之间的多轮交互。在本文中，我们提出了NEXUS，首个用于安全变换器推理的非交互式协议。该协议要求客户端在整个推理过程中只与服务器进行一轮通信：提交加密输入并接收加密结果。NEXUS引入了几个新颖的原语，包括SIMD密文压缩/解压缩、SIMD槽折叠和安全Argmax，使其在通信方面显著超越现有技术，同时保持可比的运行时性能。具体而言，它与BOLT（Oakland ’24）相比，减少了372.5倍的带宽消耗，与Bumblebee（NDSS ’25）相比，减少了53.6倍。此外，它的非交互式特性允许进行最优的硬件加速，GPU版本在运行时实现了42.3倍的加速。这使得NEXUS能够在仅用164MB带宽的情况下，在37.3秒内完成基于BERT的模型推理。\n引言 变换器模型，如GPT [48] 和 BERT [17]，已经彻底改变了人工智能领域。它们在语言翻译、内容生成和问答等多种应用中表现出色。然而，这些应用通常涉及敏感数据的处理，随着时间的推移，用户隐私的担忧也日益增加。最近，OpenAI 开发了 ChatGPT 作为一种在线推理服务，并为开发者提供了一个公共 API，使他们能够通过提交提示或消息轻松访问该平台。虽然这种方式便捷，但它对数据隐私构成了重大风险，因为用户提交的内容有时可能包含私人信息。\n安全推理是一种双方加密协议，旨在使模型推理以一种方式进行，即服务器 S 对客户端 C 提交的输入一无所知，且客户端 C 对服务器 S 的模型一无所知，除非推理结果。过去几年中，许多此类协议已被开发用于卷积神经网络（CNNs）[40]，[28]，[2]，[31]，而一些最新的研究也开始支持基于变换器的模型 [25]，[11]，[27]，[42]，[45]。\n值得注意的是，这些协议中的大多数是交互式的，要求大量的通信成本和客户端与服务器之间的多轮交互。例如，当前安全变换器推理的最先进解决方案 BOLT [45]，记录了单次推理消耗 59.61GB 带宽和 10,509 轮交互。如此庞大的通信开销显著增加了网络延迟，尤其在广域网（WAN）配置中，并使得传统的硬件加速技术，如 GPU 或 FPGA，失去效果。此外，通信负载的大小使得安全推理的财务成本非常高。根据 AWS 的定价标准 [1]，BOLT [45] 每个回复标记的成本为 5.44 美元，这意味着实际部署既昂贵又不可行。\n我们强调建立一种非交互式安全推理模型的重要性，其中客户端 C 只需提交一个加密输入，就能从服务器 S 获得加密的预测结果。对于需要实时响应的场景，现有的安全推理协议，无论是交互式还是非交互式，都无法满足速度要求。然而，非交互式协议在利用硬件加速时，在满足速度要求方面展现了潜力 [16]，[15]，[54]，[56]，[51]，[3]，[33]。在诸如数据仓库和医院诊断等非实时场景中，客户端 C 可以容忍响应的延迟，此时部署非交互式协议是可行的，而交互式协议则不可行。这是因为交互式协议要求客户端 C 的计算资源在等待期间持续占用，妨碍了 C 执行其他任务的能力。\n在非交互式安全推理的背景下，变换器与卷积神经网络（CNN）模型之间有两个关键区别。 更大规模的矩阵-矩阵乘法 之前关于私有神经网络的研究，如Gazalle [31] 和 Cheetah [28]，提出了针对全连接层的安全矩阵-向量乘法的优化协议。然而，变换器需要进行大规模的矩阵-矩阵乘法。之前的研究 [31]，[28]，[25] 通过内积计算乘法，并在矩阵-矩阵乘法的情况下采用稀疏打包来处理生成的密文。由于这种技术，密文中的大部分数据槽经常被浪费，从而引入了额外的通信开销。此外，变换器模型中矩阵的输入维度通常更高，导致需要执行更多的乘法，从而增加了计算成本。因此，开发一种既节省时间又节省空间的新矩阵乘法协议是非常有用的。 更高维度的Argmax输入 CNN和变换器的最后一层都是Argmax，其输入是一个概率向量，每个条目表示一个输出标签候选的概率（总共有m个标签）。推理输出是概率最高的标签。当前最先进的FHE（全同态加密）算法用于Argmax，由Phoenix（CCS'22）[30] 提出，其计算复杂度为O(m)。在CNN图像分类任务中，m的值通常不大（例如，在ImageNet-1k中，m = 1,000），因此这一度量被认为是可接受的。然而，对于基于变换器的NLP任务，m等于词汇表的大小，BERT中m为30,522，Llama-3-8B中m为128,256。显然，现有的算法不适用于变换器，因此需要一种低复杂度的解决方案。 A. 我们的贡献 本文提出了NEXUS，这是我们所知的首个用于安全变换器推理的非交互式协议。NEXUS的协议设计从客户端使用RNS-CKKS全同态加密（FHE）加密输入开始，使服务器能够在FHE加密的数据上评估变换器模型。我们的贡献总结如下：\n高效且优化的通信矩阵乘法 许多以前的安全推理协议，如Gazalle [31]、Cheetah [28] 和 Iron [25]，在输出密文中浪费了数据槽，从而引入了不必要的通信开销。BumbleBee [42] 通过密文交织消除了浪费的槽，但需要更多的计算。我们采用了密文压缩和解压缩策略，并利用我们矩阵乘法算法中的单项式特殊性质（参见Section-III.B）来减少通信成本。我们还提出了一种适合摊销的离线-在线计算策略（参见Section-III.C），以减少计算成本。 高效的Argmax和其他非线性函数评估 为了防止成员推理攻击 [53]，[58]，[57]，一种常见的方法是输出Argmax后的logits向量，因为它泄漏关于模型的信息最少 [53]。对于长度为m的输入（BERT中m=30,522，Llama-3-8B中m=128,256），当前最先进的协议 [30] 需要执行m次SGN操作（参见Section-II.C）和m次密文旋转。而我们的方法仅需要(log m + 1)次SGN操作和(log m + 1)次密文旋转，从而大大减少了计算开销。此外，我们还使用RNS-CKKS实现了GELU、Softmax和Layer Normalization等非线性函数。 提供了NEXUS在CPU和GPU上的端到端实现 图1展示了我们提出的协议与基线相比的改进。总之，与最先进的协议Bumblebee [42]相比，NEXUS（CPU）在计算上快了1.79倍，可以节省98.1%的通信开销，并将财务成本降低了2.38倍。利用我们协议的非交互式特性，我们进一步提供了GPU加速实现。NEXUS（GPU）将推理速度提高了42.3倍，并将财务成本降低了17.2倍，降至每个token仅0.05美元。我们的代码已经开源，地址是 https://github.com/zju-abclab/NEXUS。 Selective Network Linearization for Efficient Private Inference 选择性网络线性化以实现高效的私密推理\n摘要 私密推理（Private Inference, PI）允许在加密保护的数据上直接进行推理。尽管这种技术有望解决许多隐私问题，但由于极长的运行时间，其应用仍然十分有限。与明文推理中延迟主要由浮点运算（FLOPs）主导不同，在私密推理中，非线性函数（特别是 ReLU）是主要瓶颈。因此，实用的私密推理需要针对 ReLU 的全新优化方法。为减少 PI 的延迟，我们提出了一种基于梯度的算法，能够选择性地线性化 ReLU，同时保持预测精度。我们在多个标准的私密推理基准上评估了该算法。实验结果表明，与当前最先进的方法相比，在相同 ReLU 数量（50K）的情况下，我们的算法可提高预测精度最多达 4.25%；在相同预测精度（70%）的情况下，延迟可减少最多 2.2 倍。这些改进推进了延迟-精度空间中的帕累托前沿。除了实验结果外，我们还提出了一项“无免费午餐”定理，阐明了在保持预测精度的前提下，网络线性化的可能性及适用条件。公共代码已开放，访问地址为：https://github.com/NYU-DICE-Lab/selective_network_linearization。\n引言 基于云的机器学习框架推动了私密推理（Private Inference, PI）的发展。总体而言，私密推理的愿景是使用户能够在云服务提供商拥有的模型上高效地对其数据进行推理，同时保护双方的隐私。在推理之前，用户的数据和服务提供商的模型均通过加密技术进行了加密。\n实现这一愿景的主要难点在于深度网络中的非线性操作。使用密文进行网络中线性操作的私密执行，可以通过秘密共享技术和与输入无关的预处理阶段，使其速度几乎与对相同操作的普通（明文）评估一样快。然而，为了在网络中私密地评估 ReLU，需要使用某种版本的 Yao 的混淆电路（Garbled Circuits, GC），这导致了高昂的延迟成本和存储开销。简单来说，标准的深度网络架构（例如 ResNet）并不适合高效的私密推理，因为它们包含了过多的 ReLU 操作。（参见 Mishra et al., 2020; Ghodsi et al., 2020; 2021; Jha et al., 2021，以及第 2 节的详细讨论。）\n因此，要充分释放快速、准确且私密的神经网络推理的潜力，需要重新设计网络架构，以尽可能减少 ReLU 的数量。针对这一方向，已经涌现了许多努力。早期工作如 MiniONN（Liu et al., 2017）关注于安全协议本身，而最近的工作如 Delphi（Mishra et al., 2020）和 Circa（Ghodsi et al., 2021）则提出用其他激活函数替换 ReLU。另一类研究工作则通过神经架构搜索（NAS）设计 ReLU 高效的网络结构。例如，CryptoNAS（Ghodsi et al., 2020）采用进化式 NAS，而 Sphynx（Cho et al., 2021a）则使用微搜索 NAS。\n本文探索了一条新的路径，适用于用于成像、计算机视觉或其他感知问题的多种架构（特别是具有/不具有残差连接和 ReLU 激活的深度卷积网络）。我们研究了一个我们称为“深度网络线性化”的问题，这指的是在此类网络中精心选择一部分神经元，并消除它们的非线性（即，用恒等或线性操作替换它们的 ReLU 操作），从而在整体性能和 ReLU 操作数量之间实现最小化的权衡。需要注意的是，通过这一过程，我们并未减少网络中的参数数量。由于 ReLU 激活是简单的标量操作，在正常情况下，这一过程在网络训练或测试期间不会显著减少浮点操作（FLOP）的数量。然而，在私密推理场景下，线性化的主要优势在于减少了 GC 计算的数量。\n本文提出了一种简单的基于梯度的算法（我们称之为选择性网络线性化，Selective Network Linearization, SNL）来解决深度网络线性化问题，验证了该算法在多种标准基准上的有效性，并提供了理论依据以阐明其观察到的行为。\n目前，私密推理的最先进方法是 DeepReDuce（Jha et al., 2021）。该方法引入了深度网络中的 ReLU 丢弃（ReLU Dropping）概念，从概念上看，与网络线性化等价。他们提出的算法包含三个阶段：ReLU “筛选”、ReLU “稀疏化”和 ReLU “重塑”，每个阶段都需要多个手动设计选择，并涉及多个超参数。候选架构的搜索空间非常大；将 DeepReDuce 应用于具有 D 个阶段的 ResNet 需要训练 Ω(D) 个不同的网络，并选择准确率最高的那个。此外，DeepReDuce 所做的保留/丢弃决策是分阶段的：要么线性化整个 ReLU 层，要么完全保留整个 ReLU 层不变。\n相比之下，我们提出的技术 SNL 高度自动化，仅涉及极少的超参数。SNL 通过单一的基于梯度的训练过程实现（具体细节将在后文描述），无需在多个网络骨架上进行搜索。此外，SNL 提供了精细的控制，能够细化到像素特征图级别，以决定保留或消除哪些 ReLU。\nSNL 的核心直觉简单明了，并适用于许多深度网络。考虑任何标准架构（例如 ResNet-34），但做一个变化：将所有 ReLU 替换为参数化 ReLU（parametric ReLUs，简称 pReLUs）（He et al., 2015），其中每个 ReLU 都有一个独立的、可训练的斜率参数。此外，每个斜率参数被进一步约束为二值（0 或 1）。定义此网络后，SNL 通过标准训练过程（例如使用 SGD 或 Adam）进行优化，无需其他人工干预。\n在实际实现中需要注意一些关键点。主要挑战在于：(a) 强制 pReLU 的斜率满足二值约束，以及 (b) 确保仅保留少量的 ReLU，即斜率参数向量是稀疏的（或反稀疏的）。借鉴某些网络剪枝算法的方法（Lee et al., 2019；Cho et al., 2021b），我们通过以下方式解决这些困难：在标准训练误差损失中增加一个作用于斜率系数的 $l_1$ 惩罚项，根据时间表逐渐降低该惩罚的权重，并在最后执行一个二值化步骤，将斜率参数取整为二值。详见第 3 节。\n我们在文献中常用的私密图像分类基准数据集上验证了 SNL，结果表明 SNL 在整个准确率-延迟权衡曲线中对现有所有方法实现了帕累托支配。如图 1 所示，在 CIFAR-100 数据集的上下文中验证了这一点；我们在后文以及附录中提供了针对 CIFAR-10 和 Tiny ImageNet 的额外实验结果（及消融研究），也展示了类似的优势。详见第 4 节和附录。\n深入分析 SNL 的结果揭示了一些有趣的现象：在较高比例上，网络早期层的 ReLU 被优先线性化，而晚期层的 ReLU 则较少被线性化。这与早期 PI 工作（如 DeepReDuce，Jha et al., 2021）的结论一致。这是否暗示了网络学习中的某些基本属性？为了严谨回答这个问题，我们证明了线性化会以牺牲 3 层网络记忆容量为代价的无免费午餐定理。此外，如果网络是收缩性的，即第二层的神经元数量少于第一层（分类问题中常见的情况），那么只有在第二层较少的神经元被线性化时，选择性线性化才能保留原始网络的容量。详见第 5 节。\nSigma: Secure GPT Inference with Function Secret Sharing Sigma：通过功能秘密共享实现安全的 GPT 推理\n摘要：安全两方计算（2PC）支持安全推理，能够同时保护专有的机器学习（ML）模型及其敏感输入。然而，现有的安全推理解决方案，尤其是针对Transformer模型的解决方案，存在较高的延迟和通信开销。功能秘密共享（FSS）是一种新兴的范式，可通过预处理阶段实现高效的2PC协议。我们提出了Sigma，这是第一个基于FSS的端到端安全Transformer推理系统。通过为复杂的机器学习功能（如Softmax、GeLU和SiLU）构建新的FSS协议，并加速其在GPU上的计算，Sigma将Transformer安全推理的延迟相比现有基于预处理和GPU的最先进方法提高了12至19倍。我们展示了生成预训练Transformer（GPT）模型的首次安全推理。特别地，Sigma在38秒内执行了Meta的Llama2（HuggingFace提供，参数量为130亿），并在1.5秒内完成了GPT2的推理。\n基于Transformer的生成式语言模型近年来因其在各种自然语言任务上的卓越表现（例如，问答、摘要、语言翻译、代码生成等）[20, 21, 76]而受到广泛关注。除了确保模型和输入隐私外，对此类模型进行安全推理还带来了其他有趣的应用场景，例如“提示隐私（prompt privacy）”。人工智能公司正在投入大量精力构建能够产生良好推理结果的提示，并希望保持这些提示的隐秘性。安全推理允许持有专有提示的公司与持有敏感数据的客户，在不向彼此暴露输入的情况下，利用公开的语言模型生成推理结果。\n然而，目前用于安全推理的最先进系统在Transformer模型上表现不尽如人意。我们认为，安全机器学习推理系统必须满足以下需求：\n准确性 - 即安全推理下的准确性应与明文推理一致； 安全性 - 系统应提供标准的两方计算（2PC）安全性； 效率 - 安全推理的延迟和通信开销应较低； 可扩展性 - 系统必须能够扩展至具有数十亿参数的模型。 然而，我们发现现有系统往往无法满足（甚至多个）这些要求。现有的安全Transformer推理系统包括THE-X [25]、Iron [38] 和 CrypTen [47, 51, 84]（我们将在第8节讨论其他相关工作）。\nTHE-X：通过将基于基本函数（如 exe^x）的复杂非线性替换为简单的非线性（如 (\\max(x, 0)\\）），牺牲了准确性；此外，还通过暴露中间值牺牲了安全性。 Iron：在保持准确性和安全性的同时，通信开销巨大，即使是BERT模型也需要超过100 GB的通信量。 CrypTen：虽然利用GPU加速和预处理提升了效率，但其在线延迟和通信开销仍然显著。此外，由于使用了不安全的本地截断操作，未能提供标准的2PC安全性。而且，由于GPU内存溢出问题，它无法扩展到更大的模型。 我们的贡献 本文提出了 Sigma2 系统，它在多个维度上推进了基于Transformer模型的安全推理技术的最先进水平。与CrypTen类似，Sigma在带有预处理的两方计算（2PC）模型中运行并利用GPU加速，但其延迟和通信效率提高了一个数量级，同时提供标准的2PC安全性保证。在安全推理中，Sigma通过精确逼近复杂非线性函数，保持模型的推理准确性，并能够高效扩展至具有数十亿参数的GPT模型。\n尽管现有的基于Transformer的安全推理最先进系统 [47, 51] 已经使用GPU加速计算，它们基于秘密共享协议的设计导致了大量通信开销，即使在高带宽环境下，这些开销仍然是性能瓶颈。与传统的基于秘密共享的2PC协议相比，基于函数秘密共享（FSS）的2PC协议通信量显著降低，但计算开销较高 [16, 19]。这促使了许多近期工作 [37, 43, 73, 80] 专注于在预处理模型中设计基于FSS的专用协议，用于简单神经网络的安全推理。例如，Orca [43] 是目前卷积神经网络（CNN）安全推理的最先进系统，它证明了GPU加速可以有效处理FSS的大量计算。然而，Orca仅适用于使用简单非线性（如ReLU和Maxpool）的CNN，对Transformer中大量复杂非线性函数（如Softmax）的处理则带来不可接受的开销（详见7.1.2节）。\n在Sigma中，我们设计了针对Transformer的高效基于FSS的2PC协议。由于Transformer安全推理的延迟主要由复杂非线性（如GeLU、SiLU、Softmax和层归一化 [38]）主导，我们提出了新的FSS协议处理这些操作，并通过GPU加速实现。这些操作需要准确计算各种基本函数（如指数、倒平方根、逆运算等）。之前的工作Pika [80] 使用大型查找表（LUTs）来处理这些函数。尽管这种方法通用，Grotto [73] 指出大型查找表效率低下，并在可能的情况下使用基于自定义样条的协议。Sigma的协议通过减小查找表大小在保持准确性的同时，比Grotto更高效（详见7.1.1节）。例如，对于50位的GeLU值，Pika需要大小为 $2^{50}$ 的查找表，而Sigma仅需大小为 $2^8$ 的查找表，并且在相同威胁模型下总计算量比Grotto降低9倍。\n我们评估了基于GPT [20]、BERT [29] 和 Llama2 [77] 的模型，这些模型广泛应用于下一个单词预测和分类任务。我们的创新协议能够安全且准确地评估具有13亿参数的GPT-Neo模型（“一个由EleutherAI基于GPT-3架构复现设计的Transformer模型”[5]），推理时间为7.2秒。Sigma还支持Meta AI最近发布的Llama2模型，并在Huggingface上可用。Llama2-7B需要23秒，Llama2-13B需要38秒。Sigma运行HuggingFace上的较小GPT2模型（每月下载量数千万次）仅需1.5秒，而运行BERT模型则耗时0.1−4.5秒。总体上，Sigma在安全推理的延迟上比现有最先进技术提高了12.2−19倍。\n为了保证标准的2PC安全性，Sigma摒弃了本地截断操作，转而使用安全的忠实截断。截断广泛用于线性层（如矩阵乘法后的操作）和非线性层。我们提出了一种新的忠实截断协议（详见4.2节），其效率比现有工作 [16] 提高最多30倍。尽管我们的截断比CrypTen中的“几乎免费”的本地截断更昂贵，但由于在GeLU、SiLU和Softmax上的巨大性能提升，Sigma的端到端推理速度仍比CrypTen快10倍以上。\n我们的大规模评估得益于Sigma的前端系统，它允许用户简洁地表达所需的Transformer架构，并使用Sigma针对CPU或GPU优化的协议运行（详见第6节）。针对CPU和GPU的协议设计有所不同，我们均提供支持（详见5.1节）。事实上，即使在CPU上运行的Sigma也比在GPU上运行的CrypTen更快。我们在附录A中讨论了使用Sigma的一些实际考量。Sigma的代码已公开，地址为：https://github.com/mpcmsri/EzPC/tree/master/GPU-MPC/experiments/sigma。\nTHE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption THE-X: 基于同态加密的隐私保护Transformer推理\n摘要 随着越来越多的预训练语言模型采用云端部署，隐私问题也迅速增长，主要体现在明文用户数据（如搜索历史、医疗记录、银行账户等）的暴露。云服务用户对Transformer模型的隐私保护推理有着强烈的需求。为了保护隐私，使用同态加密（HE）仅在密文上进行计算是一种具有吸引力的选择。然而，由于Transformer模块中的复杂计算，目前的HE工具尚不支持在密文数据上进行推理。本文提出了THE-X，这是一种针对Transformer的近似方法，能够实现由流行框架开发的预训练模型的隐私保护推理。THE-X提出了一种处理Transformer网络中复杂计算的工作流程，涵盖了所有非多项式函数，如GELU、Softmax和LayerNorm。实验结果表明，我们提出的THE-X能够在加密数据上进行Transformer推理，并适用于不同的下游任务，所有任务在性能上几乎没有下降，同时享有理论上保证的隐私保护优势。\n引言 随着预训练模型在许多自然语言处理（NLP）应用中的革命性进展，如情感分析（Xu et al., 2019a）、问答系统（Yang et al., 2019b）、信息检索（Yang et al., 2019c）和文本生成（Raffel et al., 2020），许多相关技术已经被部署到云端，由工业服务提供商处理来自个人客户、小型企业和大型企业的用户数据。然而，云端预训练技术的便利性也带来了一系列隐私挑战，原因在于用户数据的敏感性。例如，用户请求中的输入文本甚至文本向量表示可能泄露私人信息，进而导致特定用户的身份泄露（Schwartz and Solove, 2011; Zhu and Han, 2020）。这种缺乏隐私保障的情况可能会阻碍注重隐私的用户将数据提供给服务提供商。因此，服务提供商可能面临无法利用用户数据发展模型的困境。此外，数据泄露和其他隐私漏洞可能导致服务提供商面临诉讼、罚款以及声誉损害。这些隐私担忧促使我们提出了THE-X，以实现Transformer模型的隐私保护推理。\n具体而言，我们识别出了预训练模型隐私保护推理中的两个挑战。第一个挑战是如何保护用户的明文数据不被第三方服务提供商访问（例如，诊疗记录或购物历史）。以往的工作应用了差分隐私（DP）（Dwork et al., 2006）及其变体来解决类似的隐私问题——最初用于统计数据库，最近也用于深度学习（DL）（Abadi et al., 2016）和NLP（Qu et al., 2021；Basu et al., 2021b；Fernandes et al., 2019；Lyu et al., 2020；Basu et al., 2021a）。然而，这种解决方案可能遭遇窃听攻击。有少数研究（Zhu and Han, 2020；Zhao et al., 2020）表明，攻击者有可能通过梯度泄露恢复原始数据。此外，隐私保护永远无法理论上保证。第二个挑战是性能问题，近期的研究如TextHide（Huang et al., 2020）和FedNLP（Lin et al., 2021）利用联邦学习（Yang et al., 2019a）在加密数据上训练模型，但代价是显著的性能下降。虽然它们集中关注训练数据的隐私保护，但并未充分探讨隐私保护推理问题。\n为了解决上述问题，我们在图1中描述了一种隐私保护推理的实践，其中一个微调后的语言模型可以通过THE-X转换为云服务模式，并在“闭眼”状态下处理用户数据。在推理过程中，用户查询的内容对Transformer模型来说是匿名的。计算结果也是密文，只有用户的私钥才能解密。此外，我们需要一个理论上保障的加密解决方案，如同态加密（HE）（Gentry, 2009），以便让服务提供商和用户在生产场景中确信隐私安全。HE的语义安全性由基于格的密码学保证，HE对密文的计算结果可以解密为与明文相同的结果，从而避免了性能降低的成本。同态加密的基本思想是在不先解密数据的情况下对加密数据进行计算，这可以在云服务场景中完全确保隐私安全。它允许将用户数据加密后外包到商业云环境进行处理。然而，由于Transformer模型中的复杂操作（例如GELU激活函数），流行的部分同态加密解决方案（仅支持加法或乘法）无法轻松适配到预训练模型的场景中。基于HE Transformer后端（Boemer et al., 2019b,a, 2020），我们设计了一系列近似组件，以完成主流Transformer骨干网络的整个推理流程。我们在GLUE基准（Wang et al., 2019）和CONLL2003任务（Tjong Kim Sang and De Meulder, 2003）上评估了THE-X对BERTtiny的表现。我们的结果表明，THE-X能够实现隐私保护推理，并且平均性能下降仅为1.49%。我们的贡献包括：\n我们是首个探索使用HE进行隐私保护Transformer推理的工作。 我们设计了一种实用且有效的近似工作流程，用于将基于Transformer的模型转换为完全由HE操作组成的函数。 一系列实验结果确认了我们提出的THE-X近似方法带来的性能损失几乎可以忽略不计。 -\n","date":"2025-01-04T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E8%B0%83%E7%A0%94%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%882/tree-3358468_1280_hu13785184439496007011.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E8%B0%83%E7%A0%94%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%882/","title":"调研论文速览2"},{"content":"调研论文速览 今天ZD哥纠正了我学习方法上的错误，不要死扣细节，扣也白扣，别人辛辛苦苦那么久的成果怎么可能让我一天或者几天看懂，我方向出现了问题，我目前要调研，而不是每一篇都看懂，所以我现在要速览一下我能找到的所有相关论文，做个总结：\n以下是新论文：\nAERO: SOFTMAX-ONLY LLMS FOR EFFICIENT PRIVATE INFERENCE AERO: 用于高效私有推理的纯 Softmax 大语言模型\n摘要 专有语言模型的广泛使用引发了用户敏感数据的隐私问题，这凸显了私有推理（Private Inference, PI）的重要性，即在加密输入上直接进行推理。然而，目前的PI方法由于非线性操作的存在，面临极高的通信和延迟开销。在本文中，我们对基于Transformer的解码器语言模型中的非线性作用进行了全面分析。我们提出了AERO，这是一种四步架构优化框架，通过系统性地移除LayerNorm和GELU等非线性操作，并减少浮点运算（FLOPs）次数，优化现有的大语言模型（LLM）架构以实现高效PI。我们首次提出了专为高效PI设计的仅使用Softmax的架构，该架构大幅降低了FLOPs。此外，我们设计了一种新颖的熵正则化技术以提升Softmax-only模型的性能。AERO实现了最多4.23倍的通信开销减少和1.94倍的延迟降低。我们通过对比当前最先进的方法验证了AERO的有效性。\n引言 动机 像ChatGPT这样的专有模型的广泛采用（Achiam等, 2023）显著提升了对用户敏感（提示）数据隐私保护的关注（Staab等, 2024；Mireshghallah等, 2024；Priyanshu等, 2023；Lauren和Knight, 2023），同时也需要防范旨在提取模型权重的攻击（Carlini等, 2024；Jovanović等, 2024）。这凸显了私有推理（Private Inference, PI）的必要性，其中用户将加密查询发送给服务提供商，而不暴露实际输入，推理直接在加密输入上进行，从而既保证输入隐私，又保护模型权重。\n尽管PI方法具有良好的前景，但当前的PI方法由于极高的延迟和通信开销而在实际应用中仍然不可行——例如，在GPT-2模型（125M参数）上，使用128个输入Token生成一个输出Token需要8.2分钟，并消耗25.3 GB的通信量（图1）；在上下文大小扩展到512时，这一数值进一步增长到30.7分钟和145.2 GB（表7）。这些开销主要来源于Transformer架构的大语言模型（LLM）中对模型性能至关重要的非线性操作，如GELU、LayerNorm和Softmax（Hou等, 2023；Lu等, 2025）。\n挑战 当前针对Transformer架构模型（如ViT、BERT）的PI解决方案要么忽略了LayerNorm的成本（Li等, 2023a；Zeng等, 2023；Zhang等, 2023；Chen等, 2023），要么通过多项式函数近似非线性操作（Zimerman等, 2024；Dhyani等, 2024）。然而，多项式近似方法存在明显的局限性：其精度对数据特定的初始猜测非常敏感（Knott等, 2021），并且其有效性仅限于狭窄的输入范围（Zimerman等, 2024）。此外，为了提高近似精度而使用高阶多项式的网络往往难以训练和优化。\n与此同时，为提升明文处理速度而设计的非线性减少方法对提升PI效率的潜力非常有限。例如，(He等, 2023；Noci等, 2023；He和Hofmann, 2024) 探索了设计无LayerNorm的LLM的架构启发式方法；然而，这些方法对激活函数选择（PI中的关键瓶颈）的广泛影响仍然鲜有研究。\n我们的技术与见解 我们对基于Transformer的大语言模型（LLMs）中非线性操作的作用进行了深入分析，主要发现如下：\n无LayerNorm的模型在前馈网络（FFN）中更倾向于使用ReLU而非GELU，这使其更适合私有推理（PI）。 在仅使用Softmax的模型中，训练不稳定性（表现为深层中的熵崩塌）可以通过对FFN权重进行归一化来避免，这在推理过程中避免了类似LayerNorm的非线性计算。 我们还观察到一种现象，称为熵过载（entropic overload），即在使用GELU的无LayerNorm模型以及仅使用Softmax的模型中，注意力头（attention heads）以更高且接近最大值的熵值被卡住的比例显著增大。我们推测，熵过载会导致注意力头缺乏多样性和专业化，从而浪费了注意力头的表征能力，并导致性能下降，这在更高的困惑度（perplexity）中得以体现。\n为缓解熵过载，我们提出了一种新颖的熵正则化技术，在训练过程中对极端熵值进行惩罚，从而避免偏离良好的熵分布。\n结果与意义 如图1所示，仅将GELU替换为ReLU，就使基础GPT-2模型的通信和延迟开销分别降低了2.68倍和1.35倍。进一步移除LayerNorm使这些收益分别提高到3.41倍和1.42倍。在Pythia模型中也观察到类似的改进（见图15）。\n在仅使用Softmax的模型中，由于FFN仅执行线性变换，将多个线性层合并为一个单一线性层，可使FFN的FLOPs减少8倍，同时显著加速计算且不增加困惑度（见图1）。此外，我们的分析表明，Softmax-only模型中早期FFN的线性变换对于训练稳定性至关重要，而较深层的FFN则可以被裁剪，从而提供了进一步减少FLOPs的机会。\n贡献 我们的主要贡献如下：\n我们全面分析了基于Transformer的大语言模型中GELU和LayerNorm的非线性作用，通过使用Shannon熵研究它们对注意力得分分布的影响，为现有LLM架构的高效PI定制提供了洞见。 我们提出了AERO，一个四阶段优化框架，并设计了仅使用Softmax的模型，大幅减少了FLOPs，实现了最高1.94倍的速度提升和4.23倍的通信开销减少。 我们提出了一种新颖的熵正则化技术，用于提升Softmax-only模型的性能，在困惑度方面实现了6%-8%的改进。 我们在广泛的上下文长度（128、256、512）和模型深度（12层和18层）下，基于从CodeParrot Face和Languini数据集（Stanić等, 2023）中提取的1.2B至4.8B训练Tokens，对GPT-2和Pythia模型（Biderman等, 2023）进行了大量实验。 结论 在本研究中，我们提出了AERO，一个四阶段设计框架，用于优化现有的大语言模型（LLM）架构，以实现高效的私有推理（PI）。我们设计了具有显著较低FLOPs的仅使用Softmax的架构，并提出了熵正则化技术以提升其性能。\n局限性 本研究主要关注于预训练性能，以困惑度（perplexity）作为主要评价指标，未包含对模型其他能力（如迁移学习或少样本学习）的实验评估。此外，所提出的仅使用Softmax的模型的有效性仅在参数量少于10亿的模型上进行了验证。\n未来工作 未来将进一步进行更广泛的实验评估，包括将AERO适配于大规模模型的研究（详见附录H）。\nCipherDM: Secure Three-Party Inference for Diffusion Model Sampling CipherDM: 用于扩散模型采样的安全三方推理\n摘要 扩散模型（Diffusion Models, DMs）在图像生成中实现了最先进的合成效果，并被应用于多个领域。然而，DMs 在使用过程中有时会严重侵犯用户隐私，因此隐私保护成为一个亟待解决的问题。直接在 DMs 中使用传统的隐私计算方案（如安全多方计算，MPC）会面临显著的计算和通信挑战。为了解决这些问题，我们提出了 CipherDM，这是第一个将 MPC 技术应用于 DMs 进行安全采样的新颖、通用框架，可广泛用于基于 DMs 的多种任务。\n我们对采样延迟的组成部分进行了深入分析，找到耗时的环节，并设计了相应的安全 MPC 协议，用于计算包括 SoftMax、SiLU 和 Mish 在内的非线性激活函数。CipherDM 在常用架构（DDPM、DDIM）上使用 MNIST 数据集以及通过 diffusers 部署的 SD 模型进行了评估。与直接在 SPU 上实现的方式相比，我们的方法运行时间提高了约 1.084× 至 2.328×，通信成本降低了约 1.212× 至 1.791×。\n代码地址：https://github.com/Zhaoxinxinzi/CipherDM\n引言 以预训练的Transformer模型 [54] 和扩散模型（Diffusion Models, DMs）[19, 50, 53] 为代表的生成模型因其在文本和图像生成任务中的出色表现而备受关注。这些模型在深度学习即服务（Deep Learning as a Service, DLaaS）[51] 模式中被广泛使用。然而，使用此类服务时也带来了隐私问题。例如，在 ChatGPT [6] 和 Stable Diffusion (SD) [47] 的案例中，用户需要向服务提供商（如拥有大量计算资源的网站或应用程序）透露其私密的提示词或图像。另一种选择是，服务提供商将其专有的训练模型参数发布给用户，后者可以在本地进行推理或采样。\n扩散模型在图像生成方面的卓越性能已在最近的多项研究中得以证明 [39, 44, 45, 47, 57]。然而，这些研究也引发了隐私问题 [39, 44, 47]。例如，Stable Diffusion 的基础性研究 [47] 指出，扩散模型使用的训练数据可能包含敏感或个人信息，而在数据收集过程中可能缺乏明确的用户同意。另一方面，Glide [39] 强调其模型能够生成虚假但高度逼真的图像，这些图像可能被用来制造令人信服的虚假信息或深度伪造（Deepfakes）。为了解决伦理问题，Glide 过滤掉了包含人物、暴力和仇恨符号的训练图像，从而降低了模型在问题场景中的误用风险。另一项研究 [44] 则通过在一个经过仔细筛选的数据集上训练模型，确保生成结果的美学质量和安全性。这些研究主要关注数据隐私和内容真实性的问题。尽管采取了措施来缓解安全性问题，但它们无法充分保证隐私信息的保护。\n传统的采样阶段 [19, 27, 52, 60] 面临用户隐私提示词或图像泄露，以及提供商模型参数泄露的风险。对于用户而言，采样过程中的提示词和图像可能包含敏感信息，例如个人肖像、身份、习惯和职业。非法组织未经授权收集此类信息会对用户隐私构成重大威胁。类似地，对于模型提供商而言，扩散模型的训练过程需要大量的时间和资源投入，模型细节（包括参数和架构）的泄露会显著影响其利益和竞争优势。因此，在采样过程中优先保护隐私至关重要。\n经典的隐私计算技术主要包括差分隐私（Differential Privacy, DP）[2, 8, 14, 24]、同态加密（Homomorphic Encryption, HE）[3, 4, 25, 49] 和安全多方计算（Secure Multiparty Computation, MPC）[10, 12, 31, 37, 38, 40]。虽然 DP 通常被认为是一种高效的方法，但它无法有效保证计算结果的精确性和准确性。此外，目前 DP 仍缺乏严格的安全性证明。HE 能够保持数据的保密性，但以较高的计算复杂性为代价。MPC 提供了一种解决方案，使得相互不信任的各方能够利用各自的私有输入协作计算某一函数，并确保只有函数的输出被揭示，任何一方都无法获得额外信息。该计算既保证了隐私又保证了正确性，既防止了恶意方获得除输出之外的信息，也确保诚实方不会接受错误的输出结果。一些研究 [13, 16, 22, 29, 59] 提出了基于 MPC 的解决方案，用于在 Transformer 模型上执行推理。然而，据我们所知，目前尚无基于 MPC 的方法用于扩散模型的安全采样。\n我们推测，缺乏相关研究的主要原因可能包括：图像生成结果的隐私评估尚不明确，将扩散模型与 MPC 框架集成的难度较大，以及在隐私计算中采样效率较低。\n基于上述观察，我们提出 CipherDM，这是首个基于 MPC 的推理框架，能够为扩散模型的采样提供严格的 MPC 保证。\nEfficient 2PC for Constant Round Secure Equality Testing and Comparison 高效的二方安全协议（2PC）用于常数轮次的安全相等性测试和比较\n摘要 安全相等性测试和比较是两种重要的原语，广泛应用于许多安全计算场景中，如隐私保护的机器学习、私有集合交集、安全数据挖掘等。在这项工作中，我们提出了新的常数轮次二方安全协议（2PC）用于安全相等性测试和安全比较。我们的协议采用在线/离线范式设计。 理论上，对于32位整数，我们的相等性测试的在线通信仅为76比特，而我们的安全比较的成本仅为384比特。我们的基准测试结果显示：（i）我们的相等性测试比Guo等人（EUROCRYPT 2023）快9倍，比加扰电路方案（EMP-toolkit）快15倍；（ii）我们的安全比较协议比Guo等人（EUROCRYPT 2023）快3倍，比Rathee等人（CCS 2020）和加扰电路方案快6倍。\nCentaur: Bridging the Impossible Trinity of Privacy, Efficiency, and Performance in Privacy-Preserving Transformer Inference 《Centaur：弥合隐私、效率和性能三者之间的难以实现的三位一体——隐私保护Transformer推理》\n摘要 随着像Transformer这样的预训练模型越来越多地部署在云平台上提供推理服务，围绕模型参数和推理数据的隐私问题日益突出。目前的隐私保护Transformer推理（PPTI）框架面临着隐私、效率和性能的“不可兼得三位一体”问题。例如，基于安全多方计算（SMPC）的解决方案提供了强有力的隐私保障，但带来了显著的推理开销和性能折衷。另一方面，使用随机置换的PPTI框架在推理效率上接近明文推理并保持准确的结果，但需要暴露一些模型参数和中间结果，从而可能导致严重的隐私泄露。单一技术难以解决这一“不可兼得三位一体”问题。为了解决这一挑战，我们提出了Centaur，一种新型的混合PPTI框架。与现有方法不同，Centaur通过随机置换保护模型参数，通过SMPC保护推理数据，利用Transformer模型的结构。通过设计一系列高效的隐私保护算法，Centaur结合了两种技术的优势，在PPTI中实现了隐私、效率和性能的更好平衡。我们在多种类型的Transformer模型和数据集上全面评估了Centaur的效果。实验结果表明，Centaur提供的隐私保护能力能够抵御各种现有的模型反演攻击方法。在性能和效率方面，Centaur不仅保持了与明文推理相同的性能，而且推理速度提高了5.0到30.4倍。\n引言 Transformer模型（Vaswani et al., 2017; Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020; Raffel et al., 2020; Liu et al., 2019; Lewis et al., 2020）正在迅速改变人类社会的运作方式。越来越多的Transformer模型，例如GPT-4（OpenAI, 2023）和Llama（Touvron et al., 2023），被部署在云平台上以提供高质量的服务，如聊天、虚拟助手和代码生成。然而，这种服务模式要求模型开发者和用户分别将模型参数和推理数据上传至云平台，从而带来了显著的隐私泄露风险。具体来说，模型参数通常代表了模型开发者的竞争优势，因此被视为个人资产。而推理数据也可能包含用户的私人信息，例如投资意图或健康状况。这些隐私风险严重阻碍了基于Transformer的大型语言模型（LLMs）在关键领域的应用。例如，在发现内部敏感代码泄露后，三星禁止其员工使用外部LLM服务。\n大量研究（Chen et al., 2022; Li et al., 2023; Zeng et al., 2022; Zhang et al., 2023; Liang et al., 2023; Hao et al., 2022; Zheng et al., 2023; Gupta et al., 2023; Dong et al., 2023; Hou et al., 2023; Ding et al., 2023; Pang et al., 2023; Luo et al., 2024）已经出现，旨在解决Transformer推理过程中模型参数和推理数据的隐私问题。然而，这些方法通常在隐私、效率和性能之间存在权衡。例如，基于密码学的技术如安全多方计算（SMPC）（Shamir, 1979; Yao, 1986; Goldreich et al., 1987）因其理论上能够同时保护模型参数和推理数据的隐私而受到广泛关注。然而，直接将现有的SMPC协议应用于实现隐私保护Transformer推理（PPTI）可能导致显著的效率低下或性能下降。例如，基于SMPC的GPT-2BASE隐私保护推理生成单个Token需要超过25分钟（Hou et al., 2023）。这种低效主要源于使用SMPC进行隐私保护推理时，与明文推理相比，显著更高的通信开销，特别是在Transformer中的非线性操作（例如Softmax、GeLU和LayerNorm）时（详见第3节）。\n为了解决这一问题，Li等人（Li et al., 2023）通过将Softmax和GeLU替换为一些SMPC友好的二次方程式，重新设计了Transformer模型，避免了非线性操作（如Softmax和GeLU中的erf、指数和最大值）的隐私保护计算。虽然这种激进的替换提高了PPTI的效率，但即使对修改后的模型进行重新训练或微调，模型性能仍会有所下降。Dong等人（Dong et al., 2023）尝试使用分段多项式逼近Transformer模型中的非线性操作。尽管这种方法能够保证模型的准确性，但使用SMPC计算分段多项式仍会带来显著的开销。Luo等人（Luo et al., 2024）整合了上述两种方法的优势，定制了一系列高效的隐私保护非线性算子，从而实现了更高效的PPTI。\n尽管这些方法通过替代或近似技术提高了PPTI中非线性层的效率，但它们很少关注线性层中SMPC引发的开销。这导致原本在PPTI中通信开销较小的线性层，在这些优化后反而成为新的通信瓶颈。因此，基于SMPC的PPTI仍然效率低下。此外，通过替代提升效率往往会导致性能下降。\n为了解决SMPC-based PPTI面临的效率和性能问题，Yuan等人（Yuan et al., 2023）提出了一种基于随机置换的方法来保护模型参数和推理数据的隐私。通过对置换后的模型参数和推理数据进行明文计算，基于置换的PPTI避免了SMPC带来的通信开销，并且由于未替代或近似任何非线性操作，其推理性能可以与明文推理相同。然而，为了确保推理过程的正确性，在推理过程中，Embedding层的参数以及部分中间结果将会被暴露，这带来了显著的隐私泄露风险（详见第3节）。\n由此可见，现有基于单一隐私保护技术的PPTI工作只能满足隐私、效率和性能三元悖论中的一项或两项需求。使用单一技术突破PPTI中隐私、效率和性能的“不可调和三难”极具挑战性。为应对这一挑战，我们提出了一种名为Centaur的新型混合PPTI框架，其目标是通过充分利用不同隐私保护策略在模型参数和推理数据上的优势，更好地平衡这一“不可调和三难”（图1）。为实现这一目标，\n隐私性：Centaur 设计了一种新的 PPTI 流程，确保 PPTI 中的所有输入、输出和中间结果均处于随机置换状态或秘密共享状态。这为模型参数和推理数据的隐私提供了有效的保护（详见第 5.1 节和第 6 节）。\n效率：Centaur 通过将大量基于随机共享的隐私保护矩阵乘法转换为明文与随机共享之间的无通信隐私保护乘法，大幅提升了 PPTI 中线性层的效率。对于非线性层，Centaur 将基于 SMPC 的高开销隐私保护非线性操作转换为基于随机置换的明文操作，并设计了一系列高效的隐私保护非线性算法。通过对线性和非线性层的高效设计整合，Centaur 在推理速度上显著超越了现有的最先进基于 SMPC 的 PPTI 框架（Li et al., 2023；Dong et al., 2023；Luo et al., 2024）（详见第 5.2 节）。\n性能：Centaur 利用中间结果在秘密共享状态与随机置换状态之间的转换，实现了 Transformer 模型中非线性算子的隐私保护精确计算。因此，Centaur 无需对原始模型结构或参数进行任何修改，即可在 PPTI 中实现与明文推理相同的性能，且无需额外的再训练或微调（详见第 5.2 节）。\n我们通过大量实验验证了 Centaur 的有效性（详见第 7 节）。具体而言，在隐私性方面，我们对多个模型和数据集进行了攻击实验，结果表明 Centaur 提供的隐私保护能够抵御现有的模型反演攻击方法。在效率和性能方面，我们在多种 Transformer 模型和数据集上测试了 Centaur，并与现有最先进的 PPTI 框架（Li et al., 2023；Dong et al., 2023；Luo et al., 2024）进行了比较。实验结果表明，与这些先进的 PPTI 框架相比，Centaur 不仅能够保持模型性能，还将推理速度提升了 5.0 至 30.4 倍。\nFaster Lookup Table Evaluation with Application to Secure LLM Inference 更快的查找表评估及其在安全大语言模型推理中的应用\n摘要 随着大语言模型（LLMs）的日益普及，用户隐私问题也愈发受到关注，因为用户提交用于推理的数据可能包含敏感信息。因此，通过安全两方计算（即安全LLM推理）运行LLMs已成为一个备受关注的热点。然而，LLMs中的许多操作（如Softmax和GELU）无法通过安全计算中的传统逻辑门来实现，而是需要利用查找表（LUTs）。这使得查找表成为安全LLM推理中的一个核心基础模块。本文提出了ROTL，一种用于查找表评估的安全两方协议。与目前最先进的查找表协议FLUTE（在Oakland \u0026lsquo;23提出）相比，ROTL在总体性能上实现了最高11.6倍的加速，在在线性能上实现了最高155倍的加速。此外，ROTL支持算术共享（这是安全LLM推理所必需的），而FLUTE仅支持布尔共享。\nROTL的核心是一种用于秘密共享旋转的全新协议，该协议允许双方在不泄露旋转偏移的情况下生成旋转后表的加法共享。我们认为这一协议具有独立的研究价值。基于ROTL，我们设计了一种新的安全比较协议，与当前最先进的协议相比，其在线性能在带宽消耗上减少了2.4倍。为了支持布尔共享，我们还对FLUTE进行了优化，将其计算复杂度从 $O(l \\cdot n^2) $降低到 $O(n \\log n + l \\cdot n)$，并将 $O(n \\log n) $的计算转移到预处理阶段。因此，与FLUTE相比，该优化在总体性能上实现了最高10.8倍的加速，在在线性能上实现了最高962倍的加速。\n引言 随着大语言模型（LLMs）的日益普及，用户隐私问题愈发受到关注，因为用户在推理阶段提供的数据可能包含敏感信息。因此，安全推理 [10], [21], [20], [23], [30], [28], [17], [14] 已成为一个重要研究方向，其目标是在推理阶段实现如下目标：服务器 (S) 无法得知客户端 (C) 的输入，而客户端仅能获取推理结果，无法得知模型的其他信息。这可以被视为一种为模型推理量身定制的安全两方计算（2PC）协议。\n然而，LLMs 中的许多操作（如 Softmax 和 GELU）无法通过 2PC 中的传统逻辑门进行计算，而是必须依赖查找表（LUT）。这使得 LUT 成为安全 LLM 推理中的一个核心基础模块。LUT 协议允许两方基于一个秘密共享的索引 i，从公共表 $x \\in \\mathbb{Z}_{2l}^n$ 中得出 $x_i$。通常会引入一个预处理阶段，用于准备一些昂贵且与输入无关的工作，以使在线阶段的计算更加高效。\nLUT 评估最常见的方法基于 “1-out-of-n” OT（选择性传输）。具体而言，S 会为 C 的 n 个可能的输入共享生成对应的 LUT 输出，并用一个随机数对这些输出进行掩码（作为 S 的输出共享）。随后，C 使用 “1-out-of-n” OT 获取自己的输出共享。尽管这一协议在计算上是高效的，但在在线阶段需要传输整个查找表。\n当前最先进的 LUT 协议（称为 FLUTE）通过将 LUT 表述转化为布尔表达式来避免传输整个查找表，然后将这些布尔表达式的电路表示为一个多输入内积进行评估 [4]。然而，FLUTE 在线阶段需要进行多输入 AND 门的评估，这带来了昂贵的计算成本。此外，FLUTE 的输入和输出仅支持布尔共享，而安全 LLM 推理需要算术共享，因为其中涉及大量矩阵乘法运算。为了用于安全 LLM 推理，FLUTE 必须结合布尔到算术（B2A）和算术到布尔（A2B）转换，而特别是 A2B 转换的代价非常高。\n我们的贡献 在本文中，我们提出了 ROTL，一种比 FLUTE 显著更快且支持算术共享的 LUT 协议。ROTL 的核心思想是在预处理阶段，C 和 S 联合将查找表 x 向右旋转 s 个元素，旋转后的表和 s 的值都由 C 和 S 秘密共享。在在线阶段，C 和 S 可以简单地恢复 (i + s)，并输出旋转表中的第 (i + s) 个元素。\n这一思路与 OTTT [19] 和 OP-LUT [9] 中的方法类似。然而，OTTT 和 OP-LUT 都需要评估昂贵的电路来旋转表：OTTT 对每一个可能的输入评估表示查找表的布尔电路，而 OP-LUT 可以被视为 GMW 协议的自然推广。与此不同，我们提出了一种显著更轻量化的查找表旋转协议。我们认识到，旋转是一种特殊的置换，因此利用了 Chase 等人提出的秘密共享置换协议 [5]，该协议已经非常轻量化，仅需 $n \\log n$ 次随机 OT 操作。通过利用旋转的固有特性，我们将所需的随机 OT 数量显著减少至仅 $\\log n$。此外，我们提出了一种方法，使得 C 和 S 可以旋转一个选择向量 $b \\in \\mathbb{Z}2^n$，而不是直接旋转查找表 $x \\in \\mathbb{Z}{2l}^n$。因此，我们使通信开销与 ll 无关。\n虽然布尔共享在 LUT 中使用较少，但它们在布尔电路的评估中仍有应用。为了完整性，我们引入了另一种名为 FLUTE+ 的 LUT 解决方案，针对布尔共享进行了优化。FLUTE+ 将 FLUTE 的计算复杂度从 O(l⋅n2)O(l \\cdot n^2) 降低到 O(nlog⁡n+l⋅n)O(n \\log n + l \\cdot n)，并将 O(nlog⁡n)O(n \\log n) 的计算转移到预处理阶段。\n除了 LUT，安全比较 是安全 LLM 推理中的另一个关键基础模块，被广泛用于截断、Softmax 和 GELU 等操作中。基于 ROTL，我们引入了一种新的安全比较协议，将主要的开销转移至预处理阶段。与当前最先进的方法相比，我们的协议在在线性能方面实现了显著的加速（具体 xxx 倍速提升）。\n我们总结了本文的主要贡献如下：\n提出了一种用于秘密共享旋转的新协议（详见第 III 节）； 提出了一种新的查找表（LUT）协议（称为 ROTL），其性能比 FLUTE 快至 155 倍，并支持算术共享（详见第 IV 节）； 对 FLUTE 进行了优化（称为 FLUTE+），实现了最高 962 倍的性能提升（详见第 V 节）； 提出了一种新的安全比较协议，与当前最先进的方法 [30] 相比，带宽需求减少了 2.4 倍（详见第 VI 节）； 将 ROTL 应用于安全大语言模型（LLM）推理（详见第 VII-E 节）； 提供了一个完整的实现以及全面的基准测试（详见第 VII 节）。 Input Reconstruction Attack against Vertical Federated Large Language Models 基于纵向联邦大语言模型的输入重构攻击\n摘要 近年来，由于 ChatGPT 的出现，大语言模型（LLMs）受到了学术界和公众的广泛关注。尽管 LLMs 在各种任务中的文本生成能力令人惊叹，但隐私问题限制了其在实际业务中的应用。更具体而言，无论是用户的输入（用户将查询发送给模型托管服务器）还是模型本身（用户下载完整模型），在使用过程中都会被暴露。纵向联邦学习（VFL）是一种解决此类问题的有前景的方法。通过将模型分为底部部分和顶部部分，分别由用户和模型提供方维护，VFL 保护了用户输入和模型知识的隐私。然而，在本文中，我们证明了在 LLMs 中，VFL 无法保护用户输入，因为从中间嵌入中重构输入既简单又低成本。实验表明，即使使用商用 GPU，也能在仅 1 秒内重构出输入句子。我们还讨论了若干可能的解决方案，以增强纵向联邦 LLMs 的隐私保护能力。\n引言 随着 ChatGPT [14] 的出现，大语言模型（LLMs）引起了公众和学术界的广泛关注。LLMs 在各种任务中表现出色，例如问答、阅读理解、文本摘要，甚至数学推理。然而，在实际应用中，LLMs 仍面临显著的隐私问题。例如，ChatGPT 的典型使用场景是用户将查询发送到 OpenAI 服务器，然后获取响应。在这种情况下，用户的查询完全暴露给了 LLM 提供商。如果查询包含敏感或有价值的信息，例如用户的个人信息或公司的机密数据，这是不可接受的。\n另一种场景是用户下载完整的 LLM 模型并自行部署。虽然这种情况下保护了用户隐私，但模型却完全暴露给了用户。考虑到 LLM 是高度有价值的资产，这种方式会侵犯模型提供商的隐私。这里的核心问题在于，数据和模型分别由两个不同的参与方持有，双方的信息都不应被对方知晓。\n一种解决此问题的方法是使用密码学技术。这些方法利用诸如多方计算协议、混淆电路和同态加密等密码学技术，来实现安全的神经网络推理或训练。尽管这些方法严格意义上完全保护了双方的隐私，但由于计算和通信开销过高，目前它们只能应用于相对简单的模型。一些前沿工作尝试使用密码学方法实现安全的 Transformer [8, 10, 4]，但即使在使用高性能服务器、理想的本地网络设置下，对于非常短的输入生成一个 token 也需要至少几分钟时间，这在实际应用中是不现实的。\n一种替代隐私保护方法是纵向联邦学习（VFL）。VFL 的典型方法是分割学习（split learning），其过程如下：\n模型提供商通过某一层将模型划分为底部模型和顶部模型。然后将底部模型发送给用户。 在推理过程中，用户将其私有输入输入到底部模型中，得到隐藏嵌入（smashed data）。随后，他将隐藏嵌入发送给模型提供商，模型提供商将其输入顶部模型并得到模型输出，最后将输出发送给用户。 因此，在分割学习中，仅隐藏嵌入和部分模型被暴露，这在一定程度上保护了双方的隐私。然而，需要注意的是，通过分割学习，用户查询的响应同样由模型提供商生成，这可能会泄露用户隐私。然而，本文关注的是用户输入的隐私，而非模型输出。\n我们展示了攻击者如何能够利用隐藏嵌入和底部模型的参数轻松重构出用户的精确输入文本。具体而言，我们首先重构第一层中的词嵌入，然后计算重构的词嵌与底部模型中的实际词嵌的相似度，找出最相似的词。这种攻击非常高效，攻击者不需要任何额外的侧信息，仅需底部模型的参数即可。这样的攻击假设在模型提供商被攻破的情况下是现实的，因为攻击者可以获得完整的模型。\n实验基于开源的 ChatGLM6B 模型进行。此外，我们还研究了在将隐藏嵌入发送给模型提供商之前添加噪声以增强输入隐私的方案。然而，实验结果表明，这种防御措施并不实际，因为模型性能也因此严重下降。\n相关工作 虽然很少有研究探讨 LLM 中的输入重构问题，但从隐藏嵌入重构输入在深度学习，特别是纵向联邦学习（VFL）中已经是一个长期研究的问题。该问题可以分为两类：黑箱和白箱。在黑箱场景中，用于生成嵌入的底部模型是未知的，而在白箱场景中，攻击者拥有底部模型，包括其结构和参数值。\n在黑箱场景中，一个基本的重构方法是训练一个重构模型，将隐藏嵌入作为输入，输出原始输入 [22, 9, 18]。其他方法包括训练一个生成器来进行反转 [9, 13]，以及训练替代模型和输入 [6]。尽管 [13] 也将反转攻击应用于语言模型，但它采用了黑箱设置，即攻击者对底部模型一无所知，但可以无限次查询模型。这要求攻击者训练一个生成模型来重构输入，过程非常耗时。相反，本文聚焦于白箱设置，因为它更加实用。\n在白箱场景中，最直接的方法是直接优化输入，因为嵌入现在是可微分的 [9, 12, 18]。然而，[18] 中的作者直接优化了一维 one-hot 单词向量，采用了 Softmax 放松技术，这导致了次优性能，因为 one-hot 向量的维度太高。相反，我们首先优化单词嵌入，然后根据单词嵌入重构单词。\nMPC-Minimized Secure LLM Inference MPC 最小化的安全 LLM 推理\n摘要 许多基于大语言模型（LLM）的推理服务存在隐私问题，要么向服务方泄露用户的输入，要么向用户泄露专有的模型权重。安全推理通过多方计算（MPC）提供了一个解决方案，但由于MPC所带来的巨大开销，现有方法对于现代LLM工作负载仍然不可行。为了解决这一开销问题，我们提出了MARILL，一个框架，通过调整LLM微调过程，最大程度地减少MPC在安全推理过程中的使用。MARILL在微调过程中引入了高层架构变更，通过去除某些昂贵的操作并将其他操作移出MPC，同时不妥协安全性，从而显著减少了推理过程中MPC所需的操作数量。因此，MARILL生成的模型在所有安全推理协议中都更加高效，并且我们的方法与针对这些操作的MPC友好近似方法相辅相成。与标准微调相比，MARILL在不同MPC设置下的安全推理中实现了3.6−11.3倍的运行时提升和2.4−6.9倍的通信提升，同时通常在下游任务中保持超过90%的性能。\n引言 基于Transformer的大型语言模型（LLMs）已经彻底改变了机器学习（ML）领域。自从ChatGPT发布以来，我们见证了大量专有LLM（如GPT-4 [57]、Claude 2 [2]、Bard [24]）和开源LLM（如Llama [69]、Mistral [34]）的发布，这些开源模型现已与专有模型竞争[11, 72, 76, 48]。最近，许多公司开始在特定领域数据上对这些模型进行微调，以提升其在下游任务（如聊天机器人、虚拟助手和协作助手）中的表现[57, 3, 15]。\n然而，使用这些微调后的模型来支持面向用户的服务却引发了显著的隐私问题。一方面，这些微调模型的提供者不希望公开其模型的权重，因为这些模型通常是在专有数据上训练的，并代表了竞争优势。另一方面，用户不希望将自己的查询发送给这些提供者，因为查询可能包含敏感或专有信息（例如受知识产权保护的代码或用户数据）。事实上，一些企业已经禁止员工使用LLM服务，例如三星最近在一名员工意外将敏感代码泄露给ChatGPT后，禁止使用外部LLM服务[62]。\n安全推理是解决这一挑战的有前景的解决方案，因为它可以通过多方计算（MPC）为双方提供隐私保护[22, 77]。关于安全推理的研究已有很多工作[56, 54, 61, 60, 70, 68, 26, 32]，这些工作提供了不同的性能和安全权衡，近期的研究集中在安全Transformer推理上[45, 73, 16, 50, 29, 25]。原则上，服务提供者可以使用这些最新的安全推理协议来支持其隐私保护服务。然而，尽管效率已取得巨大进展，这些协议对于今天的LLM仍然不切实际。例如，最先进的解决方案[25]要求在小型137M参数模型上生成第一个令牌时需要23秒和15.9GB的通信开销，对于一个典型的7B参数模型，运行时间和通信开销预计会增加到约6.5分钟和240GB，这是不可行的。\n为了最小化这一开销，之前的研究集中于MPC中计算开销较大的低级操作，并提出了针对这些操作的MPC友好近似方法（第2节）。在本工作中，我们考虑了一种正交方法，旨在通过高层架构变更来最小化MPC开销。这种架构变更不会简化操作，而是减少了在MPC中需要的昂贵低级操作的数量。关键是，这一策略并不（必要）完全消除这些操作，而是将其移出MPC，不会妥协安全性，并且其成本相对较小。我们的工作首次探索了这种高层策略，我们称之为MPC最小化。我们通过微调实现这一目标，我们的关键见解是，微调如果能够根据安全推理精心定制，可以为MPC最小化开辟出显著的机会。\n基于这一见解，我们提出了一个微调框架MARILL2，它对标准微调进行了战略性修改，并受到MPC独特特征的指导。通过MARILL微调后的模型具有以下特点：（i）MPC最小化，同时保持相同的安全性水平（第3节）；（ii）通过知识蒸馏，在ML性能上接近标准微调模型（第5节）。关键是，由于MARILL本质上是在MPC内压缩模型，生成的模型在所有安全推理协议中都显著更高效（第6.1节）。此外，如前所述，MARILL仅引入高层架构变更，这些变更与MPC友好的近似方法相辅相成。我们展示了将这些近似方法与MARILL结合使用时，效率进一步提高（第6.3节）。接下来，我们简要介绍我们的技术和它们在MPC中最小化的模型组件（加粗部分）：\n利用开源模型 如前所述，开源LLM已变得更强大，并且现在能够与专有模型竞争[11, 72, 76, 48]。因此，出现了一种趋势，越来越多的服务提供商选择使用他们的私有数据集对这些开源模型进行微调，而不是从头开始训练自己的专有模型[3, 15]。标准的微调方法会使用私有数据更新所有模型权重，这要求整个模型都必须在MPC中运行，从而无法利用公开可用的预训练权重。因此，我们提出了两种微调策略，可以有效利用公共权重来最小化MPC的使用：\n层冻结（Layer Freezing）（§ 5.1）：我们通过将微调更新（因此，私有权重）仅限于预训练模型的最终层，减少了需要在MPC中评估的Transformer层数量。我们采用这种严格的划分，因为交替使用私有层和公共层仍然需要在公共层中运行瓶颈操作，这需要在MPC中处理（§ 4），而仅仅修剪公共层会导致任务性能下降（§ 6.4）。 低秩适配（Low-rank Adaptation，LoRA）（§ 5.2）：近年来，像LoRA [30]这样的参数高效微调技术表明，通过仅训练模型权重的一小部分，可以实现与标准微调相当的任务性能。尽管LoRA旨在加速微调过程，但我们展示了一种LoRA的新应用，表明它可以被重新用于减少MPC中矩阵乘法的维度——这是自然双方设置下以及其他MPC设置中解码（B）阶段的运行瓶颈（§ 5.2）。 降低自注意力开销 我们分析了在不同MPC设置下LLM推理的成本配置，发现自注意力模块是最有效设置中的瓶颈（§ 5.3）。减少自注意力操作的标准方法是采用头修剪（head pruning）[53]。然而，我们必须修剪多达75%的头（及其对应的参数）才能取得显著的改进，但我们发现这会导致准确度大幅下降，即使在微调后也未能完全恢复（§ 6.4）。为了解决这一问题，我们引入了以下新技术：\n头合并（Head-merging）（§ 5.3）：通过将多个头合并为一个，我们减少了MPC中需要的注意力头的数量，同时，也按比例增加了每个头的维度，以保留所有参数。虽然计算量（FLOPs）看似没有变化，但我们证明了头合并实际上在MPC中匹配了头修剪的性能（§ 6.4）。这是基于这样一个关键观察：在MPC中，成为瓶颈的自注意力操作仅与头的数量有关，而与头的维度无关。我们的实验表明，如果头合并得当，头合并比头修剪在任务性能上表现得更好（§ 6.4）。 MARILL的端到端工作流程如图1所示。与标准微调相比，MARILL生成的模型在各种MPC设置的最先进安全推理框架中，运行时间提高了3.6至11.3倍，通信开销减少了2.4至6.9倍（§ 6.1）。我们在三种不同类型的任务上评估了MARILL的机器学习性能，即代码生成[8]、聊天机器人[82]和机器翻译[40]。在这些基准测试中，我们展示了MARILL通常能保持超过90%的标准微调性能（§ 6.2）。\n相关工作 安全推理协议 在这项工作中，我们重点关注基于MPC（多方计算）的神经网络安全推理协议，该领域始于SecureML的开创性工作[56]。SecureML考虑了一个仅涉及服务提供者和客户的二方设置，并且在这一设置中经过许多后续工作[56, 37, 47, 54, 61, 60, 80, 31, 4, 27, 29, 50, 58]后，性能有了显著的提升。尽管如此，二方计算（2PC）仍然带来了非常大的开销。因此，随后的工作考虑了引入一个额外的帮助方的其他设置，如具有诚实多数的三方计算（3PC）[70, 41, 63, 55, 71, 16]和带有信任经销商的二方计算（2PC-Dealer）[39, 26, 32, 25]。其他一些工作则通过利用GPU加速[39, 68, 74, 32, 25]来加速安全推理协议。最近的工作[27, 29, 50, 58, 16, 73, 25, 6]在所有这些设置中，集中于安全的变压器推理，因为它们代表了当前AI工作负载的主流。我们的工作与这些协议是正交的，可以与它们中的任何一个一起加速安全推理（附录F）。\nMPC友好的近似 一些工作[45, 56, 21, 19, 14, 9, 54, 51, 33, 59, 13, 12, 49, 42, 81]提出了针对非线性激活函数（如softmax和GeLU）的近似实现，使其更适合MPC。这些近似通常会导致模型性能的大幅下降。MPCFormer[45]提出了一种两阶段蒸馏过程来弥补这一差距。大多数这些工作[54, 33, 20, 59, 13, 12, 42, 49, 81]还使用了神经架构搜索（NAS），根据所需的精度级别，在同一网络中采用多个近似。\n我们的工作与这些近似方法是互补的，因为我们对架构进行高层次的修改，而不是底层操作。我们在§ 6.3中展示了，这些近似方法可以与MARILL结合，以进一步提升性能。此外，MARILL与这些工作的两个关键区别在于：\n尽管这些工作输出的模型中所有权重都是私有的，但MARILL生成的模型混合了公共和私有权重； 基于NAS的工作中的模型架构依赖于私有训练数据，并泄露额外的信息，而MARILL是静态配置的，与训练数据无关。 Nimbus: Secure and Efficient Two-Party Inference for Transformers Nimbus：Transformers 安全高效的两方推理\n摘要 Transformer模型因其在机器学习任务中的强大能力而受到广泛关注。它们的广泛应用引发了关于推理过程中敏感信息泄露的潜在风险。然而，在应用于Transformer时，现有的基于安全双方计算（2PC）的方法在效率上存在两方面的限制：（1）线性层中的资源密集型矩阵乘法，和（2）复杂的非线性激活函数，如GELU和Softmax。本文提出了一种新的双方推理框架Nimbus，专为Transformer模型设计。在线性层方面，我们提出了一种新的2PC范式，并结合一种编码方法，基于外积思想安全地计算矩阵乘法，相比现有的最先进协议（SOTA），该方法在性能上提高了2.9×至12.5×。在非线性层方面，通过对输入分布的新观察，我们提出了一种低阶多项式近似方法，用于GELU和Softmax，这一方法相较于最先进的多项式近似提高了2.9×至4.0×，而与没有隐私保护的非2PC推理相比，我们方法的平均准确度损失为0.08%。与最先进的双方推理方法相比，Nimbus在不同网络设置下提高了BERTbase推理的端到端性能2.7×至4.7×。\n引言 Transformer模型[36]在语言理解[19]、视觉任务[6]和聊天机器人[21]等多个机器学习任务中带来了显著的进展。随着Transformer模型处理越来越敏感的数据和任务，隐私问题成为部署中的一个重要关注点。例如，一家医院（客户端）希望使用另一家组织（服务器）的模型来增强其诊断能力。这对双方提出了隐私问题：医院要么必须上传其私密数据，要么组织需要将其专有模型发送给医院。\n近期，基于安全的两方计算（2PC）[14, 26, 16, 29]的几项工作实现了隐私保护的安全两方推理。这些工作通过让客户端和服务器共同执行“加密”输入和模型的推理，使用包括同态加密（HE）[7]、加法秘密共享等密码学技术。客户端对模型一无所知，只知道推理结果，并且客户端的输入对服务器保密。\n隐私保护伴随着大量的计算和通信开销，这是由于昂贵的密码学操作造成的。尽管现有的卷积神经网络的安全两方推理可以在几分钟内完成[8, 23, 18, 2, 34, 32, 17]，但是在Transformer模型上实现安全推理却更具挑战性，因为线性层中大量的矩阵乘法和复杂的非线性层。为了摊销线性层中HE的开销，许多工作[17, 14, 26]采用了窗口编码来模拟内积。然而，这种编码方法带来了HE密文的稀疏格式，导致冗余的通信和计算。非线性层的效率瓶颈在于如何安全地计算GELU和指数函数（包括Softmax）。之前的工作[5, 26]使用分段多项式来逼近这两个非线性函数。然而，为了保持精度，使用了高阶多项式和较大的定点精度，这导致了较大的通信开销和多轮计算。\n本文提出了一种新的安全两方推理框架Nimbus，旨在解决上述效率瓶颈。具体来说，我们的贡献总结如下：\n• 我们提出了一种客户端外积（COP）协议，以促进线性层的计算。我们的COP协议包含了两个关键创新。首先，模型权重的静态特性使得服务器可以在设置阶段将加密权重发送给客户端，从而消除了在线阶段的输入通信。其次，去除输入通信使得我们能够设计一种新颖的行级编码方案，通过外积实现同态矩阵乘法。这种编码进一步提高了同态矩阵乘法的效率，并为通信生成了紧凑的输出密文。\n• 对于非线性层，我们提出了一个新的观察结果，即它们的输入分布呈现规律性。与之前假设均匀输入分布的逼近方法不同，我们的方法减少了分配给少见输入值的逼近预算。这使得我们能够使用低阶多项式和较少的片段来逼近非线性函数。此外，低阶多项式对定点误差的敏感性较低，使我们能够采用更小的环以提高效率。我们还提出了一种新协议，能够实现小环和大环之间的自由转换。因此，我们的方法在非线性层上取得了更好的性能，同时仅产生了平均0.08%的准确性损失。\n• 我们使用流行的Transformer模型BERTbase在局域网（LAN）和广域网（WAN）设置下评估了Nimbus的性能。与最先进的工作BumbleBee[26]相比，我们在安全计算矩阵乘法（分别是GELU和Softmax）方面提高了2.9倍至12.5倍（分别是2.9倍至4.0倍）。结合所有优化，我们将安全两方推理的端到端性能提高了2.7倍至5.9倍，并减少了60%的通信成本。代码可在以下链接获取：https://github.com/secretflow/spu。\nNo Free Lunch Theorem for Privacy-Preserving LLM Inference 摘要 个人和企业在许多方面从大型语言模型（LLMs）中受益匪浅，包括PaLM、Gemini和ChatGPT等。例如，LLMs提高了生产力、降低了成本，并使我们能够专注于更有价值的任务。此外，LLMs具备了处理大量数据集、发现潜在模式，并提供推动技术和科学前沿的关键见解的能力。然而，LLMs也带来了隐私问题。用户与LLMs的互动可能会暴露他们的敏感个人或公司信息。缺乏强有力的隐私保障和法律框架可能会导致不当的数据处理或无理的侵犯，进而可能引发隐私侵犯和个人身份盗窃的风险。为了确保隐私，必须最小化共享提示与私密信息之间的依赖关系。虽然已经提出了各种随机化方法来保护提示的隐私，但与未保护的LLMs提示相比，这些方法可能会导致效用损失。因此，在实施有效保护机制时，必须评估隐私泄露风险与效用损失之间的平衡。本研究开发了一个推理隐私保护的大型语言模型（LLMs）的框架，并为研究隐私保护与效用之间的相互关系奠定了坚实的理论基础。核心观点通过一个定理加以总结，该定理被称为NFL（No-Free-Lunch）定理。\n引言 复杂的大型语言模型的出现，包括PaLM [1]和ChatGPT [2]，为个人和企业带来了巨大的好处。这些模型能够帮助我们在从信息合成到生成新内容和数据分析等多种领域的工作中取得进展。通过这样做，它们提高了我们的生产力、降低了成本，并使我们摆脱了繁琐的工作，能够专注于更有价值的任务[3]。此外，LLMs还可以帮助生成创意、设计解决方案以及促进研究与开发。例如，在医疗保健、金融和科学等领域，LLMs能够分析大量数据，识别模式，并提供有价值的见解，推动技术和科学的突破与进展。尽管LLMs带来了巨大的好处，但它们也引发了隐私问题。当用户与LLMs互动时，他们的查询可能涉及机密的个人或公司信息，这些信息会暴露给LLMs的提供方。如果用户的查询没有得到充分保护，而LLM供应商也没有得到有效监管，用户数据可能会被滥用或被未经授权的人访问，从而导致身份盗窃或其他隐私侵犯。尤其考虑到共享给AI聊天机器人或代理的敏感信息（如财务详情或健康相关讨论），这一点更加令人担忧。因此，在对LLMs进行推理时，保护隐私显得尤为重要。\n本研究聚焦于一种典型的LLM推理设置，其中LLM是黑箱的，其架构、模型参数和推理细节被隐藏在商业API和用户界面中。基于此，保护提示隐私的一种自然方法是对提示进行随机化。为解决这一问题，已有若干随机化技术[4, 5, 6]被提出，并在减少由暴露提示引发的隐私泄露方面显示了实证效果。然而，与未加保护的LLM提示相比，这些方法不可避免地会带来一定程度的效用损失。此外，现有研究缺乏必要的理论分析来量化效用损失和隐私泄露之间的折衷。减少私人信息暴露与效用损失之间的矛盾促使我们探索一个研究问题：“从理论上讲，是否有可能开发出某些保护方法，在提示LLM时既能最小化私人信息的暴露，又能尽量减少效用损失？”我们在提出的隐私保护LLM推理框架中的定理4.4中总结了这一主要发现，提出了对这一问题的反论证。我们将其命名为“无免费午餐定理”（No-Free-Lunch Theorem）。\n另一项相关研究[7]也提出了一个定理，试图解决效用与隐私之间的平衡。然而，该研究的范围主要集中在横向联邦学习中的模型训练，其中隐私泄露是基于在客户端和服务器之间传输的模型梯度来定义的。相比之下，我们的工作集中于LLM推理，其中涉及提示和LLM输出的交互发生在客户端和服务器之间，如第3节所述，我们将隐私泄露定义为对手从受保护的输入中推断原始输入的程度。这些不同的隐私泄露定义为分析效用与私人信息之间的平衡提供了新的理论视角。\n本研究的贡献总结如下： • 提出了一个旨在保护隐私的LLM推理框架，其中我们为隐私泄露（在定义3.1中概述）和效用损失（在定义3.2中概述）提供了正式定义。此外，我们将隐私保护的LLM推理问题形式化为一个约束优化问题。 • 提出了一个针对隐私保护LLM推理的“无免费午餐定理”（如定理4.4所述），该定理提供了效用损失和隐私暴露之间折衷的定量特征。具体来说，它表明，效用减少和隐私暴露的加权总和有一个下界，该下界是特定问题所决定的常数，并且不为零，这意味着为了确保所需的隐私保护水平，必须牺牲一定程度的模型性能。\n相关工作 在大语言模型（LLM）推理过程中，客户端和LLM服务器都有可能作为对手，危及彼此的隐私[8]。本研究关注一种情境，其中LLM服务器充当对手，通过分析客户端提供的提示推断出客户端的私人信息（见第3.1节）。本节简要回顾了现有关于隐私攻击、保护措施以及在大语言模型（LLM）推理领域中平衡保护隐私与保持效用的研究。\nLLM推理中的隐私攻击 在LLM推理阶段，LLM服务器可能通过分析客户端发送的提示来推断客户端的私人信息。这可以通过从单一提示中推断隐私信息，或通过操控本无害的对话来诱导用户提供包含私人和敏感信息的提示来实现[9]。在此类情境中，已经概述了多种攻击策略。例如，服务器具备发起反转嵌入攻击的能力[10, 11, 12, 13]，旨在根据提供的嵌入重建原始提示。此外，服务器还可以利用这些攻击提取敏感信息，如种族、性别和年龄等[14, 15, 16, 17, 18]。此外，托管LLM的服务器可以利用其模型从修改后的版本中辨别客户端的原始提示[5, 9]。\nLLM推理中的隐私保护 安全多方计算（SMPC）和随机化是两种主流的保护机制，用于保护客户端的敏感信息隐私。SMPC通过多方协作计算一个函数，确保每个方的输入保持机密。在LLM的背景下，SMPC可以用于加密数据上的计算，从而确保没有单一方能够访问其他方的机密输入。这些方法的重点在于提升LLM架构和SMPC协议的效率，以最小化SMPC协议所带来的大量计算和通信开销[19, 20, 21, 22, 23, 24, 25]。尽管SMPC能够确保客户端数据的机密性，但它要求LLM与客户端紧密协作，这可能限制其应用于仅通过商业API访问的LLM。随机化是另一种隐私保护机制，它通过在提示的嵌入中加入随机噪声来保护提示的隐私。例如，InferDPT方法[5]利用差分隐私（DP）机制改变用户的输入文本，从而阻止恶意LLM服务器通过推断敏感用户信息进行窃听。在InferDPT系统的背景下，用户首先对输入中的词汇进行微小但语义一致的变化。然后，将此修改后的输入转发给LLM，LLM生成回复并将其反馈给用户。用户随后利用自己的预训练模型，通过结合初始输入和LLM的回复，产生最终输出。DP-OPT技术[26]则利用先进的深度语言网络（如DLN [27]所描述）在本地模型的指导下，自主优化输入提示。在这一过程中，DP-OPT实现了一种隐私保护聚合技术，以确保提示的机密性。在另一项研究中，Staab等人[9]探讨了文本匿名化技术在保护用户数据隐私中的应用，并通过实证证据得出结论，单独采用这些方法不足以确保强有力的隐私保护。\nLLM推理中效用与隐私的权衡 此外，除了引入保护个人数据的方法外，还有一些研究关注了在与LLM交互时效用与私人信息之间的平衡。这些研究主要集中在LLM推理的两种情境。第一种情境中，LLM运行在商业API后面，仅依赖文本提示作为输入。保护机制通常包括在提示或其对应嵌入中引入随机噪声。该情境中的代表性研究包括DP-OPT[26]和InferDPT[5]。这些研究表明，随着隐私预算的减少，任务性能会下降。此外，DP-OPT还指出，利用更大的LLM可以显著缓解性能和隐私之间的权衡。在第二种情境中，LLM被分为两部分：较大的一部分部署在服务器上，较小的一部分部署在客户端。服务器和客户端协作训练这两部分LLM，在嵌入向量中加入随机噪声以保护隐私。该情境下的代表性研究包括TextObfuscator[4]和SAP[28]。这两项研究也展示了隐私与效用之间的微妙平衡。具体来说，随着随机化的增加，隐私增强，效用降低，反之亦然。这些研究的主要重点在于实证评估保密性与性能之间的平衡，而缺乏理论框架或LLM推理过程中效用与隐私平衡的数值评估。我们的研究旨在填补这一空白，通过提供理论分析和量化评估。\nOn Protecting the Data Privacy of Large Language Models (LLMs): A Survey 关于保护大语言模型（LLMs）数据隐私的调查\n摘要 大语言模型（LLMs）是复杂的人工智能系统，能够理解、生成和翻译人类语言。它们通过分析大量文本数据学习语言模式，从而能够执行写作、对话、摘要等语言任务。在处理和生成大量数据时，LLMs可能会泄露敏感信息，从而威胁到数据隐私。本文重点阐述与LLMs相关的数据隐私问题，以促进对这一问题的全面理解。具体而言，本文对数据隐私威胁进行了深入研究，涵盖了LLMs中的被动隐私泄露和主动隐私攻击两个方面。随后，我们评估了LLMs在不同阶段采用的隐私保护机制，并对其效果和局限性进行了详细审查。最后，讨论了在LLM隐私保护领域面临的挑战，并概述了未来可能的研究方向。\n引言 近年来，大语言模型（LLMs）已成为人工智能领域的关键角色，正在革新自然语言处理[1]、具身AI[3]–[5]、人工智能生成内容（AIGC）[6]、[7]等多个领域。LLMs通过在海量数据集上进行训练，具备了生成类人文本、回答复杂问题以及执行各种语言相关任务的非凡能力，且其准确性和流畅性前所未有。然而，在LLMs强大能力的背后，数据隐私问题引起了越来越多的关注[8]。一方面，LLMs可能面临被动隐私泄露的风险。用户若在聊天界面输入敏感数据，可能会无意间暴露这些信息给ChatGPT。例如，三星电子曾在三次不同的事件中，通过ChatGPT无意间泄露了公司敏感数据。此外，LLMs通常依赖大量数据进行训练，这些数据包括从互联网抓取的文本、公开可用的数据集或专有来源。这一数据聚合过程可能会引发重大的数据隐私问题，特别是在处理敏感信息或个人可识别信息（PII）时[9]。已有研究表明，LLMs可能会记忆训练数据，这引发了关于在推理过程中无意间泄露敏感信息的担忧[10]。即使采用差分隐私或联邦学习等技术来缓解训练过程中的隐私风险，模型参数中仍可能残留敏感数据的痕迹[11]。另一方面，LLMs也可能面临主动隐私攻击。将经过微调的LLMs部署到各种应用中，带来了额外的安全挑战。对预训练LLMs进行微调或适配特定任务，可能无意中暴露其漏洞，使其易于受到攻击，进而危及敏感信息的机密性、完整性或可用性[12]。例如，为了绕过模型固有的对齐机制，研究人员设计了一种提示策略，诱导GPT-3.5-turbo“偏离”产生常规响应，转而输出训练数据[13]。如后门攻击、成员推断攻击和模型反演攻击等已存在的漏洞，可以对预训练或微调后的模型进行利用，目的是非法获取敏感数据。为了展示当前的研究现状，我们在图1中概述了关于LLMs隐私保护的研究现状。考虑到关于隐私保护的学术论文和Hugging Face的模型列表，我们编制了该图中的流行LLMs列表。时间轴代表模型发布的日期，纵轴表示模型参数的大小。蓝色数据点表示文献中关于隐私保护关注较少的LLMs，而黑色数据点表示在隐私保护方面进行了研究的模型。目前，学术界对LLMs数据隐私的关注主要集中在相对较小的、知名的模型上，如2020年前版本的GPT-2[14]和BERT[15]系列。相比之下，近年来发布的具有更大参数规模的LLMs由于部分模型未公开发布，且隐私保护技术滞后于LLMs的快速发展，尚未得到充分审视。\n在本文中，我们深入探讨了大语言模型（LLMs）中的数据隐私问题，特别是从隐私泄露和隐私攻击两个方面研究潜在的隐私威胁。此外，我们还研究了相应的对策，通过全面回顾LLMs开发的三个主要阶段：预训练、微调和推理，提出了相应的隐私保护措施。我们的贡献总结如下： • 我们对学术文献中关于LLMs隐私威胁进行了全面调查，并将其分为两大类：隐私泄露和隐私攻击。 • 我们的研究包括对应用于LLMs的隐私保护方法的分析，并根据开发阶段将其分类。我们根据其所在阶段将隐私保护方法分为三组：预训练、微调和推理。在每个类别中，我们介绍了高层次的技术，解释了它们在LLMs中的应用，并提供了详细的文献回顾。我们调研的目标是为LLM开发者提供实施前沿技术的指导，以保护LLMs的隐私。\n相关工作 在本节中，我们首先介绍了现有关于LLMs开发和评估的调查研究。然后，进一步阐述了与LLMs中的隐私和安全问题最相关的工作，最后总结了我们调查的研究。\nA. LLM评估的调查 目前，一些研究已经对LLMs的开发和评估进行了调查。这些研究通常涵盖了LLMs的架构改进（如GPT系列、BERT、Transformers [16]–[22]）。例如，Li等人[16]聚焦于将LLM与智能个人助理（IPA）集成，以提高个人助手的能力。该研究深入探讨了这些代理的架构、能力、效率和安全性。Zhao等人[18]关注LLMs的四个关键方面：预训练、适配调优、利用和能力评估。它提供了LLMs的全面背景，包括术语和技术。Naveed等人[21]对LLMs进行了广泛的分析，涵盖了它们的架构、训练、应用和挑战。该研究详细探讨了LLMs的各个方面，如预训练、微调和评估，同时还讨论了LLMs在不同领域中的各种应用。Hadi等人[22]提供了LLMs的详细概述，讨论了它们的历史、训练和在医学、教育、金融和工程等领域的应用。该研究考察了LLMs的技术方面、挑战和未来潜力，包括伦理考虑和计算需求。为了理解LLMs在各种应用中的能力和局限性，一些研究进行了关于这些LLMs的全面测量[17]、[23]、[24]。Chang等人[17]对评估LLMs的方法和标准进行了全面分析，讨论了评估任务、数据集、基准和评估技术等各个方面。Guo等人[23]强调了在多个维度上全面评估LLMs的必要性，如知识和能力评估、对齐评估、安全考虑和在专业领域的应用。在[24]中，Liu等人研究了LLMs与人类价值观和社会规范的对齐情况，提出了一种详细的分类法，用于在多个维度上评估LLM的可信度，如可靠性、安全性、公平性、抵抗滥用能力、可解释性、遵守社会规范和鲁棒性。\nB. LLM安全性和隐私的调查 由于LLMs的训练依赖于大量的数据，这些数据通常包括敏感信息，因此，LLMs面临着处理隐私和安全问题的挑战[8]，[25]–[32]。Yao等人[8]全面研究了LLMs的安全性和隐私问题，并从三个方面进行了广泛的文献综述：有益的安全应用（如漏洞检测、安全代码生成）、负面影响（例如，网络钓鱼攻击、社交工程）和漏洞（例如，越狱攻击、提示攻击），以及相应的防御措施。Li等人[25]深入探讨了LLMs中的隐私问题，分类了隐私攻击，并详细介绍了防御策略。该研究还探讨了增强LLMs隐私的未来研究方向。Neel等人[26]研究了与LLMs相关的隐私风险，关注诸如敏感数据记忆和各种隐私攻击等问题。它回顾了减缓技术，并强调了LLMs隐私研究的当前状态。然而，他们主要关注通过红队测试模型来突出隐私攻击。Marshall等人[27]和Al-Hawawreh等人[28]探讨了ChatGPT在网络安全领域的作用。它们的讨论强调了其在现实世界中的应用，如增强代码安全性和检测恶意软件。Qammar等人[29]提供了关于聊天机器人发展到ChatGPT的广泛概述，并讨论了它们在网络安全中的作用，突出展示了漏洞和潜在攻击。然而，可能在特定的网络安全解决方案和针对已识别漏洞及攻击的预防措施方面缺乏深度。Schwinn等人[30]全面分析了LLMs中的旧威胁和新威胁，提供了对不断发展的对抗性攻击和防御的见解。但由于其对广泛威胁的关注，可能忽视了对特定攻击方法或防御机制的深入细节。Derner等人[31]研究了与ChatGPT相关的特定安全风险，为更好地理解其漏洞做出了贡献。然而，它可能没有与其他模型或系统进行全面比较，限制了其对ChatGPT的研究范围。Shayegani等人[32]彻底检查了对抗性攻击暴露的LLMs漏洞，提供了对未来模型改进的宝贵见解。然而，重点放在对抗性攻击上，可能导致对其他类型的漏洞或更广泛的安全问题关注较少。与现有的调查相比，我们的研究专注于解决LLMs中的数据隐私问题，提供了关于隐私威胁和隐私保护技术的全面文献回顾。我们深入研究了在不同阶段为减轻隐私威胁所采用的对策，并对LLMs数据隐私中的当前挑战和未来研究方向进行了深入讨论，旨在为该领域提供指导和参考。\nPrivacy in Large Language Models: Attacks, Defenses and Future Directions 随着深度学习和Transformer模型的进步，大语言模型（LLMs）显著提高了处理各种下游自然语言处理（NLP）任务的能力，并将这些任务统一到生成式管道中。一方面，强大的语言模型通过在海量文本数据上进行训练，为模型和用户带来了前所未有的可访问性和可用性。这些LLMs大大降低了应用开发者和用户的门槛，因为它们提供了预训练的语言理解和指令跟随能力。强大的LLM的可用性为各个领域开辟了新的可能性，包括LLM驱动的代理、虚拟助手、聊天机器人等。另一方面，开放访问这些模型也可能引发潜在的恶意和非故意的隐私风险。使这些模型成为有价值工具的能力，同样也可能被恶意利用或无意间泄露敏感信息。尽管目前有不断的努力来解决与LLM相关的安全和隐私问题，但这一问题仍未得到有效解决。\n本文旨在深入探讨当前针对LLM的隐私攻击，并根据假设的对手能力对其进行分类，以揭示LLM中存在的潜在漏洞。接着，我们将探讨为减轻这些隐私攻击风险而制定的主要防御策略。除了讨论现有的研究成果，我们还将关注随着LLM不断发展，可能出现的隐私问题。最后，我们通过强调未来研究和探索的几条有前景的方向来总结本文。通过识别这些研究方向，我们旨在激发隐私保护领域的进一步进展，并为这些强大LLM的安全与隐私意识发展做出贡献。通过这项综述，我们希望为LLM中存在的潜在漏洞提供宝贵的见解，从而强调在其开发和应用中解决隐私问题的重要性。\n引言 动机 随着自然语言处理领域深度Transformer模型的发展，预训练语言模型（LMs）标志着自然语言处理及整个社会变革时代的到来。目前，生成式大语言模型（LLMs）通过将各种自然语言处理任务结合成一个综合的文本生成框架，展现了卓越的能力。这些模型，如OpenAI的GPT-4、Anthropic的Claude 2和Meta的Llama 2，在理解和生成自然语言方面在近年来取得了重大突破。因此，这些LLMs在既定任务和实际挑战上均达到了前所未有的表现[19, 26, 49, 118, 187, 188, 211]。除了在各种应用中生成连贯且具有上下文相关性的文本外，LLMs还可以自动化许多语言相关的任务，使其成为开发者和终端用户不可或缺的工具。此外，LLMs具备广泛概括能力，可以处理大量未见过的文本语料。通过适当的指令（提示）和示范，LLMs甚至可以适应特定的上下文或应对新任务，而无需进一步调优[38, 124, 218, 271, 323]。因此，将LLMs集成到各种应用中的趋势日益明显，从科学研究到智能助手，都在广泛应用。\n除了增强的性能外，语言模型的训练数据规模也随着模型的增大而扩大。这些LLMs不仅在特定任务上对标注文本数据进行训练，还从互联网上获取大量的公共文本数据。与精心挑选的标注数据不同，从互联网上提取的自由形式文本往往数据质量较差，且可能无意中泄露个人信息。例如，与模型的简单互动可能会导致个人身份信息（PII）的意外传播[24, 106, 137, 166, 256, 328]。不幸的是，在没有相关个人知情或同意的情况下，不经意暴露PII可能导致违反现有隐私法律，如欧盟的《通用数据保护条例》（GDPR）和加利福尼亚州消费者隐私法案（CCPA）。此外，将多种应用集成到LLMs中已成为一种日益增长的趋势，旨在增强其知识基础能力。这些集成使得LLMs能够有效解决数学问题（如ChatGPT + Wolfram Alpha）、读取格式化文件（如ChatPDF），并通过搜索引擎（如New Bing）回答用户查询。当LLMs与像搜索引擎这样的外部工具结合时，新的特定领域的隐私和安全漏洞也会出现。例如，Li等人[137]报告指出，恶意攻击者可能会利用New Bing，将受害者的PII与部分信息关联起来。因此，目前LLMs中隐私泄露的程度仍然不确定。\n范围与目标 本调查研究了大语言模型（LLMs）在隐私攻击和防御方面的最新进展。与几篇近期关于LLMs隐私的调查论文[18, 43, 113, 183]相比，我们的工作提供了更全面和系统的分析。我们不仅涵盖了先前调查的内容，还整合了大语言模型领域的最新进展，确保我们的分析是最新的并且具有相关性。此外，我们还探讨了新兴的隐私保护技术和策略，如差分隐私[67]、基于密码学的方法[181]、反学习和联邦学习[125, 140, 284]。通过评估这些防御机制，我们旨在提供关于它们有效性和局限性的有价值见解。最后，在分析了攻击和防御之后，我们讨论了未来尚未研究的隐私漏洞以及可能的解决方案。\nPrivacy Preserving Prompt Engineering: A Survey 隐私保护提示工程：一项综述\n摘要 预训练语言模型（PLMs）在解决广泛的自然语言处理（NLP）任务方面展现了显著的能力。研究人员发现，这些模型的性能与其规模之间存在直接的相关性。因此，近年来这些模型的规模显著扩大，促使研究人员采用“大语言模型”（LLMs）一词来描述较大规模的PLMs。规模扩展带来了一个独特的能力——上下文学习（ICL），这是一种特殊形式的提示方法，通过展示示例来使用模型，而无需修改模型参数。尽管这一特性非常有趣，但隐私问题已成为其广泛应用的主要障碍。多项研究已经考察了与ICL和提示技术相关的隐私风险，并提出了缓解这些风险的技术。因此，有必要整理这些缓解技术，以便为社区提供帮助。本综述系统地概述了在ICL和提示过程中采用的隐私保护方法。我们回顾、分析并比较了这一范式下的不同方法。此外，我们总结了可用于开发这些框架的资源。最后，我们讨论了这些框架的局限性，并详细探讨了需要进一步研究的有前景的领域。\n引言 近年来，预训练语言模型（PLMs）的进展在各种自然语言处理（NLP）任务中展现了显著的能力，如文本分类、问答、情感分析、信息检索、摘要等[1]–[4]。其中，许多模型近期被推出，并持续获得显著的关注。例如，OpenAI的ChatGPT[5]用户数量已超过1.8亿[6]。其他常见的先进模型包括Meta的LLaMA[7]、OPT[8]、OPT-IML[9]、BigScience的BLOOM[10]、BLOOMZ[11]、Databricks的Dolly[12]等。通常，这些模型非常庞大，参数量达到数百亿，需要大量的计算资源用于训练和存储。术语“大语言模型”（LLMs）用于描述这些大规模的PLMs[13]–[15]。此外，这些模型主要通过使用来自网络、书籍、维基百科等多种开放文本资源进行预训练。本文中，我们将交替使用PLMs和LLMs这两个术语。\n虽然通用型LLMs在理解和解决一般NLP任务方面表现出了优异的能力，但在处理领域特定任务和适应特定目标时，它们有时需要更深入和更精细的处理。为使模型适应领域特定的下游任务并调整其目标，已提出了调整可学习模型参数的微调技术[16]–[18]。然而，模型微调过程中常常伴随高计算资源需求、过拟合风险、灾难性遗忘以及模型稳定性等挑战[19]。因此，在进行微调时需要谨慎。其他模型适应技术包括指令微调、提示微调、对齐微调等[20]。\nLLMs的新兴能力之一是提示（Prompting）。通过提示，LLM可以在提供自然语言指令和/或示范示例时生成预期的输出，而无需更新模型参数。最简单的提示类型是直接提示（也称为零-shot），用户通过提问的方式给出指令，并且不提供任何示例。上下文学习（ICL）是另一种提示形式，随着GPT-3的推出而提出[1]，它在提示中包括一些示范示例。ICL作为一种高效的方式，能够利用预训练或已适应的LLMs解决各种下游任务，而无需针对每个任务修改模型参数。然而，前述LLM使用技术不可避免地涉及可能被视为私人且包含敏感信息的数据。例如，考虑使用ICL预测一个人年收入是否至少为50,000美元。为了帮助LLM形成上下文并做出更好的预测，它可能会收到一个包含敏感信息的示范示例，如年龄、薪水和社会安全号码（SSN）。这种敏感信息可能会被不可信的LLM服务器或能够绕过LLM服务提供商API的对手访问。此外，由于Redis客户端开源库中的漏洞[21]，ChatGPT泄露了用户的聊天记录。\n尽管隐私挑战，如训练或微调数据的记忆化及其通过模型反演和成员推断攻击的恢复[22]–[24]，已被注意到[25]，但这些挑战与ICL所带来的隐私问题本质上不同。因此，专门解决与ICL相关的隐私挑战以及一般提示的隐私问题，已成为迫切的任务。\nRhombus: Fast Homomorphic Matrix-Vector Multiplication for Secure Two-Party Inference Rhombus：用于安全双方推理的快速同态矩阵-向量乘法\n摘要 我们提出了Rhombus，一个新的安全矩阵-向量乘法（MVM）协议，适用于半诚实的双方设置，能够无缝集成到现有的隐私保护机器学习（PPML）框架中，并作为线性层中安全计算的基础。Rhombus采用基于RLWE的同态加密（HE）与系数编码，允许消息从不仅仅是域 FpF_p 中选择，也可以从环 Z2lZ_{2^l} 中选择，后者支持在非线性层中进行更快速的计算。为了实现更好的效率，我们开发了一种输入输出打包技术，减少了由于HE与系数编码引起的通信成本约21倍，并提出了一种分割点选择技术，将旋转次数减少到矩阵维度的子线性级别。与Balla和Koushanfar（CCS'23）提出的最新协议HELiKs相比，我们的实现证明Rhombus使得整个MVM协议的性能提高了7.4倍至8倍，并将ResNet50的安全双方推理的端到端性能提升了4.6倍至18倍。\n引言 机器学习（ML）已促成了众多应用的实现。特别是，模型所有者可以为用户提供推理服务，而用户无需具备训练模型的能力，即所谓的机器学习即服务（MLaaS）。用户能够从机器学习服务中获得价值，同时模型所有者也能有效地将其服务货币化。然而，MLaaS带来了一个关于数据隐私的挑战性问题，即模型所有者可能会了解到用户的私人数据，或者用户可能会泄露模型。隐私保护机器学习（PPML）通过提供安全的双方推理解决方案，来应对这一挑战性问题（即两方可以协作进行安全推理，而无需揭示数据隐私）。同态加密（HE）是一种强大的加密原语，它使得模型所有者能够对加密的用户数据进行复杂计算。尽管已经做了很多努力[KKK+22, SFK+22, LKL+22]以提高基于HE的计算效率，但由于模型所有者必须调用非常计算密集的引导操作，仍然无法在加密数据上执行所有推理计算。因此，许多协议（例如[MZ17, LJLA17, JVC18, SGRP19, ASKG19, CGR+19, DEK19, RSC+19, MLS+20, RRK+20, KRC+, NC21, RRG+21, PSSY21, HJSK21, CZW+21, HLHD22, RBS+22, SDF+22, HLC+22, RBG+23, HLL+23, GJM+23, BK23]）已经被开发出来，以提高在结合HE和安全多方计算（MPC）技术的混合框架下执行PPML的效率。\n一个ML模型由一系列线性和非线性层组成。具体来说，线性层中的操作，例如3D卷积和全连接（FC），通常可以建模为矩阵-向量乘法（MVM），这些操作可以通过基于环学习误差（RLWE）的HE实现[ LPR10]，如先前的工作[ MZ17, JVC18, MLS+20, RRK+20, CZW+21, HLHD22]所述，而非线性层中的计算则通过基于秘密共享的MPC执行。因此，在这个混合HE-MPC框架下，每次使用HE执行MVM时，密文结果需要转换为秘密共享，以便进行非线性层中的后续计算，因此，乘法深度仅为1，且不需要引导操作。图1展示了一个基于HE的两方MVM范式，其中模型所有者（Bob）持有矩阵 W，而用户（Alice）持有向量 v，协议输出一个关于 $W \\cdot v$ 的加法秘密共享向量(r1, r0)，使得 $r0 + r1 = W \\cdot v$。\nRLWE（环学习误差）基础的HE实现MVM协议有两种编码方法：（1）NTT编码，将数论变换（NTT）应用于原始数据，然后将其映射到明文多项式的系数上；（2）系数编码，直接将原始数据映射到明文多项式的系数上。大多数先前的工作[ JVC18, RRK+20, ZXW21, BK23, PZM+23] 设计了基于HE的MVM协议，采用了NTT编码。然而，NTT编码有三个缺点。首先，与系数编码相比，NTT编码的代价更高。其次，NTT编码仅限于在域 $F_p$ 上操作。如[RRK+20]所示，在 $F_p$ 上，基于OT（盲传输）的非线性层协议的通信成本大约比在环 $Z_{2^l}$ 上高1.5倍，其中 $p \\approx 2^l$。第三，当将同态计算结果转换为加法秘密共享时，必须选择更大的HE参数，以保护在同态操作中使用的秘密值的隐私。最近的PPML协议Cheetah[HLHD22]采用系数编码来克服这些缺点，代价是引入了一个一位错误到计算结果中，但这对推理准确性没有影响，因为结果始终会被截断。然而，Cheetah将MVM或3D卷积结果编码为多个密文（每个密文仅编码结果的几个元素），这会带来较大的通信开销。特别地，点卷积（1×1滤波器大小）会给Cheetah带来较大的通信开销，占所有卷积安全推理中80%的开销，尤其是在ResNet50[HZRS15]这一流行深度神经网络（DNN）的推理中。这导致Cheetah在线性层的通信开销大于使用NTT编码的HE的最新PPML协议[ BK23]。如上所分析，我们在本工作中关注系数编码。在明文中，MVM可以视为矩阵每行与向量之间的内积。每个内积的计算通过将矩阵行和向量的元素分别放入两个独立的多项式中，然后进行相乘来实现。基于此，一种简单的做法浮现出来：Alice将她的向量 v 依次编码为一个多项式的系数，进行加密后，将密文发送给Bob。在接收到密文后，Bob将矩阵的每一行编码为明文多项式的系数，顺序上有些反转，然后将加密的向量与每一行编码后的数据相乘。结果，Bob为一个 $r \\times c$ 矩阵生成 r 个加密多项式。为了实现 O(1) 的通信复杂度，Bob通过调用打包技术（即PackLWEs[CDKS21]）将 r 个密文打包成一个密文，但代价是需要进行 r - 1 次同态旋转。总之，简单的做法需要 O(r) 次明文-密文乘法和旋转。\nSecFormer: Fast and Accurate Privacy-Preserving Inference for Transformer Models via SMPC SecFormer：通过SMPC实现快速准确的隐私保护推理用于Transformer模型\n摘要 随着Transformer模型在云平台上提供推理服务的广泛应用，隐私问题日益严峻，特别是涉及敏感数据如投资计划和银行账户详情。安全多方计算（SMPC）作为保护推理数据和模型参数隐私的有前景的解决方案应运而生。然而，SMPC在Transformer模型的隐私保护推理（PPI）中的应用常常导致显著的性能下降或速度减慢。这主要是由于Transformer架构中的大量非线性运算，这些运算不适合SMPC，并且很难有效地绕过或优化。为了解决这一问题，我们提出了一个全面的PPI框架，称为SecFormer，以实现Transformer模型的快速和准确的PPI。我们成功消除了PPI中高成本的指数和最大值操作，而不牺牲模型性能，并通过采用合适的数值计算方法开发了一系列高效的SMPC协议，提升了PPI中其他复杂的非线性函数的性能，包括GeLU、LayerNorm和重新设计的Softmax。我们的广泛实验表明，SecFormer在性能上优于MPCFormer，对于BERTBASE和BERTLARGE分别提高了3.4%和24.7%。在效率方面，SecFormer比PUMA分别快了3.57倍和3.58倍，展示了其高效性和速度。代码可通过点击此处获得。\n引言 Transformer模型（Vaswani等，2017；Devlin等，2019；Radford等，2019；Brown等，2020；Raffel等，2020；Liu等，2019；Lewis等，2020；Zeng等，2021；Ouyang等，2022；OpenAI，2023）在多种下游任务中表现出色（Chan等，2024；Jiayang等，2023；Lu等，2019；Zhang等，2020；Lu等，2020；Liu等，2020；Zhang等，2023c；Wang等，2023），并广泛应用于“模型即服务”（MaaS）范式中，向客户提供高质量的推理服务。然而，这种MaaS框架对推理数据和模型参数构成了显著的隐私风险（Li等，2023b，c，d，2024；Zhang等，2024b；Zhang和Zhu，2020）。例如，基于Transformer的服务Copilot和ChatGPT要求用户上传明文请求。这一操作不仅对用户隐私构成威胁，还可能违反相关法律法规，如欧盟的《通用数据保护条例》（GDPR）。安全多方计算（SMPC）（Shamir，1979；Yao，1986；Goldreich等，1987）在保护推理数据和模型权重隐私方面表现出巨大潜力（Gilad-Bachrach等，2016；Liu等，2017；Mishra等，2020；Rathee等，2021；Huang等，2022；Luo等，2023；Zhang等，2023a，2024a）。然而，使用SMPC进行Transformer模型的隐私保护推理（PPI）证明特别缓慢。例如，BERTBASE（Devlin等，2019）在SMPC下进行单个样本推理需要71秒，而明文推理仅需不到1秒。这种低效源于当前SMPC协议在执行Transformer模型中的非线性运算时的局限性（见第2.2节）。如图1(a)所示，我们发现Softmax和GeLU占据了PPI时间的77.03%，尽管它们在明文推理中的开销所占份额较小。这个观察结果与Wang等（2022）；Li等（2023a）的研究一致。为了缓解这一问题，Li等（2023a）通过用一些SMPC友好的二次函数替代Softmax和GeLU，重新设计了Transformer模型，绕过了Softmax和GeLU中的非线性运算（如erf、指数和最大值）的隐私保护计算。这种激进的替代大大提高了PPI效率，但不幸的是，显著降低了模型的性能，并且对于更大的模型不可扩展（见图1(b)）。一些其他研究（Dong等，2023）尝试通过设计更高效的SMPC协议来提高PPI，虽然能够保持模型性能，但仍面临昂贵的PPI开销。\n在本研究中，我们提出了一个全面的PPI框架——SecFormer，旨在通过利用Transformer模型和SMPC协议设计的优势，实现快速且准确的Transformer模型PPI。我们的研究表明，保留GeLU的精确计算显著提高了PPI性能（图1(b)）。基于这一见解，SecFormer通过模型设计绕过了Softmax中高开销的非线性PPI操作，如指数和最大值（第3.1节）。这种适应性调整结合了知识蒸馏的战略使用，使得SecFormer能够构建一个既高性能又兼容SMPC的Transformer模型。为了进一步提高PPI性能，我们转向协议设计，开发了一套高效的SMPC协议，利用适当的数值计算方法处理PPI中的其他复杂非线性函数，如GeLU、LayerNorm和重新设计的Softmax（第3.2节）。具体而言，SecFormer为GeLU引入了一种基于分段多项式和傅里叶级数的创新SMPC协议，促进了GeLU的高效准确计算。此外，SecFormer为LayerNorm和Softmax部署了高效的隐私保护平方根逆运算和除法计算，采用Goldschmidt方法（Goldschmidt，1964；Markstein，2004），并结合输入膨胀技术，绕过了非线性初始值计算。我们在多个数据集上对SecFormer进行了广泛的评估，使用Transformer模型BERTBASE和BERTLARGE。实验结果表明，与基于纯模型设计的最新方法相比，SecFormer在BERTBASE和BERTLARGE上的平均性能提升分别为3.4%和24.7%（第4.2节），同时保持了相当的效率。与仅改进SMPC协议的方法相比，SecFormer在PPI中实现了3.57倍和3.58倍的加速（第4.3节），同时维持了相当的PPI性能。\n背景与相关工作 基于SMPC的模型推理工作流程 安全多方计算（SMPC）是一种加密技术，为多个参与方之间的模型隐私保护推理（PPI）提供了有前景的解决方案（Gilad-Bachrach等，2016；Liu等，2017；Mishra等，2020；Rathee等，2021；Huang等，2022）。通常，参与方遵循诸如秘密共享（Shamir，1979；Goldreich等，1987）等加密原语来保护模型权重和推理数据。本文主要介绍了2-out-of-2秘密共享方案，因其效率高且具有代表性。具体而言，2-out-of-2秘密共享将一个秘密x分割成两个随机份额[[x]] = ([x]₀, [x]₁)，其中x = (([x]₀ + [x]₁) mod L)，确保每个份额都不会泄露关于x的信息，同时在两个份额结合时能够正确重构x。在构建SMPC协议时，份额由两个不同的参与方持有。它们相互传递掩蔽的中间结果，完成不同函数的隐私保护计算，并获得计算结果的份额。基于2-out-of-2秘密共享的PPI工作流程如图2所示，涉及三个主要利益相关者：需要保护模型权重的模型推理服务提供者、需要保护推理数据的客户端，以及执行模型PPI的SMPC引擎。SMPC引擎包含三个非共谋服务器（即参与者）：两个计算服务器Sᵢ（i ∈ {0, 1}），用于计算PPI的份额；以及一个辅助服务器T，用于生成执行SMPC协议所需的随机数。\n初始时，服务提供者和客户端将模型权重和推理数据的份额分别安全地传输给S₀和S₁（步骤1和2）。随后，计算服务器利用这些份额作为输入，并通过与T的交互计算执行SMPC协议，完成PPI，最终生成推理结果的份额（步骤3）。这些份额随后被转发给客户端（步骤4），以便客户端在本地重构推理结果（步骤5）。由于每个参与者仅拥有输入、输出或中间结果的一个份额，因此该PPI工作流程能够保证模型权重和推理数据的隐私。\n基于SMPC的Transformer模型推理的主要瓶颈 尽管上述PPI工作流程能够保证模型权重和推理数据的隐私，但在实现一些非线性操作（即Softmax、GeLU和LayerNorm）时，面临不可接受的通信开销（见表1），这些操作在Transformer模型中大量存在，并成为PPI的主要瓶颈。具体来说，对于一个向量 $x = (x_1, x_2, \\dots, x_n)$，Transformer中的Softmax将其转换为一个n维的概率分布，计算公式为：\n$\\text{Softmax}(x)[i] = \\frac{e^{x_i - \\tau}}{\\sum_{h=1}^{n} e^{x_h - \\tau}},$\n其中 $\\tau = \\max_{h=1}^{n} x_h$ 用于确保数值计算的稳定性。如表1所示，Softmax的SMPC计算面临三个主要障碍：指数运算、除法和最大值运算。需要注意的是，最大值的计算需要调用ΠLT操作logn次（Knott等，2021），因此成为最大的障碍。\nGeLU的函数定义为：\n$\\text{GeLU}(x) = x \\cdot \\frac{1}{2} \\left( 1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right) \\right),$\n其中 $\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_0^x e^{-t^2} dt$。GeLU函数的非线性部分源自$\\text{erf}$，目前没有现成的SMPC协议用于其隐私保护计算。\n对于一个向量 $x = (x_1, x_2, \\dots, x_n)$，LayerNorm函数定义为：\n$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\overline{x}}{\\sqrt{\\text{var}(x) + \\epsilon}} + \\beta,$\n其中 $\\overline{x} = \\frac{1}{n} \\sum_{h=1}^{n} x_h$，$\\text{var}(x) = \\frac{1}{n} \\sum_{h=1}^{n} (x_h - \\overline{x})^2$，$\\gamma$ 和 $\\beta$ 是两个可学习的参数，$\\epsilon$ 是一个非常小的常数，用于防止分母为零。对于SMPC而言，计算LayerNorm的主要瓶颈来自于除法和平方根运算。\nTransformer模型的高效PPI 为了缓解上述瓶颈，现有的Transformer模型PPI研究通过模型设计或SMPC协议设计提高效率。基于模型设计的研究（Chen等，2022；Li等，2023a；Zeng等，2022；Zhang等，2023b；Liang等，2023）通过替换Transformer中的非SMPC友好型非线性操作，绕过了PPI中的高开销操作。这些方案大幅提高了效率，但通常会导致模型性能显著下降。设计更高效SMPC协议的研究（Hao等，2022；Zheng等，2023a；Gupta等，2023；Dong等，2023；Hou等，2023；Ding等，2023；Pang等，2023）则通过为Transformer中的非线性操作定制高效的SMPC协议来提高PPI效率。这些方案保持了Transformer模型的性能，但仍然面临着高昂的计算和通信开销。\n作为基于模型设计的代表性工作，Li等（2023a）通过将GeLU和Softmax分别替换为$ \\text{Quad} = 0.125x^2 + 0.25x + 0.5$ 和 $$\r2\\text{Quad}(x)[i] = \\frac{(x_i + c)^2}{\\sum_{h=1}^{n} (x_h + c)^2},\r$$ 从而绕过了erf、指数和最大值的隐私保护计算。在此基础上，采用了知识蒸馏，细调后的Transformer模型作为教师，近似Transformer模型作为学生，蒸馏过程在下游任务数据上进行，从而得到一个与SMPC兼容的Transformer模型。该方法在提高PPI效率方面有效，但会导致模型性能显著下降。我们的研究表明，保持GeLU的准确计算显著提高了PPI性能。Dong等（2023）通过利用分段函数和多项式拟合，首次为GeLU函数提出了SMPC协议。然而，分段函数和多项式的计算需要多次调用ΠLT和ΠMul，导致其效率较低。\nSecure Transformer Inference Protocol 安全Transformer推理协议\n摘要 对于基于Transformer的服务（如ChatGPT），模型参数和用户数据的安全性至关重要。尽管近期在安全双方协议方面取得了进展，成功解决了Transformer模型服务中的安全问题，但由于加密开销过于庞大，其在实际中的应用仍然不可行。通过在开发两个真实世界的基于Transformer的服务过程中的实践经验，我们识别出了双方假设中固有的效率瓶颈。为了克服这一限制，我们提出了一种新颖的三方威胁模型。在该框架内，我们设计了一种半对称的基于置换的保护方案，并提出了STIP，这是首个在推理准确性不丢失的情况下实现安全Transformer推理的协议。在真实系统中对代表性Transformer模型的实验表明，STIP在安全性上具有实际可行性，并且在效率上比现有的安全双方协议高出数百万倍。\n引言 基于Transformer的推理服务是最前沿的人工智能应用[13, 23, 44]。云计算能力，如自动扩展[45]，满足了为Transformer提供服务的需求，尤其是具有数十亿参数的大型模型。因此，像OpenAI这样的主要组织选择将其基于Transformer的服务完全部署在云端[5]。然而，在隐私敏感领域，将原始数据发送到云端是不可行的，例如三星禁止使用ChatGPT后的敏感代码泄漏事件[21]。 模型拆分不够安全 模型拆分推理[30, 50, 58]通过在设备和云端之间战略性地分配神经网络层，设备将中间激活值发送到云端以继续推理。模型拆分推理避免了原始数据的传输，同时保持了效率[38, 39]。然而，研究表明，存在从中间激活值逆向工程敏感信息的潜在风险[2, 37]。 安全的双方协议产生高昂开销 近期研究探讨了通过同态加密（HE）和安全两方计算（2PC）[11, 24, 26]进行安全的Transformer推理。然而，这些协议会带来高昂的计算和设备-云端通信开销，尤其是在复杂的非线性层（如LayerNorm和ReLU）上。例如，CipherGPT使用GPT2模型生成单个token时需要25分钟的处理时间和90GiB的流量[26]。 第一性原理思维：三方威胁模型 为了克服效率瓶颈，我们使用第一性原理重新思考基本的两方假设：模型所有者和数据所有者。通过管理两个真实世界的基于Transformer的服务1，我们总结出一个一致的经验：模型开发者 ≠ 模型服务商。在这两个服务中，我们通过对开源参数[33, 51]进行微调[27]，利用收集的数据开发了Transformer模型。我们具备足够的计算能力进行离线模型开发，但缺乏为数十万用户提供大规模、长期服务的能力。因此，作为模型开发者，我们依赖第三方云平台提供模型服务。基于我们的实际开发经验，我们提出了一种新的三方威胁模型。在这个模型中，我们将模型所有者分解为两个实体：模型开发者和模型服务商。由于开发的模型是专有的，模型开发者必须保护其模型参数免受模型服务商潜在攻击[60]，因此我们假设他们不会串通。\nSTIP的洞察与设计 基于我们提出的三方威胁模型，我们开发了STIP（Secure Transformer Inference Protocol），并从两个方面进行了设计。首先，我们采用高效的特征空间置换来实现安全且等效的Transformer推理。由于推理在不可信的服务器上执行，模型参数和设备上的数据必须在上传到云端之前进行转换。基于特征空间的高效置换，我们设计了一个Transformer层的数据和参数转换方法。我们证明了使用我们提出的转换方法进行计算在数学上是等效的，从而确保没有准确性损失。其次，我们设计了一个模型开发者与数据所有者之间的半对称保护方案。这一思路源于神经网络的顺序结构。我们揭示了模型开发者只需要与数据所有者共享第一层和最后一层的相同置换，并可以独占保留中间层转换的信息。类似的半对称保护方案已经在图像加密[16]和在线购物[20]等领域得到应用。我们通过距离相关性[49]证明了STIP的隐私保护能力，并证明它抵抗暴力破解和已知明文攻击。\n贡献 我们总结了本工作的三个关键贡献如下：\n我们识别了两方设置中固有的效率瓶颈及其与现实应用的错位问题。我们提出了一种新的三方威胁模型，将模型所有者分解为两个独立的实体：模型开发者和模型服务商。 我们提出了STIP，这是第一个针对三方Transformer推理的安全协议，具有隐私泄露的理论界限并保证没有准确性损失。 我们实现了STIP，并在真实系统中对多种具有高达700亿参数的Transformer模型进行了评估。实验结果展示了STIP的效率，接近未保护的全云推理，并超越了现有的安全两方协议[11, 24, 26]数百万倍。 本工作没有提出任何伦理问题。\nTRUNCFORMER: PRIVATE LLM INFERENCE USING ONLY TRUNCATIONS 2024.12.1 \u0026ldquo;TRUNCFORMER: 仅使用截断的私有LLM推理\u0026rdquo;\n摘要 私有推理（PI）在与专有机器学习模型（如大语言模型（LLM））交互时，发挥着保障用户数据隐私的重要作用。然而，由于LLM中非线性函数带来的巨大延迟成本，PI在实践中仍然难以实现。现有的研究工作主要集中在通过近似方法提高特定LLM非线性函数（如Softmax或GeLU）的延迟性能。然而，随着新的LLM架构不断推出，新型非线性函数也随之出现，这导致PI研究者不断追赶，试图优化最新的非线性函数。我们提出了TruncFormer，一个将任何LLM转化为纯文本模拟PI的框架。我们的框架利用了LLM中非线性函数可微分的事实，这些非线性函数可以通过一系列加法、乘法和截断操作来准确近似。此外，我们将加法/乘法和截断操作解耦，并根据给定的字段大小和输入表示大小静态地确定应该在哪些地方插入截断操作。与现有的加密协议（这些协议在每次乘法操作后强制执行截断）相比，这种方法可以带来延迟性能的改进。我们将代码开源，供社区使用。\n引言 近年来，基于变换器（transformer）的巨大语言模型（LLM），如ChatGPT的常规使用量急剧增加。许多这样的模型是专有的，通过API调用进行访问，用户需要将输入数据（如提示或其他上下文）发送给模型提供者以获取推理结果。因此，当查询涉及机密数据时，这些模型不可用。克服这一问题的一种方法是使用私有推理（PI）与安全多方计算（MPC）。PI是一种使用加密技术执行机器学习推理的过程，其中数据持有者和模型所有者都无法得知对方的数据。尽管在许多先前的工作中已经展示了使用MPC进行PI的可能性，但由于高延迟成本，PI的实际可行性仍然存在争议（Evans等，2018；Liu等，2017）。从宏观上看，PI的延迟成本可以分为两个部分：字段内操作和字段外操作。传统的加密操作是在固定大小的字段中执行的，这些字段仅支持加法和乘法。这些操作比明文版本的操作要慢，但并不是PI延迟的主要原因（Ghodsi等，2020）。字段外操作，如除法、指数运算、ReLU等，是PI中延迟的主要来源，因此，现有的大多数PI协议都必须使用诸如Yao的加密电路（Yao，1986）或盲传输（Kilian，1988）等技术。然而，这两种技术都非常依赖通信，因此会显著增加实际PI实现的延迟。\n为了应对字段外PI操作带来的巨大延迟，出现了大量相关的研究工作。早期的工作如Delphi（Mishra等，2020）和Circa（Ghodsi等，2021）为字段外操作创建了多项式近似。随后，像CryptoNAS（Ghodsi等，2020）、Sphynx（Cho等，2022a）、SNL（Cho等，2022b）和SE-Net（Kundu等，2023）等方法通过适应网络架构来最小化字段外操作的数量。然而，这些方法大多考虑了涉及卷积/残差网络的计算机视觉应用，主要关注ReLU作为唯一的字段外操作。\n随着LLM和变换器架构的兴起，PI方法现在不得不应对新的字段外操作类型。这导致了一小部分、但不断增长的研究工作，专注于优化特定的操作，如Softmax或GeLU。最近，MPCFormer（Li等，2022）、PriViT（Dhyani等，2023）、MPCViT（Zeng等，2022）、和Sal-ViT（Zhang等，2023）等方法采用了类似的网络适应思想，以减少在变换器模型中加密处理非线性操作的负担。其他相关的研究包括（Hao等，2022；Hou等，2023；Zheng等，2023）；详细综述见（Luo等，2024）。然而，由于LLM研究的快速进展，新的协议或针对非线性操作的PI友好型近似方法的竞争仍在不断进行。\n我们的贡献 在本文中，我们提出了TruncFormer，一个用于LLM推理的新框架，可以与任何网络架构和任何PI协议结合使用。具体而言：\nTruncFormer接收任意变换器模型，并根据给定的字段大小和算术操作的位宽，静态地确定在可能发生溢出的地方计算截断。 TruncFormer修改了各种Crypten（Knott等，2021）对字段外操作的近似，使其适用于当今的公开LLM。 将这些内容结合起来，我们展示了TruncFormer仅通过字段内操作（加法/乘法）和截断操作，完成了LLM的私有推理。 我们使用一个纯文本库验证了TruncFormer，该库准确模拟了在PI环境下实现我们方法的成本（延迟和准确性），并测试了两个流行的LLM（Llama-7B和Gemma2B）。我们将该库开源，供社区使用（见 https://anonymous.4open.science/r/fixed_llm-D334/README.md）。 ","date":"2024-12-19T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E8%B0%83%E7%A0%94%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/blossoms-8263945_1280_hu10435487583272791281.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E8%B0%83%E7%A0%94%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/","title":"调研论文速览"},{"content":"hugo文章部署错误 报错信息 1 2 deploy Action failed with error Unexpected HTTP response: 404 之前经常因为写错yaml配置文件而报错，于是我检查了我新上传的一篇文章，检查了很多遍都没有发现错误，于是上网搜索，不是文章的错误，而是在.github/workflows/hugo_deploy.yaml 目录，有一个未正确配置的工作流 YAML 文件。\n错误原因 这里的latest要指定为自己使用的特定版本，不然当hugo更新时，自己使用的还是旧版本，就会出现问题，我的版本是0.136.5，更改后就不报错了。\n1 2 3 4 5 - name: Setup Hugo uses: peaceiris/actions-hugo@v3 with: hugo-version: \u0026#34;0.136.5\u0026#34; extended: true ","date":"2024-12-15T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/hugo%E6%96%87%E7%AB%A0%E9%83%A8%E7%BD%B2%E9%94%99%E8%AF%AF/sunflower-3550693_1280_hu8588353291531835585.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/hugo%E6%96%87%E7%AB%A0%E9%83%A8%E7%BD%B2%E9%94%99%E8%AF%AF/","title":"hugo文章部署错误"},{"content":"总结 好多论文里的东西都看不懂，要不是缺大模型的知识，要不就是缺密码学的知识，虽然我想象力丰富，但我对于那些没亲手经历过得实验真的有点想象不到，大学没有学ai的弊病在此刻体现的淋漓尽致。需要想想办法了。\nCipherGPT: Secure Two-Party GPT Inference 本文提出了CipherGPT，这是首个用于安全两方 GPT 推理的框架。该框架通过创新协议解决了大语言模型推理中隐私泄露的问题，尤其适用于一个持有私密数据、另一个持有大语言模型的场景。核心贡献包括：\n提出了专为 GPT 推理定制的安全矩阵乘法，提升了 2.5 倍的速度和 11.2 倍的带宽节省。 提出了用于安全计算 GELU 的新协议，在运行时、通信和精度上分别提高了 4.2 倍、3.4 倍和 10.9 倍。 开发了首个用于 top-k 采样的协议，并进行了完整的实现和基准测试，证明了其在隐私保护和性能方面的优势。 一个新的GPT模型，里面有的细节知识不懂，VOLE，TOP-K采样需要继续了解学习。\nDP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass 该论文提出了DP-Forward，一种结合差分隐私（DP）技术的语言模型微调与推理方法。通过在前向传播过程中应用差分隐私，DP-Forward 能够有效保护数据隐私，同时保证模型的性能。与传统方法相比，DP-Forward 采用了创新的隐私保护机制，确保了微调和推理过程中用户数据的安全。\n《DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass》提出了一种在大规模预训练语言模型中集成差分隐私（Differential Privacy, DP）的方法，重点在于保护用户隐私的同时，不影响模型的训练和推理效果。该方法通过在模型的前向传播（forward pass）阶段引入差分隐私机制，从而实现隐私保护。\n背景 差分隐私是一种数学定义，用来衡量在数据分析过程中是否泄露了个体的隐私。它的核心思想是通过在数据中添加噪声，确保即使外部观察者获得了分析结果，也无法推断出任何个体的敏感信息。在深度学习中，差分隐私技术通常通过在梯度更新中加入噪声来防止隐私泄露。\n传统的差分隐私方法通常在训练过程中对梯度或参数进行扰动（例如，添加噪声），这会影响模型的性能。DP-Forward方法则选择在推理过程中添加隐私保护，从而避免对训练过程造成干扰，并使得隐私保护更加精细和局部化。\n主要贡献 DP-Forward提出了一种新的差分隐私框架，它在前向传播阶段引入了差分隐私机制，而不是在传统的反向传播阶段进行噪声注入。这样可以在不影响训练过程的情况下保护推理过程中涉及的用户数据的隐私。\n在前向传播中引入差分隐私： 在标准的神经网络中，前向传播是将输入数据传递通过各层网络，最终输出预测结果。在DP-Forward方法中，差分隐私噪声被添加到前向传播的计算过程中，而不是在反向传播阶段。这意味着模型在进行推理时会生成含有噪声的输出，从而保障输入数据的隐私。 噪声添加方式： 具体来说，在前向传播过程中，模型会对每个输入的隐私数据应用特定的噪声生成机制。噪声的强度会根据差分隐私的定义来调节，通常需要满足特定的隐私保证，如$\\epsilon$-差分隐私（epsilon-DP）。这个噪声机制会确保即使外部观察者获取了模型输出，他们也无法推测出关于输入数据的任何敏感信息。 噪声设计： 添加的噪声设计上需要权衡隐私性和准确性。噪声的方差和尺度需要与隐私预算（privacy budget）$\\epsilon$的要求相匹配。较高的隐私预算意味着噪声较小，但隐私保护较弱；较低的隐私预算则增强了隐私保护，但可能导致模型输出的准确性下降。 隐私保护与性能的平衡： 在设计时，DP-Forward注重在隐私保护和模型性能之间找到平衡点。通过对前向传播过程的噪声控制，能够在保护隐私的同时，最大限度地保留模型的推理精度。这个平衡通常依赖于多个因素，包括模型架构、噪声级别和隐私预算。 工作流程 DP-Forward的工作流程可以分为以下几个步骤：\n模型预训练： 和传统的预训练语言模型一样，首先会对模型进行大规模预训练，通常在没有隐私保护的环境下进行。这一步骤仍然是标准的模型训练过程。 推理过程中的隐私保护： 在推理阶段，当模型接收到用户输入时，会在前向传播过程中注入差分隐私噪声。这确保了输出不包含关于输入的精确信息，从而保护了输入数据的隐私。 隐私预算的分配： 需要根据预设的隐私预算（$\\epsilon$）来确定噪声的大小。隐私预算决定了模型的隐私保护强度，通常在多个推理请求中共享隐私预算，从而在多个推理过程中分配适当的噪声。 输出： 模型经过前向传播处理后，产生带有噪声的推理结果，输出给用户。由于噪声的加入，推理结果不会泄露输入数据的敏感信息。 关键技术和挑战 隐私预算管理： 隐私预算$\\epsilon$是控制差分隐私强度的关键。需要在多个推理请求中合理管理隐私预算，以确保在保证隐私的同时，模型能够提供有用的推理结果。 噪声添加的影响： 在前向传播中加入噪声可能导致推理结果的精度下降。如何设计合适的噪声生成机制，以平衡隐私保护与推理效果之间的矛盾，是实现这一方法的关键挑战之一。 模型架构调整： 有时需要对模型进行轻微的架构调整，以便更好地与差分隐私机制兼容，尤其是在推理过程中涉及大量用户输入的情况下。 适应性和灵活性： 该方法的优点在于其灵活性，它能够适应多种不同的应用场景，不仅限于文本生成，也可以应用于其他类型的推理任务，如图像识别、语音识别等。 总结 DP-Forward提供了一种新的差分隐私集成方式，专注于在推理过程中保护用户隐私。这种方法的优势在于，它不需要改变模型的训练过程，而是在推理阶段添加噪声保护隐私。通过合适的噪声机制和隐私预算管理，DP-Forward能够在保证隐私的同时，最大程度上保持推理精度。这对于保护敏感数据的隐私，同时在实际应用中维持良好的性能，提供了有力的支持。\n对我来说还是没有达到我的目的（不给模型方数据），这个是给了，让模型方保护，导致输出是经过差分隐私的，感觉没啥用。\nIncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization IncogniText 提出了一种基于 LLM 的文本匿名化方法，旨在通过条件随机化隐私属性来增强文本隐私保护。这种方法通过将敏感属性进行随机化处理，确保用户隐私得到有效保护，同时保持文本的上下文和语义连贯性，适用于需要隐私保护的文本生成和处理任务。\n《IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization》提出了一种创新的方法，旨在通过基于大型语言模型（LLM）的隐私增强条件文本匿名化技术来保护文本中的敏感信息。该方法通过结合条件生成和隐私保护机制，有效地隐藏文本中的私密属性，同时保持文本的可读性和有效性。\n论文概述 在现代自然语言处理（NLP）应用中，文本数据通常包含敏感信息（如个人身份、地理位置、健康状况等），这些信息在处理和共享时需要得到保护。传统的文本匿名化方法通常通过替换、删除或模糊敏感信息来实现隐私保护，但这些方法往往会影响文本的上下文和可用性，甚至可能导致文本内容失真。为了克服这一挑战，IncogniText 提出了基于大型语言模型（LLM）进行的条件文本匿名化，通过“私密属性随机化”（Private Attribute Randomization）技术，在不丧失文本质量和结构的情况下保护隐私。\n主要贡献 隐私增强的文本匿名化： 该论文提出了一种新的文本匿名化方法，称为IncogniText，它通过结合条件生成模型（conditional generation model）和隐私保护技术来实现文本中敏感属性的匿名化。与传统的替换方法不同，IncogniText能够根据上下文生成与原始文本相似的、具有隐私保护的替代信息。 私密属性随机化（Private Attribute Randomization）： 论文的核心创新是私密属性随机化，它通过LLM生成新的替代值来替换文本中的私密属性（如名字、地址、电话号码等），同时确保这些替代值不会泄露任何关于原始信息的敏感数据。这种方法不仅能保护隐私，还能保持文本的流畅性和语义完整性。 条件文本生成： IncogniText使用条件生成技术来确保生成的匿名文本符合特定的上下文要求。这意味着替代的隐私信息需要满足与原始文本相似的语法结构、语义逻辑和上下文约束。这一技术的关键在于保持文本的自然性和连贯性，同时去除敏感信息。 保护隐私而不失文本有效性： 传统的匿名化技术（如直接删除或替换敏感词）可能会导致文本的可读性降低或语义失真。IncogniText通过对敏感信息进行“随机化”而非简单替换，确保了文本在隐私保护的同时，仍然可以保留原始文本的有效性和可读性。 方法详解 IncogniText的技术流程可以分为以下几个步骤：\n输入文本分析： 首先，IncogniText对输入文本进行分析，识别出其中的私密属性。这些私密属性通常包括个人身份信息、地址、电话号码、电子邮件等。在这一步，系统使用预训练的实体识别模型（NER）或其他信息提取技术来自动识别文本中的敏感信息。 条件生成模型： 对于每个私密属性，IncogniText使用大型语言模型（LLM）来生成一个新的、上下文相关的替代值。例如，如果文本中出现了某个人名，模型会根据周围的上下文生成一个与原始名字语义相近但不真实的名字。这种替代值不仅能够隐藏敏感信息，还能保持文本的流畅性。 私密属性随机化： 在生成替代值的过程中，IncogniText应用私密属性随机化的技术，即通过扰动和随机化原始敏感信息的某些部分，从而生成一个“新的”值，而这个新值在上下文中看起来与原始文本高度相似，但不会泄露原始的敏感信息。例如，在生成替代的地址时，系统可以基于地理信息来产生与原始地址相似但并不指向同一位置的地址。 输出生成： 最终，IncogniText会将修改后的文本重新组合，并确保替代信息符合语法和上下文的要求。这些生成的文本仍然具有可读性和有效性，并且隐私得到有效保护。 隐私保护机制 IncogniText通过以下几个方面加强隐私保护：\n隐私预算控制： 隐私保护的效果依赖于生成过程中的随机化程度。为了确保敏感信息不会被泄露，IncogniText采用了类似于差分隐私的隐私预算机制，通过对生成内容的扰动来控制隐私泄露的风险。 上下文一致性： 生成的替代文本与原始文本在语义和上下文中高度一致，这不仅保证了文本的自然性，也避免了由于生成文本过于生硬或不合适而泄露隐私的风险。 加密和安全性： 在敏感文本处理过程中，IncogniText可能会采用加密技术来进一步确保数据在传输和存储过程中的安全性，特别是在面临潜在的恶意攻击时。 评估与实验 论文通过一系列的实验证明了IncogniText的有效性，评估了其在多个标准文本数据集上的表现，包括：\n文本可读性和质量： 实验表明，IncogniText能够生成高质量的匿名化文本，且文本的流畅性、语法正确性与原始文本几乎没有差异。 隐私保护效果： 通过与传统的匿名化技术（如替换法）进行对比，IncogniText显著提高了隐私保护效果，减少了敏感信息泄露的风险。 模型性能： 在性能评估中，IncogniText在多个大型语言模型上进行测试，表现出较好的鲁棒性和较低的计算成本。即便是在处理较大规模的文本时，模型依然能够有效地保护隐私。 结论 《IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization》提出了一种创新的隐私保护方法，通过基于大型语言模型的条件生成和私密属性随机化技术，实现了在不损害文本质量和语义的前提下，有效地保护文本中的敏感信息。该方法为文本匿名化技术提供了一种新的思路，尤其适用于需要保留上下文信息和语义流畅性的隐私保护场景。\n这个也不是我想要的，这就像是在做文本清洗，去除敏感信息。\nPermLLM: Private Inference of Large Language Models within 3 Seconds under WAN PermLLM 提出了一个针对大语言模型的隐私保护推理框架，能够在广域网（WAN）环境下实现 3 秒内的快速推理。该框架通过高效的加密技术和优化的通信协议，确保在保护用户数据隐私的前提下，模型推理能够快速完成，具有较好的可扩展性和实时性。\n《PermLLM: Private Inference of Large Language Models within 3 Seconds under WAN》提出了一种新颖的方案——PermLLM，旨在解决在广域网（WAN）环境下进行大语言模型（LLM）推理时的隐私保护问题。论文的重点是如何在保护隐私的同时，提高大语言模型推理的效率，特别是在带宽较低的环境（如WAN）下，确保推理结果能够在3秒内返回，同时有效保证用户数据的隐私。\n论文概述 随着大语言模型（LLM）的广泛应用，推理（Inference）过程中的隐私保护变得尤为重要。在许多应用场景中，尤其是在涉及用户敏感数据时，如何确保输入的隐私不被泄露是一个关键问题。与此同时，随着云计算服务的普及，越来越多的推理请求都通过网络传输，而网络带宽（尤其是在广域网WAN环境中）较低时，如何保证推理速度和隐私保护之间的平衡，成为了一个亟待解决的问题。\nPermLLM通过创新的技术解决了这两个挑战，提出了一个高效且隐私保护的推理框架。其目标是在保证隐私的同时，使得大语言模型能够在网络延迟较高的环境下仍然实现快速推理。\n主要贡献 隐私保护的推理框架： 论文提出的PermLLM框架致力于在保护用户隐私的前提下，通过使用加密和分布式计算技术，提供快速且安全的推理结果。与传统的隐私保护方法（如同态加密和多方计算）相比，PermLLM在推理效率上具有明显的优势，尤其是在高延迟的WAN环境中。 高效的推理时间： PermLLM的核心创新之一是其在WAN环境下提供3秒内推理的能力。这一性能的突破主要得益于该框架的高效网络协议和加密技术，使得推理请求能够在低带宽和高延迟的环境下迅速完成。 基于Permute机制的私密计算： 该框架采用了一种称为Permute（Perm）*的机制，它结合了特定的*隐私保护算法（如加密、随机化）和推理优化策略**，使得在数据传输过程中不会暴露敏感信息。 WAN环境中的优化： WAN环境中由于网络延迟和带宽的限制，传统的推理方法很难满足快速响应的需求。PermLLM通过对数据流和计算流程的高度优化，显著降低了因网络延迟导致的时间浪费，保证了用户能够在较短时间内获得推理结果。 方法详解 PermLLM框架的工作流程包括以下几个关键步骤：\n输入数据的加密处理： 在PermLLM中，用户的数据（如文本、查询等）会在传输前进行加密处理。加密算法可以防止用户输入在传输过程中泄露给服务提供者或中间人。通常使用对称加密或非对称加密技术来确保输入数据的机密性。 数据传输与隐私保护： 在PermLLM框架中，输入数据通过加密的通道传输到远端服务器。在传输过程中，数据经过特定的隐私保护机制（如数据混淆或扰动）处理，以确保即使网络传输过程被监听或攻击，敏感信息依然不会泄露。 Permute机制的应用： Permute是PermLLM框架中的一个关键隐私保护机制。它通过对输入数据进行“随机化”或“重排列”，生成多个可能的版本，而每个版本的输出结果仍然能够与原始数据保持一致，但由于其加密和随机化处理，第三方无法直接推测出原始数据的内容。通过这种方式，PermLLM有效保证了用户数据在整个推理过程中的隐私。 推理过程的优化： PermLLM使用了一种高效的推理优化算法，能够在短时间内完成对输入数据的推理。该算法通过优化模型的计算过程（如减少计算节点的数量或对模型的参数进行压缩）来加快推理速度。此外，该框架还通过网络协议的优化，减少了数据传输的延迟。 快速反馈与结果解密： 在模型推理完成后，结果会通过加密传输回用户端，用户端再进行解密操作。解密后的推理结果被返回给用户，整个过程的响应时间被优化至3秒内，即使是在带宽有限、网络延迟较高的WAN环境中。 隐私保护机制 PermLLM采用了多种技术来确保隐私保护：\n同态加密（Homomorphic Encryption）： 同态加密是一种允许在加密数据上进行计算的技术。通过在加密数据上直接执行推理计算，PermLLM能够确保数据在整个推理过程中不会被解密或暴露，只有最终的推理结果会被解密并返回给用户。 安全多方计算（Secure Multi-Party Computation, SMPC）： 在某些情况下，PermLLM还可能使用SMPC技术，特别是在需要多个计算节点共同处理推理任务时。SMPC允许多个参与方协同计算，但每个参与方的输入数据始终保持私密，且无法泄露其他参与方的敏感信息。 隐私预算控制与分配： 为了确保隐私保护不会过度影响推理效率，PermLLM框架中还包含了隐私预算控制机制，允许用户根据需要灵活调整隐私保护级别和计算效率之间的平衡。 私密数据传输： 在数据传输过程中，PermLLM采用了强加密技术，如TLS协议和端到端加密，确保数据在传输过程中不会被窃听或篡改。 实验与评估 论文中展示了PermLLM在多个实际场景中的表现，包括：\n性能评估： 在WAN环境下，PermLLM展示了其在3秒内完成推理的能力。实验结果表明，即使在带宽有限的条件下，PermLLM也能够保持低延迟和高吞吐量。 隐私保护效果： 通过与传统的隐私保护方法（如普通加密）进行对比，PermLLM显著提高了隐私保护效果，同时保持了推理效率。 网络条件适应性： 在不同的网络条件（如高延迟和低带宽）下，PermLLM展现了出色的适应能力，能够调整其网络协议和计算资源，确保推理时间保持在3秒内。 总结 《PermLLM: Private Inference of Large Language Models within 3 Seconds under WAN》提出了一个创新的隐私保护推理框架PermLLM，它通过采用加密技术、优化推理过程和引入Permute隐私机制，成功地实现了在WAN环境下保护用户隐私的同时，提供高效的推理服务。该方法的突破性贡献在于，它不仅能够保证隐私保护的强度，还能够确保推理效率满足快速响应的需求（3秒内）。PermLLM在大语言模型的应用场景中具有广泛的潜力，特别是在需要保护用户隐私的环境中。\n写的太高深了，丁点看不懂，算了下一个\nPFID: Privacy First Inference Delegation Framework for LLMs 论文提出了PFID，一个针对大语言模型的隐私优先推理委托框架。该框架通过将推理过程分委托给第三方服务器来实现隐私保护，同时避免暴露用户数据。PFID 结合了隐私保护与推理效率，能够在确保隐私的同时提供高效的推理服务，适用于跨域的 LLM 应用场景。\n《PFID: Privacy First Inference Delegation Framework for LLMs》这篇论文提出了一种新的框架——PFID（Privacy First Inference Delegation），旨在解决在使用大语言模型（LLM）时的隐私保护问题，尤其是在需要将推理任务委托给外部计算资源（如云端服务）时，确保用户隐私不被泄露。\nPFID框架的核心理念是将推理任务委托给外部计算资源的同时，最大限度地保护用户的私人数据，避免数据泄露和滥用。为了实现这一目标，PFID设计了一整套机制，结合了数据保护技术、隐私保障协议以及高效的推理方法。\n论文概述 随着大语言模型（LLM）在各种应用中（如智能客服、内容生成等）取得突破性进展，越来越多的用户希望利用外部服务提供商的计算资源来进行推理。然而，使用外部计算资源时，如何保护用户数据的隐私成为了一个重要问题，特别是在云计算环境中，用户的输入数据和推理过程可能会暴露给服务提供商，造成隐私泄漏的风险。\nPFID框架通过将推理任务委托给外部计算资源时采用隐私保护技术，确保用户数据在整个推理过程中始终处于受保护的状态，防止敏感信息泄露。\n主要贡献 隐私优先的推理任务委托： PFID的核心思想是确保推理过程中的隐私保护，设计了一种隐私优先的推理任务委托机制。用户可以将LLM的推理任务委托给外部计算资源，但通过加密、隔离等手段，确保输入数据不被泄露给服务提供商。 推理过程中的隐私保护： 论文提出了多种隐私保护技术来保证推理过程中敏感信息不会泄露。这些技术包括但不限于加密技术、同态加密（HE）、安全多方计算（SMPC）等，可以防止外部计算节点访问用户的原始数据。 高效的隐私保护协议： PFID不仅关注隐私保护的强度，还特别关注推理的高效性。它设计了一种高效的隐私保护协议，能够在确保隐私的前提下，最大程度地减少加密和计算过程中的延迟，保证推理任务的快速响应。 多方合作的隐私保护： PFID框架考虑到多方协作的情况，尤其是在分布式计算环境中，多个计算节点可能共同完成推理任务。在这种情况下，PFID提供了隐私保护协议，确保每个节点都无法访问完整的数据，仅能访问必要的计算部分，从而有效降低泄露风险。 方法详解 PFID框架的设计包括以下几个关键技术组件和流程：\n任务分配与委托： 在PFID框架中，用户首先将推理任务分配给外部计算资源（如云计算服务）。这一步骤需要确保在任务委托过程中，用户数据保持加密状态，任何未经授权的计算节点都不能访问原始数据。为此，PFID框架采用了加密协议，即使在云端计算过程中，用户数据也始终保持加密状态。\n隐私保护协议： PFID提出了一种多层加密隐私保护协议，该协议包括以下几个主要组成部分：\n输入数据加密：用户的输入数据（如文本数据）在本地设备上加密后传输到云端进行推理。加密技术可以采用对称加密、非对称加密或同态加密等技术，确保数据在整个推理过程中的安全。 同态加密（Homomorphic Encryption, HE）：同态加密是支持在加密数据上直接计算的加密方式。PFID使用同态加密来确保即使云计算节点无法查看用户数据，也能对加密数据进行推理操作，从而避免数据泄露。 安全多方计算（Secure Multi-party Computation, SMPC）：当多个计算节点共同完成推理任务时，PFID采用SMPC协议来确保每个节点只能获取部分计算结果，不能知道完整的输入数据或中间计算结果，从而有效地防止隐私泄露。 快速推理优化： 为了避免隐私保护过程中产生过多的计算开销，PFID框架设计了一种高效的推理优化机制。该机制通过优化加密计算的过程，减少加密和解密的开销，使得即使在隐私保护的前提下，推理速度也能保持较快。\n具体而言，PFID优化了以下方面：\n加密数据处理优化：通过合理安排加密计算过程中的任务分配，减少了计算资源的浪费。 带宽优化：在数据传输过程中，PFID采用了高效的压缩和加密策略，减少了数据传输的时间和带宽消耗。 计算节点协同：在多方计算的环境下，PFID通过高效的任务分配和同步机制，确保各计算节点能够协同工作，减少等待时间，提高整体推理效率。 隐私监控与反馈机制： PFID框架提供了一个隐私监控和反馈机制，用户可以监控其数据在推理过程中的隐私保护状态。该机制通过日志和报告功能，让用户能够随时了解数据在推理过程中的安全性，确保其隐私得到充分保护。\n隐私保护技术 加密技术： PFID框架使用多种加密技术来保护用户数据： 对称加密：在客户端和服务器之间使用对称加密进行数据传输。 非对称加密：在云计算服务中使用非对称加密来确保数据的安全传输。 同态加密：通过同态加密，保证数据在加密状态下仍然能够被处理而不暴露原始数据。 多方计算（SMPC）： 当推理任务需要多个计算节点协作时，PFID使用多方计算协议，确保每个节点仅能访问一部分计算数据，不会泄露任何敏感信息。 隐私预算： 为了平衡隐私保护和计算效率，PFID框架引入了隐私预算的概念。通过合理设置隐私预算，用户可以调整隐私保护的强度，以适应不同的应用场景和性能要求。 实验与评估 论文通过一系列实验，验证了PFID框架在隐私保护和推理效率方面的表现。实验结果表明：\n隐私保护效果： PFID能够有效保护用户数据的隐私，尤其是在多方计算和加密操作中，避免了数据泄露的风险。 推理效率： 通过优化加密计算和网络传输，PFID在保证隐私的前提下，能够提供快速的推理结果，适用于需要实时响应的应用场景。 可扩展性： PFID框架具有良好的可扩展性，可以在多个云计算服务之间进行部署，并在大规模并发请求的情况下保持高效性能。 总结 《PFID: Privacy First Inference Delegation Framework for LLMs》提出的PFID框架是一个创新的隐私保护推理解决方案，旨在解决在委托外部计算资源进行大语言模型推理时的隐私保护问题。通过结合加密技术、同态加密、SMPC和隐私预算控制，PFID确保了用户数据的隐私不被泄露，同时提供了高效的推理服务。该框架在保护隐私的同时，能够满足实时响应需求，具有广泛的应用潜力，特别是在需要保护用户数据的敏感领域。\n这篇不错，专业对口，之后拿出来细品！\nPrivacyAsst: Safeguarding User Privacy in Tool-Using Large Language Model Agents PrivacyAsst 旨在为使用工具的大语言模型代理提供隐私保护。该系统通过在模型执行过程中加入隐私保护机制，确保用户数据在与外部工具交互时得到有效保护。该方法通过多层次的隐私防护，避免用户数据泄露，特别适用于需要访问外部工具的 LLM 代理应用。\n这篇论文《PrivacyAsst: Safeguarding User Privacy in Tool-Using Large Language Model Agents》主要探讨了在大语言模型（LLM）代理系统中保护用户隐私的技术和方法。LLM代理系统是指能够通过语言模型（如GPT）与用户进行交互并调用外部工具（如搜索引擎、数据库等）来增强其能力的系统。由于这些系统涉及到与外部资源的交互，如何保护用户隐私变得尤为重要。\n论文的主要贡献与结构 背景和动机 现代大语言模型（如GPT等）在任务执行中常常依赖外部工具，例如搜索引擎、数据库访问、API调用等。这种功能使得模型能够扩展其能力，但同时也增加了隐私泄露的风险。用户的数据可能在调用这些工具时泄露，特别是在缺乏适当隐私保护的情况下。论文的动机就是提出一种方法，能够在LLM代理使用工具的过程中保护用户的隐私。\nPrivacyAsst：隐私保护助手 论文提出了一种名为PrivacyAsst的隐私保护框架，它能够在LLM代理系统中保障用户隐私。具体来说，PrivacyAsst通过对模型与工具交互的监控和控制，确保用户的敏感数据不被泄露或不当使用。\n隐私泄露的风险 论文分析了不同类型的隐私泄露风险，尤其是在LLM代理使用外部工具的过程中。例如，用户可能会在与代理的对话中无意中透露敏感信息，而这些信息会通过调用外部工具（如API或数据库）暴露给第三方。隐私泄露的场景包括：\n用户输入的敏感信息被误用。 外部工具无法有效保护数据，导致数据暴露。 大语言模型本身在生成回答时可能泄露用户的私人信息。 PrivacyAsst的工作原理 PrivacyAsst框架的核心机制包括：\n隐私敏感性检测：框架能够自动识别用户输入中可能涉及隐私泄露的敏感信息。通过自然语言处理（NLP）技术，PrivacyAsst能检测出个人身份信息（PII）、财务信息、健康信息等。 信息屏蔽与替换：在检测到敏感信息时，PrivacyAsst会对这些信息进行屏蔽或替换。例如，将用户输入中的具体地址或联系方式用占位符代替，避免在与外部工具的交互中泄露。 工具调用监控：框架还可以监控LLM代理与外部工具的交互，确保敏感信息不被传递到这些工具中。如果需要调用工具但存在隐私泄露风险，PrivacyAsst会阻止该调用或采取适当的保护措施。 用户控制与透明度：用户可以设定隐私保护的策略，决定哪些信息可以被使用，哪些信息需要严格保护。此外，系统也会向用户提供透明的隐私保护措施，使用户能够理解和掌控自己的数据。 实验与评估 论文还进行了实验，评估PrivacyAsst框架的有效性。通过与传统的隐私保护方法进行对比，证明了PrivacyAsst能够有效地减少用户隐私泄露的风险，同时对LLM代理的功能影响最小。\n实验结果表明，PrivacyAsst能够显著降低敏感信息泄露的概率，且能够在不显著降低系统性能的情况下保证隐私保护。\n挑战与未来方向 论文最后讨论了隐私保护技术在工具使用大语言模型中的挑战和未来研究方向。这些挑战包括：\n如何应对更加复杂的隐私攻击，特别是在多方交互的情况下。 如何平衡隐私保护与系统的功能性，避免过度限制工具的使用。 如何设计更加智能和自适应的隐私保护系统，以应对不断变化的威胁模型和用户需求。 总结 《PrivacyAsst: Safeguarding User Privacy in Tool-Using Large Language Model Agents》提出了一个针对大语言模型代理使用外部工具过程中的隐私保护框架。通过智能检测、信息屏蔽与替换、工具调用监控以及用户控制，PrivacyAsst能够有效防止用户敏感信息泄露。论文的贡献在于提供了一种实用的隐私保护方法，能够在增强LLM功能的同时，确保用户数据的安全与隐私。\n这个挺有意思，防止LLM在用工具的时候给自己隐私泄露了，但就感觉怪怪的\n隐私敏感性检测和信息屏蔽方法已经在隐私保护领域使用多年，类似的技术早已被应用于各种隐私保护场景中。\n工具调用监控部分也是一种常见的技术，通常通过API网关或中间件进行操作，来确保数据不被误用。\n实际应用的局限性：外部工具的监控和数据屏蔽是有限的，很多隐私泄露的风险实际上可能来自于模型内部的知识泄漏，而不仅仅是外部工具的调用。隐私Asst没有讨论如何解决这些问题。\nPrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration 本文提出了PrivacyRestore，一种在推理过程中进行隐私移除和恢复的隐私保护方法。通过在推理的各个阶段移除敏感数据，并在推理完成后恢复数据的完整性，PrivacyRestore 能够有效保护用户隐私，同时保证推理结果的准确性和一致性。\n这篇论文《PrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration》提出了一种名为PrivacyRestore的方法，用于在大语言模型（LLM）的推理过程中保护用户隐私。论文的目标是解决当前在大语言模型应用中，尤其是用户与模型交互时，隐私泄露的潜在风险。作者提出的方法通过隐私去除和隐私恢复技术，确保用户的敏感信息在推理过程中不被泄露，同时又不影响模型的预测性能。\n论文的主要贡献与结构 背景和动机 随着大语言模型的广泛应用，如何保护用户隐私在实际应用中变得至关重要。大语言模型的强大能力依赖于它们处理和生成大量信息，这些信息可能包括用户的个人数据。如果模型在推理时泄露了用户的私人信息，可能导致严重的隐私泄露问题。现有的隐私保护方法，如差分隐私，通常无法完全满足大语言模型推理过程中的隐私需求，尤其是当模型需要高质量的推理结果时。\n论文的动机是提出一个新的隐私保护框架，通过去除敏感信息并在推理后恢复隐私，解决现有隐私保护方法在大语言模型中的局限性。\nPrivacyRestore：隐私去除和恢复 PrivacyRestore框架的核心思想是，在大语言模型的推理过程中，将用户的敏感信息从输入中去除，以防止在推理过程中泄露隐私。推理完成后，再将这些去除的隐私信息恢复回来，使得模型的输出既可以确保隐私保护，又能保持推理的准确性和相关性。\nPrivacyRestore框架可以分为两个主要部分：\n隐私去除：在模型进行推理之前，对用户输入进行处理，去除其中的敏感信息。敏感信息可能包括个人身份信息（PII）、地址、电话、银行账户等。去除这些敏感信息的目的是避免模型在推理过程中接触到这些数据，从而降低泄露的风险。 隐私恢复：在推理完成后，恢复去除的隐私信息，以确保模型的输出仍然与原始输入保持一致，而不影响推理的质量。 隐私去除与恢复的技术细节 隐私去除：隐私去除依赖于预处理技术，使用自然语言处理（NLP）方法来识别和去除输入文本中的敏感信息。具体来说，模型会通过实体识别（Named Entity Recognition, NER）技术或其他上下文分析方法识别出敏感内容，并将其替换为占位符或完全去除。例如，用户输入中提到的地址或电话号码会被替换成“[ADDRESS]”或“[PHONE]”等占位符。 隐私恢复：隐私恢复的核心是在推理过程完成后，基于模型输出的信息，恢复去除的敏感信息。这一过程会根据模型的推理结果以及输入中去除的敏感信息位置进行调整。恢复方法需要确保敏感信息的插入不会改变模型的推理结果，同时保持隐私信息的完整性。 隐私保护与推理质量的平衡 论文还探讨了隐私保护与推理质量之间的平衡问题。隐私去除和恢复可能对模型的性能产生影响，特别是当敏感信息对于模型的理解至关重要时。作者提出了一种优化策略，通过对去除的信息进行分析，确保隐私保护不会过度损害推理结果的准确性。实验表明，通过精细设计的隐私去除和恢复机制，PrivacyRestore能够在保护隐私的同时，最大限度地保留模型的推理能力。\n实验与评估 论文进行了广泛的实验评估，验证了PrivacyRestore方法的有效性。实验结果表明，在采用PrivacyRestore框架后，模型在推理过程中能够显著降低隐私泄露的风险。与传统的隐私保护方法（如直接删除敏感信息或使用差分隐私）相比，PrivacyRestore在确保隐私安全的同时，能够保持较高的推理质量。\n具体的评估方法包括：\n隐私泄露评估：评估隐私泄露的概率，特别是在模型推理时是否会泄露去除的敏感信息。 推理性能评估：通过比较模型在隐私保护前后的推理精度、召回率等指标，评估隐私保护措施对模型性能的影响。 用户体验评估：通过用户调查等方式，评估隐私保护措施对用户体验的影响，确保模型在隐私保护的同时仍能提供用户期望的服务。 挑战与未来方向 论文最后讨论了PrivacyRestore框架的挑战和未来研究方向：\n去除与恢复的精度问题：如何更加精确地去除和恢复敏感信息，尤其是在面对复杂的自然语言输入时。当前的技术可能会对一些细节进行过度简化，从而影响恢复的准确性。 多模态隐私保护：随着多模态模型（包括文本、图片、视频等）逐渐普及，隐私保护的挑战将更加复杂，如何在多模态推理中实现隐私保护是一个重要的研究方向。 个性化隐私保护策略：不同用户的隐私需求不同，如何根据用户的隐私偏好设计个性化的隐私保护策略，将是未来研究的一个重要方向。 总结 《PrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration》提出了一个创新的隐私保护框架，通过在推理过程中去除敏感信息并在结果输出后恢复隐私，解决了大语言模型在实际应用中可能存在的隐私泄露问题。该框架不仅保护了用户隐私，还尽可能地保留了模型的推理能力，实验结果证明其有效性和实用性。论文为隐私保护技术在大语言模型中的应用提供了新的思路，同时也指出了未来的研究方向。\n这个我也不敢妄加评论，但就是疑惑，谁没事干把自己的隐私信息放进去推理，自己的个人信息放进去就能推理的更准确吗？到头来你把隐私信息找出来用占位符替换，最后再给人换回去，有啥用啊？\n隐私去除已经是隐私保护领域的常见做法，很多NLP系统都通过识别和去除敏感信息来保护用户隐私。\n隐私恢复的想法也并不是非常创新。恢复去除的敏感信息通常是通过占位符来实现，虽然恢复的准确性可能是一个挑战，但方法并没有提供特别具有突破性的技术细节。\n计划 论文看得多以后会有一点点感悟，有用的论文看不懂，没用的看了想笑，确实没用，估计我以后也会加入吧，再接再厉吧！\n","date":"2024-12-15T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC8%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/flowers-8046973_1280_hu1689836452074278793.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC8%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/","title":"第8周工作总结"},{"content":"矢量隐私线性评估 VOLE 矢量隐私线性评估 (Vector Oblivious Linear Evaluation, 简称 VOLE) 是一种密码学工具，用于高效实现隐私保护的线性运算协议。VOLE 的目标是允许两个或多个参与方在彼此输入保持私密的前提下，协同完成某些线性运算任务。这种方法在安全多方计算 (MPC) 和私有推理中非常有用。\n核心概念： VOLE 通常涉及两方，分别称为发送方和接收方：\n发送方： 输入一个矢量 aa 和一个矢量 bb。 发送方知道这些值，并且希望接收方在不直接得知 aa 和 bb 的情况下，能够执行特定的线性操作。 接收方： 输入一个矢量 xx。 通过与发送方交互，接收方得到一个结果 y=ax+by = ax + b，而不会暴露自己的输入 xx。 隐私性： 发送方不会知道接收方的输入 xx。 接收方不会知道发送方的输入 aa 和 bb，只获得 ax+bax + b 的结果。 高效性： VOLE 协议的设计特别注重计算和通信效率，可以用于处理大规模矢量或矩阵运算。 工作原理： VOLE 协议通常依赖于预分布的密钥材料或使用更高级的密码技术（如差分隐私、同态加密或秘密共享）来实现：\n初始化阶段： 协议开始前，参与方可能会预共享一些随机密钥或生成关联的共享值，用于后续运算。 评估阶段： 发送方和接收方通过加密通信执行一个安全协议。 接收方从协议中获得目标值 ax+bax + b。 实际例子： 假设我们有一个广告推荐系统，其中广告平台想根据用户的兴趣分数 xx 计算推荐的广告得分 y=ax+by = ax + b，但双方不希望透露自己的隐私信息。\n发送方（广告平台）\n：\n输入参数 a=[2,3]a = [2, 3] 和偏置 b=[5,−1]b = [5, -1]。 接收方（用户）\n：\n输入兴趣向量 x=[4,7]x = [4, 7]。 通过 VOLE 协议：\n广告平台和用户执行交互，平台不知道用户的兴趣 xx，用户也不知道平台的权重 aa 和偏置 bb。 协议输出： 用户收到结果 y=ax+b=[2⋅4+5,3⋅7−1]=[13,20]y = ax + b = [2 \\cdot 4 + 5, 3 \\cdot 7 - 1] = [13, 20]。 广告平台无法得知用户的兴趣向量，用户也无法获知广告平台的参数 aa 和 bb。 应用场景： 私有机器学习推理：比如允许用户在不泄露其私有数据的前提下，利用云端模型进行推理。 安全多方计算：如在金融或医疗场景中，多个机构合作完成隐私保护的统计分析。 隐私保护的广告投放：用于确保用户的兴趣和广告平台的策略均不暴露。 这种工具能够平衡效率与隐私，在当前注重隐私的计算任务中具有重要的应用价值。\n基于样条（spline）的 GELU 近似方法 基于样条（spline）的 GELU 近似方法是一种高效且安全的计算方法，用于近似 GELU 激活函数。样条方法通过将复杂的非线性函数分解为多个线性或低阶多项式函数片段来实现快速计算，同时保持较高的精度。\nGELU 函数的定义: GELU（Gaussian Error Linear Unit）的数学定义是：\n$\\text{GELU}(x) = x \\cdot \\Phi(x)$\n其中 $\\Phi(x)$ 是标准正态分布的累积分布函数：\n$\\Phi(x) = \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]$\n由于 $\\text{erf}$（误差函数）和 $\\Phi(x)$ 的计算涉及指数函数和积分运算，其直接计算开销较大。\n基于查找表的传统方法： 传统方法通过以下步骤实现 GELU 的安全计算：\n使用查找表（LUT）近似 $e^{-x}$。 用另一张查找表近似指数函数的倒数。 每一步都需要扩展或截断位宽，以在精度和效率之间找到平衡。 这种方法尽管有效，但需要多个查找表和逐步逼近，计算流程较复杂。\n基于样条的 GELU 近似： 基于样条的方法通过分段线性或分段多项式逼近的方式，用更简单的运算替代传统的复杂计算。以下是关键步骤：\n1. 样条分段： 将输入x 的值域划分为多个区间，例如 [−3,−2][-3, -2], [−2,−1][-2, -1], [1,2][1, 2] 等。 在每个区间内，用一个简单的函数（通常是一阶或二阶多项式）来近似 GELU 的值。 2. 低阶多项式近似： 在每个区间 [xi,xi+1][x_i, x_{i+1}] 中，使用线性或二次样条拟合 GELU 函数： $\\text{GELU}(x) \\approx a_i x^2 + b_i x + c_i, \\quad x \\in [x_i, x_{i+1}]$ 这些系数 $a_i, b_i, c_i$ 是通过离线拟合和训练得到的，确保逼近误差最小。 3. 高效计算： 在线计算时，根据输入值 xx 找到对应的分段区间，并应用对应的多项式计算 $\\text{GELU}(x)$。 样条方法仅需简单的加减乘除操作，大大减少了计算复杂度。 相较传统方法的优点： 效率高：样条方法不需要查找多个查找表，也不涉及指数和倒数计算，适合硬件实现。 精度可控：通过增加分段数或使用更高阶的样条，可以灵活调整逼近误差。 更安全：在安全计算环境中，样条方法减少了中间复杂运算的泄露风险。 一个具体例子： 假设我们采用三段分段样条，分段区间为 [−2,−1][-2, -1], [−1,0][-1, 0], 和 [0,2][0, 2]，并拟合出以下二次多项式：\n对于 [−2,−1][-2, -1]:\n$GELU(x)≈0.1x2+0.5x+0.2\\text{GELU}(x) \\approx 0.1x^2 + 0.5x + 0.2$\n对于 [−1,0][-1, 0]:\n$GELU(x)≈−0.2x2+0.7x+0.1\\text{GELU}(x) \\approx -0.2x^2 + 0.7x + 0.1$\n对于 [0,2][0, 2]:\n$GELU(x)≈0.3x2+0.4x+0.3\\text{GELU}(x) \\approx 0.3x^2 + 0.4x + 0.3$\n在线计算时：\n如果 x = -1.5，它属于区间 [−2,−1][-2, -1]，代入第一个多项式： $GELU(−1.5)=0.1(−1.5)2+0.5(−1.5)+0.2=0.025\\text{GELU}(-1.5) = 0.1(-1.5)^2 + 0.5(-1.5) + 0.2 = 0.025$ 如果x = 0.5，它属于区间 [0,2][0, 2]，代入第三个多项式： $GELU(0.5)=0.3(0.5)2+0.4(0.5)+0.3=0.725\\text{GELU}(0.5) = 0.3(0.5)^2 + 0.4(0.5) + 0.3 = 0.725$ 应用场景： 硬件实现：适合加速器或 FPGA 等场景，实现高效的激活函数计算。 隐私计算：在多方计算（MPC）或同态加密中，样条方法可减少计算复杂性和通信量。 大模型推理：在内存和计算受限的设备上提高推理效率。 这种方法已经被证明能够有效平衡精度和效率，是安全计算中 GELU 近似的一种重要手段。\nSIMD（Single Instruction, Multiple Data）技术 SIMD（Single Instruction, Multiple Data）技术，是指一种通过在同一时刻对多个数据元素执行相同操作的计算技术。SIMD 允许处理器在单条指令下并行地处理多个数据，从而提高计算效率和吞吐量，尤其在执行向量化操作时能够显著加速计算过程。\nSIMD 技术的基本概念： SIMD 是一种并行计算模型，它允许在同一条指令下对多个数据元素进行批量操作。常见的 SIMD 指令集包括 Intel 的 SSE、AVX，以及 ARM 的 NEON，它们都能够将多个数据点（例如 4 个、8 个、16 个数字）装入一个寄存器，并通过单条指令同时对这些数据执行相同的操作（例如加法、乘法等）。\nSIMD 在同态加密中的应用： 在同态加密（Homomorphic Encryption, HE）中，SIMD 技术用于优化对加密数据的批量处理。比如，假设有一个密文表示的加密数据集，SIMD 可以将这些密文批量处理到单个密文中，从而摊销计算成本，提高处理速度。\n在这类加密方案中，例如 RLWE（Ring Learning With Errors） 加密方案，SIMD 可以并行地对多个加密的消息执行相同的操作，如加法、乘法等，从而加速加密数据的计算过程。例如，多个密文的处理可以通过一次向量化操作一次性完成，而不是逐个处理，从而提高效率。\n问题中的“同态旋转”： “同态旋转”指的是在同态加密中，对密文进行的某些操作（如加密数据的旋转或重新排列），在 RLWE 等加密方案中常常是通过复杂的同态操作来实现的。这种旋转操作通常涉及对加密数据结构（如向量或多项式）的某些变换。\n同态旋转的成本高：同态加密中的旋转操作非常昂贵，因为它需要对加密数据进行复杂的同态运算，同时保持加密数据的隐私性。 摊销成本：SIMD 技术通过并行处理大量数据，可以在某种程度上摊销这些昂贵的计算成本。也就是说，SIMD 让我们能够在执行加密运算时，将多个计算合并为一个操作，减少每次计算的总消耗。 然而，由于 同态旋转 操作的复杂性，这部分操作的计算仍然是同态加密中性能瓶颈之一，尤其是在需要执行多个旋转和聚合操作时，仍然需要付出较高的计算开销。\n总结： 在加密算法中，SIMD 技术通过批量并行处理多个数据元素，能够提高对加密数据（例如 RLWE 密文）的处理效率。而同态旋转操作在同态加密中是一个复杂的操作，通常需要昂贵的计算资源。尽管 SIMD 技术可以在一定程度上摊销这一成本，但旋转操作仍然是一个计算瓶颈，导致性能受到一定限制。\n","date":"2024-12-09T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E5%AF%86%E7%A0%81%E5%AD%A6%E9%83%A8%E5%88%86%E9%97%AE%E9%A2%982/wave-7726187_1280_hu1193120050624582333.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E5%AF%86%E7%A0%81%E5%AD%A6%E9%83%A8%E5%88%86%E9%97%AE%E9%A2%982/","title":"密码学部分问题2"},{"content":"总结 最近运气不太好啊，生病连着生病，两周感冒刚好，准备重新出发了，又不知道为何整出来荨麻疹了，一晚一晚的睡不着觉，很是头大啊。\n上次和学长讨论完，要找一个细致的方向，但最近看了看论文，发现越来越难了，很多细节都不懂，比如密码学的一些东西，感觉需要恶补很多东西啊，不然基础的不懂，他们整体构筑起来的论文我就有点看不懂。\n这周看了一个BumbleBee: 大型变换器模型的安全双方推理框架，感觉有一点点感悟，但不多，由于生病折磨，这周就没怎么看，感觉缺的背景知识有点多，需要拿出时间来补一补。\n还看了一个论文：InferDPT：面向黑盒大语言模型的隐私保护推理，这个比较好理解，但我却想不到，想来也是因为缺少背景知识的缘故，他把要发送的文档进行差分隐私发过去，把gpt的结果拿过来给自己的模型，再生成一次结果，这样就保证了大模型一方拿不到完整数据，而且有不错的效率保证。\nCipherGPT：安全的两方 GPT 推理还没看完\n计划 继续看看论文，补一补背景知识。\n","date":"2024-12-08T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC7%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/leaves-5598709_1280_hu2639622986206436163.png","permalink":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC7%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/","title":"第7周工作总结"},{"content":"Hadamard product是什么？ Hadamard product（哈达玛积），也称为元素乘积，是指两个相同维度的矩阵或向量之间的逐元素相乘。与矩阵乘法不同，Hadamard积不涉及矩阵的行列运算，而是直接对对应位置的元素进行相乘。\n如果有两个矩阵 $A = [a_{ij}]$ 和$B = [b_{ij}]$，它们的Hadamard积$C = A \\circ B$ 是一个同样大小的矩阵，其中每个元素 $c_{ij} = a_{ij} \\cdot b_{ij}$，即： $$\rC = A \\circ B = \\begin{bmatrix} a_{11} \\cdot b_{11} \u0026 a_{12} \\cdot b_{12} \u0026 \\dots \u0026 a_{1n} \\cdot b_{1n} \\\\ a_{21} \\cdot b_{21} \u0026 a_{22} \\cdot b_{22} \u0026 \\dots \u0026 a_{2n} \\cdot b_{2n} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ a_{m1} \\cdot b_{m1} \u0026 a_{m2} \\cdot b_{m2} \u0026 \\dots \u0026 a_{mn} \\cdot b_{mn} \\end{bmatrix}\r$$ Hadamard积常用于各种机器学习和深度学习的操作，如神经网络中的门控机制（例如LSTM和GRU），以及图像处理中的卷积操作等。\n注意，Hadamard积仅适用于两个维度相同的矩阵或向量。\n什么是秘密共享? 秘密共享（Secret Sharing）是一种密码学技术，用于将一个秘密（如密钥、密码或敏感数据）分割成多个部分，并将这些部分分发给不同的参与者。只有在一定数量的参与者合作时，才能恢复原始的秘密。通过这种方式，秘密共享可以增强安全性，因为即使某些参与者的部分被泄露或丢失，其他部分仍然无法恢复完整的秘密。\n主要类型 阈值秘密共享（Threshold Secret Sharing） 这种方法要求至少$t$ 个参与者合作才能恢复秘密。每个参与者得到一个秘密份额，只有在$t$ 个或更多参与者联合时，才能重构出原始秘密。\nShamir’s Secret Sharing\n：这是最著名的阈值秘密共享方案，由Adi Shamir在1979年提出。其核心思想是通过多项式插值来实现秘密的分割和恢复。\n假设原始秘密是一个 ss，可以选择一个 $(t−1)$ 次多项式 $f(x)$，其中常数项 $f(0) = s$ 即为秘密。 然后根据多项式生成 $n$ 个不同的点分发给 $n$ 个参与者。 任何 $t$ 个参与者可以使用拉格朗日插值法恢复原始秘密。 对称秘密共享 在这种方法中，每个参与者获得一个共享密钥，这些密钥通过某种方式结合来恢复原始秘密。常见的算法包括加密技术、异或操作等。\n加密的秘密共享 这种方法利用加密技术对秘密进行分割，确保即使单个参与者泄露了他们的份额，其他人也无法访问秘密。例如，通过同态加密（homomorphic encryption）或基于门限加密的秘密共享方案。\n应用场景 密钥管理：通过将密钥分散到多个地点或参与者之间，提高密钥管理的安全性。例如，如果加密系统的密钥被分为多个部分，只有通过参与者之间的协作才能解密数据。 防止单点故障：秘密共享技术可以防止某个单一参与者或设备故障导致秘密丢失。通过适当选择阈值，可以确保即使部分参与者不可用，剩余的参与者仍能恢复秘密。 分布式计算：在分布式环境中，多个节点可以合作进行秘密的计算或解密，而不需要暴露各自的数据。例如，在加密计算或保密计算（Secure Multi-Party Computation, MPC）中，多个计算节点共同参与计算秘密，但每个节点仅持有部分数据，保护了每个节点的隐私。 区块链与去中心化系统：在去中心化系统中，秘密共享可用于增强安全性和隐私保护。例如，在某些区块链协议中，多个节点可以共同持有私钥的份额，用于签名交易或执行智能合约，只有满足一定数量的节点签名才能完成操作。 优点和缺点 优点：\n增强安全性：通过分割秘密，即使某些份额泄露，其他人也无法恢复原始秘密。 容错性：可以容忍一定数量的参与者失效或泄露信息，只要满足阈值条件，秘密仍能恢复。 灵活性：可以根据需求调整参与者数量和阈值。 缺点：\n参与者管理复杂：需要管理和协调多个参与者。 计算成本：某些秘密共享方案，如Shamir的方案，可能需要额外的计算来分割和恢复秘密。 通信开销：分发和恢复秘密可能需要较大的通信开销，尤其是在参与者数量多时。 总的来说，秘密共享技术是提升系统安全性和隐私保护的重要工具，广泛应用于多种安全和加密场景中。\n任何 ttt 个参与者可以使用拉格朗日插值法恢复原始秘密,这是怎么做到的? 要理解如何通过拉格朗日插值法来恢复原始秘密，首先我们需要了解Shamir的秘密共享的工作原理。简单来说，Shamir的秘密共享方案通过将秘密转换为一个多项式的常数项，并通过给定的点分发给参与者。在恢复秘密时，tt 个参与者可以使用这些点和拉格朗日插值法来重建原始多项式，从而恢复秘密。\nShamir的秘密共享原理 构造多项式：首先，选择一个秘密 ss 作为常数项，然后构造一个 $(t-1)$ 次的多项式 $f(x)$，使得：\n$f(x) = a_0 + a_1 x + a_2 x^2 + \\dots + a_{t-1} x^{t-1}$\n其中，$f(0) = a_0 = s$ 为秘密。其余的系数 $a_1, a_2, \\dots, a_{t-1}$ 是随机选择的。\n生成份额：然后，选择 $n$ 个不同的 xx 值（例如$x_1, x_2, \\dots, x_n$），并计算对应的 $f(x_i)$ 值。每个参与者将收到一对$(x_i, f(x_i))$，即他们的“份额”。这些份额分别分配给 $n$ 个参与者。\n重建秘密：任何 $t$ 个份额就足以恢复原始秘密。通过拉格朗日插值法，可以使用这 $t$ 个点（$x_i, f(x_i)$）来求解多项式的常数项 $f(0)$，从而恢复秘密。\n拉格朗日插值法 拉格朗日插值法是一个用于构造多项式的数学方法，给定一组数据点$(x_1, y_1), (x_2, y_2), \\dots, (x_t, y_t)$，我们可以构造一个唯一的多项式，使得它通过所有这些点。\n在Shamir的秘密共享中，我们有$t $个点 ($(x_i, f(x_i)$)，要恢复秘密 $f(0)$ 即常数项，我们可以通过以下拉格朗日插值公式来计算：\n$f(0) = \\sum_{i=1}^{t} f(x_i) \\cdot \\ell_i(0)$\n其中，$\\ell_i(x)$ 是第 $i$ 个拉格朗日基多项式，定义如下：\nℓ$\\ell_i(x) = \\prod_{\\substack{1 \\leq j \\leq t \\ j \\neq i}} \\frac{x - x_j}{x_i - x_j}$\n为了恢复 $f(0)$，我们将 $x = 0$ 代入上式：\n$\\ell_i(0) = \\prod_{\\substack{1 \\leq j \\leq t \\ j \\neq i}} \\frac{0 - x_j}{x_i - x_j} = \\prod_{\\substack{1 \\leq j \\leq t \\ j \\neq i}} \\frac{-x_j}{x_i - x_j}$\n然后，通过上面得到的每个 $\\ell_i(0)$ 值，乘上对应的 $f(x_i)$，并将所有结果求和，就能得到 $f(0)$，即原始的秘密。\n例子 假设有三个参与者（$t = 3$），他们分别获得了以下份额：\n$(x_1 = 1, f(x_1) = 5)$ $(x_2 = 2, f(x_2) = 8)$ $(x_3 = 3, f(x_3) = 11)$ 我们要恢复秘密 $f(0)$，即常数项。\n计算每个 $\\ell_i(0)$：\n$\\ell_1(0) = \\frac{(0 - 2)(0 - 3)}{(1 - 2)(1 - 3)} = \\frac{6}{2} = 3$\n$\\ell_2(0) = \\frac{(0 - 1)(0 - 3)}{(2 - 1)(2 - 3)} = \\frac{3}{-1} = -3$\n$\\ell_3(0) = \\frac{(0 - 1)(0 - 2)}{(3 - 1)(3 - 2)} = \\frac{2}{2} = 1$\n计算秘密：\n$f(0) = f(x_1) \\cdot \\ell_1(0) + f(x_2) \\cdot \\ell_2(0) + f(x_3) \\cdot \\ell_3(0)$\n$f(0) = 5 \\cdot 3 + 8 \\cdot (-3) + 11 \\cdot 1 = 15 - 24 + 11 = 2$\n因此，原始的秘密 $f(0) = 2$。\n总结 通过拉格朗日插值法，$t$ 个参与者的份额可以恢复原始的秘密。该过程利用了每个参与者的份额和拉格朗日基多项式的计算，以重建出秘密所对应的多项式，并提取出常数项。\n2-out-of-2 加法秘密共享方案是什么？ 2-out-of-2加法秘密共享方案是一种非常基础的秘密共享方案，专门用于将一个秘密分配给两个参与者，并且要求两个参与者必须共同合作才能恢复秘密。在这种方案中，秘密通过加法分割为两个部分，每个参与者都持有一个秘密份额，只有两个参与者联合时才能恢复原始秘密。\n工作原理 在2-out-of-2加法秘密共享方案中，假设原始秘密为 $s$，我们将这个秘密分割成两个部分，分别给两个参与者。我们使用以下方法进行分割：\n假设原始秘密是 $s$，我们随机选择一个数 rr 作为第一个份额，计算第二个份额 $s\u0026rsquo; = s - r$。 然后，将这两个份额分别分配给两个参与者： 参与者1收到份额 $r$ 参与者2收到份额 $s\u0026rsquo; = s - r$ 恢复秘密 为了恢复秘密 $s$，两个参与者需要将他们的份额相加：\n$s = r + (s - r)$\n通过这种方式，两个参与者的份额加在一起，恢复出原始的秘密 $s$。\n特点 加法操作：在这个方案中，秘密的分割和恢复都基于加法。每个参与者只知道一个加法部分，只有通过相加才能恢复出原始秘密。 完全安全：如果只有一个参与者知道自己的份额，那么他无法单独恢复秘密，因为他只有一个部分，缺少了另一个部分的信息。 简单实现：这是一个非常简单和高效的秘密共享方案，适用于只有两个参与者的场景。 例子 假设有一个秘密 $s = 100$，我们将其分配给两个参与者。\n随机选择一个份额 $r = 30$，那么第二个份额 $s\u0026rsquo; = 100 - 30 = 70$。\n参与者1得到份额$r = 30$，参与者2得到份额 $s\u0026rsquo; = 70$。\n为了恢复秘密，参与者1和参与者2将他们的份额相加：\n$s = 30 + 70 = 100$\n应用 2-out-of-2加法秘密共享可以应用于很多场景，尤其是需要确保两个参与者必须合作才能访问某个秘密的情况。例如：\n安全存储密钥：将加密密钥分成两个部分，分别存储在不同的地点，只有两个存储位置都可访问时，密钥才可以恢复。 两方合谋：在某些加密协议中，两个参与者需要共同合作解密数据，单独任何一个参与者都无法获得数据的任何信息。 总结 2-out-of-2加法秘密共享方案通过将秘密分割成两个部分，确保只有在两个参与者合作的情况下才能恢复原始秘密。这种方案简单易实现，但仅适用于参与者数量为2的场景。如果需要更高的容错能力（比如 t-out-of-n 的方案），则需要使用更复杂的算法，如Shamir的秘密共享。\noblivious transfer是什么？ **Oblivious Transfer（OT，盲传输）**是一种重要的加密协议，广泛应用于多方计算（Secure Multi-Party Computation, MPC）和隐私保护计算领域。其基本思想是在两方或多方之间进行信息传输时，保证发送方和接收方的隐私。特别地，Oblivious Transfer 保护了发送方的信息不被泄露给接收方，而接收方也不会知道其他的消息，只有选择的信息才能被接收。\nOblivious Transfer的基本概念 在OT协议中，存在两个主要参与方：\n发送方（Sender）：拥有若干个消息，并希望将其中一个消息传递给接收方，但不希望接收方得知其他消息。 接收方（Receiver）：接收方可以选择想要接收的消息，并且只会获得它所选择的消息，但发送方无法知道接收方选择了哪一个。 有几种常见的Oblivious Transfer类型，其中最常见的有以下两种：\n1. 1-out-of-2 Oblivious Transfer (1-2 OT) 在1-out-of-2 OT协议中，发送方有两个消息 $m_0$ 和 $m_1$，接收方选择一个索引（例如 0 或 1），然后接收方获得选择的消息，而发送方并不知道接收方选择了哪个消息。\n工作流程： 发送方将两个消息 $m_0 $和 $m_1 $发送给接收方，但不直接告诉接收方哪个消息是哪个。 接收方选择一个索引（0 或 1），并且只获得对应索引的消息。接收方不会知道其他的消息内容。 发送方无法知道接收方选择了哪一个消息，接收方只能获得所选的消息。 安全性：\n发送方隐私性：发送方无法知道接收方选择了哪个消息。 接收方隐私性：接收方无法知道未选择的消息，也无法从其它信息中推测出未选的消息。 2. k-out-of-n Oblivious Transfer (k-n OT) 在k-out-of-n OT协议中，发送方有 n 个消息$m_1, m_2, \u0026hellip;, m_n$，接收方可以选择 k 个消息，并且获得这 k 个消息，但不透露选择的其他 n-k 个消息。接收方只会知道自己选择的 k 个消息，而发送方不会知道接收方选择了哪些消息。\nOblivious Transfer的应用 安全多方计算（MPC）：在多方计算中，OT协议用于保护各方的隐私，使得各方可以在不知道对方输入的情况下共同计算某个函数的结果。例如，在两个公司进行联合数据分析时，可以使用OT协议来确保各自的数据不被泄露。 私密信息检索（Private Information Retrieval, PIR）：PIR是指用户在不透露查询内容的情况下，从数据库中检索信息。OT协议可以用于构建PIR方案，确保查询者的隐私得到保护。 数字货币和区块链：OT协议也可以用于增强区块链系统中的隐私保护，尤其是在匿名交易中。它可以让交易双方在不暴露其具体信息的情况下，完成加密验证和交易。 加密货币钱包：例如，在一些加密货币钱包中，OT协议可用于确保钱包私钥的安全转移，同时防止第三方在转移过程中获得不必要的敏感信息。 OT协议的实际构建 OT协议可以通过不同的加密技术实现，常见的方式包括：\n公钥加密：例如使用 RSA 加密或椭圆曲线加密来实现 OT 协议。 同态加密：可以通过同态加密协议来构建OT协议，以便在加密状态下进行数据传输。 零知识证明：通过零知识证明机制，保证消息传输过程中各方的信息不被泄露。 总结 **Oblivious Transfer（盲传输）**是一种重要的加密协议，它保证了信息在传输过程中各方的隐私性。特别地，发送方和接收方都不会得知对方的敏感信息，只有接收方能够获得自己所选择的消息。这一协议被广泛应用于多方计算、私密信息检索、数字货币隐私保护等领域，是现代密码学和隐私保护技术的核心组件之一。\n秘密共享中使用的 $2^l$ 模数是什么？ 在秘密共享（Secret Sharing）方案中，特别是在 Shamir 的秘密共享协议中，$2^l$ 模数通常指的是一个大素数或一个足够大的整数，用于确保秘密能够在模运算中适当地加密和解密。在这种情况下，$2^l$ 表示对一个长度为 $l$ 的二进制数据进行处理时所需要的模数。\n具体来说，$l$ 是密钥或秘密的比特长度，因此 $2^l$ 是一个足够大的数，通常用于分割秘密的范围。例如，在 Shamir 的方案中，秘密通常被视为一个 $l$ 比特的数值，而将其分割成多个份额时，使用 $2^l$ 来确保数值保持在所需范围内。这也意味着，每个份额的大小由 $2^l$ 限定，从而保证了秘密的安全性和正确的恢复。\n简而言之，$2^l$ 模数的使用帮助在加密和共享过程中处理大的整数，确保秘密在分发和恢复时不被泄露，同时也方便进行数学运算。\n","date":"2024-12-02T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E5%AF%86%E7%A0%81%E5%AD%A6%E9%83%A8%E5%88%86%E9%97%AE%E9%A2%98/wave-7726187_1280_hu1193120050624582333.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E5%AF%86%E7%A0%81%E5%AD%A6%E9%83%A8%E5%88%86%E9%97%AE%E9%A2%98/","title":"密码学部分问题"},{"content":"总结 这一周又没干啥正经事，学科前沿讲座结课了，整了一下结课论文，把软件工程实训汇报完了，有惊无险，但却有点轻微感冒+上火+口腔溃疡，痛不欲生，虽已习惯了高三时期留下的旧疾，但得了口腔溃疡还是痛。\n这周又读了几篇论文，还是没有找到与我要研究东西特别相近的论文，都是关于深度学习的，没有大模型这方面的，这周继续探索一下。\n计划 计划就是完成上周以及上上周未完成的计划，继续干\n\u0026ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption\u0026rdquo;\n该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。 \u0026ldquo;Federated Learning with Encrypted Data\u0026rdquo;\n这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。 \u0026ldquo;Secure Inference with Deep Learning Models\u0026rdquo;\n该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。 \u0026ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning\u0026rdquo;\n本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。 \u0026ldquo;Secure and Private Inference of Deep Learning Models\u0026rdquo;\n这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。 \u0026ldquo;Privacy-Preserving Machine Learning: A Practical Approach\u0026rdquo;\n本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。 \u0026ldquo;Secure Outsourced Computation in Machine Learning\u0026rdquo;\n这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。 \u0026ldquo;Differentially Private Inference in Machine Learning\u0026rdquo;\n本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。 \u0026ldquo;Towards Secure and Private Machine Learning: A Survey\u0026rdquo;\n这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。 \u0026ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning\u0026rdquo;\n该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。 \u0026ldquo;Secure and Private Model Inference: A Survey\u0026rdquo;\n本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。 ","date":"2024-11-17T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC6%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/ai-generated-8787240_1280_hu8301059065001962622.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC6%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/","title":"第6周工作总结"},{"content":"🎵总结 这一周没干啥正经事，周一周二库库读了两天论文，初步了解了一下安全推理的基础知识（就看了一点点），周三图书馆闭馆，晚上上课，所以周三休息一天，周三下午参加了理想汽车AI算法实习生的面试，想找一个这种实习边干活边学习，可惜这并不是啥也不会就能行的，还得学习，沉淀一下。一开始挺紧张的，在脑海里思考了好多遍我该怎么解释，表现自己，结果面试官让我开始自我介绍，他就一只在笑😁（牙很齐很白），不知道他在笑什么，但是看他笑得那么开心我就不紧张了，问了一下transformer为什么要加入位置编码来表示位置信息，LLM微调除了LoRA还有什么，还了解那些大模型，一个也不会，哈哈哈，真不是啥也不会就可以找工作的，不过此行并非失败，而是挺有收获的，面试官一直说，没关系，不会咱就换一个，下去把这个搞清楚就行了，搞得我有点不好意思，在问问题的过程中，面试官还顺带给我解释解释，增长了一些知识，最后我反问到该怎么系统的学习一下才能找到实习（我真是个鬼才🤣，问面试官这问题）\n面试官说：建议我先不要着急找面试，用几个月学习一下，把基础的机器学习和深度学习过一遍，代码复现都能看懂，还问了我研究生方向是大模型安全，看看这方面的论文，把基础知识掌握扎实，上Kaggle上参加或者复现两个项目放到简历里，会更加出色。\n非常感谢面试官的指导，没有想象中的KPI面试，不会有脸色看，更像是前辈教育后辈如何学习🤩。\n周四上午起了个大早直奔图书馆找同学，同学给占了一个之前我没有坐过的位置，有点风，恰好我没有棉袄，差点冻死，于是下午就回了宿舍，周四周五周六就在宿舍待着，写软件工程的文档，哎呦，忒难写，还得画图，周日棉袄到位，在图书馆待了一天，把所有文档完结，做了一个PPT，顺手的事。\n于是这一周就这么过完了\n🎧阅读 Secure and Private Machine Learning: A Survey of Techniques and Applications\nDifferential Privacy: A Survey of Results\n🥁计划 计划就是完成上周未完成的计划，开干！\n\u0026ldquo;Secure and Private Machine Learning: A Survey\u0026rdquo;\n这篇论文综述了在机器学习中实现安全和隐私保护的各种方法，包括使用同态加密和安全多方计算的技术。 \u0026ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption\u0026rdquo;\n该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。 \u0026ldquo;Federated Learning with Encrypted Data\u0026rdquo;\n这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。 \u0026ldquo;Secure Inference with Deep Learning Models\u0026rdquo;\n该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。 \u0026ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning\u0026rdquo;\n本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。 \u0026ldquo;Secure and Private Inference of Deep Learning Models\u0026rdquo;\n这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。 \u0026ldquo;Privacy-Preserving Machine Learning: A Practical Approach\u0026rdquo;\n本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。 \u0026ldquo;Secure Outsourced Computation in Machine Learning\u0026rdquo;\n这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。 \u0026ldquo;Differentially Private Inference in Machine Learning\u0026rdquo;\n本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。 \u0026ldquo;Towards Secure and Private Machine Learning: A Survey\u0026rdquo;\n这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。 \u0026ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning\u0026rdquo;\n该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。 \u0026ldquo;Secure and Private Model Inference: A Survey\u0026rdquo;\n本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。 ","date":"2024-11-10T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC5%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/ai-generated-7926621_1280_hu17162743065600781340.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC5%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/","title":"第5周工作总结"},{"content":"总结 这一周高强度阅读把书都读完了，但是没看代码，后果是有点记不住，等有需要时再去看吧。\n这周看了大模型内容比较多，主要是为了快点看完相关内容找ZD哥确定毕设方向，早点开始，做的好一点。\n周五下午找了ZD哥确定方向，看完方向人有点蒙，可能是大模型看得多，安全那方面看得少，导致我不知道这个方向在干什么，于是开始沉思，思考了半天感觉确实不会，找gpt老师询问，没有得到结果，故有些惆怅，周日又坐在实验室上了一天的软件工程实训，还好马上就结束了，太烦了。今天是周一，先把上周周报做了\n我又重新换了问法，问了gpt老师，这次得到了满意的结果，找到了几篇论文，这周就读这几篇。\n感觉这才是周报，之前的都是书的摘抄。\n阅读 LLM预训练数据准备\ntransformer模型架构（不知道是不是我本，这个我从不同的地方看了听了好多遍，现在才刚有点感觉）\nInstruction Tuning\n提示学习（实在不行，没有研究天赋就去干点轻松的(bushi)）\n解码与部署\n后面就是补了两篇差分隐私的文献，读完感觉对要研究的方向还是不清楚，故又找了几篇：\n计划 \u0026ldquo;Secure and Private Machine Learning: A Survey\u0026rdquo;\n这篇论文综述了在机器学习中实现安全和隐私保护的各种方法，包括使用同态加密和安全多方计算的技术。 \u0026ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption\u0026rdquo;\n该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。 \u0026ldquo;Federated Learning with Encrypted Data\u0026rdquo;\n这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。 \u0026ldquo;Secure Inference with Deep Learning Models\u0026rdquo;\n该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。 \u0026ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning\u0026rdquo;\n本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。 \u0026ldquo;Secure and Private Inference of Deep Learning Models\u0026rdquo;\n这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。 \u0026ldquo;Privacy-Preserving Machine Learning: A Practical Approach\u0026rdquo;\n本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。 \u0026ldquo;Secure Outsourced Computation in Machine Learning\u0026rdquo;\n这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。 \u0026ldquo;Differentially Private Inference in Machine Learning\u0026rdquo;\n本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。 \u0026ldquo;Towards Secure and Private Machine Learning: A Survey\u0026rdquo;\n这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。 \u0026ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning\u0026rdquo;\n该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。 \u0026ldquo;Secure and Private Model Inference: A Survey\u0026rdquo;\n本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。 ","date":"2024-11-03T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/colorful-2609978_1280_hu7268197494349331923.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/","title":"第4周工作总结"},{"content":"💐解码与部署 大语言模型是通过文本生成的方式进行工作的。在自回归架构中，模型针对输入内容逐个单词生成输出内容的文本。这个过程一般被称为 解码。\n🌸解码策略 大语言模型的生成方式本质上是一个概率采样过程，需要合适的解码策略来生成合适的输出内容。\n🌹语言模型解码的背景知识 模型M每次根据当前上下文词元序列𝒖=[𝑢1,𝑢2,··· ,𝑢𝑡]建模下一个词的概率分布𝑃（即公式$O = softmax(W^LY_L)$的输出），然后根据一定的解码策略选择下一个词𝑢′，之后再将𝒖和𝑢′作为新的上下文重复上述步骤，直到生成结束词元或者达到长度上限为止。在这个流程中，解码策略将主要关注如何基于概率分布𝑃选择合适的下一个词𝑢′。自回归的解码策略并不局限于特定架构，常见的编码器-解码器、因果解码器和前缀解码器均可适用。\n🌺贪心搜索 目前常见的大语言模型主要是通过语言建模任务进行预训练的。基于这种训练方式，一种直观的解码策略是贪心搜索（Greedy Search）。具体来说，贪心搜索在每个生成步骤中都选择概率最高的词元， 其可以描述为以下形式:\n贪心搜索示意图\r🌻概率采样 该方法根据模型建模的概率分布采样得到下一个词元，旨在增强生成过程中的随机性和结果的多样性：\n在图中展示了下一个词元的概率分布，虽然单词“coffee”被选中的概率较高，但基于采样的策略同时也为选择其他单词（如“water”、“tea”等）留有一定的可能性，从而增加了生成文本的多样性和随机性。\n🌼贪心搜索的改进 贪心搜索在每个生成步骤中均选择最高概率的词元，这可能会由于忽略在某些步骤中概率不是最高、但是整体生成概率更高的句子而造成局部最优。\n束搜索. 在解码过程中，束搜索（BeamSearch）[1]会保留前𝑛个具有最高概率的句子，并最终选取整体概率最高的生成回复。这里的𝑛被称为束大小（Beam Size）。当 𝑛 = 1，束搜索就退化为贪心搜索。如图9.3所示（𝑛=2），第一步保留了概率最高的两个词“coffee” 和 “water” 作为候选；第二步基于 “coffee” 和 “water” 均进行扩展，得到模型在两个上下文内容下的概率分布，最后选择联合概率最高的两个句子 “coffee then” 和 “coffee and” 作为候选。在下面的生成步骤中，将会继续基于这两个候选去进行扩展，每次都选择联合概率最高的两个句子。最后，当两个束的句子均生成结束后，选择整体生成概率最高的候选句子作为最终的输出。在实践中，束的数量通常设定在3到6的范围内，设置过大的束会显著增加运算开销，并可能会导致性能下降。\n长度惩罚. 由于束搜索中需要比较不同长度候选句子的概率，往往需要引入长度惩罚（LengthPenalty）（亦称为长度归一化）技术。如果没有长度惩罚，传统的束搜索会倾向于生成较短的句子，因为每生成一个单词，都会乘以一个小于1的概率，使得句子的生成概率逐渐变小。因此，可以在生成概率的计算中引入长度惩罚，通过将句子概率除以其长度的指数幂𝛼，对于句子概率进行归一化处理，从而鼓励模型生成更长的句子。在实践中，𝛼通常设置为0.6到0.7之间的数值。\n重复惩罚.为了缓解贪心搜索重复生成的问题，可以使用𝑛-元惩罚（𝑛-gram Penalty）来强制避免生成重复的连续𝑛个词元，实践中𝑛通常设置为3到5之间的整数。进一步地，研究人员还提出了相对“温和”的惩罚机制来降低生成重复词元的概率，而不是“一刀切”地完全避免某些短语的生成，如出现惩罚（Presence Penalty）和频率惩罚（FrequencyPenalty）。具体地，出现惩罚在生成过程中会将已经生成词元的logits减去惩罚项𝛼来降低该词元之后生成的概率。频率惩罚相较于出现惩罚，会记录每个词元生成的数目，然后减去出现次数乘以惩罚项𝛼，因此如果一个词元生成得越多，惩罚也就越大。在实践中，𝛼的取值范围通常在0.1到1之间。这些重复惩罚方法不止适用于贪心搜索，对于随机采样也均适用。\n🌷随机采样的改进策略 基于概率采样的方法会在整个词表中选择词元，这可能会导致生成不相干的词元。为了进一步提高生成质量，可以进一步使用一些改进的采样策略，减少具有极低概率词汇对于生成结果的影响。\n温度采样（TemperatureSampling）. 为了调节采样过程中的随机性，一种有效的方法是调整softmax函数中的温度系数。具体来说，公式5.11中的$l = W^LyL$ 称为logits，调整温度系数后的softmax计算式如下： $$\rP(u_j | \\mathbf{u}_i) = \\frac{\\exp(l_j / t)}{\\sum_{j'} \\exp(l_{j'} / t)}\r$$ 其中，$l_{j^′ }$表示每个候选词元的logit，𝑡是温度系数。具体来说，降低温度系数𝑡会使得概率分布更加集中，从而增加了高概率词元的采样可能性，同时降低了低概率词元的采样可能；当温度系数𝑡设置为1时，该公式退化为标准的随机采样方法；而当𝑡趋近于0时，实际上等同于贪心搜索，即总是选择概率最高的词。此外，当𝑡趋近于无穷大时，温度采样会退化为均匀采样。\n🌱低资源部署策略 由于大模型的参数量巨大，在解码阶段需要占用大量的显存资源，因而在实际应用中的部署代价非常高。这里介绍一种常用的模型压缩方法，即模型量化（Model Quantization），来减少大模型的显存占用，从而使得能够在资源有限的环境下使用大模型.\n🌲量化基础知识 在神经网络压缩中，量化通常是指从浮点数到整数的映射过程[2]，目前比较常用的是8比特整数量化，即INT8量化。针对神经网络模型，通常有两种类型的数据需要进行量化，分别为权重量化（也称为模型参数量化）和激活（值）量化，它们都以浮点数形式进行表示与存储。\n🌳量化的数学表述 量化的过程可以表示为一个函数，该函数将连续的输入映射到离散的输出集合。一般来说，这个过程涉及到四舍五入或截断等近似操作。下面介绍一个一般形式的量化函数： $$\rx_q = R(x/S)-Z.\r$$ 通过上述数学变换，量化算法将浮点数向量𝒙转化为量化值𝒙𝒒。其中，𝑆表示缩放因子，用于确定裁剪范围，𝑍表示零点因子，用于确定对称或非对称量化，𝑅(·) 表示将缩放后的浮点值映射为近似整数的取整操作。一般来说，裁剪范围对于量化性能有很大影响，通常需要根据实际数据分布进行校准，可以通过静态（离线）或动态（运行时）方式。\n作为上述变换的逆过程，反量化（Dequantization）对应地从量化值中恢复原始值，该过程首先加上零点因子，然后乘以缩放因子： $$\r\\tilde{\\mathbf{x}} = \\mathbf{S} \\cdot (\\mathbf{x}_q + \\mathbf{Z})\r$$ 进一步，可以定义量化误差是原始值𝒙和恢复值$\\tilde{\\mathbf{x}}$之间的数值差异：$Δ=∥𝒙−\\tilde{\\mathbf{x}}∥^2_2$。\n🌴量化的策略 基于上述量化函数的定义形式，下面介绍一些对于量化函数常见的分类与实现策略。\n均匀量化和非均匀量化.根据映射函数的数值范围是否均匀，可以将量化分为两类：均匀量化和非均匀量化。均匀量化是指在量化过程中，量化函数产生的量化值之间的间距是均匀分布的。这种方法通常用于将连续的数据转换为离散的表示，以便在计算机中进行高效处理。与此不同，在非均匀量化方法中，它的量化值不一定均匀分布，可以根据输入数据的分布范围而进行调整。其中，均匀量化方法因其简单和高效的特点而在实际中被广泛使用。\n对称量化和非对称量化. 根据零点因子𝑍是否为零，均匀量化可以进一步分为两类：对称量化（𝑍=0）和非对称量化（𝑍≠0）。对称量化与非对称量化的一个关键区别在于整数区间零点的映射，对称量化需要确保原始输入数据中的零点（𝑥=0）在量化后仍然对应到整数区间的零点。而非对称量化则不同，根据前面的公式可以看出此时整数区间的零点对应到输入数值的𝑆·𝑍。为了方便讨论，这里以一个常见的8比特量化为例进行介绍。如图9.6(a)和图9.6(b)所示，对称量化将输入数值𝒙通过一个映射公式转换为八比特整数的表示范围内，如果是有符号整数，则该范围可以设置为[−128,127]，适用于𝒙的数值大致分布在零点两侧的情况，如果是无符号整数，则设置为[0,255]，适用于输入数值基本都分布在零点一侧的情况。\n量化粒度的选择. 量化算法通常针对一个批次的数据进行处理，其中批次的规模大小就反应了量化粒度，可以由算法设计人员进行选择。在神经网络模型中，输入数据和模型权重通常是以张量的形式进行表示与组织的。首先，如果每个张量只定义一组量化参数（即𝑆和𝑍），这称为按张量量化。为了进一步提高量化的精度，可以针对每个张量定义多组量化参数，例如可以为权重矩阵的列维度（也称为“通道”）设置特定的量化参数，称为按通道量化。还有一些研究工作采用了更细粒度的量化方案，对一个通道的数值细分为多个组，即按组的方式进行量化。在神经网络量化中，从按张量到按组，量化粒度越来越小，且使用较小的粒度通常可以提高量化的准确性，有效保持原始模型的性能。但是由于引入了更多的量化参数，在使用时会带来额外的计算开销。相反，按张量量化的粒度较粗，可能会引入较大的误差，但由于在硬件实现上更加简单，也是一种常见的量化粒度选择策略。在实践中，量化粒度需要根据具体任务和模型进行选择，应该采用可以平衡量化准确性以及额外计算开销的合适粒度。\n🌵大模型训练后量化方法 基于上述的量化基础知识，本部分将主要介绍大语言模型相关的量化方法。通常来说，模型量化方法可以分为两大类，即量化感知训练（Quantization-Aware Training,QAT）和训练后量化（Post-Training Quantization,PTQ）。从方法的名字可以看出，量化感知训练方法需要更新权重进而完成模型量化，而训练后量化方法则无需更新模型权重。与小型语言模型相比，在为大语言模型设计或选择量化方法时需要侧重关注两个主要因素。首先，大语言模型由大规模神经网络参数组成，在设计量化算法时需要考虑到所需要花费的计算开销。一般来说，训练后量化方法需要更少的算力开销，实践中应用更为广泛。其次，大语言模型中具有不同的数值分布特征（如激活值中存在较大的数值），这使得对于大语言模型进行低比特量化变得非常困难，特别是对于激活值。\n🌿权重量化 首先介绍主要面向模型权重的量化方法。其中，主流的权重量化方法通常是基于逐层量化的方法进行设计的，旨在通过最小化逐层的重构损失来优化模型的量化权重，可以刻画为：$\\text{argmin}_{\\mathbf{W}} | \\mathbf{X} \\mathbf{W} - \\mathbf{X} \\mathbf{W} |_2^2$，其中𝑾，𝑾 分别表示原始权重和量化后的权重，𝑿为输入。\n为了有效地优化该目标函数，GPTQ[3]的基本想法是在逐层量化的基础上，进一步将权重矩阵按照列维度分组（例如128个列为一组），对一个组内逐列进行量化，每列参数量化后，需要适当调整组内其他未量化的参数，以弥补当前量化造成的精度损失。在具体的实现中，GPTQ还进一步采用了特殊设计的优化方法来加速整个过程，如延迟批次更新、Cholesky重构等。GPTQ可以在3比特或4比特精度下实现对于大语言模型的有效权重量化。\n进一步，AWQ[4]发现在逐层和逐组权重量化过程中，对于模型性能重要的权重只占全部参数的一小部分（0.1%∼1%），并且应该更加关注那些与较大激活值维度相对应的关键权重，因为它们对应着更为重要的特征。为了加强对于这部分关键权重的关注，AWQ方法提出引入针对权重的激活感知缩放策略。具体来说，AWQ的优化目标将逐层的重构损失$| \\mathbf{X} \\mathbf{W} - \\mathbf{X} \\mathbf{W} |_2^2$修改为：$\\left| \\left( \\text{diag}(\\mathbf{s})^{-1} \\cdot \\mathbf{X} \\right) \\cdot \\mathbf{Q}(\\mathbf{W} \\cdot \\text{diag}(\\mathbf{s})) - \\mathbf{X} \\mathbf{W} \\right|_2^2$，其中𝑄 为量化函数。通过引入缩放因子𝒔，AWQ算法可以使得量化方法更为针对性地处理关键权重所对应的权重维度。\n☘️模型蒸馏 模型蒸馏（ModelDistillation）的目标是将复杂模型（称为教师模型）包含的知识迁移到简单模型（称为学生模型）中，从而实现复杂模型的压缩。一般来说，通常会使用教师模型的输出来训练学生模型，以此来传递模型知识。以分类问题为例，教师模型和学生模型在中间每一层会输出特征表示（特指神经网络模型），在最后一层会输出针对标签集合的概率分布。模型蒸馏的核心思想是，引入额外的损失函数（称为蒸馏损失函数），训练学生模型的输出尽可能接近教师模型的输出。在实际应用中，蒸馏损失函数通常与分类损失函数（交叉熵损失函数）联合用于训练学生模型。下面首先介绍传统的知识蒸馏方法，再介绍其在大语言模型中的应用。\n🍀参考文献 [1]. CARNEGIE-MELLON UNIV PITTSBURGH PA DEPT OF COMPUTER SCIENCE. Speech Understanding Systems. Summary of Results of the Five-Year Research Effort at Carnegie-Mellon University. 1977.\n[2]. Amir Gholami et al. “A Survey of Quantization Methods for Efficient Neural Network Inference”. In: arXiv preprint arXiv:2103.13630 (2021).\n[3]. EliasFrantaretal.“GPTQ:AccuratePost-TrainingQuantizationforGenerativePre-trained Transformers”. In: arXiv preprint arXiv:2210.17323 (2022).\n[4]. Ji Lin et al. “AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration”. In: arXiv preprint arXiv:2306.00978 (2023).\n","date":"2024-11-01T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/decoderbs/roses-7314623_1280_hu10924616842157365117.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/decoderbs/","title":"解码与部署"},{"content":"🎄提示学习 经过预训练、指令微调和人类对齐后，我们接下来讨论如何通过提示学习方法来有效地使用大语言模型解决实际任务。目前常用的方法是设计合适的提示（Prompting），通过自然语言接口与大模型进行交互。在现有研究中，任务提示的设计主要依靠人工设计和自动优化两种策略来实现。为了更好地 解决未见过的任务，一种典型的提示方法是上下文学习（In-contextLearning,ICL），它将任务描述与示例以自然语言文本形式加入到提示中。此外，思维链提示（Chain-of-Thought, CoT）作为一种增强技术，将一系列中间推理步骤加入到提示中，以增强复杂推理任务的解决效果。\n🎆基础提示 因为大语言模型的微调代价较高，基于自然语言的提示方法已经成为了使用大语言模型解决下游任务的主要途径。由于提示的质量在很大程度上会影响大语言模型在特定任务中的表现，因此一系列工作深入研究了通过人工设计或自动优化的方法来生成合适的任务提示。\n🎇人工提示设计 针对特定任务设计合适的任务提示，这一过程被称为“提示工程”（Prompt Engineering）。\n🧨关键要素 一般而言，针对大语言模型的提示设计需要考虑四个关键要素，即任务描述、输入数据、上下文信息和提示策略。\n任务描述.任务描述部分展示了大语言模型应当遵循的具体指令。一般来说，用户应该使用清晰的、具体的表达来描述任务目标。进一步，某些特定任务还需要对于输入或输出的格式进行更详细的说明，可以使用关键词或者特殊符号来强 调特殊设置以指导大语言模型更好地完成任务。 输入数据. 通常情况下，用户可以直接使用自然语言描述输入数据的内容。对于特殊形式的输入数据，则需要采用合适的方法使其能够被大语言模型读取与理解。例如，对于结构化数据（如知识图谱、表格等），通常使用线性化方法将其转换为易于处理的文本序列[264]。此外，由于结构化数据具有较好的组织形式，可以使用编程代码中的数据结构进行存储表示，将结构化数据中的属性表示为数据结构中的变量。基于代码表示的结构化数据可以使用外部工具（如程序执行器）进行准确地读取。 上下文信息. 除了任务描述和输入数据外，上下文信息对某些特定任务也非常重要。例如，搜索引擎可以为开放问答任务提供参考文档，可以通过将检索到的参考文档以上下文信息的形式引入提示作为大语言模型的输入。在引入外部信息时，需要对于这些信息进行合适的格式化，以加强大语言模型对它们的利用。此外，上下文学习中的任务示例数据也有助于提升大语言模型处理复杂任务的能力，大模型可以通过这些示例数据学习任务目标、输出格式以及输入和输出之间的映射关系。 提示策略. 针对不同的大语言模型设计合适的提示策略对于激发模型解决特定任务的能力非常重要。在某些情况下，添加特定的前缀或后缀有助于引导大语言模型解决复杂任务。例如，使用前缀“让我们一步一步地思考”可以激发大语言模型的逐步推理能力，而使用前缀“你是这项任务（或这个领域）的专家”可以提高大语言模型在某些特定任务（或领域）中的表现。此外，对于对话式的大语言模型（例如ChatGPT），由于其使用了大量对话数据进行训练，因此更合适的做法是将提示拆分为多个子任务提示，以多轮对话的方法逐步输入给大语言模型。 ✨自动提示优化 人工设计提示虽然比较直接，但是需要耗费较多的人工成本，同时要求设计人员具有丰富的提示工程经验。此外，大语言模型对于提示设计比较敏感，人工设计的提示有时很难获得最优的效果，还可能导致任务性能的下降。需要注意的是，由于大语言模型参数量巨大，并且很多工作机制已经与传统预训练模型有着较大的差异，许多提示优化方法已经不再适用于大语言模型。\n🎉离散提示优化 离散提示通常是由一系列自然语言词元组成的完整句子表达（如“请根据提供的检索信息回答下列问题”）。然而，在离散的词元空间中进行组合搜索，不仅时间复杂度高，而且可能导致非最优的搜索结果。下面将介绍四种常见的离散提示优化方法，能够提升离散任务提示的有效性与搜索效率。\n基于梯度的方法. 这类方法通过梯度更新技术以最大化模型的似然分数来优化离散提示的搜索过程。一种早期的代表性方法[1]使用梯度引导技术，首先将提示初始化为一系列“[MASK]”标记，然后迭代地将提示中的词元替换为词典中的其他词元，通过词元替换产生的对数似然变化来近似估计梯度，进而为提示的每个位置贪心搜索出最佳的词元。由于该方法对提示的每个位置都进行所有候选词元的替换和梯度评估，因此需要模型进行多次前向和后向计算，导致搜索过程的效率较低。为了改进搜索效率，可以将离散词元转化为连续嵌入表示（又称为“软词元”），使用梯度直接对连续嵌入参数进行优化，最后将每个连续嵌入映射为词典中最邻近的离散词元。 基于强化学习的方法. 为了实现更有效的离散词元选择，另一种解决方法是将离散提示优化问题转换为强化学习问题，并使用强化学习算法进行求解。具体来说，可以将预训练语言模型作为强化学习中的策略网络并依次生成提示中的词元。在提示生成结束之后，策略网络可以获得任务特定的奖励信号，该奖励信号可通过强化学习算法用于策略网络参数的训练。在实践中，可以设计不同类型的奖励信号，比如真实标签与基于提示的预测标签是否一致、生成文本与给定条件的匹配程度。在最后的测试阶段，基于训练好的策略网络，可以采用贪心搜索策略来生成任务提示中的每个词元。 基于编辑的方法. 这类方法主要关注如何通过编辑现有的提示来改进模型的性能，通常是基于模型在目标任务上的表现来判断提示的好坏。它特别适用于无法直接访问模型内部状态（如梯度）的情况，例如只能通过API调用的模型。在这类方法中，通常需要事先定义好编辑操作，然后迭代地对提示进行修改，直至达到最大迭代轮次或者模型最佳性能。提示的关键要素包括任务描述、输入数据、上下文信息和提示策略。因此，常用的提示编辑操作有修改任务描述、添加或删除上下文任务示例、调整输入到输出的标签映射器（例如可以使用“positive/negative”或者“正/负”表示二分类）等。此外，提示编辑操作也可以根据不同的场景或者需求进行设计，以适配下游具体任务。整体流程可以概述如下：基于预定义的编辑操作，在现有提示的基础上修改得到新提示，并输入至模型得到目标任务上的表现，根据表现筛选出合适的提示。由于上述过程可能需要迭代进行，可以只选择少量测试样例来评估模型表现，以减少计算开销。 基于大语言模型的方法. 由于大语言模型具有通用的任务能力，因此可以将提示优化看作一个待求解的任务，进而直接使用大语言模型作为提示生成器来生成或改进提示[2,3]。基于大语言模型的自动提示生成框架将提示优化过程看作是一个由大语言模型指导的黑盒优化问题。该框架首先利用提示生成模型（用于生成提示指令的大语言模型）基于少量上下文示例生成一批候选的任务指令。随后，使用“目标模型”（用于下游测试的大语言模型）对这些候选指令在目标任务上的表现进行逐一评估。在评估过程中，可以采用模型困惑度或任务准确率作为衡量指令质量的指标。上述过程可以通过基于蒙特卡洛搜索的多轮优化策略进行扩展。在每一轮迭代中，根据模型表现对候选指令进行筛选得到高评分指令，并利用大语言模型生成与高评分指令相似的新指令，从而扩展候选指令集。迭代完成后，选择模型表现最佳的候选指令作为最终使用的提示。然而，上述方法没有充分考虑提示的整个历史改进轨迹，因此可能在提示搜索过程中陷入局部最优或者产生效果震荡，无法生成更好的提示。为了解决这一问题，可以将所有改进的历史提示及其分数纳入提示优化过程，以指导大语言模型逐步生成更好的新提示。 🎊连续提示优化 与离散提示不同，连续提示由一组连续空间中的嵌入向量组成，可以根据下游任务的损失直接通过梯度更新进行优化。值得注意的是，已有连续提示优化的工作主要是基于预训练语言模型开展的，由于大语言模型参数量巨大，连续提示受到的关注较为有限。已有的连续提示优化研究通常依赖于有监督学习方法。当数据稀缺的情况下，还可以采用迁移学习方法来缓解目标任务标注数据不足的问题。\n监督学习方法. 这类方法将连续提示向量视为可训练的模型参数，基于下游任务数据，通过最小化交叉熵损失来优化连续提示。Prefix-tuning [4] 会在语言模型的每个Transformer 层预置一串前缀（即一组可训练的连续向量），而Prompt-tuning[5]只会在输入层加入可训练的提示向量。 通过固定语言模型的大规模参数而只微调这些连续的提示向量，可以有效节省训练所需要的参数量。然而，这些提示优化方法通常与输入无关，缺乏对于输入语义的充分考虑。 迁移学习方法. 有监督学习方法通常需要充足的训练数据来学习最优的任务提示，很难在数据稀缺场景下获得较好的模型性能。为了解决这个问题，基于提示的迁移学习方法首先为若干个具有代表性的源任务学习一个所有任务共享的连续提示，然后使用该提示初始化目标任务的提示，这可以为下游任务的提示优化提供良好的初始点。然而，这种方法存在一定的局限性，它在解决目标任务的所有实例时都使用了相同提示，而即使是一个精心优化过的提示也未必适合所有的任务实例。为了解决这一问题，可以为每个源任务独自学习任务特定的连续提示（而不是所有源任务共享），在解决目标任务的实例时，可以采用注意力机制等方式学习目标实例与每个源任务提示的相关性权重系数，对若干个源任务的提示向量进行加权组合，将组合后的新提示（为连续向量形式）用于帮助模型解决当前任务实例。 🎋上下文学习 在GPT-3的论文[6]中，OpenAI研究团队首次提出上下文学习（In-context learning,ICL）这种特殊的提示形式。目前，上下文学习已经成为使用大语言模型解决下游任务的一种主流途径。下面将详细介绍这一提示方法。\n🎍上下文学习的形式化定义 根据GPT-3论文中所给出的描述[6]，上下文学习使用由任务描述和（或）示例所组成的自然语言文本作为提示。图10.1展示了上下文学习的提示构建过程。首先，通过自然语言描述任务，并从任务数据集中选择一些样本作为示例。其次，根据特定的模板，将这些示例按照特定顺序组合成提示内容。最后，将测试样本添加到提示后面，整体输入到大语言模型以生成输出。基于任务描述以及示例信息，大语言模型无需显式的梯度更新即可识别和执行新的任务。\n形式上，我们使用𝐷𝑘={𝑓(𝑥1,𝑦1),\u0026hellip;, 𝑓(𝑥𝑘,𝑦𝑘)}来表示由𝑘个样本构成的一组示例数据，其中𝑓(𝑥𝑘,𝑦𝑘)是一个函数，负责将第𝑘个任务样本转换为自然语言提示。给定任务描述𝐼、示例𝐷𝑘以及新的输入𝑥𝑘+1，大语言模型生成答案ˆ 𝑦𝑘+1的过程可以通过下面的公式来表述：\n值得一提的是，上下文学习与指令微调之间存在着紧密的联系，因为它们都涉及将任务或样本转化为自然语言形式供大语言模型进行处理。在原始的GPT-3论文中，作者将上下文学习的提示定义为任务描述和示例的组合，这两部分均为可选。按照这个定义，如果大语言模型仅通过任务描述（即任务指令）来解决未见过的任务，也可以被看作是上下文学习的一种特例。两者的主要区别是，指令微调需要对大语言模型进行微调，而上下文学习仅通过提示的方式来调用大语言模型解决任务。此外，指令微调还可以有效提升大语言模型在执行目标任务时的上下文学习能力，尤其是在零样本场景下（即仅依赖任务描述而无需额外的示例）。\n🎁思维链提示 思维链提示[7,8]是一种高级提示策略，旨在增强大语言模型在各类复杂推理任务上的表现。常见的推理任务包括算术推理[9]、常识推理[9]以及符号推理[7]等多种任务。与上下文学习方法仅使用⟨输入，输出⟩二元组来构造提示不同，思维链提示进一步融合了中间的推理步骤来指导从输入到输出的推理过程。下图展示了一个思维链提示的具体例子。\n思维链提示技术的演化过程\r🎑参考文献 [1]. Taylor Shin et al. “AutoPrompt: Eliciting Knowledge from Language Models with Auto matically Generated Prompts”. In: EMNLP. 2020.\n[2]. Yongchao Zhou et al. “Large Language Models are Human-Level Prompt Engineers”. In: ICLR. 2023.\n[3]. Chengrun Yang et al. “Large Language Models as Optimizers”. In: arXiv preprint arXiv: 2309.03409 (2023).\n[4]. XiangLisa Li and Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Gen eration”. In: ACL. 2021.\n[5]. Brian Lester, Rami Al-Rfou, and Noah Constant. “The Power of Scale for Parameter Efficient Prompt Tuning”. In: EMNLP. 2021.\n[6]. TomB.Brown et al. “Language Models are Few-Shot Learners”. In: NeurIPS. 2020.\n[7]. JasonWeietal. “Chain of Thought Prompting Elicits Reasoning in Large Language Mod els”. In: arXiv preprint arXiv:2201.11903 (2022).\n[8]. Zheng Chu et al. “A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future”. In: arXiv preprint arXiv:2309.15402 (2023).\n[9]. Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. “A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers”. In: ACL. 2020.\n[10]. AlonTalmoret al. “CommonsenseQA: A Question Answering Challenge Targeting Com monsense Knowledge”. In: NAACL-HLT. 2019.\n","date":"2024-11-01T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/prompt_learning/birch-trees-8345812_1280_hu1913448854319438606.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/prompt_learning/","title":"提示学习"},{"content":"指令微调（Instruction Tuning） 指令微调（Instruction Tuning）是指使用自然语言形式的数据对预训练后的大语言模型进行参数微调，这一术语由谷歌研究员在2022年的一篇ICLR论文中正式提出[1]。在另外一些参考文献中，指令微调也被称为有监督微调（Supervised Fine-tuning）[2] 或多任务提示训练（MultitaskPromptedTraining）[3]。指令微调过程需要首先收集或构建指令化的实例，然后通过有监督的方式对大语言模型的参数进行微调。经过指令微调后，大语言模型能够展现出较强的指令遵循能力，可以通过零样本学习的方式解决多种下游任务。\n🏔️指令数据的构建 一般来说，一个经过指令格式化的数据实例包括任务描述（也称为指令）、任务输入-任务输出以及可选的示例。\n⛰️基于现有的NLP任务数据集构建 学术界围绕传统NLP任务（如机器翻译、文本摘要和文本分类等）发布了大量的开源数据集合，这些数据是非常重要的监督学习数据资源，可以用于指令数据集的构造。通常来说，这些NLP数据集都包括输入和输出两个主要部分。\n例如，在中英翻译任务中，输入是“大语言模型已经成为机器学习的一个重要研究方向”，而相应的输出则是“Large language models have become one important research direction for machine learning”。\n为了生成指令化的训练数据，一个非常关键的步骤就是为上述的“输入-输出”对数据添加任务描述信息，用于指导模型去理解任务目标以及相关信息。在上述的例子中，可以向中译英的翻译数据集中添加指令， 例如“请把这个中文句子翻译成英文”。通过上述操作，就可以将一个NLP任务的数据实例全部通过自然语言形式进行表达，进而数据实例可以被用于大语言模型的指令微调。\n现有NLP数据集的指令格式化示意图\r为了更好地标注NLP指令微调数据，研究 人员开发了众包平台PromptSourcehttps://github.com/bigscience-workshop/promptsource，它能够高效地支持标注人员为不同数据集创建、共享及验证任务描述。\n🌋基于日常对话数据构建 尽管通过指令格式化已有的NLP数据集能够获得大量的指令数据实例，但是这些数据的多样性比较局限，与人类的真实需求也不能很好匹配。为此，研究人员开始使用用户在日常对话中的实际需求作为任务描述。例如，InstructGPT[2]将用户提交给OpenAI API的查询作为任务描述。由于这些用户查询源自于真实应用场景，均采用自然语言形式进行表达，因此特别适合大模型学习指令跟随能力。为了进一步增加任务的多样性，OpenAI还雇佣标注者创作更多的真实生活任务，包括开放式生成、开放式问答、头脑风暴等任务，然后由另一组标注者来回答这些问题作为输出。OpenAI最终将指令（用户真实查询或者人工标注的任务）和期望的输出（人工编写的答案）配对作为一个训练实例。但是，OpenAI没有对外开放所使用的指令数据。\n🗻基于合成数据构建 Self-Instruct \u0026mdash;\u0026mdash;-\u0026gt; Yizhong Wang et al. “Self-Instruct: Aligning Language Model with Self Generated In structions”. In: arXiv preprint arXiv:2212.10560 (2022).\nEvol-Instruct \u0026mdash;\u0026mdash;-\u0026gt; Can Xu et al. “WizardLM: Empowering Large Language Models to Follow Complex In structions”. In: arXiv preprint arXiv:2304.12244 (2023).\n🏖️指令微调的训练策略 🏜️优化设置 指令微调中的优化器设置（AdamW或Adafactor）、稳定训练技巧（权重衰减和梯度裁剪）和训练技术（3D并行、ZeRO和混合精度训练）都与预训练保持阶段一致，可以完全沿用。\n不同之处：\n目标函数. 预训练阶段通常采用语言建模损失，优化模型在每一个词元上的损失。而指令微调可以被视为一个有监督的训练过程，通常采用的目标函数为序列到序列损失，仅在输出部分计算损失，而不计算输入部分的损失。 批次大小和学习率.考虑到预训练阶段已经学习到了能够展现较好性能的模型参数，指令微调阶段通常只需要使用较小的批次大小和学习率对模型进行小幅度的调整。例如InstructGPT(175B)微调的批次大小为8，学习率恒定为5.03×10−6；Alpaca (7B) 微调的批次大小为128，学习率预热到2×10−5，然后采用余弦衰减策略。 多轮对话数据的高效训练.对于一个多轮对话数据，通常的训练算法是将其拆分成多个不同的对话数据进行单独训练。为了提升训练效率，可以采用特殊的掩码机制来实现多轮对话数据的高效训练。在因果解码器架构中，由于输入输出没有明显的分界，可以将所有一个对话的多轮内容一次性输入模型，通过设计损失掩码来实现仅针对每轮对话的模型输出部分进行损失计算，从而显著减少重复前缀计算的开销。 🏝️数据组织策略 平衡数据分布：最常见的方法是样本比例混合策略，即把所有数据集进行合并，然后从混合数据集中等概率采样每个实例。例如，研究者建议混合使用NLP任务数据（如FLANv2）、对话数据（如ShareGPT）和合成数据（如GPT4-Alpaca），来进行大模型的指令微调。\n多阶段指令数据微调：首先使用大规模NLP任务指令数据对模型进行微调，然后再使用相对多样的日常对话指令和合成指令进一步微调。为了避免能力遗忘问题，可以在第二阶段中添加一些NLP指令数据。\n结合预训练数据与指令微调数据：为了使得微调过程更加有效和稳定，可以在指令微调期间引入预训练数据和任务，这可以看作是对于指令微调的正则化。\n🏛️参数高效的模型微调 通过指令微调，大语言模型能够更好地学习遵循和执行人类指令。然而，由于大语言模型的参数量巨大， 进行全参数微调（需要较多的算力资源开销）在本节中，我们将讨论如何针对大语言模型进行参数高效微调（Parameter-efficient Fine-tuning），也称为轻量化微调 （Lightweight Fine-tuning）。在现有文献中，参数高效微调[4,5,6] 是一个重要的 研究方向，旨在减少需要训练的模型参数量，同时保证微调后的模型性能能够与全量微调的表现相媲美。下面将首先介绍常用于Transformer架构的参数高效微调方法，然后以LoRA微调方法为例介绍参数高效微调的代码实现。\n🏘️低秩适配微调方法 LoRA微调示意图\r🏬LoRA基础 大语言模型中包含大量的线性变换层，其中参数矩阵的维度通常很高。研究人员[6]发现模型在针对特定任务进行适配时，参数矩阵往往是过参数化（Over-parametrized）的，其存在一个较低的内在秩。为了解决这一问题，LoRA[6] 提出在预训练模型的参数矩阵上添加低秩分解矩阵来近似每层的参数更新，从而减少适配下游任务所需要训练的参数。给定一个参数矩阵𝑾，其更新过程可以一般性地表达为以下形式： $$\rW = W_0 +ΔW\r$$ 其中，$W_0$是原始参数矩阵，Δ𝑾是更新的梯度矩阵。LoRA的基本思想是冻结原始矩阵$W_0 ∈ R^{H \\times H}$，通过低秩分解矩阵$A ∈ R^{H \\times H}$和$B ∈ R^{H \\times H}$来近似参数更新矩阵$Δ𝑾 = A·B^T$，其中𝑅≪𝐻是减小后的秩。在微调期间，原始的矩阵参数$W_0$不会被更新，低秩分解矩阵𝑨和𝑩则是可训练参数用于适配下游任务。在前向传播过程中，原始计算中间状态$h=W_0·x$的公式修改为： $$\rh = W_0·x + A·B^T·x\r$$ 在训练完成后，进一步将原始参数矩阵$W_0$和训练得到的权重𝑨和𝑩进行合并：$W = W_0 + A·B^T$，得到更新后的参数矩阵。因此，LoRA微调得到的模型在解码过程中不会增加额外的开销。\n🏦LoRA变种 在原始的LoRA实现中，每个低秩矩阵的低秩参数𝑅都被设置为固定且相同的数值，并且在训练过程中无法进行调整，这种设定忽略了不同的秩在微调任务中可能产生的差异化影响。因此，通过这种方式训练得到的低秩矩阵往往并非最优解。AdaLoRA[7]讨论了如何更好地进行秩的设置。它引入了一种动态低秩适应技术，在训练过程中动态调整每个参数矩阵需要训练的秩同时控制训练的参数总量。具体来说，模型在微调过程中通过损失来衡量每个参数矩阵对训练结果的重要性，重要性较高的参数矩阵被赋予比较高的秩，进而能够更好地学习到有助于任务的信息。相对而言，不太重要的参数矩阵被赋予比较低的秩，来防止过拟合并节省计算资源。尽管LoRA能够有效地节省显存，但对于参数规模达到上百亿级别的模型而言，其微调所需的成本仍然相当高昂。QLoRA[8]将原始的参数矩阵量化为4比特，而低秩参数部分仍使用16比特进行训练，在保持微调效果的同时进一步节省了显存开销。根据上一小节的分析，对于给定参数量为𝑃的模型，QLoRA微调所需要的显存由LoRA微调所需要的2𝑃进一步下降为0.5𝑃。因此通过QLoRA技术，可以在一张A6000(48GB)的GPU上微调65B的模型，接近16比特模型微调的性能。\n🏡其他高效微调方法 适配器微调、前缀微调、提示微调\n参考文献 [1]. Jason Wei et al. “Finetuned Language Models are Zero-Shot Learners”. In: ICLR. 2022.\n[2]. Long Ouyang et al. “Training language models to follow instructions with human feed back”. In: arXiv preprint arXiv:2203.02155 (2022).\n[3]. VictorSanhetal. “Multitask Prompted Training Enables Zero-Shot Task Generalization”. In: ICLR. 2022.\n[4]. XiangLisa Li and Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Gen eration”. In: ACL. 2021.\n[5]. Brian Lester, Rami Al-Rfou, and Noah Constant. “The Power of Scale for Parameter Efficient Prompt Tuning”. In: EMNLP. 2021.\n[6]. EdwardJ.Huetal. “LoRA:Low-RankAdaptation of Large Language Models”. In: ICLR. 2022.\n[7]. Qingru Zhang et al. “Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning”. In: arXiv preprint arXiv:2303.10512 (2023).\n[8]. TimDettmersetal.“QLoRA:EfficientFinetuningofQuantizedLLMs”.In:arXivpreprint arXiv:2305.14314 (2023).\n","date":"2024-10-31T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/instructiontuning/apples-1776744_1280_hu10159539994416574822.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/instructiontuning/","title":"Instruction Tuning"},{"content":"模型架构 大语言模型架构配置表\u003e\r🍈Transformer 模型 当前主流的大语言模型都是基于Transformer模型进行设计的。Transformer是由多层的多头自注意力（Multi-headSelf-attention）模块堆叠而成的神经网络模型。原始的Transformer 模型由编码器和解码器两个部分构成，而这两个部分实际上可以独立使用，例如基于编码器架构的BERT模型[1]和解码器架构的GPT模型[2]。与BERT等早期的预训练语言模型相比，大语言模型的特点是使用了更长的向量维度、更深的层数，进而包含了更大规模的模型参数，并主要使用解码器架构，对于Transformer 本身的结构与配置改变并不大。\n🍉输入编码 在Transformer 模型中，输入的词元序列(𝒖 = [𝑢1,𝑢2,\u0026hellip;,𝑢𝑇]) 首先经过一个输入嵌入模块（InputEmbeddingModule）转化成词向量序列。具体来说，为了捕获词汇本身的语义信息，每个词元在输入嵌入模块中被映射成为一个可学习的、具有固定维度的词向量$v_t$ ∈$R^H$。由于Transformer的编码器结构本身无法识别序列中元素的顺序，位置编码（PositionEmbedding,PE）被引入来表示序列中的位置信息。给定一个词元$u_t$，位置编码根据其在输入中的绝对位置分配一个固定长度的嵌入向量$p_t$ ∈$R^H$。然后，每个词元对应的词向量和位置向量将直接相加，生成了最终的输入嵌入序列𝑿=[𝒙1,\u0026hellip;,𝒙𝑇]，并且被传入到后续层中：$x_t=v_t+p_t$.\n通过这种建模方法的表示，Transformer 模型可以利用位置编码 𝒑𝑡 建模不同词元的位置信息。由于不同词元的位置编码仅由其位置唯一决定，因此这种位置建模方式被称为绝对位置编码。尽管绝对位置编码能够一定程度上建模位置信息，然而它只能局限于建模训练样本中出现的位置，无法建模训练数据中未出现过的位置，因此极大地限制了它们处理长文本的能力。\n🍋多头自注意力机制 多头自注意力是Transformer模型的核心创新技术。相比于循环神经网络（Recurrent Neural Network, RNN）和卷积神经网络（ConvolutionalNeuralNetwork,CNN）等传统神经网络，多头自注意力机制能够直接建模任意距离的词元之间的交互关系。作为对比，循环神经网络迭代地利用前一个时刻的状态更新当前时刻的状态，因此在处理较长序列的时候，常常会出现梯度爆炸或者梯度消失的问题。而在卷积神经网络中，只有位于同一个卷积核的窗口中的词元可以直接进行交互，通过堆叠层数来实现远距离词元间信息的交换。\n多头自注意力机制通常由多个自注意力模块组成。在每个自注意力模块中，对于输入的词元序列，将其映射为相应的查询（Query,𝑸）、键（Key,𝑲）和值（Value,𝑽）三个矩阵。然后，对于每个查询，将和所有没有被掩盖的键之间计算点积。这些点积值进一步除以$\\sqrt{D}$进行缩放（𝐷是键对应的向量维度），被传入到softmax函数中用于权重的计算。进一步，这些权重将作用于与键相关联的值，通过加权和的形式计算得到最终的输出。在数学上，上述过程可以表示为： $$\rQ = XW^Q,\r$$$$\rK = XW^K,\r$$$$\rV = XW^V,\r$$$$\rAttention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{D}})V.\r$$与单头注意力相比，多头注意力机制的主要区别在于它使用了𝐻组结构相同，但映射参数不同的自注意力模块。输入序列首先通过不同的权重矩阵被映射为一组查询、键和值。每组查询、键和值的映射构成一个“头”，并独立地计算自注意力的输出。最后，不同头的输出被拼接在一起，并通过一个权重矩阵$W^O$∈$R^{H \\times H}$ 进行映射，产生最终的输出。如下面的公式所示： $$\rhead_n = Attention(XW^Q_n,XW^K_n,XW^V_n)\r$$$$\rMHA = Concat(head_1,...,head_N)W^O\r$$由上述内容可见，自注意力机制能够直接建模序列中任意两个位置之间的关系，进而有效捕获长程依赖关系，具有更强的序列建模能力。另一个主要的优势是，自注意力的计算过程对于基于硬件的并行优化（如GPU、TPU等）非常友好，因此能够支持大规模参数的高效优化。\n🍏前馈网络层 为了学习复杂的函数关系和特征，Transformer 模型引入了一个前馈网络层（Feed Forward Netwok, FFN），对于每个位置的隐藏状态进行非线性变换和特征提取。具体来说，给定输入𝒙，Transformer中的前馈神经网络由两个线性变换和一个非线性激活函数组成： $$\rFFN(X) = σ(XW^U + b_1)W^D + b_2\r$$ 其中$W^U$ ∈ $R^{H \\times H}$ 和$W^D$ ∈ $R^{H \\times H}$ 分别是第一层和第二层的线性变换权重矩阵，$b_1$ ∈ $R^{𝐻^′}$ 和 $b_2$ ∈ $R^H$ 是偏置项，𝜎是激活函数（在原始的Transformer中，采用 ReLU 作为激活函数）。前馈网络层通过激活函数引入了非线性映射变换，提升了 模型的表达能力，从而更好地捕获复杂的交互关系。\nTransformer 架构图\r🍐编码器 在Transformer 模型中，编码器（Encoder）的作用是将每个输入词元都编码成一个上下文语义相关的表示向量。编码器结构由多个相同的层堆叠而成，其中每一层都包含多头自注意力模块和前馈网络模块。在注意力和前馈网络后，模型使用层归一化和残差连接来加强模型的训练稳定度。其中，残差连接（Residual Connection）将输入与该层的输出相加，实现了信息在不同层的跳跃传递，从而缓解梯度爆炸和消失的问题。而LayerNorm则对数据进行重新放缩，提升模型的训练稳定性。编码器接受经过位置编码层的词嵌入序列𝑿作为输入，通过多个堆叠的编码器层来建模上下文信息，进而对于整个输入序列进行编码表示。由于输入数据是完全可见的，编码器中的自注意力模块通常采用双向注意力，每个位置的词元表示能够有效融合上下文的语义关系。在编码器-解码器架构中，编码器的输出将作为解码器（Decoder）的输入，进行后续计算。形式化来说，第𝑙层（𝑙∈{1,\u0026hellip;,𝐿}）的编码器的数据处理过程如下所示： $$\rX^′_l = LayerNorm(MHA(X_{l-1})+X_{l-1})\r$$$$\rX_l = LayerNorm(FFN(X^′_l)+X^′_l)\r$$其中，$X^′_l$ 和 $X_l$ 分别是该Transformer层的输入和输出，$X^′_l$是该层中输入经过多头注意力模块后的中间表示，LayerNorm表示层归一化。\n🥥解码器 Transformer 架构中的解码器基于来自编码器编码后的最后一层的输出表示以及已经由模型生成的词元序列，执行后续的序列生成任务。与编码器不同，解码器需要引入掩码自注意力（MaskedSelf-attention）模块，用来在计算注意力分数的时候掩盖当前位置之后的词，以保证生成目标序列时不依赖于未来的信息。除了建模目标序列的内部关系，解码器还引入了与编码器相关联的多头注意力层，从而关注编码器输出的上下文信息$X_L$。同编码器类似，在每个模块之后，Transformer 解码器也采用了层归一化和残差连接。在经过解码器之后，模型会通过一个全连接层将输出映射到大小为𝑉的目标词汇表的概率分布，并基于某种解码策略生成对应的词元。在训练过程中，解码器可以通过一次前向传播，让每个词元的输出用于预测下一个词元。而在解码过程，解码器需要经过一个逐步的生成过程，将自回归地生成完整的目标序列。解码器的数据流程如下所示： $$\rY^′_l = LayerNorm(MaskedMHA(Y_{l-1})+Y_{l-1})\r$$$$\rY^\"_l = LayerNorm(CrossMHA(Y^′_l,X_L)+Y^′_l)\r$$$$\rY_l = LayerNorm(FFN(Y^\"_l)+Y^\"_l)\r$$其中，$Y_{l-1}$ 和 $Y_l$ 分别是该Transformer 层的输入和输出，$Y^′_l$ 和 $Y^\u0026quot;_l$ 是该层中输入经过掩码多头注意力MaskedMHA和交叉多头注意力CrossMHA模块后的中间表示，LayerNorm 表示层归一化。然后将最后一层的输入𝒀𝐿映射到词表的维度上： $$\rO = softmax(W^LY_L)\r$$ 其中，𝑶 ∈$R^{H \\times V}$ 是模型最终的输出，代表下一个词在词表上的概率分布；$W^L$ ∈ $R^{H \\times V}$ 是将输入表示映射到词汇表维度的参数矩阵，而$W^LY_L$是概率化前的中间值，通常被称为logits。\n[1]. JacobDevlin et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. In: NAACL-HLT. 2019.\n[2]. Alec Radford et al. “Improving language understanding by generative pre-training”. In: OpenAI Blog (2018).\n","date":"2024-10-29T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/transformermodelbase/house-7497002_1280_hu17957794167668257846.png","permalink":"https://JiangZhiyu-1024.github.io/p/transformermodelbase/","title":"transformer模型架构"},{"content":"文献阅读 每周都完不成上次的任务，杂事太多了，尽快完成： Detecting formal thought disorder by deep contextualized word representations 竟然要收费，不看了，让gpt给讲一下\n这篇论文《通过深度上下文化词表示检测形式思维障碍》研究了如何使用深度学习中的上下文词嵌入模型（如ELMo）来检测精神疾病患者的形式思维障碍（Formal Thought Disorder, FTD）。FTD是一种影响语言表达和思维逻辑的精神疾病症状，常见于精神分裂症患者。\n论文主要利用深度上下文化词嵌入模型，如ELMo，通过生成词在句子中的嵌入表示，捕捉语境中词的多义性和丰富的语义信息。研究团队通过分析患者的语言数据，构建模型来区分正常语言与存在思维障碍的语言表达，从而实现自动化的FTD检测。\n研究的核心贡献是使用更复杂的语义表示（如上下文化词嵌入）来解决传统方法无法有效处理的词义歧义问题。\nDistributed Representations of Words and Phrases and their Compositionality 《Distributed Representations of Words and Phrases and their Compositionality》是由Mikolov等人在2013年提出的一篇重要论文，介绍了Word2Vec模型的改进和创新。这篇文章主要讨论了如何通过分布式词表示（distributed word representations）更好地捕捉单词和短语的语义信息，并且提出了两种核心模型：CBOW（Continuous Bag of Words） 和 Skip-gram。\n这篇论文的主要贡献包括：\nSkip-gram和CBOW模型：论文提出了两种用于生成词向量的架构。Skip-gram模型的目标是根据一个词预测其上下文，而CBOW模型则根据上下文词来预测目标词。这两种模型能够有效地捕捉单词之间的语义关系。 层次Softmax和负采样：为了提高训练大规模语料库的效率，作者引入了层次Softmax和**负采样（negative sampling）**技术，这大幅降低了模型的计算复杂度，使得大规模训练变得可行。 词和短语的组合性：论文不仅处理了单词的表示，还处理了短语的表示，通过数据驱动的方式将多词短语映射为词向量，捕捉更复杂的语义结构。 向量运算反映语义关系：Word2Vec生成的词向量能够通过简单的向量运算表达词语之间的关系。例如，向量运算 vec(\u0026quot;King\u0026quot;) - vec(\u0026quot;Man\u0026quot;) + vec(\u0026quot;Woman\u0026quot;) ≈ vec(\u0026quot;Queen\u0026quot;) 说明了这种分布式表示在捕捉语义关系上的强大能力。 LLM BOOK 找到一本书，准确的来说是一个github仓库，组织者把近些年的LLM相关的东西都整进去了，好全，写的真好，库库看吧那就\n先把资源放在这里：RUCAIBox/LLMSurvey: The official GitHub page for the survey paper \u0026ldquo;A Survey of Large Language Models\u0026rdquo;.\n第一部分 背景与基础知识 语言模型的发展历程（P16） LLM大语言模型那肯定是从LM语言模型来的，语言模型的发展又经历了：\n统计语言模型（StatisticalLanguageModel, SLM） 神经语言模型（NeuralLanguageModel,NLM） 预训练语言模型（Pre-trainedLanguageModel,PLM） 大语言模型（LargeLanguageModel, LLM） LLM的特点（P19） 具有较为丰富的世界知识 具有较强的通用任务解决能力 具有较好的复杂任务推理能力 具有较强的人类指令遵循能力 具有较好的人类对齐能力 具有可拓展的工具使用能力 大语言模型对于产业应用带来了变革性的技术影响，将会催生一个基于大语言模型的应用生态系统。\n第二部分 预训练 大语言模型的第一个训练阶段是预训练，预训练语料的规模和质量对于提升大语言模型的能力至关重要。\n数据来源 通用文本数据和专用文本数据。通用文本数据涵盖了网页、书籍和对话文本等。由于通用文本数据规模较大、多样性强且易于获取，大多数大语言模型都会收集大量的通用文本数据，以增强其语言建模能力。此外，为了进一步提升大语言模型在特定专业任务上的表现，人们还将预训练语料的范围扩展至更专业的数据集，如多语数据、科学数据和代码数据等。\n通用文本数据 现有大语言模型预训练数据中各种数据来源的比例分布图\u003e\r专用文本数据 多语文本、 科学文本、代码。\n典型的预训练数据预处理流程图\u003e\r数据预处理 构建并使用系统化的数据处理框架（如开源库Data-Juicer）\n具体操作（P74）\n附录 softmax是啥？ Softmax是一种常用的函数，特别是在多分类任务中，用来将一个向量中的元素转换为0到1之间的概率分布。其核心作用是： 将实数映射为概率分布：Softmax 接受一个实数向量，输出一个与输入维度相同的向量，其中每个元素的值表示该元素作为某个类别的概率，且这些概率的和为1。这样就可以根据这些概率来进行分类预测。\n公式：Softmax 函数的定义如下： $$\r\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\r$$ 其中，\\( z_i \\) 是输入向量中的某个元素，\\( e^{z_i} \\) 是该元素的指数值，分母是所有输入元素的指数值的和。这个函数将每个 \\( z_i \\) 映射为一个在 \\( (0,1) \\) 之间的概率，并且所有输出的概率之和为1。\n作用机制：\n指数放大效果：Softmax 函数通过对输入值取指数（\\( e^x \\)），会将较大的输入值放大更多，而较小的值则被进一步压缩，突出更大的概率。 归一化：Softmax 保证所有输出的概率之和为1，符合概率的定义。 应用场景：在神经网络的最后一层，特别是分类任务中，Softmax 常用于多类分类问题，比如在自然语言处理（NLP）中的词分类、图像识别中的对象分类等。它将网络的输出转化为易于理解的概率值，帮助模型决定输入属于哪个类别。\n举例 假设我们有一个模型输出三个值 \\( [2.0, 1.0, 0.1] \\)，通过 Softmax 转换后得到的结果是大约 \\( [0.71, 0.26, 0.03] \\)，其中最大的值0.71表示模型认为输入属于第一个类别的概率最大。\n简言之，Softmax 是一种将输出转换为概率分布的工具，特别适合多分类问题的场景。\n看了半天论文，感觉没有一个时间线很难受，不清楚那些论文之间的关系，问问gpt： 基础概念与理论 先掌握大模型背后的基本理论，这些知识是理解后续论文的基础：\n语言模型的基础： 经典论文：《Attention Is All You Need》（Transformer） 相关概念：自注意力机制、序列到序列模型、词嵌入（如Word2Vec、GloVe） 自回归模型与自编码器： GPT 系列（GPT, GPT-2, GPT-3）的原理和应用 BERT 及其衍生模型的预训练与微调方式 扩展学习与生成式任务： 《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》 生成式预训练模型的设计与任务应用（如文本生成、机器翻译） 模型的训练与优化 学习如何高效地训练大模型，并且了解模型的优化技术：\n微调与参数高效训练： Efficient Fine-Tuning Techniques for Large Models: Adapter, LoRA 等 探索模型压缩技术（知识蒸馏、量化、剪枝等） 对比学习与自监督学习： 《SimCLR》、《MoCo》 等代表性论文，理解对比学习在大模型中的应用 模型对齐与安全 在大模型发展过程中，模型对齐（alignment）和伦理问题变得越来越重要：\n模型对齐与价值观嵌入： 《InstructGPT: Training language models to follow instructions with human feedback》 RLHF（强化学习通过人类反馈）等对齐策略 模型安全与公平性： 探讨模型偏见问题以及在实际应用中的安全隐患 视觉-语言模型与多模态模型 学习大模型在跨模态任务中的应用，比如视觉-语言任务：\nCLIP（Contrastive Language-Image Pretraining）：理解图像和文本共同表征的训练方法 DALL-E、Stable Diffusion：生成式视觉任务的模型设计与应用 视觉语言对话系统：如《Visual ChatGPT》 前沿研究与新趋势 在深入理解基本模型后，可以关注大模型领域的前沿研究方向：\n大模型与知识图谱的结合：如何将知识图谱融入语言模型 领域自适应模型：如何通过特定领域数据进行模型的进一步微调 检索增强生成（RAG）：如《RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》 实战应用与开源框架 阅读开源项目中的技术文档，参与开源项目（如Hugging Face的模型库） 学习如何应用大模型解决实际问题，并尝试训练和微调自己的模型 这样，你可以从基础概念、模型训练到前沿研究和实战应用，逐步深入，既能掌握理论又能结合实践。这个过程可以帮助你筛选出重要的论文和研究方向。\n对齐是什么意思？ 在大型语言模型（LLM）中，“对齐”通常指的是模型的输出与人类意图或价值观的匹配程度。具体来说，它涉及确保模型在生成文本时能够产生符合人类期望的、有益的和安全的内容。对齐的目标是减少模型生成有害、误导或不准确的信息。\n对齐可以通过多种方法实现，包括：\n训练数据的选择和清洗：使用高质量的数据集进行训练，确保模型接触到的内容符合人类的价值观和期望。\n强化学习：利用人类反馈来微调模型，使其在生成内容时更符合人类的意图。\n安全性和伦理性评估：在模型发布之前，对其输出进行评估，以识别和修正潜在的偏见或有害内容。\n通过这些方法，可以提高模型在特定任务或应用场景中的可靠性和安全性。\narXiv 论文是什么？ arXiv 是一个开放获取的电子预印本存档，主要用于科学研究领域，特别是在物理学、数学、计算机科学、统计学、金融学、生物学和其他相关学科。研究者可以在 arXiv 上免费上传和分享他们的研究论文，通常是在正式出版之前的版本。\narXiv 的特点包括：\n快速发布：研究者可以迅速分享他们的研究成果，而不必等待同行评审和正式出版的过程。\n开放获取：任何人都可以访问和下载论文，无需订阅或支付费用。\n广泛的学科覆盖：涵盖多个领域，促进跨学科的交流和合作。\n社区反馈：研究者可以在正式发表之前收到来自社区的反馈，帮助他们改进论文。\narXiv 的论文通常以“预印本”的形式存在，因此它们可能尚未经过同行评审。这意味着在引用这些论文时，研究者需要谨慎，了解其可能的局限性。\n在arXiv上发还未出版的论文不怕被别人剽窃成果吗？ 在 arXiv 上发布未出版的论文确实存在被剽窃的风险，但研究者通常认为在 arXiv 上共享研究成果是有益的，原因如下：\n时间戳和公开记录：一旦论文被上传到 arXiv，上传的时间戳可以作为知识产权的证明。这在某种程度上可以保护研究者的原创性。\n学术社区的支持：arXiv 是一个活跃的学术社区，研究者通常会关注相关领域的论文，并且对剽窃行为持有负面态度。学术界普遍重视诚信，剽窃通常会受到严厉的谴责。\n早期反馈：通过发布预印本，研究者可以获得来自同行的反馈，从而改进自己的工作。这也有助于在正式出版之前建立声誉。\n增加曝光率：在 arXiv 上发布可以提高研究的可见性，吸引潜在合作者或资助者。\n通用人工智能系统是什么？ 通用人工智能系统（AGI, Artificial General Intelligence）指的是一种能够理解、学习和应用知识，完成任何人类所能完成的智力任务的人工智能系统。与当前的狭义人工智能（Narrow AI）相比，后者在特定任务或领域表现出色（如图像识别、自然语言处理等），AGI 的目标是实现一种更广泛的智能，具备跨领域的学习和适应能力。\nAGI 的特点包括：\n自主学习：能够从有限的数据中学习并推理，具备自我改进的能力。\n多任务处理：能够在多个不同的领域和任务中表现出色，而不局限于特定的应用。\n常识推理：具备理解和应用常识的能力，能够处理复杂的、不确定的环境。\n灵活性和适应性：能够适应新情况和挑战，灵活地应对不同的环境和需求。\nAGI 的发展仍然是一个前沿研究领域，目前尚未实现。许多研究者在探索如何构建这样的系统，同时也讨论其潜在的伦理和社会影响，例如对就业、隐私和安全的影响。\n思考 随着看的东西多了，和使用chatgpt，我隐隐感觉到，由于chatgpt的语言模型主要是通过预测下一个最可能的词来生成文本，这一过程称为自回归生成（autoregressive generation）。导致gpt经常会顺着你的话去说，即有更小的概论去反对你的话，尽量让你说的话看起来正确，当然，如果你说的是：“太阳从西边升起”，那么他会义无反顾的反驳你。\n","date":"2024-10-26T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/third-week/cards-8594729_640_hu5982296781466901225.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/third-week/","title":"第3周工作总结"},{"content":"第一次成功创建个人博客 回忆 想当年其实我自己创建过，跟着一个教程做的，但当时才大一，很多东西都不懂，哼哧哼哧做了半天，最后不知道哪里不行，没有成功，于是放弃了，改用CSDN写，但现在我又有时间了，保研结束，该拾掇拾掇自己了，我觉得个人博客就像自己的一个小花园，写文章就好像在里面种花，精心打理后很有成就感，所以这次花了一天时间重新建起了我的花园，圆了大一时的梦，顺便做个记录，好记性不如烂笔头，忘了回来查查。\n资料 这次是跟着B站一个up主建立的博客，讲的非常清楚，把他的视频链接放在这里。\nhttps://www.bilibili.com/video/BV1bovfeaEtQ?vd_source=9cfc0ca3d84e1ed0064763e32a183f15\n记录 文章存放 content/post放的是文章\n文章名 文章都叫index.md\n日期问题 每个文章前面要有插入一个yaml块，里面放一些信息，日期这里一开始有一个非常搞心态的问题，因为我发现我随便测试日期，显示的日期都是10-10，09-09这种，日期和月份一样，后来检查了半天才发现，视频中up主在修改配置文件hugo.yaml时，把日期格式修改为了2006-01-02，而我顺手就写了个2006-01-01，我发现他把我这个格式理解成了月份和日期要一样，于是产生了那个令人哭笑不得的现象。\n深刻理解配置文件 今天才知道仓库的actions存放的是一些自动执行的脚本，一开始新建文章的开头配置文件没有写好，导致脚本运行一直出错，现在改好之后，看不到错的记录了，一片绿很养眼\nslug 这个头配置文件里的slug: firstblog是给链接起个名字，我这个内容是从我另一个博客复制过来的，于是他俩冲突了，显示的内容都杂在一起了，坏了，刚交上去就出bug，正好记上\n收获 对github仓库加深了理解\n对一些配置文件的存在有了新的认识\n致谢 感谢up主：Letere-莱特雷\n对，因为对这个博主的英文名字很感兴趣，于是给自己起了一个名字Zion Blaze,我感觉very古德，比我的损友老马起的foolish fishy强一w倍，明天联系我的律师起诉他。\n感谢chatgpt同学，今天依旧稳定发力，明天开始继续研究LLM，争取有朝一日让gpt同学感谢感谢我。\n","date":"2024-10-25T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/firstblog/liuying5_hu10731212018240133723.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/firstblog/","title":"记第一次创建我的个人博客"},{"content":"文献阅读 看了知乎的科普文章，先从经典的论文开始看：\nAttention Is All You Need 《Attention is All You Need》是由Vaswani等人在2017年提出的一篇重要论文，首次引入了Transformer模型。以下是其主要内容的总结：\n背景：传统的序列到序列模型（如RNN和LSTM）在处理长序列时存在瓶颈。Transformer模型通过自注意力机制克服了这些限制，使得模型可以并行处理数据，提升了训练效率。\n模型架构：\nEncoder-Decoder结构：Transformer由两个主要部分组成：编码器和解码器。编码器将输入序列转化为上下文向量，解码器则根据上下文生成输出序列。 自注意力机制：每个输入的表示都通过自注意力机制与其他输入进行交互，从而捕捉序列中的依赖关系。每个位置的表示通过加权求和得到，权重由输入的相似度计算得到。 多头注意力：模型使用多个注意力头并行计算不同的表示，能够更全面地捕捉信息。 位置编码：由于Transformer没有循环结构，论文引入了位置编码，提供序列中词语的位置信息，以保留词序信息。\n训练与优化：Transformer使用了残差连接和层归一化，以提高训练的稳定性和收敛速度。论文中还介绍了基于Adam优化器的训练过程。\n实验结果：Transformer在机器翻译任务上取得了显著的性能提升，并且在其他NLP任务中也展示了强大的通用性。\n影响：该论文奠定了现代NLP研究的基础，推动了基于Transformer的预训练语言模型（如BERT、GPT等）的发展。\n差分隐私深度学习(Deep Learning with Differential Privacy) 《Deep Learning with Differential Privacy》是由Martin Abadi等人在2016年提出的一篇重要论文，探讨了在深度学习中实现差分隐私的策略。以下是其主要内容的总结：\n背景：\n差分隐私：一种保护用户数据隐私的技术，旨在确保模型在训练时无法推断出任何单个用户的敏感信息。即使攻击者知道模型和数据集的其余部分，也无法有效地获取关于某个特定用户的信息。 深度学习与隐私问题：深度学习模型通常依赖大量数据，这使得保护用户隐私变得尤为重要。 模型与训练：\n噪声注入：通过在模型的梯度更新中添加噪声，可以达到差分隐私的效果。噪声的大小与隐私预算（ε）相关，预算越小，隐私保护越强，但模型的性能可能会受到影响。 Privacy Accounting：论文提出了一种计算隐私损失的框架，确保在每次训练步骤中监控和控制隐私损失。 算法设计：\nDP-SGD（Differentially Private Stochastic Gradient Descent）：论文提出了一种改进的随机梯度下降算法，结合了梯度裁剪和噪声注入，以实现差分隐私。这种方法确保在训练过程中，个别数据对模型的影响是被限制的。 实验结果：\n论文通过实验展示了DP-SGD在图像分类任务上的有效性，同时比较了不同噪声级别对模型性能的影响，证明了在保持合理准确性的同时实现差分隐私是可行的。 应用与展望：\n论文强调了在各类机器学习应用中（如医疗、金融等）保护隐私的重要性，提出将差分隐私技术与深度学习结合是一个有效的解决方案，并鼓励未来的研究在这一领域的进一步探索。 1 2 3 感觉偏向数学一点，全是公式和证明，好难看懂。 差分隐私顾名思义就是用来防范差分攻击的 加入噪声，改变原来的概率分布 Deep Reinforcement Learning from Human Preferences 《Deep Reinforcement Learning from Human Preferences》是由Christiano等人在2017年提出的一篇重要论文，探讨了如何利用人类偏好来训练强化学习模型。以下是其主要内容的总结：\n背景：\n传统的强化学习方法通常依赖于奖励信号来指导学习，但设计有效的奖励函数在复杂任务中往往困难且耗时。 人类可以提供更直观的偏好信息，这为训练强化学习代理提供了一种新途径。 方法概述：\n人类偏好收集：论文提出了一种方法，通过收集人类对不同行为的偏好来生成奖励信号。人类评估多个策略的表现，从中选择更好的策略，以此来建立奖励模型。 奖励模型：利用收集到的人类偏好数据，训练一个深度神经网络来预测代理在给定状态下的奖励。这种奖励模型可以用来指导强化学习算法。 训练过程：\n深度强化学习：在获得奖励模型后，使用深度强化学习算法（如Proximal Policy Optimization，PPO）来训练代理。代理通过与环境交互，不断优化其策略。 迭代学习：通过不断收集新的偏好数据并更新奖励模型，实现迭代学习，使得代理能够逐步提升性能。 实验结果：\n论文在多个复杂任务（如Atari游戏和模拟环境）中展示了该方法的有效性，代理能够通过学习人类的偏好，超越基于手动设计的奖励函数的表现。 结果表明，利用人类偏好进行训练能够加速学习过程，并提升代理的最终性能。 意义与展望：\n该方法展示了人类偏好在强化学习中的潜力，提出了一种新的思路来解决奖励设计问题。 论文强调了未来研究中结合人类反馈和偏好的重要性，特别是在涉及复杂决策和安全性问题的领域。 附录 残差连接（residual connection）和层归一化（layer normalization）是什么？ 残差连接（Residual Connection）和 层归一化（Layer Normalization）是深度学习中常用的两种技术，主要用于提高神经网络的训练效率和性能。下面是它们的定义和作用：\n残差连接（Residual Connection） 定义：残差连接是一种将输入直接与输出相加的结构，通常在深度神经网络中使用。公式表示为： $$\r\\text{Output} = \\text{Layer}(x) + x\r$$ 其中 \\(x\\) 是输入，\\(\\text{Layer}(x)\\) 是通过某个层（如卷积层或全连接层）处理后的输出。\n作用：\n缓解梯度消失问题：在非常深的网络中，梯度可能会随着层数增加而消失，导致网络难以训练。残差连接提供了一条捷径，使得梯度可以更容易地通过网络传播。 加速收敛：通过引入直接路径，残差连接有助于提高网络的收敛速度。 提高模型性能：它允许模型学习恒等映射，这使得训练更深的网络成为可能，提升了模型的表现。 层归一化（Layer Normalization） 定义：层归一化是一种对网络中每一层的激活进行归一化的技术，它通过计算每个样本的均值和标准差来调整激活值。层归一化的公式为： $$\r\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta\r$$ 其中：\n\\(x\\) 是输入。 \\(\\mu\\) 是输入的均值。 \\(\\sigma\\) 是输入的标准差。 \\(\\epsilon\\) 是一个小常数，避免除以零。 \\(\\gamma\\) 和 \\(\\beta\\) 是可学习的参数，用于缩放和平移归一化的结果。 作用：\n减少内部协变量偏移：层归一化帮助减少模型训练过程中每层输入的变化，使得网络在训练时更加稳定。 提高训练速度：通过标准化输入，层归一化可以使得训练过程更加平滑，从而加快收敛速度。 适用于变长序列：与批量归一化（Batch Normalization）不同，层归一化可以直接应用于变长的输入序列，适合于RNN和Transformer等模型。 总结 残差连接主要通过提供捷径来缓解深层网络中的梯度消失问题，并加速训练过程。 层归一化则通过标准化激活值来提高训练的稳定性和速度。这两者结合使用，能够显著提升深度学习模型的表现。 ","date":"2024-10-20T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/second-week/leaves-6729056_640_hu5688234115053010817.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/second-week/","title":"第2周工作总结"},{"content":"工作总结： 文献阅读 综述类： 2023年国内大模型发展综述与趋势研判_赵子忠\nAIGC大模型测评综述：使能技术、安全隐患和应对_许志伟\nAI大模型发展综述_张乾君\n深入： 《A Neural Probabilistic Language Model》是Yoshua Bengio等人在2003年发表的一篇重要论文，提出了一种基于神经网络的语言模型方法。这篇论文的主要目标是通过神经网络来改进传统的语言模型。\n传统的语言模型的任务是估计一个词序列的概率： P(w1,w2,…,wT) 在这种情况下，模型需要根据上下文词来预测当前词的概率。传统的做法一般是通过统计方法（如n元语法模型）来估计这些概率，但这类方法有一个很大的问题：它们无法处理高维数据，尤其是在词汇量非常大的情况下，计算复杂度极高，且泛化能力有限。\nBengio等人提出了一种新的神经网络语言模型，目的是克服这个问题。这个模型的核心思想是使用一个多层感知器来学习词的分布式表示（即词向量），并基于这些词向量来估计概率。具体过程大致如下：\n词嵌入：每个词通过一个嵌入矩阵被映射为一个连续向量，这种向量的维度比词汇表要小得多。这一步的目的是将离散的词映射到一个连续的低维空间，减少计算复杂度。\n神经网络建模：利用一个前馈神经网络（通常是多层感知器），根据前面的若干个词的词向量，来预测下一个词的条件概率。通过这个网络，模型可以学习到词与词之间的相似性和上下文关系。\n训练：模型通过最大化训练数据的似然估计来进行训练。具体来说，模型会调整嵌入矩阵和网络的参数，使得预测的概率尽可能接近真实的词序列概率。\n这篇论文的创新之处在于提出了“词向量”的概念，并展示了使用神经网络来处理语言建模问题的潜力。这为后来的深度学习在自然语言处理（NLP）中的应用奠定了基础。\n附录 什么是语言模型 语言模型（Language Model, LM）是自然语言处理中的一个核心概念，其主要任务是对一个词序列的概率分布进行建模。简单来说，语言模型的目标是估计一段文本或句子出现的概率，或者根据前面的词来预测下一个词。\n语言模型的定义 给定一个词序列 $$\rP( w_1, w_2, \\dots, w_T )\r$$ ，语言模型的目标是计算这个序列的联合概率： $$\rP(w_1, w_2, \\dots, w_T)\r$$ 这通常可以分解为条件概率的乘积： $$\rP(w_1, w_2, \\dots, w_T) = P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_1, w_2) \\cdot \\dots \\cdot P(w_T | w_1, w_2, \\dots, w_{T-1})\r$$ 这意味着每个词的概率取决于它前面的词。一个好的语言模型能够通过学习语料库中的模式，准确地捕捉这些条件概率。\n语言模型的用途 语言模型有广泛的应用场景，包括但不限于：\n文本生成：语言模型可以用来自动生成自然语言文本。例如，给定一部分句子，模型可以预测并生成合理的下一个词，从而逐步生成完整的句子或段落。\n机器翻译：在机器翻译中，语言模型帮助翻译系统生成符合目标语言语法的翻译结果。\n语音识别：语言模型能够帮助语音识别系统，在可能的识别结果中选择最合适的词序列。\n拼写纠正：在文本输入或搜索引擎中，语言模型可以用于纠正拼写错误，提供更符合语言习惯的结果。\n传统语言模型 在神经网络语言模型之前，最常用的语言模型是n元语法模型（n-gram Model）。n元语法模型通过假设一个词的概率只与它前面的n-1个词有关来简化问题。举个例子，三元语法模型（trigram model）认为： $$\rP(w_T | w_1, w_2, \\dots, w_{T-1}) \\approx P(w_T | w_{T-2}, w_{T-1})\r$$这种简化降低了计算复杂度，但也限制了模型的表达能力，因为它只能捕捉到短距离的上下文信息，无法充分考虑长距离的依赖关系。\n神经网络语言模型 传统语言模型的一个主要问题是词汇量增大时，模型的计算复杂度随之增加。为了解决这个问题，Yoshua Bengio等人在《A Neural Probabilistic Language Model》论文中提出了使用神经网络进行语言建模的想法。这种神经网络模型通过将词表示为词向量（word embeddings），并通过深度学习模型来捕捉更复杂的语境和词间关系，从而解决了传统语言模型的许多不足之处。\n总之，语言模型的核心任务就是对自然语言的概率进行建模，并通过学习语言中的统计模式来预测文本。\n什么是纬度灾难？ 维度灾难（Curse of Dimensionality）是指在处理高维数据时，随着数据维度增加，问题的复杂性呈指数级增长，导致很多传统方法在高维空间中变得低效或不可行的现象。\n其影响包括：\n数据稀疏性：在高维空间中，数据点变得非常稀疏，因为数据的总量相对于空间的体积变得极其小，导致模型难以从有限的数据中捕捉到模式和关系。简单来说，随着维度的增加，需要的样本量也随之爆炸性增长，否则模型可能会过拟合或无法泛化。\n计算复杂度增加：在高维空间中，计算距离、相似性等操作的复杂度大大增加，使得很多算法在高维情况下效率低下。\n模型的泛化能力下降：由于数据点过于稀疏，模型很难学习到有用的特征，从而可能导致泛化能力下降。\n具体来说，维度灾难对机器学习、统计学、最优化等领域带来了极大的挑战，尤其是当模型需要处理很多特征或变量时。为应对这个问题，研究人员提出了许多方法，如降维技术（如主成分分析PCA）和分布式表示（如词嵌入），以减少有效维度或通过分布式方法来表示复杂的数据结构。\n有概率为什么要引入似然的概念，他们有什么不同？ 概率和似然虽然看起来相似，但在统计学中它们有不同的定义和用途：\n概率（Probability） 概率是用来描述随机事件发生的可能性。在给定模型参数的情况下，概率告诉我们某个事件或观测结果发生的机会有多大。\n假设我们有一个随机变量 \\( X \\)，它取某个值 \\( x \\) 的概率可以表示为 \\( P(X = x|\\theta) \\)，其中 \\( \\theta \\) 是模型的参数。例如，如果我们有一个硬币掷出的模型，参数 \\( \\theta \\) 可以是硬币正面朝上的概率（比如 \\( \\theta = 0.5 \\)）。\n核心问题：给定模型的参数 \\( \\theta \\)，事件 \\( X = x \\) 发生的概率是多少？ 似然（Likelihood） 似然是从反方向来考虑的。它描述的是在已知观测数据的前提下，模型参数的可能性。具体来说，似然是给定观测数据后，模型参数如何使观测数据变得最可能的一个度量。\n假设我们已经观察到数据 \\( X = x \\)，现在我们想知道，在不同的模型参数 \\( \\theta \\) 下，这个数据出现的可能性有多大。似然可以表示为 \\( L(\\theta|X = x) \\)，或者更直观地写作 \\( P(X = x|\\theta) \\)，虽然数学表达式相同，但意义不同——在这里我们关注的是参数 \\( \\theta \\) 的值使观测到的数据最可能。\n核心问题：给定观测到的数据 \\( X = x \\)，模型参数 \\( \\theta \\) 有多大可能是正确的？ 概率和似然的区别 概率：我们知道参数 \\( \\theta \\)，希望知道某个事件 \\( X \\) 发生的概率。\n问题：给定参数，事件的概率是多少？ 似然：我们知道事件（观测数据），希望推断出最有可能的参数 \\( \\theta \\)。\n问题：给定观测数据，哪个参数最可能是正确的？ 例子 抛硬币的例子 概率：假设你有一枚硬币，已知它是公平的，即 \\( \\theta = 0.5 \\)，那么掷硬币得到正面的概率是 \\( P(\\text{正面}) = 0.5 \\)。\n似然：现在，你掷硬币10次，得到8次正面和2次反面。你不知道硬币的公平性（即不知道 \\( \\theta \\) 的值）。你想根据这次实验结果估计硬币正面朝上的概率 \\( \\theta \\)。此时，你需要用似然来衡量在不同 \\( \\theta \\) 值下，观测结果（8次正面，2次反面）出现的可能性有多大，从而找到最有可能的 \\( \\theta \\)。\n似然函数 \\( L(\\theta|X) \\) 可能在某个 \\( \\theta \\) 值处达到最大值，这个 \\( \\theta \\) 就是最能解释观测数据的参数值。\n总结 概率用于给定模型参数时预测事件的发生可能性。 似然用于在已知观测数据时，推断哪个参数最能解释这些数据。 ","date":"2024-10-13T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/first-week/banlan_hu6307248181568134095.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/first-week/","title":"第1周工作总结"}]