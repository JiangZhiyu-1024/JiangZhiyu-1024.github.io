[{"content":"InferDPT：面向黑盒大语言模型的隐私保护推理 摘要 以ChatGPT为代表的大型语言模型（LLMs）极大简化了文本生成任务。然而，它们也引发了关于隐私风险的担忧，如数据泄露和未经授权的信息收集。现有的隐私保护推理解决方案面临着与计算时间和通信成本相关的实际挑战。本文提出了InferDPT，这是第一个针对黑盒LLM的隐私保护推理的实用框架，旨在实现文本生成中的差分隐私。InferDPT包括两个关键模块：“扰动模块”利用差分隐私机制生成扰动的提示，从而实现与黑盒LLM的隐私保护推理；“提取模块”则受到知识蒸馏和我们观察到的现象的启发，从扰动生成结果中提取一致且连贯的文本，确保成功完成文本生成。为了解决与先前差分隐私机制易受嵌入逆转攻击的隐私问题，我们引入了RANTEXT，这是一种新型的差分隐私机制，集成在InferDPT的扰动模块中，提出了“随机邻接列表”（RANdom adjacency list）概念，用于对提示中的文本进行扰动。实验结果表明，InferDPT的文本生成质量与非隐私的GPT-4相当，而RANTEXT在隐私与效用之间的权衡上超越了现有的最先进机制，即SANTEXT+和CUSTEXT+。即使在隐私参数ε值为6.0的情况下，RANTEXT也能实现超过90%的平均隐私保护率，抵御嵌入逆转攻击，比SANTEXT+高出0.58倍，比CUSTEXT+高出3.35倍。\nI. INTRODUCTION 近年来，大语言模型（LLMs）的快速发展引起了全球学术界和工业界的广泛关注 [1]。ChatGPT [2] 作为一个显著的例子，在2023年11月6日，由OpenAI首席执行官Sam Altman在公司首次开发者大会上宣布，已达到每周1亿活跃用户的里程碑 [3]。ChatGPT的广泛流行极大地方便了人们的日常工作和生活。用户通过API或网页界面与ChatGPT互动，用于生成文本，应用场景包括但不限于撰写文章、记录日常工作活动、以及为新产品撰写广告文案 [4]。然而，技术是把双刃剑。虽然LLMs提供了无与伦比的便利和实用性，但它们也带来了关于隐私泄露的重大担忧。有些情况下，LLMs的滥用导致了严重的隐私侵犯。例如，三星员工泄露了公司机密的会议记录和关于未发布产品的敏感数据 [5]。此外，最近发生的一起事件中，GPT-3.5意外泄露了个人的自拍照 [6]。这些事件重新激发了公众对上传个人数据到LLMs可能带来的隐私风险的担忧 [7]。因此，解决上传查询内容（在LLMs文本生成背景下称为“prompt”）的隐私问题变得至关重要。通常，文本生成任务中的prompt包括一个基本的写作指令和上传的私人文档1（根据微软AI Builder文档的定义 [8]）。\nExisting Solutions 以往的研究未能在实际文本生成任务中的推理过程中保护prompt中文档的隐私，尽管它们已经探讨了语言模型的隐私保护技术。如表I所示，CipherGPT [9] 在变换器架构模型中利用同态加密技术，实现在加密数据上的推理。尽管这些技术在理论上可以用于隐私保护的文本生成任务，但由于计算时间和通信成本的显著问题，它们在实际应用中存在局限性。TextObfuscator [10] 和 DP-Forward [11] 在分割学习中的数据传输过程中加入噪声。然而，这些方法主要设计用于分类任务，且不适用于模型拥有者（如OpenAI [14]）未公开LLM架构细节的黑盒场景，这通常出于模型的知识产权和商业价值考虑。\n另一方面，SANTEXT+ [12] 和 CUSTEXT+ [13] 利用差分隐私（DP）技术 [15]，将文本中的敏感词替换为语义上相近的固定词汇集中的词，这在DP的背景下称为“词邻接列表”。这些方法同样设计用于隐私保护的分类任务，能够容忍差分隐私噪声引入的显著信息失真。对于本文探讨的隐私保护文本生成任务 [16]，即使是prompt中的轻微信息失真，也会导致生成文本的连贯性和一致性问题，因此SANTEXT+ 和 CUSTEXT+ 对此类任务并不直接有效。此外，我们在图6中的实验结果表明，SANTEXT+ 和 CUSTEXT+ 易受嵌入反演攻击 [17] 的影响：即使在隐私参数ε设置为0.01的极端情况下，攻击者仍然可以从SANTEXT+或CUSTEXT+中恢复出40%的原始私密词。这一现象的原因在于：（1）在SANTEXT+中，一定比例的词汇没有被扰动；（2）在CUSTEXT+中，每个词汇都有一个固定的小型词邻接列表，默认为20，这导致了词汇未被替换的概率较高，从而引发隐私泄漏。\nOur Proposal 为了保护在黑盒LLM推理过程中文档的隐私，并解决差分隐私（DP）引发的信息偏差问题，我们提出了一个框架——InferDPT，用于文本生成任务。InferDPT的基本思路来源于知识蒸馏 [18] 和我们的观察，后者通过实验结果在第IV-B节中得到支持：DP扰动生成的prompt中的词汇，在原始prompt的多个部分中共享相同的词汇。此外，它们之间相同词汇的数量与隐私参数ε呈正相关。InferDPT由扰动模块和提取模块组成。在扰动模块中，InferDPT采用差分隐私机制（如SANTEXT+ 和 CUSTEXT+）对原始文档进行扰动，得到扰动后的prompt，并将其上传至远程LLMs。在提取模块中，InferDPT部署了一个本地模型，该模型比远程LLMs更轻量且能力较弱。该本地模型提取并重构远程LLMs生成的扰动结果。利用扰动后的生成结果作为参考，根据原始文档推断文本，InferDPT不仅保护了prompt的隐私，而且通过蒸馏远程大语言模型的能力，提升了本地模型生成的文本质量。\n为了解决SANTEXT+和CUSTEXT+在抵抗嵌入反演攻击中的脆弱性，我们开发了RANTEXT。RANTEXT是一个新颖的差分隐私机制，集成到InferDPT的文本扰动过程中。RANTEXT引入了“随机邻接列表”的概念，用于令牌级扰动。对于每个令牌，它使用拉普拉斯分布 [19] 动态确定随机邻接列表的大小，然后从该列表中采样一个新令牌，以替换原始的私密令牌。\n为了评估我们方案的效用和隐私保护能力，我们在GPT-4 [14]（当前最流行的黑盒大语言模型之一）上进行了实验，针对三个数据集进行实际的开放式文本生成任务。我们发现，现有的差分隐私攻击策略对RANTEXT的效果不足。我们提出了一种自适应攻击——GPT推理攻击，利用了GPT-4的能力。\nOur Contributions 我们总结了主要贡献如下：\n我们提出了 InferDPT，这是首个针对黑盒大语言模型隐私保护推理的实用框架，在文本生成中实现了差分隐私。 我们开发了 RANTEXT，一种新颖的局部差分隐私指数机制，集成到InferDPT的文档扰动过程中。RANTEXT解决了现有差分隐私机制易受嵌入反演攻击的问题，进一步增强了对隐私威胁的防御能力。 我们在第VI-B节中针对三个数据集进行了实际开放式文本生成任务的实验。实验结果表明，当隐私参数ε设置为3.0，且使用3.89GB的本地模型时，InferDPT在三个指标上的生成质量与GPT-4相当。 我们在第VII节评估了四类隐私威胁。特别是，当隐私参数ε设置为6.0，并选择嵌入反演攻击的前10个候选项时，RANTEXT的平均隐私保护率超过90%，相比CUSTEXT+提升了3.35倍，相比SANTEXT+提升了0.58倍。 II. PRELIMINARIES A. Large Language Models 大语言模型（LLMs）是先进的人工智能系统，训练于庞大的数据集。它们旨在理解、生成和解释人类语言，展示了在各种与语言相关任务中的卓越多功能性。通常，LLMs根据用户上传的$prompt P_{\\text{ro}} $生成文本$ \\text{Gen}$。这些模型有不同类型，包括像ChatGPT [2] 和Claude [20] 这样的闭源商业服务，以及像Llama [21] 和Vicuna [22] 这样的开源模型。本文重点关注闭源LLMs，并旨在解决它们在开放式文本生成任务中的黑盒推理过程中的隐私问题。\n具体而言，在开放式文本生成任务 [23] 中，这些黑盒LLMs的作用是根据$prompt P_{\\text{ro}} $继续生成文本 $\\text{Gen}$，以提高文本生成质量，并根据多维度指标进行评估。详细来说，给定一个$prompt P_{\\text{ro}} = \\text{Insw} \\parallel \\text{Doc}$，其中Insw是基础写作指令，$Doc =\\langle x_i \\rangle_{i=1}^L$ 是由L个令牌或词 $x_i$ 组成的原始文档，分别属于令牌词汇表 $V_t$ 和词汇表 V，LLMs承诺提供推理函数 $\\text{Infer}: P_{\\text{ro}} \\to \\text{Gen} $来生成文本。\nB. (Local) Differential Privacy \u0026amp; Exponential Mechanism 差分隐私 [15] 是一种隐私保护概念。作为其最流行的模型之一，ε-局部差分隐私（ε-LDP）允许数据拥有者在将数据上传到任何不受信任的聚合器之前，使用随机机制 $M(\\cdot)$ 对其数据进行本地扰动。\n定义1 (ε-局部差分隐私 [24])：在ε-LDP中，给定隐私参数 $\\epsilon \\geq 0$，如果一个随机机制 M 满足以下条件，对于任何两个输入 $x, x\u0026rsquo; \\in X$ 和任何可能的输出 $y \\in Y$，则称 M 是ε-LDP合规的：\n$$\\frac{ \\text{Pr}[M(x) = y] }{ \\text{Pr}[M(x') = y] } \\leq e^\\epsilon$$通常，较小的 $\\epsilon$ 值提供更高的隐私保护，但以减少数据效用为代价。此外，这里的一个关键定义是输入集 X。在以往的自然语言处理研究 [12], [13] 中，大多数研究者假设词汇表中的任意一对词汇共享相同的输入集 X 和输出集 Y。我们观察到，这种定义导致了效用和隐私之间的权衡问题。本文在第V-B节中使用随机邻接列表重新定义了ε-LDP的输入集。\n定义2 (指数机制 [25])：对于给定的评分函数$u : X \\times Y \\to \\mathbb{R}$，如果随机机制 $M(\\cdot)$ 满足以下条件，对于任何输入 $x \\in X $和任何可能的输出 $y \\in Y$，则 M 是ε-LDP合规的：\n$\\text{Pr}[y | x] \\propto \\exp \\left( \\frac{\\epsilon \\cdot u(x, y)}{2 \\Delta_u} \\right)$\n其中，敏感度 $\\Delta_u$ 定义为：\n$\\Delta_u = \\max_{x, x\u0026rsquo; \\in X, y \\in Y} |u(x, y) - u(x\u0026rsquo;, y)|$\n评分函数 u 在不同场景下可能有所不同。通常，我们可以调整 u 的上界来将 $\\Delta_u$ 设置为特定的实数，其中 $\\Delta_u$ 表示评分函数 u 的敏感度。类似地，$\\epsilon$ 值越小，隐私保护能力的安全性越高，但数据的效用越低。当选择较小的 $\\epsilon$ 时，评分函数 $u(x, y)$ 对任何扰动结果的输出概率不再起决定性作用。\nIII. PROBLEM STATEMENT A. Threat Model 我们考虑的场景是LLM平台（如ChatGPT）是一个诚实但好奇的对手，称为Adv。用户（记作Usr）打算上传一个prompt，并调用Adv的推理服务 $\\text{Infer}: P_{\\text{ro}} \\to \\text{Gen}$，以完成由开源模型执行不佳的文本生成任务。在这里，Gen表示由Adv生成的文本。上传的$P_{\\text{ro}} = \\text{Insw} \\parallel \\text{Doc}$ 代表Usr的原始prompt，包含Insw（基础写作指令）和$\\langle x_i \\rangle_{i=1}^L$（由L个令牌或单词 $x_i$ 组成的原始文档，分别属于令牌词汇表 $V_t$ 和词汇表 V）。根据以往的研究 [10], [17]，隐私信息与每个令牌或单词相关。为了保护原始文档Doc中的每个令牌或单词，Usr对Doc应用差分隐私 [15]，生成一个扰动后的文档Docp。因此，Usr上传扰动后的$P_{\\text{ro}}^p = \\text{Insw} \\parallel \\text{Doc}^p$。此外，Usr可以部署一个能力较弱的语言模型，而不是LLMs。为了保护模型的商业价值，Adv不会透露LLM的内部架构或参数，而只会暴露其令牌词汇表，以便在推理过程中进行计费验证。\n给定一个$P_{\\text{ro}} = \\text{Insw} \\parallel \\text{Doc}$，Adv的目标是获得Doc中每个令牌或单词。以一个草拟文章的prompt为例，Adv承诺执行文本生成任务，但可能意图窃取其实验结果以进行未经授权的收集。此外，我们假设Adv完全了解差分隐私算法的细节，除了原始文档Doc之外。Adv预计会利用DP中的漏洞发起攻击，旨在基于扰动版本Docp恢复Doc中的每个单词或令牌。表II总结了本文中经常使用的符号。\nB. Existing Solutions and Limitations 现有的解决方案，如SANTEXT+ [12] 和 CUSTEXT+ [13]，主要集中在分类任务中的隐私保护模型训练：\nSANTEXT+ 在分类任务的训练过程中实现了差分隐私（DP）。在SANTEXT+中，部分单词不受DP的干扰。它仅将敏感单词替换为来自单词邻接列表（整个词汇表）中的其他单词，基于度量本地差分隐私 [26]。最终，这会导致在嵌入反演攻击 [17] 下发生隐私泄露。 CUSTEXT+ 在分类任务的训练过程中对所有单词进行扰动。它采用指数机制 [25]，将每个单词原封不动地替换为来自单词邻接列表中在嵌入距离上接近的单词。与SANTEXT+相比，CUSTEXT+将单词邻接列表的大小减少为一个默认的小数字。然而，单词邻接列表的大小较小使得它容易受到嵌入反演攻击。 为了保护在文本生成任务推理过程中的文档隐私并解决由DP噪声引入的信息失真问题，我们提出了一个框架InferDPT（见第四节）。我们还提出了一种指数机制RANTEXT（见第五节），作为解决SANTEXT+和CUSTEXT在抵御嵌入反演攻击时存在漏洞的方案。\nIV. THE INFERDPT FRAMEWORK A. Overview 我们提出了InferDPT框架，旨在保护文本生成任务中LLM推理过程中的隐私。如图1所示，InferDPT由两个模块组成：\n扰动模块：保护隐私。通过差分隐私生成扰动提示，并将文档（Doc）中的每个单词或标记替换为在嵌入距离上接近的单词或标记。 提取模块：保持实用性。通过一个本地语言模型，从扰动生成的文本中提取连贯一致的文本，并将其重构为与原始提示对齐的输出。本地语言模型的能力低于黑盒LLM。 InferDPT的设计面临两个主要挑战：\n为原始文档Doc提供强大的隐私保护。为了解决这个隐私挑战，InferDPT的扰动模块利用差分隐私机制，顺序地将原始文档Doc中的每个单词或标记替换为嵌入距离上接近的替代词。 在语义扰动下保持文本的实用性。这一挑战比第一个更为艰难。为了解决这个挑战，我们在LLMs上进行了大量实验。我们发现一个现象：如果将原始提示生成文本中出现的所有单词构成一个集合，那么在扰动提示生成的文本中，很多单词也属于这个集合。随着扰动的减少，这个数字逐渐增加。这个现象表明，扰动输出包括了由GPT-4直接生成的文本中的单词。为了正式描述这个现象，我们提出了关于差分隐私扰动输出的以下观察结果。 B. Key Observations Observation. 考虑一个局部差分隐私的随机函数 $M(\\cdot)$，满足以下条件：对于词汇表 V（或标记词汇表 $V_t$）中的任意单词（或标记） x，以及属于 x 的单词邻接列表（或随机邻接列表）中的 y 和 z，如果 $d(x, y) \\geq d(x, z)$，则有：\n$P_r[M(x) = y] \\leq P_r[M(x) = z]$\n其中，$d(\\cdot)$ 测量两个输入之间的语义相似度，输出值越小表示相似度越大。\n原始文档 Doc 被 $M(\\cdot)$ 扰动，得到扰动文档 $Doc_p$。如果 Doc 和 $Doc_p$ 共享相同的推理服务 $Infer(\\cdot)$ 和写作指令 Insw，则它们形成提示 $P_ro = Insw | Doc$ 和扰动提示 $P_ro_p = Insw | Doc_p$。扰动生成结果 $Gen_p = \\langle g_i \\rangle_{i=1}^K = Infer(P_ro_p)$ 满足以下条件：\n$Infer_j(P_ro) = \\langle h(j)i \\rangle{i=1}^K$\n期望集合（Expected set）为：\n$Expected set = N \\left[ j=1 \\left{ h(j)_i | h(j)_i \\in Infer_j(P_ro) \\right} \\right]$\n交集集合（Intersection set）为：\n$Intersection set = { g_i | g_i \\in Expected set \\text{ and } g_i \\notin stopwords }$\n其中，$g_i $和 $h(j)i$ 属于词汇表 V，$\\langle g_i \\rangle{i=1}^K$ 代表由 K 个 $g_i$ 组成的文本，$\\langle h(j)i \\rangle{i=1}^K$ 代表由K 个 $h(j)_i$ 组成的文本，N 是大于 1 的整数，stopwords [27] 是在文本分析中通常因信息量低而被忽略的常见词，Count(·) 计算集合的大小，Corr(·) 是测量两个变量之间关系的相关系数，其值范围为 -1（完全负相关）到 1（完全正相关）。\n该观察指出，如果期望集合是从原始文本生成结果的 N 次迭代中构建的，那么期望集合中出现在扰动生成结果中的单词数量将与 ε 呈正相关。为了验证这一点，我们进行了以下实验。\n经验验证。我们通过收集来自 GPT-4 [14] 在 CNN/Daily Mail 数据集 [29] 上生成 100 次的原始提示输出中的 100 个单词来得到期望集合。原始提示包括一个基本写作指令和一个由 50 个标记组成的原始文档，如图 1 所示。我们利用 SANTEXT+ [12]、CUSTEXT+ [13] 和第 V 节中介绍的 RANTEXT 生成了在不同 ε 值下，GPT-4 生成的 100 个单词的扰动输出。我们计算了来自 GPT-4 扰动和非隐私生成结果中属于期望集合的单词数量。图 2 显示了实验结果。可以看出，随着 ε 的增大和扰动的减少，三种机制中期望集合中的单词数量显著增加。这验证了观察结果，确认了期望集合中出现在扰动生成结果中的单词数量与 ε 呈正相关。\n基于这一观察，InferDPT 只需要一个提取模块来提取和重构来自远程黑盒 LLM 的扰动输出，提取其能力以完成文本生成任务。在接下来的子节中，我们将深入探讨 InferDPT 的这两个模块。\nC. 扰动模块 InferDPT 的扰动模块采用差分隐私机制，通过从一个集合中选择与嵌入距离接近的单词或标记来替换每个单词或标记。这个集合被称为词邻接列表 Cw（使用 SANTEXT+ [12] 和 CUSTEXT+ [13]）或随机邻接列表 Cr（使用 RANTEXT）。如表 III 和表 IV 所示，大多数被扰动的单词或标记并不是它们原始形式的同义词，而是与原始单词在嵌入距离上接近。\n考虑一个随机机制 M(·)，其中 ε ≥ 0。给定一个私密文档 Doc = ⟨xi⟩L i=1，由 L 个标记或单词组成，其中每个 xi ∈ V 或 xi ∈ Vt。扰动模块通过将每个 xi 替换为随机输出 yi = M(xi, Cw(xi)) 或 yi = M(xi, Cr(xi)) 来获取扰动文档 Docp = ⟨yi⟩L i=1。扰动文档的示例见图 1。扰动模块的详细过程在算法 1 中概述。\n在 InferDPT 的扰动模块中，采用了三种不同的差分隐私机制：SANTEXT+、CUSTEXT+ 和 RANTEXT。经过扰动后，Usr 上传一个扰动的提示 P rop = Insw ∥ Docp，其中包含写作指令 Insw 和扰动文档 Docp，如图 1 所示。然后，黑盒 LLM 会返回扰动生成结果 Genp = Infer(P rop) 给用户。\nD. 提取模块 如图 1 所示，由扰动提示生成的文本与原始文档 Doc 部分不一致，并且在语义上不连贯。值得注意的是，尽管扰动生成 Genp 和原始生成 Gen 共享相同的文本，但没有原始文档信息，难以将 Genp 与原始文档 Doc 对齐，因为关键信息已经被扰动。为了获得对齐的生成结果，InferDPT 的提取模块使用一个被认为是可信的本地语言模型，且不会引发隐私泄露问题。该本地模型比远程黑盒 LLM 小且能力较弱，便于部署。用户随后将原始文档 Doc 和扰动生成结果 Genp 输入，生成与原始提示对齐的最终输出。\n值得注意的是，本地语言模型本身可以生成对齐的生成结果，但由于其能力有限，生成质量不尽如人意。通过使用扰动生成结果 Genp，我们可以将远程黑盒 LLM 的能力提取到我们的本地小型语言模型中。提取模块的提示可以在附录 A 中找到。\n基于上述描述，我们可以对 InferDPT 有一个全局的理解。需要注意的是，扰动模块可以采用现有的差分隐私机制，如 SANTEXT+ [12] 和 CUSTEXT+ [13]。然而，正如在 III-B 节中分析的，这两种机制容易受到嵌入反演攻击 [17]。为了解决这个问题，我们将在下一节引入 RANTEXT。\nV. RANTEXT 机制 A. 概述 我们设计了 RANTEXT 机制，以解决差分隐私机制在嵌入反演攻击下的漏洞。如图 3 所示，RANTEXT 包含两个步骤：\n计算随机邻接列表：此步骤通过两项操作为每个原始 token 计算一个随机邻接列表：计算随机嵌入和欧几里得距离。随机邻接列表中的所有 token 共享相同的输入集。 通过 ε-LDP 采样扰动 token：此步骤为每个原始 token 采样一个扰动 token，并通过 ε-LDP 从其随机邻接列表中替换文档中的原始 token，从而获得扰动文档。 正如在 III-A 节中提到的，LLM 会暴露其 token 词汇表 Vt 用于推理服务的计费验证。利用 token 词汇表 Vt 和字节对编码 (BPE) 算法 [30]，用户可以获得 LLM 的 tokenizer(·) 算法。给定原始文档 Doc，RANTEXT 首先使用 LLM 的 tokenizer(·) 算法将 Doc 转换为 tokens ⟨ti⟩L i=1，其中 ti ∈ Vt：\n$\\text{Tokenset} = \\langle t_i \\rangle_{i=1}^{L} = \\text{tokenizer}(Doc).$\n为了保护 Doc 的隐私，RANTEXT 丢弃 Doc 中不属于 Vt 的 tokens，并使用指数机制来替换每个剩余的 token，将其替换为与其独有的随机邻接列表中的某个 token 在嵌入距离上接近的 token：\n$\\text{Tokensetp} = \\langle r_i \\rangle_{i=1}^{l} = \\langle M(t_i) \\rangle_{i=1}^{l},$\n其中，$r_i \\in Vt$，Tokensetp 表示扰动后的 token 集合，$C_t(t_i)$ 表示 $t_i$ 的随机邻接列表。RANTEXT 将扰动 token 集合 Tokensetp 中的 tokens 连接起来，得到一个扰动文档 Docp，从而实现隐私保护。\nB. 计算随机邻接列表 为了正式定义随机邻接列表，我们首先给出随机邻接嵌入的定义：\n定义 3 (随机邻接嵌入)。给定 token $t \\in V_t$，其随机邻接嵌入定义如下：\n$C_e(t) = {e_b \\mid d_e(e_b, \\varphi(t)) \u0026lt; d_e(\\hat{\\varphi}(t), \\varphi(t)), e_b \\in \\mathbb{R}^N },$\n其中，$e_b \\in \\mathbb{R}^N$ 表示任何 $N$ 维的实数向量。函数 $d_e(\\cdot)$ 用于计算两个向量之间的距离，定义为：\n$d_e(a, b) = \\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^2}.$\n函数 $\\varphi: V_t \\to \\mathbb{R}^N$ 将任何给定的 token 映射到 $N$ 维实数向量空间中的一个向量。函数 $\\hat{\\varphi}(t) = \\varphi(t) + Y$，其中随机向量 $Y$ 满足概率密度：\n$Y \\sim f(x) = Z \\cdot 2^{\\Delta\\varphi} \\cdot \\exp\\left(- \\frac{|x|}{\\Delta\\varphi} \\right)$,Z={ϵ如果 ϵ\u0026lt;2,alog⁡(b⋅ϵ+c)+d否则,$Z = \\begin{cases} \\epsilon \u0026amp; \\text{if} \\ \\epsilon \u0026lt; 2, \\ a \\log(b \\cdot \\epsilon + c) + d \u0026amp; \\text{else}, \\end{cases}$\n其中，$\\Delta\\varphi$ 是函数 $\\varphi(\\cdot)$ 的敏感性，$a \\approx 0.0165, b \\approx 19.0648, c \\approx -38.1294, d \\approx 9.3111$。\n给定一个 token $t \\in V_t$，要计算其随机邻接嵌入，我们需要完成两步计算：\n步骤 1：计算随机嵌入。我们使用拉普拉斯分布 [19] 构造随机向量 $Y$。我们独立地将 $Y$ 添加到 $\\varphi(t)$ 的每个维度中，从而获得原始私有 token $t$ 的随机嵌入 $\\hat{\\varphi}(t) = \\varphi(t) + Y$。\n步骤 2：计算欧几里得距离。我们计算 $\\varphi(t)$ 和 $\\hat{\\varphi}(t)$ 之间的欧几里得距离，记作 $d_e(\\hat{\\varphi}(t), \\varphi(t))$。随机邻接嵌入由那些与 $\\varphi(t)$ 的欧几里得距离小于 $d_e(\\hat{\\varphi}(t), \\varphi(t))$ 的嵌入组成。\n我们使用 $Y$ 动态地确定随机邻接列表的大小。随机向量 $Y$ 的详细构造过程可以在附录 B 中找到。通过对随机邻接嵌入的定义，我们进一步给出随机邻接列表的定义：\n定义 4 (随机邻接列表)。给定 token $t \\in V_t$，其随机邻接列表定义如下：\n$C_r(t) = { t\u0026rsquo; \\mid \\varphi(t\u0026rsquo;) \\in C_e(t), t\u0026rsquo; \\in V_t }.$\n给定一个 token $t \\in V_t$，其随机邻接列表由 token 词汇表 $V_t$ 中的任意 token $t\u0026rsquo;$ 组成，其中 $t\u0026rsquo;$ 的嵌入 $\\varphi(t\u0026rsquo;)$ 与 $t$ 的嵌入 $\\varphi(t)$ 之间的欧几里得距离小于 $t$ 的随机嵌入与 $t$ 的嵌入 $\\varphi(t)$ 之间的欧几里得距离。\n定理 1。给定一个 token $t \\in V_t$ 和任意 token $t\u0026rsquo; \\in V_t$，存在一个 RANTEXT 的随机邻接列表 $C_r(t)$，使得 $t\u0026rsquo; \\in C_r(t)$。\n定理 1 表明，RANTEXT 解决了 CUSTEXT+ [13] 中固定且小的邻接列表带来的隐私错误。它确保一个 token $t$ 可以被 $V_t$ 中的任何 token 替换。RANTEXT 中不确定的随机邻接列表的潜在大小等同于整个 token 词汇表 $V_t$ 的大小。定理 1 的证明见附录 B。\nC. 通过 ε-LDP 采样扰动的 Token 在 SANTEXT+ [12] 中，一部分词汇没有受到差分隐私（DP）的扰动。为了避免原始文本中的隐私泄露问题，RANTEXT 对 Tokenset = ⟨$t_i$⟩$L$$_{i=1}$ 中的每个 token 进行扰动。为了扰动 token $t_i$，RANTEXT 使用满足 ε-LDP 的指数机制 [25]，从 $C_r(t_i)$ 中选择一个新的 token 来替代原始的 token。对于任何不属于 $V_t$ 的特殊 token $t_s$，RANTEXT 会将其丢弃，以确保 Docp 中没有特殊 token 泄露。\n为了保证扰动文档的有效性，RANTEXT 中指数机制的随机机制 $M(·)$ 需要满足以下条件：\n$d(x, y) \\geq d(x, z) \\Rightarrow \\Pr[M(x) = y] \\leq \\Pr[M(x) = z],$\n其中 $x \\in V_t$，$y$ 和 $z$ 属于 $x$ 的随机邻接列表。$d(·)$ 衡量两个输入之间的语义相似度，输出越小，表示相似度越高。\n算法 2 RANTEXT 机制\n输入: Token 集合 $Tokenset = \\langle t_i \\rangle^L_{i=1}$，token 词汇表 $V_t$，隐私参数 $\\epsilon$，嵌入函数 $\\varphi(\\cdot)$，距离函数 $d_e(\\cdot)$，随机向量 $Y$； 输出: 扰动后的文档 $Doc_p$；\n初始化 $Tokenset_p \\leftarrow \\emptyset$；\n对于 $i = 1$ 到 $L$，执行：\n​\t如果 $t_i \\notin V_t$，则：\n​\t丢弃 token $t_i$；\n​\t跳过当前循环；\n​\t结束如果；\n​\t采样一个随机向量 $Y$；\n​\t计算嵌入 $eb_t \\leftarrow \\varphi(t_i)$；\n​\t计算随机嵌入 $eb_n \\leftarrow eb_t + Y$；\n​\t计算欧几里得距离 $d_{\\text{threshold}} \\leftarrow d_e(eb_n, eb_t)$；\n​\t计算随机邻接集 $C_e(t_i) = {eb | d_e(eb, eb_t) \u0026lt; d_{\\text{threshold}}, eb \\in \\mathbb{R}^N }$；\n​\t计算随机邻接列表 $C_r(t_i) = {t_i\u0026rsquo; | \\varphi(t_i\u0026rsquo;) \\in C_e(t_i), t_i\u0026rsquo; \\in V_t }$；\n​\t对于每个 $t_i\u0026rsquo; \\in C_r(t_i)$，执行：\n​\t计算 $d_{t_i\u0026rsquo;} \\leftarrow d_e(\\varphi(t_i), \\varphi(t_i\u0026rsquo;))$；\n​\t计算评分函数 $u(t_i, t_i\u0026rsquo;) \\leftarrow 1 - \\frac{d_{t_i\u0026rsquo;}}{d_{\\text{threshold}}}$；\n​\t更新总分 $p_{\\text{total}} \\leftarrow p_{\\text{total}} + \\exp \\left( \\frac{\\epsilon}{2} \\cdot u(t_i, t_i\u0026rsquo;) \\right)$；\n​\t结束对于；\n​\t对于每个 $t_i\u0026rsquo;\u0026rsquo; \\in C_r(t_i)$，执行：\n​\t计算条件概率 $p(t_i\u0026rsquo;\u0026rsquo;|t_i) \\leftarrow \\frac{\\exp \\left( \\frac{\\epsilon}{2} \\cdot u(t_i, t_i\u0026rsquo;\u0026rsquo;) \\right)}{p_{\\text{total}}}$；\n​\t结束对于；\n​\t从随机邻接列表中采样 $r_i \\sim p(t_i\u0026rsquo;\u0026rsquo;|t_i)$；\n​\t将新 token $r_i$ 添加到扰动 token 集合 $Tokenset_p$ 中；\n结束对于；\n将 $Tokenset_p = \\langle r_i \\rangle^L_{i=1}$ 连接起来，得到 $Doc_p$；\n输出扰动后的文档 $Doc_p$；\n为了实现这一点，RANTEXT 中随机机制 $M(\\cdot)$ 的评分函数 $u(\\cdot)$ 如下所述：\n给定一个 token $t$，RANTEXT 认为在扰动 token $t$ 时，$C_r(t)$ 中的任何两个 token 都共享相同的输入集和输出集。对于任意两个 token $x, y \\in C_r(t)$，评分函数为：\n$u(x, y) = 1 - \\frac{|d_e(\\varphi(x), \\varphi(t)) - d_e(\\varphi(y), \\varphi(t))|}{d_e(\\varphi(t), \\hat{\\varphi}(t))} \\tag{16}$\n根据方程 (11) 和方程 (14)，可以得出：\n$|d_e(\\varphi(x), \\varphi(t)) - d_e(\\varphi(y), \\varphi(t))| \u0026lt; d_e(\\varphi(t), \\hat{\\varphi}(t)) \\tag{17}$\n$0 \\leq \\frac{|d_e(\\varphi(x), \\varphi(t)) - d_e(\\varphi(y), \\varphi(t))|}{d_e(\\varphi(t), \\hat{\\varphi}(t))} \u0026lt; 1 \\tag{18}$\n结合方程 (16) 和方程 (17)，可以推导出：\n$0 \u0026lt; u(x, y) \\leq 1 \\tag{19}$\n$\\Delta u = 1 \\tag{20}$\n$$\rP_r[y|x] = \\frac{\\exp \\left( \\frac{\\epsilon}{2} \\cdot u(x, y) \\right)}{\\sum_{y' \\in C_r(t)} \\exp \\left( \\frac{\\epsilon}{2} \\cdot u(x, y') \\right)} \\tag{21}\r$$$$\r= \\frac{\\exp \\left( \\frac{\\epsilon}{2} \\cdot \\left( 1 - \\frac{|d_e(\\varphi(x), \\varphi(t)) - d_e(\\varphi(y), \\varphi(t))|}{d_e(\\varphi(t), \\hat{\\varphi}(t))} \\right) \\right)}{\\sum_{y' \\in C_r(t)} \\exp \\left( \\frac{\\epsilon}{2} \\cdot \\left( 1 - \\frac{|d_e(\\varphi(x), \\varphi(t)) - d_e(\\varphi(y'), \\varphi(t))|}{d_e(\\varphi(t), \\hat{\\varphi}(t))} \\right) \\right)} \\tag{22}\r$$$$\ru(t, y) = 1 - \\frac{d_e(\\varphi(y), \\varphi(t))}{d_e(\\varphi(t), \\hat{\\varphi}(t))} \\tag{23}\r$$$$\rP_r[y|t] = \\frac{\\exp \\left( \\frac{\\epsilon}{2} \\cdot \\left( 1 - \\frac{d_e(\\varphi(t), \\varphi(y))}{d_e(\\varphi(t), \\hat{\\varphi}(t))} \\right) \\right)}{\\sum_{y' \\in C_r(t)} \\exp \\left( \\frac{\\epsilon}{2} \\cdot \\left( 1 - \\frac{d_e(\\varphi(t), \\varphi(y'))}{d_e(\\varphi(t), \\hat{\\varphi}(t))} \\right) \\right)} \\tag{24}\r$$RANTEXT 的详细过程如算法 2 所示。此外，RANTEXT 满足 ε-LDP 的定义，符合第 IV-B 节的观察条件：\n定理 2：给定隐私参数 $\\epsilon \\geq 0$ 和 token $t$ 的随机邻接列表 $C_r(t)$，对于任意输入 $x, x\u0026rsquo; \\in C_r(t)$ 和输出 $y \\in C_r(t)$，RANTEXT 的随机机制 $M$ 满足：\n$\\frac{P_r[M(x) = y]}{P_r[M(x\u0026rsquo;) = y]} \\leq e^{\\epsilon} \\tag{25}$\n定理 2 证明了，给定 token $t$ 的随机邻接列表 $C_r(t)$，RANTEXT 机制满足 ε-LDP。定理 2 的证明见附录 B。\nVI. 实验 A. 实验设置 数据集： 对于开放式文本生成任务，我们使用了两个经典的NLP数据集：CNN/Daily Mail [29] 用于新闻文章，Wikitext-103-v1 [31] 用于维基百科文章。对于实际应用，我们使用了ArXiv数据集 [32] 来撰写科学论文。这些数据集涵盖了大量的事件和个人。\n基准： InferDPT 是首个实际应用于文本生成任务中实现差分隐私的隐私保护推理框架 [33]。由于目前没有其他同类型的框架，因此我们没有将 InferDPT 与其他框架进行比较。对于扰动模块中的差分隐私机制，我们将 RANTEXT 与现有的最先进机制，SANTEXT+ [12] 和 CUSTEXT+ [13] 在其默认设置下进行比较。\n评估指标： 遵循以往的开放式文本生成工作 [23]，[33]，我们使用文章的前 50 个标记作为原始文档 Doc，这些需要保护的部分。我们使用 Doc 的继续写作部分，称为 Gen，长度为 100 个标记。标记由 GPT-2 的分词器函数 [34] 进行计数。与 [35] 一致，采用了以下三种指标来评估生成文本在开放式生成任务中的质量：\n多样性 (Diversity) 该指标通过计算独特的n-gram重复率来衡量文本的多样性，计算公式如下： $\\text{diversity} = \\frac{1}{4} \\sum_{n=2}^4 \\frac{| \\text{unique n-grams(Gen)} |}{| \\text{total n-grams(Gen)} |}$\n较低的得分表示模型容易产生重复内容，而较高的得分则表明模型使用了更广泛的词汇。\nMAUVE [36] 该指标用于评估语言模型生成的文本与人工编写的目标继续文本之间的相似性。在该指标中，较高的得分是期望的。 连贯性 (Coherence) 连贯性通过计算文档 Doc 和继续文本 Gen 的嵌入向量之间的余弦相似性来衡量，计算公式如下： $\\text{COH(Doc, Gen)} = \\frac{\\text{SimCSE(Doc)} \\cdot \\text{SimCSE(Gen)}}{| \\text{SimCSE(Doc)} | \\cdot | \\text{SimCSE(Gen)} |}$\n其中，SimCSE(x) 代表预训练模型 [37]。\n实现 我们在一个集群上运行实验，配备有 NVIDIA RTX A6000 GPU 和 Intel Xeon Gold 6130 2.10 GHz CPU。所有机制的实现均使用 Python 语言完成。对于黑盒推理，我们使用 GPT-4 [14] 作为远程大语言模型，温度参数设置为 0.5。相应地，GPT-4 的标记词汇表使用 cl100k base [38]。我们将 cl100k base 的前 11,000 个英文标记作为 Vt。对于嵌入函数 φ(·)，我们选择 text-embedding-ada-002 [39]，该函数使用与 GPT-4 训练过程中相同的标记词汇表 cl100k base。我们使用 Vicuna-7b-4bit [40] 和 Llama2-7b-4bit [21] 作为提取模块的本地语言模型，温度参数设置为 0.5。\nB. 效用评估 (Evaluation of Utility) 我们评估了在扰动模块中使用不同差分隐私机制生成的 InferDPT 输出质量，使用提取模块中的 Vicuna-7b-4bit (3.89GB) 在不同数据集上的表现。表 V 显示了 InferDPT 生成的质量与非隐私的 GPT-4 生成质量的比较：\n尽管上传的提示（prompt）受到了差分隐私的扰动，但 InferDPT 生成的文本质量与非隐私的 GPT-4 生成的文本质量相当，并且优于本地模型的输出。这证明了 InferDPT 的有效性。 GPT-4 生成的文本质量通常优于本地模型直接生成的文本。 在相同隐私参数 ε 下，InferDPT 中使用 RANTEXT 生成的文本质量与 CUSTEXT+ 相当，并且优于 SANTEXT+。 我们还测量了 InferDPT 每次推理的时间成本，结果见表 VI。实验结果表明，InferDPT 提供了隐私保护，同时没有产生巨大的时间开销。我们还调查了 InferDPT 是否能够在不同本地模型中工作。\n正如表 VII 所示，实验结果证明 InferDPT 在不同模型中仍然有效。\n我们进一步比较了 InferDPT 使用 Vicuna-7b-4bit 最终生成文本与 GPT-4 从原始提示生成的输出之间的余弦相似度。比较结果显示在表 VIII 中。在相同隐私参数 ε 下，三种数据集上的 RANTEXT 扰动生成的文本的余弦相似度接近于表现最好的 CUSTEXT+。\n此外，我们还研究了隐私参数 ε 对 RANTEXT 中随机邻接列表大小的概率分布的影响。如图 4 所示，我们使用 Cr/Vt 来表示随机邻接列表在整个标记词汇表中的比例。随着 ε 增加，RANTEXT 更倾向于出现较小尺寸的随机邻接列表。\nVII. 针对隐私威胁的防御 (Defense Against Privacy Threat) A. 输入推断攻击 (Input Inference Attack) 在输入推断攻击中 [12]，攻击者使用预训练的 BERT 模型从扰动版本的文档 Docp 恢复原始文档 Doc。BERT 模型通过掩码语言建模 [28] 进行训练，能够预测原始标记，方法是依次将扰动文本中的每个标记替换为特殊标记 \u0026ldquo;[MASK]\u0026quot;。这种方法利用了 BERT 理解上下文的能力，使其能够推断出被掩码的标记。如果输出标记与输入标记匹配，则攻击成功。随后，我们计算所有攻击的成功率，记为 rats。差分隐私的隐私保护度定义为 1 − rats。\n如图 5 所示，与 SANTEXT+ 和 RANTEXT+ 相比，RANTEXT 在输入推断攻击中提供了更好的隐私保护。实验结果表明，在 ε 值范围为 0.01 到 18.0 之间，RANTEXT 提供超过 80% 的隐私保护。特别是在 CNN/Daily Mail 数据集上，当 ε 值为 18.0 时，RANTEXT 的隐私保护是 SANTEXT+ 的 1.11 倍，是 CUSTEXT+ 的 1.41 倍。我们分析了实验结果，发现 BERT 无法识别 GPT-4 的标记。为了更全面地评估 RANTEXT 的安全性，我们在第 VII-C 节提出了利用 GPT-4 能力的自适应攻击方法，即 GPT 推理攻击。\nB. 嵌入反转攻击 (Embedding Inversion Attack) 嵌入反转攻击 [17] 计算扰动文档中每个标记的嵌入与词汇表中其他标记嵌入之间的距离，返回与其欧几里得距离最接近的前 K 个标记。实验在 top K = 1 和 10 的条件下进行。图 6 表明，在这两种条件下，SANTEXT+ 和 CUSTEXT+ 都容易受到嵌入反转攻击，表明它们提供的隐私保护水平相对较低。即使在 ε = 0.01 时，这些方法也只能为超过 40% 的原始文档提供隐私保护。随着 top K 从 1 增加到 10，SANTEXT+ 和 CUSTEXT+ 的隐私保护率变化不大。另一方面，RANTEXT 由于其随机邻接列表的设计，有效地阻止了攻击者利用邻接信息进行成功攻击，从而在此类攻击中表现出更强的隐私保护。\nC. 自适应攻击：GPT推理攻击 (GPT Inference Attack) RANTEXT 对 GPT-4 的标记词汇表进行扰动。由于 GPT-4 能识别所有标记，因此假设 GPT-4 可以更好地恢复被 RANTEXT 扰动的文本。因此，我们提出了一种自适应攻击方法——GPT推理攻击。在该方法中，攻击者将扰动文本输入 GPT-4，并指示其恢复每个标记。如果恢复的标记与原始标记一致，则攻击成功。GPT推理攻击的提示见附录 D。图 5 显示了 GPT推理攻击的结果。GPT-4 在所有测试中比 BERT 具有更高的攻击成功率。这可能是由于 GPT-4 的更大规模和更好的理解能力，使其在推理攻击中更有效。在面对 GPT 推理攻击时，SANTEXT+ 和 CUSTEXT+ 的隐私保护率低于 RANTEXT，而 RANTEXT 保持了最佳的隐私保护。\nD. 扰动生成中的隐私泄露 我们进一步讨论了原始文档 Doc 是否会通过扰动生成结果 Genp 泄露。图 7 显示了 Doc 和 Genp 之间的余弦相似度。实验结果表明，RANTEXT 保持了原始文档 Doc 与扰动生成结果 Genp 之间的低语义相似度，表明通过扰动结果泄露隐私的风险较低。此外，我们通过检查原始文档中的 n-gram 词是否在扰动输出中重复来衡量隐私泄露。在原始文本和扰动输出中都找到的 n-gram 词被视为泄露。如表 IX 所示，即使是非隐私提示，原始文档的隐私泄露也不到 11%。\nE. 隐私与效用之间的权衡 随后，我们比较了 RANTEXT、CUSTEXT+ 和 SANTEXT+ 在隐私与效用之间的权衡。我们使用每种机制在顶部 1 的嵌入倒转攻击下的隐私保护率作为衡量每个方案隐私级别的指标。我们比较了在相同生成文本质量下，RANTEXT、CUSTEXT+ 和 SANTEXT+ 在 CNN/Daily Mail 数据集上的隐私保护水平，使用 Vicuna-7b-4bit（3.89GB）作为提取模块。如图 8 所示，RANTEXT 在相同的生成文本质量下提供了最佳的隐私保护，超过了 SANTEXT+ 和 CUSTEXT+。总结来说，RANTEXT 相较于 SANTEXT+ 和 CUSTEXT+ 展现了在差分隐私机制下对各种攻击的强大隐私保护，同时能够生成高质量的文本。\nVIII. 相关工作 CipherGPT [9] 已将同态加密 [41] 应用于基于 Transformer 的语言模型，能够在加密数据上执行推理。然而，这导致了一个目前无法完全解决的问题：显著的计算时间和通信成本：推理一个令牌需要 24 分钟和 93 GB 的带宽，导致部署变得不切实际。PromptPATE [42] 和 DP-OPT [43] 利用差分隐私（DP）重建用于分类任务的数据集，从而在提示学习（调优）过程中保护训练数据的隐私。然而，它们专注于分类任务，未能解决差分隐私噪声引入的信息失真问题。Tang 等人 [44] 提出了一种差分隐私方法，用于生成隐私保护示例以进行上下文学习。他们部署了一个大型语言模型，通过少量生成的差分隐私推理来重建私密示例。他们的工作也主要集中在分类任务上。\nIX. 结论 本文探讨了在黑箱大语言模型执行文本生成任务时隐私泄露的挑战，并提出了 InferDPT 作为一种潜在解决方案。此外，我们提出了 RANTEXT，一种新型的差分隐私算法，旨在通过使用指数机制增强大语言模型中的用户隐私保护。我们期望我们的解决方案和研究成果能够为当前的隐私挑战提供技术性见解，并为未来在新兴大语言模型中的隐私保护探索提供启示。\n","date":"2024-12-06T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/liverworts-8616125_1280.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/inferdpt%E9%9D%A2%E5%90%91%E9%BB%91%E7%9B%92%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E6%8E%A8%E7%90%86/","title":"InferDPT：面向黑盒大语言模型的隐私保护推理"},{"content":"BumbleBee: 大型变换器模型的安全双方推理框架 摘要 基于大型变换器模型的技术已在自然语言处理、计算机视觉等许多实际任务中实现了最先进的性能。然而，随着它们处理的数据和任务敏感性增加，隐私问题在模型部署过程中成为了一个主要关注点。在这项工作中，我们关注于在两方设置下进行私密推理的场景，其中一方持有私密输入，另一方持有模型。我们介绍了BumbleBee，一个快速且适合通信的双方私密变换器推理系统。我们的贡献主要有三点：首先，我们提出了优化的矩阵乘法协议，相较于以往技术，显著降低了80%-90%的通信成本。其次，我们开发了一种针对变换器模型中非线性激活函数的高效协议构建方法。所提出的激活协议在处理速度上实现了显著提升，同时与之前的两种方法相比，通信成本降低了80%-95%。最后，我们对五个变换器模型进行了广泛的基准测试。BumbleBee通过评估LLaMA-7B模型展示了其能力，使用CPU生成一个token大约需要14分钟。我们的结果进一步表明，BumbleBee在性能上超越了Iron（NeurIPS22）一个数量级，并且比BOLT（Oakland24）快三倍，同时通信成本仅为BOLT的十分之一。\n引言 基于大型变换器模型，如BERT [19]、GPT [60] 和 ViT [22]，在许多实际任务中实现了最先进（SOTA）的性能，包括人员重识别 [47]、语音助手 [14] 和代码自动补全 [75]。随着变换器模型处理的数据和任务越来越敏感，隐私问题已成为模型部署中的主要关注点之一。私密推理旨在保护模型权重免受用户的干扰，同时确保服务器无法获取用户私密输入的信息。许多近期的研究引入了基于安全多方计算（MPC）[8]、[27]的加密框架，以实现深度学习模型（如卷积神经网络（CNN）[36]、[2]、[41]、[62]）和变换器模型的私密推理[48]、[33]、[79]、[73]。尽管安全双方计算（2PC）可以高效地在几分钟内推理CNN，但在变换器模型上的私密推理则带来了新的挑战，特别是在通信开销方面。例如，[33]、[30]等研究表明，对12层BERT模型的单次私密推理可能需要高达90 GB的通信。此外，[73，表1]报告称，12层ViT模型的单次私密推理可能需要交换约262 GB的消息。因此，这些通信密集型方法对高带宽的需求不可或缺。我们总结了开发高效、通信友好的2PC框架，用于大规模变换器私密评估的两个主要挑战。\n大规模矩阵的乘法 变换器模型的推理可能涉及数百次大矩阵的乘法。例如，自然语言处理（NLP）变换器使用嵌入表将一组单词查询转换为数值表示。嵌入表查找可以定义为矩阵乘法 $EV$，其中 $E$ 的每一行是一个对应于输入查询中每个单词索引的独热向量。换句话说，这种乘法的维度可以解析为：单词数 × 词汇表大小 × 嵌入大小，这比卷积神经网络（CNN）中的矩阵要大得多。由于相对于NLP变换器而言，视觉变换器通常具有更大的嵌入大小，因此也可能涉及较大的矩阵。现有的大多数加密协议用于私密矩阵乘法，都依赖于“不可知转移”（Oblivious Transfer，OT）[18]、[57]、[59] 或同态加密（Homomorphic Encryption，HE）[41]、[12]、[36]、[35]。然而，这两种方法各有其局限性。基于OT的私密矩阵乘法方法计算时间较少，但需要传输大量消息；另一方面，基于HE的方法计算量显著增加，但比OT方法更加适合通信。第一个挑战是开发一种既快速又通信友好的矩阵乘法协议。\n更复杂的激活函数 与CNN中使用的简单ReLU激活函数不同，变换器模型包含复杂的激活函数，如softmax、Gaussian Error Linear Unit（GeLU）和Sigmoid Linear Unit（SiLU）。这些激活函数的计算需要基本的数学函数，如指数运算、除法和双曲函数。尽管研究人员已经为这些基本函数开发了特定的协议[11]、[61]、[62]、[44]，但直接在变换器模型中使用这些协议仍然不切实际。主要原因是变换器模型中的激活数量非常庞大。例如，GPT2模型[15]的单次推理需要计算大约 3.9 × 10⁶ 次点对点的GeLU激活。我们的第二个挑战是为这些复杂的激活函数设计高效的2PC协议。\nA. 技术细节\n高效的线性函数协议 我们提出了一种叫做“不可知线性变换”（Oblivious Linear Transformation，OLT）的原语。我们将OLT描述为一个双方协议，其中两方分别提供私密矩阵 $Q \\in \\mathbb{Z}_2^{* 2l}$ 和 $V \\in \\mathbb{Z}_2^{* 2l}$，并生成它们之间的共享 $QV$。使用OLT，我们可以在模 $2l$ 的环上实现两个加法共享矩阵的乘法。基于同态加密（HE）的方法[36]、[54]提供了一个很好的起点。这些方法的一个重要局限性是显著的通信开销，这来自于输出密文的“稀疏”格式。更具体地说，[36]、[54]中的每个输出密文加密了一个长向量。然而，乘法结果只需要该向量中的一小部分元素。为了进行解密，仍然需要传输整个加密向量。为了克服这一不足，我们提出了一种压缩过程，通过同态方式将加密向量中不必要的条目归零，从而将多个“稀疏”向量合并为一个“密集”向量。因此，通信开销减少，因为需要发送的密文数量变少。与之前的密文压缩方法[13]相比，我们的压缩过程快了大约50倍。总体而言，我们观察到与[36]、[54]相比，通信成本减少了80%-90%。\n除了矩阵乘法，点对点乘法 $x_0 \\cdot y_0, x_1 \\cdot y_1, \\dots$ 也是变换器推理中的一个重要计算。许多基于同态加密的点对点乘法协议[18]、[63]需要设置一个相对较大的明文模数$t$。例如，[18]、[63]都设置HE参数 $t \u0026gt; 22l+40$ 来加密来自 $\\mathbb{Z}_2^l$ 的值。在这项工作中，我们提出了一种模数提升函数，用于统一底层HE的秘密共享模数和明文模数。提升函数使我们能够在HE密文上执行模 $2l$ 的算术运算。这使我们能够选择较小的HE参数，即 $t \\approx 22l$。我们的实验结果表明，在[63]中应用提升函数后，性能提高了1.3倍。\n$$\r\\text{SiLU}(x) \\approx \\begin{cases} -10^{-5} \u0026 \\text{if } x \u003c -8 \\\\\rP(x) \u0026 \\text{if } -8 \u003c x \\leq -4 \\\\\rQ(x) \u0026 \\text{if } -4 \u003c x \\leq 4 \\\\\rx - 10^{-5} \u0026 \\text{if } x \u003e 4 \\end{cases}\r$$ 接着，我们基于两个关键的见解提出了优化方法。首先，我们利用激活函数的平滑性来提高效率。例如，对于输入 $x = -3.98$，即便使用（错误的）第二段，也能得到类似的结果，即 $P(-3.98) \\approx Q(-3.98)$。这种平滑性为我们设计一种高效的评估方法提供了空间，尽管会引入轻微的误差。其次，我们提出了优化措施，以提高在对相同输入点评估多个多项式时的摊销效率。具体来说，我们优化后的激活协议比当前在[61]、[53]中详细描述的数值方法快了9到20倍，通信成本减少了80%到95%。\nB. 贡献\n总结来说，我们做出了三项关键贡献：\n我们开发了一个基于同态加密（HE）的矩阵乘法协议，特别针对模 $2l$ 运算进行了优化，以提升通信效率。事实上，我们的协议已被证明与现有的基于HE的方法相比，通信成本减少了80%。此外，我们的协议还具有高速度。例如，私密矩阵乘法操作（维度为 $128 \\times 768 \\times 768$）可以在1秒钟内完成，使用一个中等规格的云实例。 我们实现了所有提出的协议，并开发了一个新的MPC后端，称为BumbleBee，并将其集成到SPU库[53]中。为了进行比较，我们还基于SPU库实现了一个基准版本，使用了几种最先进的2PC协议。通过简单的对比，图1展示了与基准版本相比，提出的协议所带来的改进。总的来说，我们在SIGMA[30]的基础上减少了84%的通信成本，在BOLT[58]的基础上减少了90%，在Iron[33]的基础上减少了92%。 BumbleBee使得私密变换器推理变得易于使用。现有的工作[48]、[33]、[58]仅考虑了BERT系列模型。与之对比，我们成功地在5个预训练变换器模型上运行了BumbleBee，利用了HuggingFace网站上提供的模型权重和Python程序，包括BERT-base、BERT-large、GPT2-base、LLaMA-7B和ViT-base。我们还在四个公开数据集上评估了BumbleBee的准确性。所有实验都使用了我们提出的协议，而不是通过模拟进行的。我们提供了可重现的实现，网址为：https://github.com/AntCPLab/OpenBumbleBee。我们的方法为精确且可行的私密变换器推理提供了有力证据，即使在不改变神经网络结构的情况下，也能实现这一目标。 II. 基础知识\nA. 符号约定\n我们用 $x = y$ 表示 $x$ 等于 $y$，用 $x := y$ 表示将 $y$ 的值赋给变量 $x$。对于一个交互协议 $\\Pi$，我们用 $JxK \\gets \\Pi$ 表示协议的执行。我们用 $[n]$ 来表示集合 ${0, \\dots, n - 1}$，其中 $n \\in \\mathbb{N}$。对于一个集合 $D$，$x \\in R^D$ 表示 $x$ 是从 $D$ 中均匀随机采样的。我们使用 $\\lceil \\cdot \\rceil$、$\\lfloor \\cdot \\rfloor$ 和 $\\lfloor \\cdot \\rceil$ 来分别表示天花板函数、地板函数和四舍五入函数。逻辑与和异或分别表示为 $\\land$ 和 $\\oplus$。令 $1{P}$ 表示指示函数，当谓词 $P$ 为真时，$1{P}$ 为 1，$P$ 为假时，$1{P}$ 为 0。我们用带有“帽”符号的小写字母，例如 $\\hat{a}$，来表示多项式，用 $\\hat{a}[j]$ 表示多项式 $\\hat{a}$ 的第 $j$ 个系数。我们用点符号 $\\cdot$，例如 $\\hat{a} \\cdot \\hat{b}$，来表示多项式的乘法。我们用 $Z_q = Z \\cap [0, q)$ 来表示 $q \\geq 2$ 时的整数模 $q$。同余式 $x \\equiv y \\mod 2^l$ 将简写为 $x \\equiv_l y$。对于一个二次幂数 $N$，和 $q \u0026gt; 0$，我们写 $R_q$ 来表示整数多项式环 $R_q = Z_q[X] / (X^N + 1)$。我们用粗体字母如 $\\mathbf{a}$、$\\mathbf{M}$ 来表示向量和矩阵，用 $a[j]$ 来表示向量 $\\mathbf{a}$ 的第 $j$ 个分量，用 $M[j, i]$ 来表示矩阵 $\\mathbf{M}$ 的 $(j, i)$ 项。Hadamard 乘积表示为 $\\mathbf{a} \\odot \\mathbf{b}$。\nB. 加密原语\n$$\rJxK⋅JyK≡l(JxK0+JxK1)⋅(JyK0+JyK1)≡lJxK0JyK0+JxK1JyK1+JxK0JyK1+JxK1JyK0JxK \\cdot JyK \\equiv_l (JxK_0 + JxK_1) \\cdot (JyK_0 + JyK_1)\\equiv_l JxK_0JyK_0 + JxK_1JyK_1 + JxK_0JyK_1 + JxK_1JyK_0\r$$ 其中，混合项 $JxK_0 JyK_1$ 和 $JxK_1 JyK_0$ 是通过同态加密计算的。对于一个实值 $x ̃ \\in \\mathbb{R}$，我们首先将其编码为一个定点值 $x = \\lfloor x ̃ 2^f \\rfloor \\in [-2^{l-1}, 2^{l-1})$，并在指定精度 $f \u0026gt; 0$ 下进行秘密共享。我们用 $J·; f K$ 明确表示 $f$ 位定点精度的共享值。当 $l = 1$ 时，我们使用 $JzK_B$ 表示布尔共享。此外，当所有权与上下文无关时，我们省略下标，仅写 $JxK$ 或 $JzK_B$。\n2.遗忘传输：我们依赖于遗忘传输（OT）来进行非线性计算。在一般的 1-out-of-2 OT 协议（$2^1$-OT）中，发送方输入两个长度为 $l$ 位的消息 $m_0$ 和 $m_1$，接收方输入一个选择位 $c \\in {0, 1}$。协议结束时，接收方学习到消息 $m_c$，而发送方则什么也不知道。当发送方的消息相关时，相关 OT（COT）在通信上更为高效 [6]。在我们的加法 COT 中，发送方输入一个函数 $f(x) = x + \\Delta$，其中 $\\Delta \\in Z_{2^l}$，接收方输入选择位 $c$。协议结束时，发送方学到 $x \\in Z_{2^l}$，而接收方学到 $x + c \\cdot \\Delta \\in Z_{2^l}$。在本工作中，我们使用 Ferret 协议 [77] 来实现低通信量的 COT。\n3.基于格的加法同态加密：同态加密（HE）允许在不知晓解密密钥的情况下，计算函数 $F(x)$ 的加密结果。在本研究中，我们使用基于环学习与误差（RLWE）的同态加密方案 [52]。RLWE方案由一组公共参数 $HE.pp = {N, q, t}$ 定义。\nKeyGen：生成RLWE密钥对 $(sk, pk)$，其中私钥 $sk \\in R_q$，公钥 $pk \\in R_{q^2}$。 加密：RLWE密文表示为多项式元组 $(\\hat{b}, \\hat{a}) \\in R_{q^2}$。我们使用 $RLWE_{q,t} pk(\\hat{m})$ 来表示在公钥 $pk$ 下加密的 $m \\in R_t$。 加法 (⊞)：给定两个RLWE密文 $ct_0 = (\\hat{b}_0, \\hat{a}_0)$ 和 $ct_1 = (\\hat{b}_1, \\hat{a}_1)$，它们分别加密了 $m_0, m_1 \\in R_t$，并使用相同的密钥，则操作 $ct_0 \\oplus ct_1$ 计算 RLWE 元组 $(\\hat{b}_0 + \\hat{b}_1, \\hat{a}_0 + \\hat{a}_1) \\in R_q^2$，并且该元组可以解密为 $\\hat{m}_0 + \\hat{m}_1 \\mod R_t$。 乘法 (⊠)：给定一个RLWE密文 $ct = (\\hat{b}, \\hat{a})$，它加密了 $m \\in R_t$，以及一个普通多项式 $\\hat{c} \\in R_t$，则操作 $ct \\otimes \\hat{c}$ 计算元组 $(\\hat{b} \\cdot \\hat{c}, \\hat{a} \\cdot \\hat{c}) \\in R_{q^2}$，并且该元组可以解密为$ \\hat{m} \\cdot \\hat{c} \\mod R_t$。 SIMD 编码：通过选择一个素数 $t$，使得 $t \\equiv 1 \\mod 2N$，SIMD 技术 [66] 允许将 $v, u \\in Z_t^N$ 的 N 个元素向量转换为多项式 $\\hat{v}, \\hat{u} \\in R_t$。多项式 $\\hat{v} \\cdot \\hat{u}$ 可以解码为按点相乘 $u \\odot v \\mod t$。在私有评估的上下文中，SIMD 技术可以通过一个因子 $1/N$ 来摊销按点相乘的成本。我们用 $\\hat{v} := SIMD(v)$ 来表示SIMD编码，并用 $SIMD^{-1}(\\cdot)$ 表示解码函数。 自同态变换：给定一个RLWE密文 $ct \\in R_q^2$，它加密了一个多项式 $\\hat{m}(X) \\in R_t$，以及一个奇数 $g \\in [1, 2N)$，操作 $ct\u0026rsquo; := Auto(ct, g)$ 计算新的密文 $ct\u0026rsquo; \\in R_q^2$，解密后得到 $m\u0026rsquo;(X) = \\hat{m}(X^g) \\in R_t$。 C. 基于Transformer的模型\n许多现代语言预处理器，如GPT [15] 和 BERT [19]，由一个输入嵌入层和多个Transformer层组成 [70]。类似地，视觉Transformer（ViTs）[22], [5] 也采用了类似的架构，不过它们没有输入嵌入层。基于Transformer的模型主要有两个计算模块：多头注意力机制和前馈神经网络。此外，层归一化（Layer Normalization）用于两个连续层之间。\n多头注意力：一个注意力机制 $Attention(Q, K, V)$ 计算的是 $softmax(QK^\\top + M)V$，它可以被描述为将一个查询 $Q$ 和一组键值对 $(K, V)$ 映射为一个加权和。这里，$Q, K, V$ 是输入矩阵的不同线性变换。多头注意力的变体计算 $H$ 个并行的注意力：$Attention(Q_j, K_j, V_j)$，其中 $j \\in [H]$，然后将这些 $H$ 个结果矩阵拼接起来。\n层归一化：对于一个向量 $x \\in \\mathbb{R}^d$，令 $\\mu = \\frac{1}{d} \\sum_{j} x[j] \\in \\mathbb{R}$ 和 $\\sigma = \\sum_{j \\in [d]} (x[j] - \\mu)^2 \\in \\mathbb{R}$。层归一化表示为：\n$LayerNorm(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma} + \\beta$\n其中 $\\gamma, \\beta \\in \\mathbb{R}$ 是两个超参数。\n前馈神经网络：前馈神经网络通常包括两个线性变换，并在它们之间应用激活函数，即：\n$FFN(X) = W_0 \\cdot F(W_1 \\cdot X)$\n其中，$F: \\mathbb{R}^* \\to \\mathbb{R}^*$ 是一个激活函数，如 GeLU 或 SiLU。\nD. 威胁模型与私密推理\n类似于之前的工作 [41], [55], [62], [36], [3]，我们针对一个静态且半诚实的概率多项式时间（PPT）对手，遵循理想/现实世界范式 [10]。也就是说，我们考虑一个计算能力有限的对手，在协议执行开始时腐化了其中一个方，并遵循协议规范，但试图学习有关诚实方输入的附加信息。BumbleBee 调用了几个小规模私密计算的子协议，概要见表 I。为了简化协议描述和安全性证明，我们使用混合模型描述 BumbleBee。一个调用功能 $F$ 的协议称为在“$F$-混合模型”中。此外，BumbleBee 中某些函数为了更好的效率进行了近似计算。根据定义 [25]，如果一个协议的近似计算揭示的输入信息不比 $F$ 本身更多，则该协议构成了 $F$ 的私密近似。\n在私密 2PC 推理（图 2）中，服务器 $S$ 持有一个 transformer 模型，而客户端 $C$ 向该模型发送查询，如一段文本。假设 $S$ 和 $C$ 均为半诚实模型，BumbleBee 使得 $C$ 只能得知两项信息：transformer 的架构和推理结果。$S$ 要么可以得知结果，要么什么都不能得知，具体取决于应用场景。关于 $C$ 的私密输入和 $S$ 的模型权重的所有其他信息应保持秘密。威胁模型的正式定义见附录。\n安全矩阵乘法 在私有变换器推理中，我们需要两种类型的矩阵乘法：\n共享矩阵与明文矩阵的乘法。例如，对于每个Transformer模块（参见图2），我们计算共享输入矩阵（由前一个模块计算得出）与服务器的明文权重矩阵之间的乘法。 注意力机制内部两个秘密共享矩阵的乘法。 我们提出了一种原语，称为\u0026quot;隐式线性变换\u0026quot;（Oblivious Linear Transformation，OLT），用于实现这两种矩阵乘法。我们将OLT描述为一种二方协议，分别从两个方获取两个私有矩阵Q和V，并生成它们之间的共享矩阵JQVK。通过OLT原语，我们可以使用一次OLT执行计算共享矩阵JQK和明文矩阵V之间的乘法。另一方面，对于两个共享矩阵的乘法，我们需要进行两次OLT。也就是说，我们通过两个OLT计算两个混合项JQK1 · JVK0和JQK0 · JVK1。\n基于同态加密（HE）的OLT现有方法 我们的出发点是许多研究中使用的基于同态加密（HE）的OLT方法，例如[54]、[36]、[33]（以下简称为KRDY风格）。KRDY需要两个函数： πlhs : Zkw×mw 2l 7→ R2l 和 πrhs : Zmw ×nw 2l 7→ R2l，用于将矩阵编码为可以使用RLWE加密的多项式系数。假设 1 ≤ kw · mw · nw ≤ N，这两个函数定义如下： qˆ := πlhs(Q) 和 vˆ := πrhs(V)，使得 (1) qˆ[0] = Q[i, j]，当 i = 0 且 j = 0 时， qˆ[N + i · kw · mw − j] = −Q[i, j]，当 i = 0 且 j ∈ [mw]/{0} 时， qˆ[i · kw · mw − j] = Q[i, j]，当 i \u0026gt; 0 且 j ∈ [mw] 时， vˆ[k · mw + j] = V[j, k]，当 j ∈ [mw] 且 k ∈ [nw] 时。 qˆ 和 vˆ 的所有其他系数设置为0。 乘积多项式 qˆ · vˆ 直接给出结果矩阵 QV 的一些系数，如以下命题所示。\n命题 1. [54，改编] 假设 1 ≤ kw · mw · nw ≤ N。给定两个多项式 qˆ = πlhs(Q)，vˆ = πrhs(V) ∈ R2l，乘法 U ≡l Q · V 可以通过在环 R2l 上计算多项式的乘积 uˆ = qˆ · vˆ 来评估。即 U[i, k] = uˆ[i · mw · nw + k · mw]，对于所有 i ∈ [kw]，k ∈ [nw]。\n图3提供了πlhs和πrhs编码方法背后的基本思想的简单示意图。在OLT的背景下，我们让P0使用P0的密钥将同态加密的RLWEpk0(πlhs(Q))发送给P1。然后，P1可以通过单次同态乘法，即RLWEpk0(πlhs(Q))⊠πrhs(V)，同态地评估矩阵乘法 Q · V。为了将加密后的矩阵转换为算术共享，P0和P1接着共同调用FH2A功能（参见表I）。当矩阵形状 k·m·n \u0026gt; N 时，我们可以将其拆分成更小的子矩阵，并将KRDY应用于每对对应的子矩阵。我们将划分窗口表示为(kw, mw, nw)。\n在通信成本方面，KRDY需要交换 O(min(km / kwmw, mn / mwnw) + kn / kwnw) 个密文。与[55]、[62]中使用的其他基于HE的OLT方法相比，KRDY OLT不需要任何旋转（在HE中旋转操作非常昂贵），因此相对高效。然而，KRDY OLT的结果中有许多“无用”的系数。根据命题1，结果多项式uˆ中只有kwnw个系数是“有用的”，而N个系数中KRDY仍然需要传输整个uˆ的RLWE密文进行解密，这可能会浪费通信资源。为了提供一个例子，我们考虑一个在变换器模型中常用的维度集合，如k = 16，m = 768，n = 3072。在这种情况下，KRDY协议可能需要交换约25MB的密文。这可能是一个可观的通信开销，因为变换器模型通常包含数百个这样的超大规模矩阵。接下来的部分将详细讨论减少这种通信负担的方法。\n通过密文打包的首次尝试 我们的首次尝试是将PackLWEs [13]过程应用于KRDY，我们将其称为KRDY+。PackLWEs过程允许我们从多个RLWE密文中选择任意系数，并通过执行同态自同构将它们组合成一个单一的RLWE密文。在KRDY+的情况下，我们可以将密文数量从 O(kn/(kwnw)) 减少到 O(kn/N)，代价是 O(kn) 的同态自同构操作。再次考虑矩阵形状 (k, m, n) = (16, 768, 3072)。现在，KRDY+交换大约2.9 MB的密文，相较于KRDY的25 MB，这是一个显著的减少。然而，在HE中，自同构操作非常缓慢，实际上对于具有较大kn的矩阵，KRDY+使用的自同构可能比同态乘法本身更为计算密集。\n密文交织优化 我们在矩阵乘法的背景下提出了针对PackLWEs过程[13]的专门优化。其主要思路是，与其按照PackLWEs过程逐个“挑选”有用的系数，我们更倾向于“清理”结果密文中的无用系数，然后将它们合并成一个单一的密文。我们简要描述清理过程。让我们以 N = 8 和 aˆ(X) = ∑ᵢ=0⁷ aiXi 为例。根据定义，自同构Auto(ˆa, 9)生成 ∑ᵢ=0⁷ aiXi·9，这相当于 ∑ᵢ=0³ a2iX2i − ∑ᵢ=0³ a2i+1X2i+1 mod X8 + 1。在这里，奇数位置的系数符号被翻转。因此，评估 ˆa + Auto(ˆa, N + 1) 会消除奇数索引的系数，并将偶数索引的系数加倍。更一般地，令我们计算 ˆa + Auto(ˆa, N/2j + 1) 对于任何 N 的2的幂因子。再次以N = 8为例，我们考虑 2j = 2。Auto(ˆa, 8/2 + 1) 等于 ∑ᵢ=0⁷ aiXi·5 ≡ a0 − a2X2 + a4X4 − a6X6 + ∑ᵢ≠0 mod 2 aiXi·5。简而言之，2j倍数位置的系数符号被翻转。假设对于所有 i ≠ 0 mod 2j 的 ˆa[i] 系数已经为零。那么，ˆa + Auto(ˆa, N/2j + 1) 会消除2j的奇数倍位置的系数，同时加倍偶数倍位置的系数。我们定义一个函数 ZeroGap(ˆa, 2r)，它重复公式 r 次 ˆa := aˆ + Auto(ˆa, N/2j + 1) 对于 j = 0, 1, · · · , r − 1。执行这些操作后，结果是一个多项式，其中只有在2r倍数位置的系数是非零的。然而，这些系数被2r的倍数缩放。为了修正这个缩放，我们首先将多项式乘以逆缩放因子 2−r mod q。这隐式地要求使用奇数模数q，这在使用模数 q ≡ 1 mod 2N 的基于RLWE的HE中通常是满足的。此外，ZeroGap过程要求两个需要的系数之间的间隔是2的幂数。这等同于πlhs和πrhs编码中的分区窗口 mw。请注意，只要满足 1 ≤ kwmwnw ≤ N，可以自由选择这个分区窗口。\n有了ZeroGap过程，我们提出了我们的关键贡献之一，即InterLeave过程。InterLeave过程将2r个多项式交织成一个单一的多项式，输入系数以2r步的跨度排列。我们首先给出InterLeave的简单版本，如图4所示。在这个简单版本中，我们分别对每个输入多项式应用ZeroGap过程（步骤2到步骤5）来清理无关系数。然后，我们通过简单的旋转-求和计算（步骤6到步骤8）得到最终结果。我们想强调的是，密文域中的系数旋转可以比SIMD旋转[32]更高效地实现。事实上，图4计算了 O(r · 2r) 次自同构操作，将 N 个系数合并成一个多项式。图5给出了一个InterLeave（简单版本）的玩具示例。\n最终版本。我们现在展示图6中的优化版本，它将图4中的复杂度从 O(r · 2r) 次自同构操作降低到了 O(2r) 次自同构操作。我们将展示这个优化版本的直觉。直观上，利用自同构函数的双线性性质，将两个具有相同索引的自同构操作合并为一个自同构操作是合理的。即，对于任何有效的索引 g，两个自同构操作的和 Auto(ˆa, g) + Auto(ˆb, g) 等于对多项式和的自同构操作。然而，应用相同自同构索引的多项式在求和步骤（图4的步骤8）之前，需要先进行不同单位的右移（即图4的步骤6）。为了解决这个问题，我们利用以下方程。即，当 ˆb[i] = 0 对于所有 i ≠ 0 mod 2j 时，\n$$\rAuto(ˆa,N/2j+1)+Auto(ˆb,N/2j+1)⋅X2j=Auto(ˆa−ˆb⋅X2j,N/2j+1)Auto(ˆa, N/2j + 1) + Auto(ˆb, N/2j + 1) \\cdot X^{2j} = Auto(ˆa - ˆb \\cdot X^{2j}, N/2j + 1)\r$$$$\rAuto(ˆb,N/2j+1)⋅X2jAuto(ˆb, N/2j + 1) \\cdot X^{2j}\r$$$$\rAuto(−ˆb⋅X2j,N/2j+1)Auto(-ˆb \\cdot X^{2j}, N/2j + 1)\r$$ 这样，我们就可以利用双线性特性，将具有相同索引的两个自同构操作合并为一个自同构操作。\n提出的OLT协议 我们现在在算法1中描述我们的OLT协议。前三个步骤与KRDY风格的协议 [54]，[36] 类似。具体来说，我们将一个大矩阵分割成子块，并使用 πlhs 和 πrhs 将每个块编码为多项式。然而，在我们的方案中，我们故意选择了一个2的幂次方的分区窗口 mw。我们协议与他们的主要区别在于，在步骤4中使用了 InterLeave 操作，以减少RLWE密文的数量，从 O(kn/(kwnw)) 降低到 O(kn/N )。解析交织多项式并构造结果矩阵的过程在附录的图8中进行了说明。为了在计算和通信开销之间取得良好的平衡，选择合适的分区窗口 mw 是至关重要的。较小的 mw 值可能会在步骤1中过度增加通信开销，而较大的 mw 值会导致步骤4中更多的同态自同构操作。给定矩阵形状 (k, m, n) 和分区窗口 (kw, mw, nw)，步骤1中发送的密文数量为 n1 := m′·min(k′, n′)。执行 InterLeave 所需的同态自同构操作总数为 n2 := ⌈k′n′/mw⌉mw。交织后，步骤5中发送的密文数量为 n3 := ⌈ k′n′ / mw ⌉。为了选择合适的分区窗口，我们可以通过最小化以下目标函数来优化：\n$\\ argmin_{kw, mw, nw} ; PC \\cdot n2 + PB \\cdot (n1 + n3)$\n其中，mw 必须是2的幂，并且满足 $1 \\leq kw \\cdot mw \\cdot nw \\leq N$ 的约束。这里，PC 表示计算一个同态自同构操作的成本，PB 表示发送一个密文的成本。较小的 PC/PB 比率可能表明计算能力强大或带宽受限的情景。在这种情况下，我们倾向于选择较大的 mw。相反，如果带宽充足，可以选择较小的 mw 值，以减轻密文交织所需的时间。\n复杂度。根据我们的经验结果，选择 mw ≈ √N 是一个可行的选择。然后，算法1中的协议需要大约 O(kn/√N) 次同态自同构操作用于密文交织。相比之下，KRDY+ 基准协议需要 O(kn) 次同态自同构操作。举个例子，设 mw = 26 且 N = 213。在这种设置下，我们协议的密文压缩时间大约是 KRDY+ 密文压缩时间的 1/26 ≈ 1.6%。KRDY+ 和我们协议的通信开销都是 O(kn/N) 个密文。请注意，BOLT 的主模OLT可能根据矩阵维度 [58] 需要更少的同态自同构操作 O(pk²m²n/N²)。\n安全性。我们仅增加了一个密文压缩步骤来减少通信开销，我们发送的信息是 KRDY 协议的子集，因此我们的安全级别与其相同。定理1的证明参考了 [54]，[36]。\n算法 1 提议的不可知线性变换 \\( $ \\Pi_{OLT} $\\)\n$$ Q \\in \\mathbb{Z}^{k \\times m} $$$$ 2^l $$$$ V \\in \\mathbb{Z}^{m \\times n} $$$$ 2^l $$。\n$$ JUK $$$$ U \\equiv_l Q \\cdot V $$$$ pp = (\\text{HE.pp}, pk, (kw, mw, nw)) $$ 大小 \\( mw \\) 是一个 2 的幂，且\n$$ 1 \\leq kw \\cdot mw \\cdot nw \\leq N $$ $$ k' = \\lceil \\frac{k}{kw} \\rceil $$$$ m' = \\lceil \\frac{m}{mw} \\rceil $$$$ n' = \\lceil \\frac{n}{nw} \\rceil $$$$ \\tilde{m} = \\lceil \\frac{k' \\cdot n'}{mw} \\rceil $$ 注意：如果\n$$ k' \u003e n' $$，则交换发送方和接收方的角色。 步骤：\n$$ Q_{\\alpha, \\beta} \\in \\mathbb{Z}^{kw \\times mw} $$$$ \\hat{q}_{\\alpha, \\beta} := \\pi_{\\text{lhs}}(Q_{\\alpha, \\beta}) $$$$ ct'_{\\alpha, \\beta} := \\text{RLWE}_{q, 2l}^{pk} (\\hat{q}_{\\alpha, \\beta}) $$\n发送给 \\( R \\)。\n$$ V_{\\beta, \\gamma} \\in \\mathbb{Z}^{mw \\times nw} $$$$ \\hat{v}_{\\beta, \\gamma} := \\pi_{\\text{rhs}}(V_{\\beta, \\gamma}) $$\n其中 \\( \\beta \\in [m'] \\) 且 \\( \\gamma \\in [n'] \\)。\n$$ \\{ct'_{\\alpha, \\beta}\\} $$$$ c[\\alpha \\cdot n' + \\gamma] := \\bigoplus_{\\beta \\in [m']} ct'_{\\alpha, \\beta} \\odot \\hat{v}_{\\beta, \\alpha} $$\n对于 \\( \\alpha \\in [k'] \\)，\\( \\gamma \\in [n'] \\)。\n$$ \\tilde{c}[\\theta] := \\text{InterLeave}([c[\\theta \\cdot mw], c[\\theta \\cdot mw + 1], \\cdots \\mid \\{z\\} mw]) $$\n对于 \\( \\theta \\in [\\tilde{m}] \\)。\n▷ 当 \\( k' \\cdot n' \\) 无法被 \\( mw \\) 整除时，填充零。\n$$ \\hat{c}_{i, 0}, \\hat{c}_{i, 1} \\leftarrow \\text{FH2A}(\\tilde{c}[i]) $$$$ JUK_l := \\text{ParseMat}(\\hat{c}_{0, l}, \\hat{c}_{1, l}, \\cdots) $$\n来获取各自的分享。\n定理 1. 算法 1 中的协议 ΠOLT 在 FH2A 混合模型下，能够在半诚实对手存在的情况下，私密地实现 OLT 功能。\nE. 进一步优化\n批量矩阵乘法：回顾一下，变换器模型中的每个多头注意力涉及 H \u0026gt; 1 个并行的矩阵乘法，例如，Qj · Kj，其中 j ∈ [H]。当这些结果矩阵较小时，即 |Qj · Kj| ≤ N/2 时，我们倾向于应用 InterLeave 对批量乘法的密文进行压缩。简而言之，批量乘法的 O(H) 个 RLWE 密文将进一步打包成 O((H · kn)/N) 个密文。 动态压缩策略：我们采用一种动态的密文压缩策略，使我们能够在减少通信成本和增加计算成本之间取得平衡。具体来说，当只有一个 RLWE 密文需要发送时，即 ⌈k/kw⌉ · ⌈n/nw⌉ = 1，我们不使用任何密文压缩。另外，当 kn ≪ N 时，我们使用 PackLWEs 而不是 InterLeave，因为前者在小规模情况下可以运行得更快。 基于 RLWE 的更快批量 OLE 除了矩阵乘法，变换器模型中还需要标量乘法（或按批次处理的 Hadamard 乘积）。我们使用符号 Batch OLE（bOLE）[17] 来描述一个二方计算协议，该协议接收一个发送方 S 的向量 x 和接收方 R 的向量 y，并生成它们 Hadamard 乘积的秘密共享 Jx ⊙ yK。在私有推理的背景下，我们证明了带误差的 bOLE 变体（bOLEe）是足够的。我们可以使用更小的 RLWE 参数构建更高效的 bOLEe 协议。我们的 bOLEe 可能会在最终输出中引入最低有效位（LSB）误差。然而，由于在多方计算（MPC）中使用了定点表示，这些 LSB 误差将通过后续的截断被移除，因此我们的方法不会影响整体准确性。与 [63] 中基于 RLWE 的 bOLE 类似，我们还应用了 SIMD 技术 [66] 来实现 bOLE，但具有更好的摊销效率。在 [63] 中，发送方 S 对其私有输入 x ∈ ZN 2l 进行预处理为 SIMD(x)，并将密文 RLWEq,t S (SIMD(x)) 发送给接收方 R。然后，R 回应密文 RLWEq,t S (SIMD(x)) ⊠ SIMD(y) ⊞ SIMD(r) 给 S，r 是一个掩码向量，r ∈ ZN 。实际上，这里的算术运算 x⊙y+r 是在模 t 下进行的，尽管 x 和 y 中的值来自环 Z2l 。由于模 t 与 2l 不可整除，掩码向量 r 应从更大的环中采样以提供统计安全性。具体来说，[63] 从 ZN 22l+σ 中采样 r，以提供 σ 位的统计安全性。此外，为了防止加法溢出，他们需要设置一个较大的明文模 t \u0026gt; 22l+σ+1。我们展示了如何避免这种额外的 σ 位开销，代价是引入 1 位的 LSB 误差。基本上，我们使用提升函数在质数明文模 t 上引入一个中间层的 2l 模。 Lift(x) : ZN 2l → ZN t 通过 ⌊ t / 2l · x⌉ mod t Down(y) : ZN t → ZN 2l 通过 ⌊ 2l / t · y⌉ mod 2l\n命题 2. 如果 t \u0026gt; 22l，则对于任意 x, y ∈ Z2l，有 Down(Lift(x)·y mod t) ≡l x·y。也就是说，我们可以在质数模 t 上进行模 2l 运算。\n证明. 只需证明误差项可以被舍入为零。Lift(x) · y mod t 可以写为 t / 2l · ((x · y mod 2l) + re · y，其中 re 是舍入误差，|re| ≤ 1/2。然后，误差项被舍入为 ⌊ 2l / t · (re · y)⌉ = 0，当 t \u0026gt; 22l 且 y \u0026lt; 2l 时。 现在，我们可以使用信息学随机掩码 r ∈ ZtN 代替统计掩码。结果中可能会引入 1 位误差。\n命题 3. 设 u′ := Down(Lift(x) · y − r mod t)，v′ := Down(r)，其中 x, y ∈ Z2l 且 r ∈ Zt。如果 t \u0026gt; 22l 且 r 在 Zt 上均匀分布，则 u′ + v′ ≡l x · y + e，其中 e ∈ {0, ±1}。\n算法 2 带误差的 bOLE 协议 ΠbOLEe\n输入： 发送方 S：x ∈ ZN 2l ，私钥 sk。 接收方 R：y ∈ ZN 2l 。 公共参数 pp = {N, t}，其中 t = 1 mod 2N 为质数，且 t \u0026gt; 22l，以及公共密钥 pk。\n输出： JzK ∈ ZN 2l，使得 ∥z − x ⊙ y mod 2l∥∞ ≤ 1。\n步骤：\nS 将 RLWEq,t pk (xˆ) 发送给 R，其中 xˆ := SIMD(Lift(x))。 R 计算 yˆ := SIMD(y)。 收到密文 RLWEq,t sk (xˆ) 后，R 计算 ct := RLWEq,t pk (xˆ) ⊠ yˆ。 JuˆK ← FH2A(ct) 将其转换为算术共享。假设 S 的共享为 JuˆK0 ∈ Rt，R 的共享为 JuˆK1 ∈ Rt。请注意，我们让 FH2A 函数来捕捉电路隐私（参见备注 1）。 S 输出 Down(SIMD−1(JuˆK0))。 R 输出 Down(SIMD−1(JuˆK1))。 该证明基本遵循 [36, 完整版本，附录 C] 中的类似论证。\n定理 2. 算法 2 中的协议 ΠbOLEe 在 FH2A 混合模型下，能够在半诚实对手存在的情况下，私密地实现 bOLEe 功能（参见表 I）。 定理 2 的正确性简单地遵循 SIMD 打包和命题 3。对于具体的改进，我们的 bOLEe 协议在 l = 64 时需要 t ≈ 2128，而 [63] 的方法需要更大的 t ≈ 2168。我们的实证结果表明，我们的协议比他们的协议快约 1.3 倍。\n激活函数协议 我们首先描述用于 GeLU 函数的协议。本节中描述的方法和优化也可以应用于其他函数，如 SiLU 和 ELU（见附录 D）。\nA. 高斯误差线性单元（GeLU）\nGeLU 函数的常见定义是：\n$GeLU(x)=0.5x(1+tanh⁡(2π(x+0.044715x3))).\\text{GeLU}(x) = 0.5x(1 + \\tanh\\left(\\frac{\\sqrt{2}}{\\pi}(x + 0.044715x^3)\\right)).$\n$$\r\\text{Seg4GeLU}(x) = \\begin{cases}\r-\\epsilon \u0026 \\text{if } x \u003c -5 \\\\\rP_3(x) \u0026 \\text{if } -5 \\leq x \\leq -1.97 \\\\\rP_6(x) \u0026 \\text{if } -1.97 \\leq x \\leq 3 \\\\\rx - \\epsilon \u0026 \\text{if } x \u003e 3\r\\end{cases}\r$$ 其中 $P_b(x)$ 是近似 GeLU 函数的 b 次多项式，定义在一个较短的区间上。例如，设定 $\\epsilon = 10^{-5}$，我们绘制了 Seg4GeLU 如图 7a 所示。从图中可以看到，Seg4GeLU 很好地近似了 GeLU 函数。\n我们在算法 3 中的 GeLU 协议基本上遵循（2），使用 Fless 和 Fmux 进行分支选择。为了进一步提高效率，我们讨论并引入了三个独立的优化，这些优化在之前的研究工作 [58]、[49]、[24]、[21] 中未提及。\n近似分支选择：第一个优化利用了激活函数的平滑性。具体来说，我们首先找到近似多项式 $P_3(x)$ 和 $P_6(x)$，使得 $P_3(x) \\approx P_6(x)$，特别是在枢轴点附近，例如在我们的案例中，$x = -1.97$。公式（2）中的选择可以通过一个小于协议 $1{x \u0026lt; y}$ 来私密地实现，其中 $x, y \\in Z_{2l}$。基于这些观察，我们建议计算小于位时忽略一些低位 f′ 位，以减少通信开销。这是一个常见的优化方法，用于在固定点值的秘密共享上执行比较。然而，确保正确的近似多项式至关重要，因为超出枢轴点的波动可能会破坏近似。经验表明，近似的分段选择有助于将算法 3 中 GeLU 协议的通信开销减少 5%。 批量（近似）分支选择：我们的第二个优化是针对 [46] 和 [36] 中基于 OT 的 MSB 协议。该协议使用公式 $MSB(JxK_0) \\oplus MSB(JxK_1) \\oplus 1{(JxK_0 \\mod 2^{l-1}) + (JxK_1 \\mod 2^{l-1}) \\geq 2^{l-1}}$ 计算算术共享 $JxK$ 的 MSB，其中最后一位的计算需要一个 $M_1$-OT2。因此，对于公式（2）中的分支选择，我们需要 3 次 $M_1$-OT2 调用。请注意，这些“批量”比较是通过一个秘密共享和多个明文阈值进行的。通过这种安排，我们可以将 3 次 $M_1$-OT2 调用合并为一次 $M_1$-OT6，即在 6 位消息上进行 1-of-M OT。尽管这种 OT 组合可能不会显著减少通信开销，但根据我们的实验，它可以将 GeLU 的计算时间减少 35%。这是因为在使用 Ferret OT 时，调用一次 $M_1$-OT2 和调用一次 $M_1$-OT6 的运行时间相似。 优化多项式评估：对于公式（2）中的 $P_3(x)$ 和 $P_6(x)$ 的评估，我们建议使用更快的平方协议 $\\Pi_{\\text{square}}$ 来计算所有偶次幂项，如 $x^2$、$x^4$ 和 $x^6$。实际上，在 2PC 中，执行平方操作的成本是标准乘法操作的一半。此外，我们还提出通过减少一半的通信成本来优化奇次幂项的计算。举个例子，考虑给定 $JxK$ 和 $Jx^2K$ 时，如何计算 $Jx^3K$。我们可以使用两次 bOLE 调用来计算 $Jx^3K$，即 $FbOLE(JxK_0, Jx^2K_1)$ 和 $FbOLE(Jx^2K_0, JxK_1)$。对于算法 2 中的 bOLE 构造，P0 将 SIMD(Lift(JxK_0)) 和 SIMD(Lift(Jx^2K_0)) 的密文发送给 P1。我们注意到，当计算 $Jx^2K$ 和 $Jx^4K$ 时，这两个密文已经被发送给 P1。因此，为了计算立方项，玩家可以跳过算法 2 中的步骤 1，按照其余步骤相同地执行。 权衡。在我们的评估中，我们证明了这三种优化可以使时间减少35%，通信减少7%。需要注意的是，这些改进伴随着近似误差的增加。具体来说，我们的GeLU评估的平均ULP误差约为11，而[58]的平均ULP误差为4。此外，我们在四个transformer模型中测试了提出的GeLU协议，结果表明推理精度的降级非常小，不超过1%。我们认为这是精度和效率之间的合理权衡。\n$$\rsoftmax(x)[i]=exp⁡(x[i]−xˉ)∑jexp⁡(x[j]−xˉ),\\text{softmax}(x)[i] = \\frac{\\exp(x[i] - \\bar{x})}{\\sum_j \\exp(x[j] - \\bar{x})},\r$$ 其中 $\\bar{x}$ 是输入向量 $x$ 的最大元素。对于二维矩阵，我们对其每一行向量应用公式(3)。值得注意的是，公式(3)中的所有指数操作的输入都是负数。我们利用负操作数来加速私有softmax。具体来说，我们通过简单的裁剪分支来近似指数操作：\n对于 $x \\in [T_{\\text{exp}}, 0]$，我们有 $\\exp(x) \\approx 1 + \\frac{x}{2^n}$； 对于 $x \u0026lt; T_{\\text{exp}}$，则 $\\exp(x) \\approx 0$。 对于裁剪范围 $T_{\\text{exp}}$，我们简单地设置 $T_{\\text{exp}}$ 使得 $\\exp(T_{\\text{exp}}) \\approx 2^{-f}$，其中 $f$ 是定点精度。假设我们设置 $f = 18$，那么我们设置 $T_{\\text{exp}} = -13$，因为 $\\exp(-13) \u0026lt; 2^{-18}$。当 $T_{\\text{exp}}$ 固定后，我们可以经验性地设置泰勒展开的次数 $n$。例如，在我们的实验中，对于 $T_{\\text{exp}} = -13$，我们设置 $n = 6$，以使平均误差在 $2^{-10}$ 之内。此外，我们还使用上一节提出的近似“小于”操作进行分支选择，因为负输入的指数操作也是平滑的。通过调用 $F_{\\text{tnrunc}}$ 可以实现除以 $2^n$，而 $2^n$ 的幂运算是通过一系列的 $\\Pi_{\\text{square}}$ 来计算的。\nC. 混合位宽评估 在运行我们的激活协议之前，我们可以先将共享值切换到较小的环 $l\u0026rsquo; \u0026lt; l$ 以节省通信开销。大环 $Z_{2^l}$ 到小环 $Z_{2^{l\u0026rsquo;}}$ 的共享转换可以通过局部设置 $JxK_l \\mod 2^{l\u0026rsquo;}$ 来完成。反方向的转换，我们结合了[61]中的环扩展协议和[16]中的启发式优化。在我们的实现中，每次从 $Z_{2^{l\u0026rsquo;}}$ 到 $Z_{2^l}$ 的转换将在两轮中交换大约 $O(2^{(l - l\u0026rsquo;)})$ 位。这个混合位宽评估的主要问题是乘法引起的溢出。例如，双精度定点 $2^f$ 可能已经大于 $l\u0026rsquo;$。为了解决这个问题，我们必须做出两个妥协：\n使用额外一次对 $F_{\\text{trunc}}$ 的调用来降低激活函数中的定点精度； 缩小近似区间或增加分支数，以便可以使用较低次数的多项式。 $$\rGeLU(x)≈P4(x)=0.5x+∑i=04ci∣x∣iforx∈[−2.7,2.7].\\text{GeLU}(x) \\approx P_4(x) = 0.5x + \\sum_{i=0}^{4} c_i |x|^i \\quad \\text{for} \\quad x \\in [-2.7, 2.7].\r$$ 在我们的一些实验中，我们对GeLU应用了这个 $P_4(x)$，但对于SiLU函数，我们仍然需要使用两个多项式。\nRELATED WORK Low Communication OLT 许多利用同态SIMD [66] 技术的工作可以直接用于OLT功能，并具有相对较小的通信开销，例如 [32]、[12]、[39]、[35] 等。SIMD技术要求使用质数明文模数 $t$，而不是秘密共享中使用的 $2^l$ 模数。可以利用中国剩余定理接受来自 $Z_{2^l}$ 的秘密共享，但这会使得同态加密侧的计算和通信开销大大增加。另一种构建低通信OLT的策略是使用向量隐式线性评估（VOLE）[9]、[77]、[7]，正如CipherGPT [34]中提出的。然而，这种基于VOLE的方法要求较大的矩阵维度 $k \\approx 10^7$ 才能实现低（摊销）通信，因此只适用于自回归变换器。\nNon-linear Functions 对于softmax函数，[45]、[68] 也使用泰勒级数 $(1 + x/2^n)^{2^n}$ 来近似指数运算，只是他们没有应用范围裁剪。例如，CryptGPU根据对一些数据集的检查经验性地设置了 $n = 9$。SiRNN [61] 使用查找表进行初步猜测，并通过几次牛顿迭代进行修正。这些方法提供了精确的指数运算，但在2PC中计算开销较大。[80] 使用另一种数值方法近似softmax函数。然而，它可能需要超过64轮乘法，这使得它在通信上非常密集。Kelkar等人[42]提出了一种新的2PC指数协议，但由于失败概率，它的局限性在于必须使用较大的环 $l \\approx 128$ 或严格约束输入域（例如，$|x| \\leq 5$）。[49] 通过加扰电路[78]使用12个一次多项式来近似激活函数。最近的研究方法也[20]、[21]、[24]、[58] 使用多个低次多项式来近似GeLU函数。然而，这些方法都没有考虑GeLU函数的平滑性以及批量比较来减少分支选择的开销。\nPrivate Transformer Inference (2PC) Iron [33]、BOLT [58] 和 CipherGPT [34] 是为变换器设计的2PC推理框架。这三个框架都大量复用了SiRNN框架 [61]、[23] 中基于OT的协议来评估激活函数。BOLT [58] 被认为是安全的双方变换器推理的最先进框架。BumbleBee与BOLT共享一些设计元素，例如非线性函数的分段近似和使用同态加密（HE）的私有矩阵乘法。然而，BumbleBee在通信效率方面超越了BOLT（见图1）。BumbleBee优于BOLT的主要原因总结如下：\n共享类型一致的乘法 BOLT的矩阵乘法协议在质数模数$p$上运行，而他们的非线性协议则在环模数$2^l$上运行。在私有推理过程中，它们必须在不同的共享类型之间来回切换。为了集成环到质数的转换（例如，参考[42]），BOLT需要一种特殊的共享乘法，其中输出的位宽可以大于输入的位宽。具体来说，BOLT复用了EzPC框架[23]、[61]中的基于OT的协议来实现这种不均匀的共享乘法。相比之下，在BumbleBee中，矩阵乘法协议和共享乘法协议都在环模数$2^l$上运行。这些共享类型一致的乘法使我们能够基于HE构建更高效的共享乘法协议。现有的工作如[63]已经表明，基于HE的共享乘法协议在通信方面比基于OT的协议高效5到6倍。需要注意的是，我们进一步优化了[63]的通信开销，提升了1.3倍。\n平方操作的成本是标准乘法的一半 为了私密评估分段函数，我们需要对秘密共享输入$JxK$评估低次多项式。BOLT采用霍纳法则来评估多项式。例如，BOLT评估一个四次多项式如下：\n$P4(JxK)=(((((a4⋅JxK+a3)⋅JxK)+a2)⋅JxK)+a1)⋅JxK+a0,P_4(JxK) = (((((a_4 \\cdot JxK + a_3) \\cdot JxK) + a_2) \\cdot JxK) + a_1) \\cdot JxK + a_0$,\n这需要3次标准的共享乘法。BOLT通过Motzkin多项式预处理将标准乘法的次数减少到2次。相比之下，在BumbleBee中，我们倾向于利用平方操作，因为执行平方操作的成本是标准乘法操作的一半。例如，我们需要两个平方操作（用于二次项和四次项）以及“一半”的标准乘法（用于三次项）来评估四次多项式，更不用说更高效的共享乘法协议了。我们评估低次多项式的方式也比BOLT使用的霍纳法则更高效，尤其是在私有计算的背景下。\nLower Communication OT 最后一个优势来自于OT协议的具体选择。具体来说，BOLT选择使用IKNP OT [38]作为其基础OT协议，而我们则利用Ferret OT [77]。Ferret OT的通信开销比IKNP OT小，但代价是更多的本地计算。我们在工程上进行了努力，将Ferret OT集成到多核CPU中，以最小化其运行时间，包括跨多个线程的同步。\n从上述总结来看，我们强调了支持环模数的矩阵乘法协议的重要性。正是我们乘法协议中的共享类型一致性，使得BumbleBee能够显著超越BOLT。\n私有变换器推理 ((2+1)-PC) 最近的研究，如[48]、[31]、[67]、[30]，已经探讨了双方（例如S和C）可以访问受信任第三方（TTP）帮助的场景。这些方法通常将私有计算过程分为两个独立的阶段：初步预处理阶段，随后是评估阶段。值得注意的是，在预处理阶段，TTP负责生成并分发双方随后在评估阶段使用的所有相关随机性。参照[67]的命名，我们将这种TTP辅助的计算过程称为(2+1)-PC，以便与我们的2PC设置清晰区分。\n私有变换器推理 (3PC) PrivFormer [3] 和 PUMA [21] 是为变换器设计的两种基于三方设置 [4]、[56] 的私有推理框架。具体来说，PrivFormer通过使用MPC友好的替代方案（即ReLU注意力机制）替代了softmax注意力，但这需要对模型进行微调。PUMA与BumbleBee共享一些设计元素，如GeLU的分段近似和指数函数的泰勒近似。简而言之，这些3PC方法不使用OT和HE，但通信开销比我们的方案要大。其他相关工作[79]、[48]、[3]考虑了使用不同近似结构的替代模型，这些模型在MPC中更易计算。然而，已有研究表明粗略的近似可能会显著降低模型的准确性[43]，因此他们的工作需要进行模型微调。BumbleBee的一个优势是它不需要任何模型微调；相反，我们的重点是在给定的预训练变换器模型上进行私有推理。值得注意的是，我们的方法也可以适配到他们的模型中，从而实现更好的性能。\nVII. 评估\n模型与数据集 我们在5个变换器模型上评估BumbleBee，包括四个NLP模型，即BERT-base、BERT-large [19]、GPT2-base [15]、LLaMA-7B [69]，以及一个计算机视觉模型ViT-base [74]、[22]。这些模型通过三个超参数进行参数化：块数B、表示的维度D和头数H。我们直接复用了来自公开源的训练模型。为了展示BumbleBee的有效性，我们在4个数据集上进行了私有推理，包括来自GLUE基准的CoLA、RTE和QNLI（用于NLP任务），以及用于图像分类的ImageNet-1k [64]。具体来说，ImageNet-1k数据集是一个1000个不同类别的分类任务，而来自GLUE基准的三个数据集是二分类任务。\n评估指标 在2PC设置下，我们不区分“离线”和“在线”成本，如一些先前的(2+1)-PC工作所做的那样[55]、[30]。我们报告端到端的运行时间，包括通过网络传输密文的时间。但我们没有包括从硬盘加载模型所花费的时间。我们测量了总通信量，包括双方发送的所有消息。我们使用1GB = 210MB = 230字节。\n实验环境 本文描述的实验主要在两台阿里巴巴云实例（ecs.g7.16xlarge）上进行，这些实例配备了64个vCPU，主频为2.70GHz，内存为256GB。我们尽可能使用多线程。为了模拟不同的网络条件，我们通过Linux中的流量控制命令操控了云实例之间的带宽。具体而言，我们在两种网络设置下进行了基准测试：局域网（LAN），带宽为1Gbps（单向1Gbps），延迟为0.5ms，以及广域网（WAN），带宽为400Mbps，延迟为4ms。\n具体参数 我们将秘密共享的l设置为64，将定点精度f设置为18。在对GeLU和指数运算进行混合环优化时，我们将l′设置为32，f′设置为12。具体来说，我们对BERT-base、GPT2-base和ViT-base模型应用了GeLU和指数运算的混合环优化，而对于其他较大的模型，我们仅对GeLU/SiLU激活进行优化。我们扩展了Yacl库中的Ferret实现[1]，以支持各种应用级的OT类型，例如M 1-OT6。对于RLWE，我们使用SEAL库[65]并借助[37]加速在Intel CPU上的计算。对于近似小于运算，我们在50位输入上评估了MSB协议，以进行(2)中的区段选择。对于近似指数运算，我们将Texp设置为-14，并使用n = 6。有关实现参数的更多细节，请参见附录。\n可用性 我们提供了可复现的实现，网址为：https://github.com/AntCPLab/OpenBumbleBee。\n","date":"2024-12-03T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/bumblebee-%E5%A4%A7%E5%9E%8B%E5%8F%98%E6%8D%A2%E5%99%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%89%E5%85%A8%E5%8F%8C%E6%96%B9%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/cottage-2955582_1280_hu10908540004532869761.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/bumblebee-%E5%A4%A7%E5%9E%8B%E5%8F%98%E6%8D%A2%E5%99%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%89%E5%85%A8%E5%8F%8C%E6%96%B9%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/","title":"BumbleBee 大型变换器模型的安全双方推理框架"},{"content":"Hadamard product是什么？ Hadamard product（哈达玛积），也称为元素乘积，是指两个相同维度的矩阵或向量之间的逐元素相乘。与矩阵乘法不同，Hadamard积不涉及矩阵的行列运算，而是直接对对应位置的元素进行相乘。\n$$\rC = A \\circ B = \\begin{bmatrix} a_{11} \\cdot b_{11} \u0026 a_{12} \\cdot b_{12} \u0026 \\dots \u0026 a_{1n} \\cdot b_{1n} \\\\ a_{21} \\cdot b_{21} \u0026 a_{22} \\cdot b_{22} \u0026 \\dots \u0026 a_{2n} \\cdot b_{2n} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ a_{m1} \\cdot b_{m1} \u0026 a_{m2} \\cdot b_{m2} \u0026 \\dots \u0026 a_{mn} \\cdot b_{mn} \\end{bmatrix}\r$$ Hadamard积常用于各种机器学习和深度学习的操作，如神经网络中的门控机制（例如LSTM和GRU），以及图像处理中的卷积操作等。\n注意，Hadamard积仅适用于两个维度相同的矩阵或向量。\n什么是秘密共享? 秘密共享（Secret Sharing）是一种密码学技术，用于将一个秘密（如密钥、密码或敏感数据）分割成多个部分，并将这些部分分发给不同的参与者。只有在一定数量的参与者合作时，才能恢复原始的秘密。通过这种方式，秘密共享可以增强安全性，因为即使某些参与者的部分被泄露或丢失，其他部分仍然无法恢复完整的秘密。\n主要类型 阈值秘密共享（Threshold Secret Sharing） 这种方法要求至少$t$ 个参与者合作才能恢复秘密。每个参与者得到一个秘密份额，只有在$t$ 个或更多参与者联合时，才能重构出原始秘密。\nShamir’s Secret Sharing\n：这是最著名的阈值秘密共享方案，由Adi Shamir在1979年提出。其核心思想是通过多项式插值来实现秘密的分割和恢复。\n假设原始秘密是一个 ss，可以选择一个 $(t−1)$ 次多项式 $f(x)$，其中常数项 $f(0) = s$ 即为秘密。 然后根据多项式生成 $n$ 个不同的点分发给 $n$ 个参与者。 任何 $t$ 个参与者可以使用拉格朗日插值法恢复原始秘密。 对称秘密共享 在这种方法中，每个参与者获得一个共享密钥，这些密钥通过某种方式结合来恢复原始秘密。常见的算法包括加密技术、异或操作等。\n加密的秘密共享 这种方法利用加密技术对秘密进行分割，确保即使单个参与者泄露了他们的份额，其他人也无法访问秘密。例如，通过同态加密（homomorphic encryption）或基于门限加密的秘密共享方案。\n应用场景 密钥管理：通过将密钥分散到多个地点或参与者之间，提高密钥管理的安全性。例如，如果加密系统的密钥被分为多个部分，只有通过参与者之间的协作才能解密数据。 防止单点故障：秘密共享技术可以防止某个单一参与者或设备故障导致秘密丢失。通过适当选择阈值，可以确保即使部分参与者不可用，剩余的参与者仍能恢复秘密。 分布式计算：在分布式环境中，多个节点可以合作进行秘密的计算或解密，而不需要暴露各自的数据。例如，在加密计算或保密计算（Secure Multi-Party Computation, MPC）中，多个计算节点共同参与计算秘密，但每个节点仅持有部分数据，保护了每个节点的隐私。 区块链与去中心化系统：在去中心化系统中，秘密共享可用于增强安全性和隐私保护。例如，在某些区块链协议中，多个节点可以共同持有私钥的份额，用于签名交易或执行智能合约，只有满足一定数量的节点签名才能完成操作。 优点和缺点 优点：\n增强安全性：通过分割秘密，即使某些份额泄露，其他人也无法恢复原始秘密。 容错性：可以容忍一定数量的参与者失效或泄露信息，只要满足阈值条件，秘密仍能恢复。 灵活性：可以根据需求调整参与者数量和阈值。 缺点：\n参与者管理复杂：需要管理和协调多个参与者。 计算成本：某些秘密共享方案，如Shamir的方案，可能需要额外的计算来分割和恢复秘密。 通信开销：分发和恢复秘密可能需要较大的通信开销，尤其是在参与者数量多时。 总的来说，秘密共享技术是提升系统安全性和隐私保护的重要工具，广泛应用于多种安全和加密场景中。\n任何 ttt 个参与者可以使用拉格朗日插值法恢复原始秘密,这是怎么做到的? 要理解如何通过拉格朗日插值法来恢复原始秘密，首先我们需要了解Shamir的秘密共享的工作原理。简单来说，Shamir的秘密共享方案通过将秘密转换为一个多项式的常数项，并通过给定的点分发给参与者。在恢复秘密时，tt 个参与者可以使用这些点和拉格朗日插值法来重建原始多项式，从而恢复秘密。\nShamir的秘密共享原理 构造多项式：首先，选择一个秘密 ss 作为常数项，然后构造一个 $(t-1)$ 次的多项式 $f(x)$，使得：\n$f(x) = a_0 + a_1 x + a_2 x^2 + \\dots + a_{t-1} x^{t-1}$\n其中，$f(0) = a_0 = s$ 为秘密。其余的系数 $a_1, a_2, \\dots, a_{t-1}$ 是随机选择的。\n生成份额：然后，选择 $n$ 个不同的 xx 值（例如$x_1, x_2, \\dots, x_n$），并计算对应的 $f(x_i)$ 值。每个参与者将收到一对$(x_i, f(x_i))$，即他们的“份额”。这些份额分别分配给 $n$ 个参与者。\n重建秘密：任何 $t$ 个份额就足以恢复原始秘密。通过拉格朗日插值法，可以使用这 $t$ 个点（$x_i, f(x_i)$）来求解多项式的常数项 $f(0)$，从而恢复秘密。\n拉格朗日插值法 拉格朗日插值法是一个用于构造多项式的数学方法，给定一组数据点$(x_1, y_1), (x_2, y_2), \\dots, (x_t, y_t)$，我们可以构造一个唯一的多项式，使得它通过所有这些点。\n在Shamir的秘密共享中，我们有$t $个点 ($(x_i, f(x_i)$)，要恢复秘密 $f(0)$ 即常数项，我们可以通过以下拉格朗日插值公式来计算：\n$f(0) = \\sum_{i=1}^{t} f(x_i) \\cdot \\ell_i(0)$\n其中，$\\ell_i(x)$ 是第 $i$ 个拉格朗日基多项式，定义如下：\nℓ$\\ell_i(x) = \\prod_{\\substack{1 \\leq j \\leq t \\ j \\neq i}} \\frac{x - x_j}{x_i - x_j}$\n为了恢复 $f(0)$，我们将 $x = 0$ 代入上式：\n$\\ell_i(0) = \\prod_{\\substack{1 \\leq j \\leq t \\ j \\neq i}} \\frac{0 - x_j}{x_i - x_j} = \\prod_{\\substack{1 \\leq j \\leq t \\ j \\neq i}} \\frac{-x_j}{x_i - x_j}$\n然后，通过上面得到的每个 $\\ell_i(0)$ 值，乘上对应的 $f(x_i)$，并将所有结果求和，就能得到 $f(0)$，即原始的秘密。\n例子 假设有三个参与者（$t = 3$），他们分别获得了以下份额：\n$(x_1 = 1, f(x_1) = 5)$ $(x_2 = 2, f(x_2) = 8)$ $(x_3 = 3, f(x_3) = 11)$ 我们要恢复秘密 $f(0)$，即常数项。\n计算每个 $\\ell_i(0)$：\n$\\ell_1(0) = \\frac{(0 - 2)(0 - 3)}{(1 - 2)(1 - 3)} = \\frac{6}{2} = 3$\n$\\ell_2(0) = \\frac{(0 - 1)(0 - 3)}{(2 - 1)(2 - 3)} = \\frac{3}{-1} = -3$\n$\\ell_3(0) = \\frac{(0 - 1)(0 - 2)}{(3 - 1)(3 - 2)} = \\frac{2}{2} = 1$\n计算秘密：\n$f(0) = f(x_1) \\cdot \\ell_1(0) + f(x_2) \\cdot \\ell_2(0) + f(x_3) \\cdot \\ell_3(0)$\n$f(0) = 5 \\cdot 3 + 8 \\cdot (-3) + 11 \\cdot 1 = 15 - 24 + 11 = 2$\n因此，原始的秘密 $f(0) = 2$。\n总结 通过拉格朗日插值法，$t$ 个参与者的份额可以恢复原始的秘密。该过程利用了每个参与者的份额和拉格朗日基多项式的计算，以重建出秘密所对应的多项式，并提取出常数项。\n2-out-of-2 加法秘密共享方案是什么？ 2-out-of-2加法秘密共享方案是一种非常基础的秘密共享方案，专门用于将一个秘密分配给两个参与者，并且要求两个参与者必须共同合作才能恢复秘密。在这种方案中，秘密通过加法分割为两个部分，每个参与者都持有一个秘密份额，只有两个参与者联合时才能恢复原始秘密。\n工作原理 在2-out-of-2加法秘密共享方案中，假设原始秘密为 $s$，我们将这个秘密分割成两个部分，分别给两个参与者。我们使用以下方法进行分割：\n假设原始秘密是 $s$，我们随机选择一个数 rr 作为第一个份额，计算第二个份额 $s\u0026rsquo; = s - r$。 然后，将这两个份额分别分配给两个参与者： 参与者1收到份额 $r$ 参与者2收到份额 $s\u0026rsquo; = s - r$ 恢复秘密 为了恢复秘密 $s$，两个参与者需要将他们的份额相加：\n$s = r + (s - r)$\n通过这种方式，两个参与者的份额加在一起，恢复出原始的秘密 $s$。\n特点 加法操作：在这个方案中，秘密的分割和恢复都基于加法。每个参与者只知道一个加法部分，只有通过相加才能恢复出原始秘密。 完全安全：如果只有一个参与者知道自己的份额，那么他无法单独恢复秘密，因为他只有一个部分，缺少了另一个部分的信息。 简单实现：这是一个非常简单和高效的秘密共享方案，适用于只有两个参与者的场景。 例子 假设有一个秘密 $s = 100$，我们将其分配给两个参与者。\n随机选择一个份额 $r = 30$，那么第二个份额 $s\u0026rsquo; = 100 - 30 = 70$。\n参与者1得到份额$r = 30$，参与者2得到份额 $s\u0026rsquo; = 70$。\n为了恢复秘密，参与者1和参与者2将他们的份额相加：\n$s = 30 + 70 = 100$\n应用 2-out-of-2加法秘密共享可以应用于很多场景，尤其是需要确保两个参与者必须合作才能访问某个秘密的情况。例如：\n安全存储密钥：将加密密钥分成两个部分，分别存储在不同的地点，只有两个存储位置都可访问时，密钥才可以恢复。 两方合谋：在某些加密协议中，两个参与者需要共同合作解密数据，单独任何一个参与者都无法获得数据的任何信息。 总结 2-out-of-2加法秘密共享方案通过将秘密分割成两个部分，确保只有在两个参与者合作的情况下才能恢复原始秘密。这种方案简单易实现，但仅适用于参与者数量为2的场景。如果需要更高的容错能力（比如 t-out-of-n 的方案），则需要使用更复杂的算法，如Shamir的秘密共享。\noblivious transfer是什么？ **Oblivious Transfer（OT，盲传输）**是一种重要的加密协议，广泛应用于多方计算（Secure Multi-Party Computation, MPC）和隐私保护计算领域。其基本思想是在两方或多方之间进行信息传输时，保证发送方和接收方的隐私。特别地，Oblivious Transfer 保护了发送方的信息不被泄露给接收方，而接收方也不会知道其他的消息，只有选择的信息才能被接收。\nOblivious Transfer的基本概念 在OT协议中，存在两个主要参与方：\n发送方（Sender）：拥有若干个消息，并希望将其中一个消息传递给接收方，但不希望接收方得知其他消息。 接收方（Receiver）：接收方可以选择想要接收的消息，并且只会获得它所选择的消息，但发送方无法知道接收方选择了哪一个。 有几种常见的Oblivious Transfer类型，其中最常见的有以下两种：\n1. 1-out-of-2 Oblivious Transfer (1-2 OT) 在1-out-of-2 OT协议中，发送方有两个消息 $m_0$ 和 $m_1$，接收方选择一个索引（例如 0 或 1），然后接收方获得选择的消息，而发送方并不知道接收方选择了哪个消息。\n工作流程： 发送方将两个消息 $m_0 $和 $m_1 $发送给接收方，但不直接告诉接收方哪个消息是哪个。 接收方选择一个索引（0 或 1），并且只获得对应索引的消息。接收方不会知道其他的消息内容。 发送方无法知道接收方选择了哪一个消息，接收方只能获得所选的消息。 安全性：\n发送方隐私性：发送方无法知道接收方选择了哪个消息。 接收方隐私性：接收方无法知道未选择的消息，也无法从其它信息中推测出未选的消息。 2. k-out-of-n Oblivious Transfer (k-n OT) 在k-out-of-n OT协议中，发送方有 n 个消息$m_1, m_2, \u0026hellip;, m_n$，接收方可以选择 k 个消息，并且获得这 k 个消息，但不透露选择的其他 n-k 个消息。接收方只会知道自己选择的 k 个消息，而发送方不会知道接收方选择了哪些消息。\nOblivious Transfer的应用 安全多方计算（MPC）：在多方计算中，OT协议用于保护各方的隐私，使得各方可以在不知道对方输入的情况下共同计算某个函数的结果。例如，在两个公司进行联合数据分析时，可以使用OT协议来确保各自的数据不被泄露。 私密信息检索（Private Information Retrieval, PIR）：PIR是指用户在不透露查询内容的情况下，从数据库中检索信息。OT协议可以用于构建PIR方案，确保查询者的隐私得到保护。 数字货币和区块链：OT协议也可以用于增强区块链系统中的隐私保护，尤其是在匿名交易中。它可以让交易双方在不暴露其具体信息的情况下，完成加密验证和交易。 加密货币钱包：例如，在一些加密货币钱包中，OT协议可用于确保钱包私钥的安全转移，同时防止第三方在转移过程中获得不必要的敏感信息。 OT协议的实际构建 OT协议可以通过不同的加密技术实现，常见的方式包括：\n公钥加密：例如使用 RSA 加密或椭圆曲线加密来实现 OT 协议。 同态加密：可以通过同态加密协议来构建OT协议，以便在加密状态下进行数据传输。 零知识证明：通过零知识证明机制，保证消息传输过程中各方的信息不被泄露。 总结 **Oblivious Transfer（盲传输）**是一种重要的加密协议，它保证了信息在传输过程中各方的隐私性。特别地，发送方和接收方都不会得知对方的敏感信息，只有接收方能够获得自己所选择的消息。这一协议被广泛应用于多方计算、私密信息检索、数字货币隐私保护等领域，是现代密码学和隐私保护技术的核心组件之一。\n秘密共享中使用的 $2^l$ 模数是什么？ 在秘密共享（Secret Sharing）方案中，特别是在 Shamir 的秘密共享协议中，$2^l$ 模数通常指的是一个大素数或一个足够大的整数，用于确保秘密能够在模运算中适当地加密和解密。在这种情况下，$2^l$ 表示对一个长度为 $l$ 的二进制数据进行处理时所需要的模数。\n具体来说，$l$ 是密钥或秘密的比特长度，因此 $2^l$ 是一个足够大的数，通常用于分割秘密的范围。例如，在 Shamir 的方案中，秘密通常被视为一个 $l$ 比特的数值，而将其分割成多个份额时，使用 $2^l$ 来确保数值保持在所需范围内。这也意味着，每个份额的大小由 $2^l$ 限定，从而保证了秘密的安全性和正确的恢复。\n简而言之，$2^l$ 模数的使用帮助在加密和共享过程中处理大的整数，确保秘密在分发和恢复时不被泄露，同时也方便进行数学运算。\n","date":"2024-12-02T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E5%AF%86%E7%A0%81%E5%AD%A6%E9%83%A8%E5%88%86%E9%97%AE%E9%A2%98/wave-7726187_1280_hu1193120050624582333.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E5%AF%86%E7%A0%81%E5%AD%A6%E9%83%A8%E5%88%86%E9%97%AE%E9%A2%98/","title":"密码学部分问题"},{"content":"基于秘密共享的隐私保护和可验证深度学习推理 摘要 深度学习推理作为实现深度学习模型利用的关键环节，通常被部署为面向资源受限客户端的云端框架。然而，现有的云端框架存在严重的信息泄漏问题，或导致通信成本显著增加。在本研究中，我们提出了一种隐私保护的深度学习推理方法，能够在低通信和计算成本的前提下，同时保护输入数据和模型参数的隐私。此外，用户可以以较小的开销验证结果的正确性，这对关键应用尤为重要。具体而言，通过设计安全的子协议，我们引入了一个新层，用于协作执行推理过程中涉及的安全计算。在秘密共享的协作下，我们将可验证数据注入输入中，使得能够检查返回的推理结果的正确性。我们通过理论分析以及基于 MNIST 和 CIFAR10 数据集的大量实验结果，验证了所提出的隐私保护和可验证深度学习推理（PVDLI）框架的优越性。\n引言 近年来，深度神经网络（DNN）在多个领域取得了显著进展，例如目标检测【39,26】、目标分类【18,10】、机器翻译【6,7】和人脸识别【40,33】。随着数据集规模的不断扩大，深度学习的计算强度也随之成比例增长。尽管近期在GPU硬件、网络架构和算法方面取得了重要突破，对于资源受限的客户端而言，大规模DNN的训练和推理仍需耗费无法接受的长时间和巨大的计算资源。\n幸运的是，许多DNN框架建议利用云服务来完成大量计算。然而，这些基于云的框架也带来了新的安全和隐私挑战。隐私泄露可能发生在训练阶段和推理阶段。针对训练阶段的隐私问题，已有许多隐私保护框架被提出【27,8,28,37,23,4,13,19】；而针对推理阶段的隐私问题，也有相关研究【46,12,16,32,42】。本文主要聚焦于与本研究最相关的推理阶段的隐私问题。\n在深度学习的推理阶段，隐私泄露主要来源于输入数据和深度模型本身。具体而言，输入数据通常包含大量敏感信息，这些信息应对云服务器保持隐私，因为云服务器可能无法完全信任。此外，经过充分训练的模型参数也不应暴露给未经授权的方。由于经过充分训练的模型因其高昂的训练成本而被视为重要资产，因此保护模型参数隐私与保护输入数据隐私同等重要。\n为了解决这些问题，Ocia等【24,25】和Chi等【5】提出将神经网络拆分为两部分：一部分在本地执行，另一部分在云端运行。Xu等【43】和Shen等【36】建议使用混淆神经网络【43】或数据变形【36】来在本地保护输入数据隐私。此外，为了提供更高的安全性，Zhang等【46】和Tian等【41】引入了同态加密（HE）【2】，以加密推理中部分计算。他们将所有DNN计算分为两类：线性计算和非线性计算。在线性计算在加密域中由云服务器完成。\n尽管上述框架保护了输入隐私，但并未考虑模型参数的隐私。近年来，一些旨在同时保护输入和模型参数隐私的框架得到了研究。相关工作【12,9,15】利用同态加密（HE）对整个网络进行加密，并在加密域中执行推理。Juvekar等【16】提出了一种框架，结合了混淆电路（GC）【44】和HE，用于同时保护输入和模型的隐私，与框架【32,29,31,30】类似。此外，许多工作借助安全多方计算（SMPC）来实现输入和模型参数的保护。Ma等【22】提出了在两个非共谋服务器下的完全非交互隐私保护推理框架。Liu等【21】提出了MiniONN，用于将训练好的神经网络转换为高效的无泄漏神经网络。Shamsabadi等【35】研究了一种用于分类的私有DNN训练和推理框架，其中基于秘密共享方案【34】的私有推理在两个非共谋服务器之间进行。\n此外，云服务器可能会出于经济利益返回无效结果。在许多基于深度学习的应用中，例如自动驾驶和金融风险评估，无效结果可能导致灾难性后果。因此，为用户提供一种机制验证云服务器返回结果的正确性显得尤为重要。同时，执行验证所引入的开销应尽可能低。\n为此，我们提出了一种隐私保护且可验证的深度学习推理（PVDLI）框架，能够在低通信和计算成本下同时保护输入和模型参数的隐私。此外，用户还可以以较小的开销验证推理结果的正确性。受软件设计模式中的代理模式【11】启发，我们引入了N+1个节点，称为代理层，用于协作执行推理中涉及的安全计算。为安全地执行激活函数（非线性操作），我们在微调阶段用多项式近似激活函数【3】。通过在输入中注入标记的可验证数据，我们可以以高概率有效验证推理结果的正确性。借助秘密共享，注入的验证数据与正常数据无法区分，从而防止仅针对验证数据返回正确结果的潜在攻击。\n理论分析表明，即使在参与者之间存在共谋的情况下，所提出的框架仍能够针对“诚实但好奇”的参与者提供输入和模型参数的可验证性和隐私性。此外，我们还提供了大量实验结果以验证所提出的PVDLI框架的优越性。\n本文其余部分组织如下：第2节介绍相关基础知识。第3节给出了系统模型和设计目标。第4节阐述了所提出的子协议。第5节详细描述了PVDLI框架。第6节提供了基于设计的安全实验的可验证性分析和安全性分析。第7节展示了实验结果，最后在第8节进行总结。\n预备知识 深度学习旨在从高维数据中提取复杂特征，并利用这些特征构建一个能够将输入与输出关联起来的模型。通常，深度学习架构由多层网络构成，以便将低层次特征通过非线性函数计算为更抽象的特征。每一层的输出与前一层相连接。\n在图1中，我们展示了一个深度神经网络（DNN）推理的示例。通过执行一系列计算，DNN输出对数值（logit），随后通过对该对数值执行softmax函数生成输入的类别预测概率。\n在不失一般性的情况下，我们详细描述了VGG16模型【38】中一个典型块的组成。该块包含线性计算、批归一化、激活函数和池化操作。\n线性计算 $$x \\in \\mathbb{R}^n$$$$\rz = W x + b \\tag{1}\r$$$$W$$$$b$$ 分别表示权重和偏置。由于卷积操作可以高效地转化为矩阵乘法，因此我们在线性计算中仅考虑公式 (1)。\n批归一化 $$\ru = c z + b \\tag{2}\r$$$$c$$$$b$$ 是经过充分训练的固定参数。\n激活函数 $$\rt = \\text{ReLU}(u) \\tag{3}\r$$池化 $$\\text{Avg}$$$$\ry = \\text{Avg}(t) \\tag{4}\r$$随后，池化层的输出被送入下一层进行后续计算。最终，DNN 网络输出一个对数值（logit），并通过 softmax 函数生成类别预测概率。\n系统模型与设计目标 在本节中，我们详细介绍所提出框架的设计目标和系统模型。\n设计目标 我们提出了一个隐私保护且可验证的深度学习推理框架，涉及三个实体：模型所有者（O）、用户（U） 和 N+1 个节点（称为代理层），代理层位于用户与模型所有者之间。一般来说，所提出的 PVDLI 框架面临的安全威胁主要来自代理层的行为。假设框架在**“诚实但好奇”**的设置下运行，这意味着攻击者会遵循框架规范，但可能对输入数据和模型参数产生兴趣。此外，从可验证性的角度来看，代理层中的某些节点可能会偷懒，即生成随机或错误结果以节省计算资源。\n为了实现框架的隐私保护和可验证性，我们确定了以下四个设计目标：\n隐私\n用户的输入数据必须对代理层保密。 模型的参数不应被代理层泄露。 效率\n用户的计算复杂度应显著低于直接进行完整推理计算的复杂度。 各参与方之间的通信成本应尽可能降低。 可验证性\n用户必须能够成功验证推理结果的正确性。作弊的代理层生成的错误结果不能以非可忽略的概率通过验证。 准确性\n模型推理的准确性应接近于在单一服务器上执行深度学习推理时的准确性。 系统模型 $$E = \\{E_0, E_1, \\dots, E_N\\}$$ 作为代理层，其灵感来源于代理模式【11】。在软件设计模式中，代理【11】是一个连接客户端和实际服务对象的中介对象。类似地，所提出的代理层提供逻辑操作的访问，同时隐藏输入数据和模型参数的信息。\n框架的伪代码见算法1。具体而言：\n$$F$$$$H = \\{h_1, h_2, \\dots, h_M\\}$$$$M$$ 是网络中的总参数数量。\n$$x$$$$F$$ 中进行推理。\n代理层（E）\n辅助节点（$$E_0$$）\n负责与各计算节点之间的通信，收集和分发推理过程中涉及的临时结果。 计算节点（$$\\{E_1, \\dots, E_N\\}$$）\n负责执行深度神经网络（DNN）的推理计算。 在推理开始之前，模型所有者（O）会与用户（U）协商选择一个由 1 个辅助节点（$$E_0$$） 和 N 个计算节点（$$\\{E_1, \\dots, E_N\\}$$） 组成的代理层（E）。\n算法 1: 执行 PVDLI 的步骤 输入：\n安全参数 $$k$$ 输入数据 $$x \\in \\mathbb{Z}_p^n$$ 深度学习模型 $$F$$ 计算节点数量 $$N$$ 输出：\n推理结果 $$y$$ 或错误标志 $$\\perp$$ 模型所有者 $$O$$ 和 用户 $$U$$ 选择一个由 $$N+1$$ 个节点组成的代理层 $$E = \\{E_0, E_1, \\dots, E_N\\}$$。 用户 $$U$$ 生成一个可验证数据集 $$D$$。 模型所有者 $$O$$ 通过多项式近似获取近似模型 $$F$$，然后将模型参数 $$H$$ 分成 $$N$$ 份：\n$$H^{(1)}, H^{(2)}, \\dots, H^{(N)} \\gets \\text{Split}(H, k, N)$$。 用户 $$U$$ 将可验证数据 $$\\{X, V, r_v\\}$$ 插入到输入数据中：\n$$\\{X, V, r_v\\} \\gets \\text{Insert}(x, D)$$。 用户将混合数据 $$X$$ 分成 $$N$$ 份：\n$$X^{(1)}, X^{(2)}, \\dots, X^{(N)} \\gets \\text{Split}(X, k, N)$$。 代理层执行一系列深度学习推理计算，得到 logits：\n$$L^{(1)}, L^{(2)}, \\dots, L^{(N)} \\gets \\{f(X^{(1)}, H^{(1)}), \\dots, f(X^{(N)}, H^{(N)})\\}$$。 用户 $$U$$ 聚合 logits：\n$$R \\gets \\text{Aggregation}(L^{(1)}, L^{(2)}, \\dots, L^{(N)})$$。 用户 $$U$$ 提取结果：\n$$y = R_{r_v}$$ （推理结果），\n$$R' = \\{R_i \\mid i \\in \\{1, 2, \\dots, N\\} \\setminus r_v\\}$$ （可验证结果）。 如果可验证结果 $$R'$$ 满足 $$R' = V$$： 返回 $$y$$ 作为输入数据 $$x$$ 的推理结果。 否则： 返回错误标志 $$\\perp$$。 结束。 $$O$$$$F$$$$H$$$$N$$ 份，并将这些份额分发到代理层。这些模型参数的份额是通过一种简单而有效的秘密共享方案【34】生成的。\n$$U$$$$x$$$$X$$$$U$$$$X$$$$N$$ 份，并分发到代理层以保护输入隐私。\n$$H^{(i)}$$$$X^{(i)}$$$$E_i$$$$L^{(i)}$$$$U$$$$U$$$$R$$$$U$$$$U$$$$x$$$$y$$；否则，拒绝结果并输出错误。\n提出的 PVDLI 框架包含以下六个关键模块： $$O$$$$U$$$$N+1$$$$E = \\{E_0, E_1, \\dots, E_N\\}$$$$U$$$$D$$，用于后续验证。\n$$O$$$$F$$$$k$$$$N$$ 份，并将它们分发给代理层中的计算节点。关于近似和份额分发的详细内容将在后续部分提供。\n$$k$$$$U$$$$D$$$$V$$$$r_v$$$$x$$$$X$$$$X$$$$N$$$$X^{(i)}$$$$E_i$$。\n$$X^{(i)}$$$$H^{(i)}$$$$E_i$$$$1 \\leq i \\leq N$$$$E_i$$$$L^{(i)}$$$$U$$。\n$$L^{(1)}, \\dots, L^{(N)}$$$$U$$$$X$$$$F$$$$R$$。\n$$R$$$$r_v$$$$U$$$$V$$$$x$$$$y$$$$\\perp$$。\n基于秘密共享的安全子协议 本质上，深度学习推理由一系列数学操作完成。在我们提出的框架中，为了避免输入数据和模型参数的泄露，所有操作必须以隐私保护的方式进行。基于秘密共享方案，我们设计并实现了三个安全计算子协议：安全加法协议、安全除法协议和安全矩阵乘法协议。\n安全加法协议（SecAdd）： $$A \\in \\mathbb{Z}_p^{u \\times n}$$$$B \\in \\mathbb{Z}_p^{v \\times n}$$$$A^{(1)}, B^{(1)}, A^{(2)}, B^{(2)}, \\dots, A^{(N)}, B^{(N)}$$$$C^{(1)}, C^{(2)}, \\dots, C^{(N)}$$$$C^{(1)} + C^{(2)} + \\dots + C^{(N)} = A + B$$。在执行过程中，不会交换任何中间值。\n$$E_i$$$$A$$$$B$$$$A^{(i)}, B^{(i)}$$$$1 \\leq i \\leq N$$$$E_i$$$$C^{(i)}$$$$E_0$$ 不参与计算。\n安全除法协议（SecDiv）： $$k \\in \\mathbb{Z}_p$$$$A \\in \\mathbb{Z}_p^{u \\times n}$$$$A^{(1)}, A^{(2)}, \\dots, A^{(N)}$$$$C^{(1)}, C^{(2)}, \\dots, C^{(N)}$$$$C^{(1)} + C^{(2)} + \\dots + C^{(N)} = A / k$$。\n$$E_i$$$$A$$$$A^{(i)}$$$$1 \\leq i \\leq N$$$$E_i$$$$k$$$$C^{(i)}$$。\n安全矩阵乘法协议（SecMul）： $$A \\in \\mathbb{Z}_p^{u \\times n}$$$$B \\in \\mathbb{Z}_p^{v \\times n}$$$$A^{(1)}, B^{(1)}, A^{(2)}, B^{(2)}, \\dots, A^{(N)}, B^{(N)}$$$$C^{(1)}, C^{(2)}, \\dots, C^{(N)}$$$$C^{(1)} + C^{(2)} + \\dots + C^{(N)} = A \\cdot B$$。\n$$E_0$$。辅助节点仅与计算节点通信，用于收集和分发乘法过程中的临时结果。\n$$E_i$$$$A^{(i)}, B^{(i)}$$$$1 \\leq i \\leq N$$$$A$$$$B$$$$P \\in \\mathbb{Z}_p^{u \\times n}, Q \\in \\mathbb{Z}_p^{n \\times v}, O \\in \\mathbb{Z}_p^{u \\times v}$$$$O = P \\cdot Q$$。\n$$E_i$$$$P, Q, O$$$$P^{(i)}, Q^{(i)}, O^{(i)}$$$$1 \\leq i \\leq N$$$$P, Q, O$$$$E_0$$）都是保密的。随机矩阵的份额可以通过生成份额的算法【8】轻松生成。这些辅助随机矩阵可以在离线阶段高效地准备，这意味着通信和计算成本不会增加。\n安全矩阵乘法过程 如图 3 所示，该协议包含以下四个部分：\n生成辅助随机矩阵份额 (GenAux): $$\\{P^{(i)}, Q^{(i)}, O^{(i)}\\}_{i=1}^{N}$$\n给定安全参数 $$k$$，代理层协商选择辅助随机矩阵的份额 $$\\{P^{(i)}, Q^{(i)}, O^{(i)}\\}_{i=1}^{N}$$，其中 $$P^{(i)}, Q^{(i)}, O^{(i)}$$ 表示矩阵 $$P \\in \\mathbb{Z}_p^{u \\times n}$$、$$Q \\in \\mathbb{Z}_p^{n \\times v}$$ 和 $$O \\in \\mathbb{Z}_p^{u \\times v}$$ 的第 $$i$$ 个份额，且 $$O = P \\cdot Q$$。代理层的计算节点 $$E_i$$ 只能拥有对应的份额 $$P^{(i)}, Q^{(i)}, O^{(i)}$$，其中 $$1 \\leq i \\leq N$$。 对输入矩阵份额进行掩码处理 (Mask): $$E_i$$$$1 \\leq i \\leq N$$$$\rE^{(i)} = A^{(i)} - P^{(i)} \\quad;\\quad F^{(i)} = B^{(i)} - Q^{(i)} $$$$E_i$$$$E^{(i)}, F^{(i)}$$$$E_0$$$$1 \\leq i \\leq N$$。\n重构并分割结果 (Split): $$E_0$$$$N$$$$\\{E^{(i)}, F^{(i)}\\}_{i=1}^{N}$$$$E$$$$F$$$$\rE = \\sum_{i=1}^{N} E^{(i)} \\quad;\\quad F = \\sum_{i=1}^{N} F^{(i)} \\quad;\\quad G = E \\cdot F $$$$E_0$$$$G$$$$\\{G^{(i)}\\}_{i=1}^{N}$$$$E_0$$$$E, F, G^{(i)}$$$$E_i$$$$1 \\leq i \\leq N$$）。\n计算最终份额 (Cal): $$E_i$$$$1 \\leq i \\leq N$$$$E, F, G^{(i)}$$$$\rS^{(i)} = E \\cdot Q^{(i)} \\quad;\\quad T^{(i)} = P^{(i)} \\cdot F $$$$E_i$$$$\rC^{(i)} = G^{(i)} + S^{(i)} + T^{(i)} + O^{(i)} $$$$\rC = \\sum_{i=1}^{N} C^{(i)} = A \\cdot B $$ 安全矩阵乘法的正确性证明： $$\rC = \\sum_{i=1}^{N} C^{(i)} = \\sum_{i=1}^{N} (G^{(i)} + S^{(i)} + T^{(i)} + O^{(i)}) = G + S + T + O $$$$\r= (E \\cdot F) + (E \\cdot Q) + (P \\cdot F) + (P \\cdot Q) $$$$\r= (A - P) \\cdot (B - Q) + (A - P) \\cdot Q + P \\cdot (B - Q) + P \\cdot Q $$$$\r= A \\cdot B\r$$$$P, Q, O$$ 可以离线准备并重复使用，这降低了所有计算方之间的通信成本。\n提出的框架 Init（N） → （E; D) 首先，模型所有者O与用户U协商，选择一个包含N + 1个节点的代理层，其中包括一个辅助节点E₀和N个计算节点E₁, \u0026hellip;, EN进行后续的协同计算。在这里，合理的假设是N ≥ 2，这是由于所采用的秘密共享方案提出的要求。需要注意的是，节点数量不会影响推理的准确性，因为所有节点只涉及安全子协议，这些协议已经被证明与原始计算等效。然而，节点数量确实会影响隐私保护的水平。具体来说，节点数量越多，当节点发生串通时，隐私保护的级别越高。更多的安全性分析将在第6节中提供。 接下来，为了提供可验证性，用户U生成一个可验证的数据集D = {d₁, \u0026hellip;, dL}。这里，可验证数据集可以离线生成并且可重复使用。每个数据项dᵢ的要求是，它的推理结果（标签）可以很容易地估计或获得。由于通过秘密共享方案可以很好地保护输入数据的隐私，从代理层的角度来看，所有输入数据都是随机值。因此，代理层无法区分可验证数据dᵢ和输入x。选择可验证数据dᵢ时有很多选择；它可以是标记的测试数据，甚至是可以轻松验证的虚拟数据（噪声或无意义的数据）。\nDeployModelðF; k; NÞ → H(1), \u0026hellip;, H(M) $$f(x) = a_0 + a_1x + ... + a_d x^d$$\n其中，所有的系数$a_0, a_1, \u0026hellip;, a_d$在多项式中都是可训练的。替代后，模型所有者O继续通过少量迭代微调模型F，以获得新的近似模型F。令$H = {h_1, \u0026hellip;, h_M}$表示深度学习模型F中所有权重变量的平坦化向量，其中M表示模型中的参数数量。为了保护模型参数$H$的隐私，我们提出使用一个简单而有效的N-out-of-N加法秘密共享方案[34]。具体来说，模型所有者O将模型参数$H$分割成N个份额，并将份额$H(i)$分配给代理层中的计算节点$E_i$，其中$1 \\leq i \\leq N$。通过这种方式，代理层中的每个计算节点只拥有模型参数$H$的一个份额。具体地，生成参数份额的过程包括以下两个部分：\nSetup 1k → Ko: 给定安全参数k，模型所有者O从指定的密钥空间${0, 1}^k$生成一个秘密密钥Ko。加密算法的资源需求和攻击者破解安全的概率可以通过安全参数k来表达。\n$$Alg(H; ro; N) → H(1), ..., H(N) \\in \\mathbb{Z}_M^N$$\n最后，模型所有者O将份额$H(i)$分别分配给代理层中的计算节点$E_i$，其中$1 \\leq i \\leq N$。\nGenSplitDataðx; D; k; NÞ → X(1), \u0026hellip;, X(N); V; rv 在给定安全参数k和可验证数据集D的情况下，用户U将可验证数据和输入数据混合生成混合数据X。然后，用户U将混合数据X分割成N个份额，并将它们分发到代理层。具体来说，为了提供可验证性，用户U从数据集D中随机选择J个可验证数据$I_1, \u0026hellip;, I_J \\in \\mathbb{R}^{n \\times J}$。这里，令$V = {V_1, \u0026hellip;, V_J}$表示这J个可验证数据的标签。接着，用户U随机选择一个索引$rv$，其中$1 \\leq rv \\leq J$，并将输入数据$x$与可验证数据$I$混合，构造混合数据$X = {I_1, \u0026hellip;, I_{rv-1}, x, I_{rv}, \u0026hellip;, I_J} \\in \\mathbb{R}^{n \\times (J+1)}$。\n类似地，从所有长度为k的比特串中随机选择一个秘密密钥$K_u$，然后用户U使用[8]中的份额生成算法将混合数据X分割成N个份额$X(1), \u0026hellip;, X(N)$。此外，用户U将份额$X(i)$分别分发给代理层中的计算节点$E_i$，其中$1 \\leq i \\leq N$。在这种情况下，代理层中的每个计算节点只访问混合数据X的一个份额，这意味着可以提供输入数据的隐私保护。由于混合数据被分割成多个份额，且可验证数据与输入数据可以完美混合，暗示着攻击者无法将它们分开。\nCompute H(1), \u0026hellip;, H(N); X(1), \u0026hellip;, X(N) → L(1), \u0026hellip;, L(N) 在本模块中，代理层使用输入和模型参数的份额安全地执行深度学习推理。基于秘密共享方案，我们提出了三种安全子协议：SecAdd、SecDiv和SecMul，其详细信息已在第4节中提供。如第2节所述，深度学习推理中的操作包括线性计算、批量归一化、激活和池化。所有这些计算都可以通过利用提出的子协议来安全执行。为了不失一般性，我们考虑VGG16中的一个典型模块[38]，该模块由线性计算、批量归一化、激活和平均池化组成，如图4所示。为了提供可验证性，我们将一批数据X（包括原始输入x和可验证数据I）输入网络进行推理。此模块的计算可以表示为：\n$$ Z = W \\cdot X + B $$$$ U = c \\cdot Z + b $$$$ T = a_1 \\cdot U + \\dots + a_d \\cdot U_d $$$$ Y = \\text{Avg}(T) $$其中，Avg表示平均操作，c和b是经过训练并固定的。随后，我们将详细介绍如何在代理层中安全地执行这些计算。\n线性计算\n回顾一下，每个计算节点$E_i$拥有模型参数的份额$H(i)$和输入数据的份额$X(i)$，它们包含输入数据$x$和可验证数据$I$的份额。首先，计算节点$E_i$提取并重新格式化$H(i)$，以获得权重矩阵$W(i) \\in \\mathbb{Z}_u^{n \\times p}$和偏置$b(i) \\in \\mathbb{Z}_u^p$。这里，由于输入数据$X(i) \\in \\mathbb{Z}_n^{(J+1) \\times p}$（而不是原始输入$x \\in \\mathbb{Z}_n^p$）的变化，偏置$b(i) \\in \\mathbb{Z}_u^p$需要扩展为矩阵$B(i) \\in \\mathbb{Z}_u^{(J+1) \\times p}$以进行批量推理，其中$B(i) = \\left[ b(i) , \\cdots , b(i) \\right]$。然后，为了执行线性计算$Z = W \\cdot X + B$，代理层仅需执行安全矩阵乘法和安全加法，表示为：\n$$ \\text{SecMul}(W(i), X(i)) \\rightarrow C(1), \\dots, C(N) $$$$ \\text{SecAdd}(C(i), B(i)) \\rightarrow Z(1), \\dots, Z(N) $$输出$Z(1), \\dots, Z(N)$是$Z = W \\cdot X + B$的份额。\n批量归一化\n在进行线性计算后，批量归一化被应用于解决内部协变量偏移。由于批量归一化中的参数在推理阶段是固定的，因此批量归一化操作可以作为线性计算（如式（14）所述）进行。类似地，代理层中的每个计算节点$E_i$可以从$H(i)$中提取并重新格式化，以获得份额$c(i)$和$b(i)$，其中$1 \\leq i \\leq N$。然后，代理层执行安全矩阵乘法和安全加法来完成批量归一化，表示为：\n$$ \\text{SecMul}(c(i), Z(i)) \\rightarrow C(1), \\dots, C(N) $$$$ \\text{SecAdd}(C(i), b(i)) \\rightarrow U(1), \\dots, U(N) $$激活\n如第5.2节所述，我们已经用近似多项式替代了激活函数[3]。通过这种方式，非线性操作被转化为线性计算。由于它与上述线性计算类似，我们只提供主要过程。具体来说，在式（16）中，系数$a_1, a_2, \\dots, a_d$从$H(i)$中提取。逐元素的指数运算可以通过多个乘法来安全实现。因此，激活的计算可以通过安全地执行一系列安全的乘法和加法来通过多项式进行逼近。最终，每个计算节点$E_i$输出一个份额$T(i)$，其中$1 \\leq i \\leq N$。\n平均池化\n平均池化的主要过程是对矩形区域中的值进行加和并计算平均值。它由加法和除法与池大小组成。因此，我们也可以利用安全加法和安全除法来完成平均池化。最终，深度学习推理中的所有计算可以逐层执行。最后，在最后一层，每个计算节点$E_i$输出一个logit值份额，表示为$L(i)$，其中$1 \\leq i \\leq N$。\n聚合 当用户U接收到代理层的所有logit值份额$L(1), \u0026hellip;, L(N)$时，用户U将它们聚合以获得深度模型的logit值。\n$$ L = \\frac{1}{N} \\sum_{i=1}^{N} L(i) $$最终推理结果$R$取决于深度学习任务的类型。如果深度学习模型是预测任务，则$R = L$。如果是分类任务，则$R = \\text{softmax}(L)$。\n验证 在聚合所有份额以获得推理结果$R$后，用户$U$检查推理的正确性。具体来说，用户$U$在输入密钥$rv$后，提取第$rv$列向量$y = R_{rv}$。令$R$表示矩阵$R$的其余部分。然后，用户$U$将结果$R$与标签$V$进行比较。如果相同，用户$U$接受$y$作为输入$x$的推理结果。否则，用户$U$拒绝并输出错误$?$。需要注意的是，如果没有提供输入隐私，这意味着代理层可以轻松地区分可验证数据和输入数据$x$，则代理层可以输出可验证数据的正确结果和输入数据的错误结果，从而破坏可验证性。因此，验证的成功与输入隐私的保护密切相关。\n","date":"2024-11-18T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E5%9F%BA%E4%BA%8E%E7%A7%98%E5%AF%86%E5%85%B1%E4%BA%AB%E7%9A%84%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E5%92%8C%E5%8F%AF%E9%AA%8C%E8%AF%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E7%90%86/dolomites-2897602_1280_hu5388518749094321124.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E5%9F%BA%E4%BA%8E%E7%A7%98%E5%AF%86%E5%85%B1%E4%BA%AB%E7%9A%84%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E5%92%8C%E5%8F%AF%E9%AA%8C%E8%AF%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E7%90%86/","title":"基于秘密共享的隐私保护和可验证深度学习推理"},{"content":"总结 这一周又没干啥正经事，学科前沿讲座结课了，整了一下结课论文，把软件工程实训汇报完了，有惊无险，但却有点轻微感冒+上火+口腔溃疡，痛不欲生，虽已习惯了高三时期留下的旧疾，但得了口腔溃疡还是痛。\n这周又读了几篇论文，还是没有找到与我要研究东西特别相近的论文，都是关于深度学习的，没有大模型这方面的，这周继续探索一下。\n计划 计划就是完成上周以及上上周未完成的计划，继续干\n\u0026ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption\u0026rdquo;\n该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。 \u0026ldquo;Federated Learning with Encrypted Data\u0026rdquo;\n这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。 \u0026ldquo;Secure Inference with Deep Learning Models\u0026rdquo;\n该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。 \u0026ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning\u0026rdquo;\n本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。 \u0026ldquo;Secure and Private Inference of Deep Learning Models\u0026rdquo;\n这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。 \u0026ldquo;Privacy-Preserving Machine Learning: A Practical Approach\u0026rdquo;\n本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。 \u0026ldquo;Secure Outsourced Computation in Machine Learning\u0026rdquo;\n这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。 \u0026ldquo;Differentially Private Inference in Machine Learning\u0026rdquo;\n本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。 \u0026ldquo;Towards Secure and Private Machine Learning: A Survey\u0026rdquo;\n这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。 \u0026ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning\u0026rdquo;\n该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。 \u0026ldquo;Secure and Private Model Inference: A Survey\u0026rdquo;\n本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。 ","date":"2024-11-17T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC6%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/ai-generated-8787240_1280_hu8301059065001962622.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC6%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/","title":"第6周工作总结"},{"content":"使用同态加密和联邦学习的隐私保护机器学习 Haokun Fang 和 Quan Qian 是在隐私保护机器学习、同态加密、联邦学习等领域具有一定研究成果的学者。以下是他们的简要介绍：\nHaokun Fang：\nHaokun Fang 是一位专注于机器学习和加密技术的研究者。他的研究兴趣涵盖了隐私保护机器学习、联邦学习、同态加密等前沿技术。尤其是在如何通过加密技术保障数据隐私的研究领域，Fang 通过提出和改进各种算法，推动了隐私计算技术的发展。他也致力于如何在不暴露数据的情况下进行分布式计算和机器学习。 Quan Qian：\nQuan Qian 是另一位活跃在机器学习与信息安全领域的学者，专注于隐私保护技术和加密算法。他的研究方向包括同态加密、联邦学习、数据隐私保护以及分布式学习等。他是隐私保护机器学习的领域内的知名学者之一，并在多个高影响力的会议和期刊上发表了相关研究成果。Qian 对于如何将加密技术与机器学习结合，以实现在保护隐私的前提下有效的数据分析，做出了重要贡献。 两位学者的共同研究方向是将同态加密和联邦学习结合，探索如何在确保数据隐私的情况下，进行高效且可扩展的机器学习模型训练和推理。他们的工作对于推动隐私保护技术的发展具有重要意义。\n摘要： 随着机器学习的巨大成功，隐私保护成为了一个重要的关注点。本文提出了一种基于部分同态加密和联邦学习的多方隐私保护机器学习框架，命名为PFMLP。其核心思想是所有学习方仅通过同态加密传输加密的梯度。通过实验，PFMLP训练出的模型在准确率上几乎没有差异，偏差小于1%。考虑到同态加密的计算开销，本文采用了一种改进的Paillier算法，能够加速训练过程25%-28%。此外，本文还详细讨论了加密密钥长度、学习网络结构、学习客户端数量等方面的比较。\n关键词：多方机器学习；隐私保护机器学习；同态加密\n引言 在大数据时代，数据隐私已成为最重要的问题之一。迄今为止，已经存在许多安全策略和加密算法，旨在确保敏感数据不会受到泄露。此外，其中大多数安全策略假设只有拥有密钥的人才能访问机密数据。然而，随着机器学习，尤其是集中式机器学习的广泛应用，为了训练有效的模型，数据需要被收集并传输到一个中央点。因此，对于那些私人和敏感数据，它将不可避免地面临数据泄露的风险。因此，如何在没有数据泄露的情况下对私有数据集进行机器学习，是共享智能的关键问题。基于隐私保护，具有多方隐私保护的机器学习可以帮助各方用户在确保自身数据安全的前提下，共同学习彼此的数据[1-3]。其中，联邦学习[4,5]是一个典型的例子，它能够在多方计算的背景下解决隐私问题。本文提出了一种基于同态加密的隐私保护机器学习算法，命名为PFMLP。基本上，该模型通过多方隐私保护下的梯度学习共同训练。在每次迭代中，模型通过梯度下降进行优化，并通过传输梯度从其他用户的数据中学习。然而，正如[6]中提到的成员推断攻击，训练中的恶意用户可能会使用明文梯度训练一个影像模型，从而危及其他用户的数据安全。因此，我们引入了同态加密来防御这一攻击，允许在不解密的情况下对加密数据进行计算。此外，同态操作后的解密结果等价于对明文数据的操作[7]。由于在整个同态操作过程中无法识别操作的数据，因此可以保证隐私数据的安全。本文提出的基于同态加密的多方隐私保护机器学习在实际应用中具有广泛的场景。此外，本文的主要贡献如下：\n提供了一种结合同态加密和联邦学习的多方隐私保护机器学习框架，在模型训练过程中实现数据和模型的安全保护。此外，所提出的框架能够在多方联合学习时保持隐私数据的安全。 验证了通过我们提出的算法训练的模型与传统方法训练的模型在准确率上相似。从MNIST和金属疲劳数据集的实验结果来看，准确率偏差不超过1%。 关于同态加密的时间开销，分析了不同密钥长度和网络结构的影响，发现随着密钥长度的增加或结构的复杂化，时间开销会增加。性能与安全性水平之间的权衡需要更多关注。 本文其余部分的组织结构如下：第2节简要总结了相关工作；第3节从安全性、交互性和网络结构的角度详细讨论了联邦网络算法和Paillier联邦网络算法；第4节展示了实验结果，第5节总结了全文。\n相关工作 分布式机器学习 分布式机器学习是一种多节点的机器学习，旨在提高性能、增加准确性，并轻松扩展数据到大规模。2013年在NIPS会议上提出了一种分布式机器学习框架[8]，该框架提出了一种状态同步并行模型，解决了普通同步或在海量数据和模型尺寸下训练的问题。2015年，Xing等人提出了一个通用框架，系统地解决了大规模机器学习中的数据和模型并行挑战[9]。Xie等人提出了一种有效的因子广播（SFB）计算模型，在分布式学习大矩阵参数化模型中既有效又高效[10]。Wei等人通过最大化机器间在给定网络带宽下的通信效率，最小化并行误差，同时确保大规模数据并行机器学习应用的理论融合[11]。Kim等人提出了一个分布式框架STRADS，优化了经典分布式机器学习算法的吞吐量[12]。在分布式深度学习中，2012年Jeffrey等人提出了谷歌的第一代深度学习系统Disbelief，并将模型拆分到32个节点进行计算[13]。2013年，分布式机器学习中的数据和模型并行性被引入到深度学习中，并在InfiniBand网络中实现[14]。2014年，Seide等人从理论上比较了分布式SGD（随机梯度下降）训练在模型和数据并行中的效率，并指出增加小批量的大小可以提高数据训练的效率[15,16]。\n安全多方计算与同态加密 由于分布式机器学习基于中心任务调度，数据对系统是透明的，因此数据隐私无法得到有效保护。一般来说，分布式学习涉及多方计算，通常将复杂或未知的计算过程交给第三方。1986年，Yao提出了基于百万富翁问题的Garbred电路方法，可以用于解决一般问题，包括几乎所有的双方密码问题[17]。此后，1998年，Goldreich提出了安全多方计算（SMPC）的概念[18]。至今，SMPC被视为密码学的一个子领域，能够使分布式各方在不透露自己私有输入和输出的情况下共同计算任意功能。目前，同态加密已成为SMPC中常用的方法。1978年，Rivest等人提出了同态加密的概念，应用于银行业务[19]。作为第一个公钥加密系统，著名的RSA（Rivest-Shamir-Adleman）具有乘法同态性[20,21]。1999年，Paillier算法被发明[22]，由于Paillier满足加法同态性，它已广泛应用于云加密检索、数字拍卖、数字选举及其他隐私保护应用中。2009年，Craig Gentry首次提出了一种基于理想格的全同态加密（FHE）算法，满足加法同态性和乘法同态性[23]。由于FHE具有极高的安全性，它已被广泛应用[24–26]。尤其是在云计算中，同态加密为隐私保护作出了巨大贡献[27]。此外，差分隐私也是一种通过向样本中添加噪声来防止隐私泄漏的隐私保障技术[28–30]。由于引入噪声，当数据量较小时，噪声的影响不可避免地会影响模型训练。如何减少这种影响是一个大挑战。\n联邦学习 针对数据隐私保护和多方联合学习，2016年谷歌提出了一种机器学习方法，命名为联邦学习[31]。作为一种多方协作的机器学习方法，联邦学习逐渐引起了研究界和工业界的广泛关注[32,33]。最初，联邦学习的目的是帮助Android用户解决本地更新模型的问题。此外，联邦学习可以应用于机器学习的各个领域。2019年，谷歌科学家提到，他们基于TensorFlow构建了一个用于移动设备领域联合学习的可扩展生产系统[34]。此外，2019年还提出了更多相关工作。Wang关注了在数据分布在多个边缘节点时，如何学习模型参数而无需将原始数据发送到中央节点的问题[35]。也有一些工作专注于联邦迁移学习，例如[36]中设计的框架，可以灵活地应用于各种安全多方机器学习。关于性能，[37]提出了一个框架SecureBoost，准确率几乎与五种隐私保护方法相当。联邦学习已广泛应用于各个领域。例如，谷歌设计的Gboard系统实现了键盘输入预测，同时保护隐私并帮助用户提高输入效率[38,39]。在医疗领域，患者的医疗数据是敏感的，因此联邦学习非常有用[40,41]。此外，联邦学习还可应用于自然语言处理[42]和推荐系统[43]等领域。此外，近年来，在隐私保护机器学习方面也有许多值得关注的工作。Zhou等人提出了使用差分隐私保护机器学习中的隐私，并使用SMC减少差分隐私引起的噪声[44,45]。2020年，Zhang等人提出了一种batchcrypt算法，该算法基于FATE框架的优化[46]，它将一批量的量化梯度编码为长整型，然后一次性加密，从而通过减少计算量提高了加密和解密效率。Wei Ou等人提出了一个基于同态加密的贝叶斯机器学习垂直联邦学习系统，该系统能够达到单一联合服务器训练模型的90%的性能[47]。\n方法与算法 基于联邦思想的多样本协同学习 联邦学习的思想是，在数据孤立的情况下，通过在训练过程中中间变量的交互，利用其他方的数据来优化自己的模型，如图1所示。从数据划分的角度，联邦学习可以分为两类：水平联邦学习（样本扩展）和垂直联邦学习（特征扩展）。\n水平联邦学习：水平联邦学习是通过样本扩展进行的机器学习。假设 \\( D \\) 表示数据，\\( X \\) 表示特征，\\( Y \\) 表示样本，\\( I \\) 表示数据索引。水平联邦学习可以表示为：\n$$X_i = X_j, Y_i = Y_j, I_i \\neq I_j, \\forall D_i, D_j, i \\neq j \\tag{1}$$这表示不同的用户有不同的数据，这些数据可能有交集也可能没有交集。水平联邦学习的主要思想是帮助多个用户使用自己的数据共同训练一个可靠的模型，同时确保数据的隐私和安全。然而，对于样本扩展，所有方的数据需要先对齐，以确保所有参与训练的方具有相同的特征域。这有助于所有方构建相同的模型架构并同步迭代。类似地，对于垂直联邦学习，所有参与者都有不同特征的样本。\n联邦网络算法 本文提出的联邦学习网络的主要目标是通过在训练过程中传递中间变量，帮助所有方共同训练相同的模型。考虑到大多数神经网络是通过梯度下降进行训练的，这里我们选择梯度作为其中间变量。尽管梯度不能直接表示所有数据，但它可以表示模型与数据之间的关系，有助于模型的训练。联邦学习网络的架构如图2所示，包含一个计算服务器和多个学习客户端。\n学习客户端 对于学习客户端，它们拥有自己的私有数据，并且假设所有数据已对齐，它们与其他学习参与者的数据量化维度一致。学习客户端的主要功能包括：与其他客户端初始化相同的初始模型、在本地训练数据、在训练过程中提取梯度、与计算服务器计算梯度、收集服务器的响应、传递结果、更新模型，并反复迭代，直到模型收敛。\n计算服务器 计算服务器是学习过程中的一个中介平台。其主要功能包括：接收来自多个学习客户端的梯度信息、对梯度进行计算、整合多个模型学到的信息，并将结果分别传输给每个学习客户端。\n联邦多层感知机算法 在这里，我们提出了一种基于传统多层感知机的联邦多层感知机算法（FMLP）。FMLP可以通过共享梯度，在多方数据隔离环境中为每个客户端训练一个简单的模型。多层感知机，也称为深度前馈网络，是一种典型的深度学习模型。其架构示例如图3所示。\n所有涉及算法的参数及其含义如表1所示。\n表1. PFMLP算法中的参数及描述\n参数 说明 1 $x$：数据集中的样本 2 $\\theta$：模型的参数 3 $f_p$：前馈过程 4 $out$：每次迭代的输出 5 $f^*$：激活函数 6 $loss$：损失函数 7 $c$：通过损失函数计算的损失 8 $e$：最小误差 9 $bp$：反向传播过程 10 $grad$：反向传播过程计算的梯度 11 $lr$：学习率 假设模型的参数为 $\\theta = {\\omega_1, \\cdots, \\omega_n, b_1, \\cdots, b_n}$，训练的学习率为 $lr$。数据集可以表示为 $x = {x_1, \\cdots, x_n}$。模型的目标是逼近一个分布 $f^*$。网络的前馈过程是计算训练输出，定义为：\n$$\rout = f_p(x, \\theta)\r$$计算输出与理想值之间距离的损失函数可以定义为：\n$$\rc = loss(f^*(x), out) \\tag{3}\r$$反向传播的功能是计算梯度，并将其从损失函数向后传播，帮助网络根据梯度调整参数，从而减少输出值与理想值之间的误差。反向传播过程可以定义为：\n$$\rgrad = bp(x, \\theta, c) \\tag{4}\r$$模型更新过程是根据反向传播得到的梯度调整网络参数，可以表示为：\n$$\r\\theta' = \\theta - lr \\cdot grad \\tag{5}\r$$通过联邦网络实现的多层感知机（MLP），我们可以得到一个联邦多层感知机（FMLP）。然后，MLP 模型的副本存储在每个学习客户端的本地内存中。它包含一个输入层，具有 $x$ 个单元，$n$ 个隐藏层，每个隐藏层有 $y$ 个单元，以及一个输出层，具有 $z$ 个单元。$x$ 的大小取决于输入数据的特征维度，$z$ 的大小取决于网络所需的输出，具体依赖于实际应用的目标输出。\n计算服务器的主要功能是融合梯度数据，帮助模型加速梯度下降，同时学习来自每个客户端的数据。在模型更新之前，每个学习客户端将梯度传递给计算服务器进行模型训练。此外，计算服务器整合所有客户端的梯度数据，并返回计算得到的新梯度给每个客户端用于模型更新。最后，当每个客户端的损失小于 $e$ 时，模型收敛。此外，所有客户端都可以获得相同的联邦模型。FMLP的具体步骤见算法1。\n算法 1 联邦多层感知机\n输入：数据集 $x$\n输出：模型 $\\theta_{final}$\n初始化模型参数 $\\theta$ 对于每次迭代 $i$，执行以下步骤：\n3. 前馈传播：$out_i = f_p(x_i, \\theta_i)$\n4. 计算损失：$c_i = loss(f^*(x_i), out_i)$\n5. 如果 $c_i \u0026lt; e$，则\n6. 跳出循环\n7. 否则\n8. 反向传播：$grad_i = bp(x_i, \\theta_i, c_i)$\n9. 将梯度发送给计算服务器并获取新梯度\n10. 更新：$\\theta_{i+1} = \\theta_i - lr \\cdot grad_{new}$\n11. 结束 结束循环 返回带有参数 $\\theta_{final}$ 的模型 Paillier 联邦网络 本文提出的联邦网络允许多个方在数据孤立的情况下进行协同机器学习。然而，在实际情况中，攻击者所需要的不仅仅是参与者提供的数据，还有多个方训练的最终模型。根据Shokri等人在2017年提出的成员推断攻击，攻击者可以入侵服务器，并从服务器中的数据推断出若干个影子模型。基于集成学习的思想，攻击者可以最终通过这些影子模型得到一个与实际合作训练的模型相似的预测。换句话说，在这种情况下，联邦模型只能解决数据安全问题，而不能解决模型安全问题。因此，为了保证模型的安全性，可以将同态加密引入联邦学习中。\n此外，同态加密的核心思想是，在对明文 $a$ 进行加密得到密文 $c$ 后，在密文空间内对 $c$ 执行某些操作的结果，相当于在明文空间内对 $a$ 执行相同操作的结果。加密操作可以表示为：\n$$\rE(a) \\oplus E(b) = E(a \\otimes b) \\tag{6}\r$$在公式 (6) 中，$E$ 表示加密算法，$a$ 和 $b$ 表示两个不同的明文，$\\oplus$ 和 $\\otimes$ 表示操作符。如果操作是乘法操作，那么同态加密满足乘法同态性，例如 RSA 算法 [20]。如果操作是加法操作，那么同态加密算法满足加法同态性。Paillier 算法是最著名的一种 [22]。此外，如果算法同时满足加法和乘法同态性，那么该加密算法满足完全同态性 [23]。由于在多层感知机（MLP）中我们需要对梯度数据进行求和，因此可以使用 Paillier 算法进行同态加密。\nPaillier 算法 如上所述，Paillier 加密是一种部分同态加密，满足加法同态性。它可以分为三部分：密钥生成、加密和解密。\n密钥生成：首先，选择两个足够大的素数 $p$ 和 $q$，它们的长度相等，并且满足 $\\text{gcd}(p \\cdot q, (p - 1) \\cdot (q - 1)) = 1$。然后，计算 $n$ 和 $\\lambda$，如下所示： $$\rn = p \\cdot q \\tag{7}\r$$$$\r\\lambda = \\text{lcm}(p - 1, q - 1) \\tag{8}\r$$接着，随机选择一个整数 $g$，使得 $g \\in Z^*_{n^2}$，从而使得 $n$ 能够整除 $g$ 的阶。然后，定义 $L(x)$ 来计算 $\\mu$，如下所示：\n$$\rL(x) = \\frac{x - 1}{n} \\tag{9}\r$$$$\r\\mu = (L(g^\\lambda \\mod n^2))^{-1} \\mod n \\tag{10}\r$$到此为止，我们可以得到公钥为 $(n, g)$，私钥为 $(\\lambda, \\mu)$。\n加密：假设明文为 $m$，密文为 $c$，使用公钥进行加密的过程可以表示为： $$\rc = g^m \\cdot r^n \\mod n^2 \\tag{11}\r$$ 解密：使用私钥解密密文 $c$，得到明文 $m$ 的过程为： $$\rm = L(c^\\lambda \\mod n^2) \\cdot \\mu \\mod n \\tag{12}\r$$改进的 Paillier 算法 然而，由于 Paillier 算法在进行加密和解密时的高复杂度，这将影响网络训练的效率。因此，我们使用了改进版本的 Paillier，并且在 [48] 中详细证明了优化的正确性和效率。\n密钥生成：使用 $\\alpha$ 作为除数，如果 $\\lambda$ 替换了私钥中的 $\\lambda$ 位置，我们可以修改公钥中的 $g$，并确保 $g$ 的阶为 $\\alpha n$。\n加密：假设明文为 $m$，密文为 $c$，$r$ 为随机正整数，并且满足 $r \u0026lt; \\alpha$。改进的加密过程可以表示为：\n$$\rc = g^m \\cdot (g^n)^r \\mod n^2 \\tag{13}\r$$ 解密：解密过程可以表示为： $$\rm = L(c^\\alpha \\mod n^2) \\cdot L(g^\\alpha \\mod n^2)^{-1} \\mod n \\tag{14}\r$$从上述算法可以看出，使用 $\\alpha$ 代替 $\\lambda$ 的最大优势在于解密过程中。幂运算的次数从 $2 \\cdot \\lambda$ 次减少为 $2 \\cdot \\alpha$ 次。由于 $\\alpha$ 是 $\\lambda$ 的除数，时间开销显著减少。原生 Paillier 算法的计算复杂度为 $O(|n|^3)$，而改进后的 Paillier 算法的计算复杂度为 $O(|n|^2|\\alpha|)$ [49]。\nPaillier 联邦网络的架构 在这里，我们使用 Paillier 加密来保护梯度数据。因此，即使攻击者破坏了计算服务器，他们也无法从每个学习客户端获取梯度数据的具体信息。此外，攻击者无法利用这些加密的梯度数据来训练影子模型。由于 Paillier 加密需要密钥对，为了生成和管理密钥对，我们在算法中加入了一个密钥管理中心（KMC）。Paillier 联邦网络的架构如图 4 所示，它包括 KMC、计算服务器和多个学习客户端。\nPaillier 联邦多层感知机 (PFMLP) PFMLP 的基本结构与 FMLP 非常相似。由于 PFMLP 需要与 KMC 进行交互，学习客户端在训练开始之前应该向 KMC 发送请求。KMC 确认每个参与者都在线，然后生成密钥对并将其返回给学习客户端。收到密钥对后，每个学习客户端基于加密数据进行多方机器学习。PFMLP 的流程图如图 5 所示。与 FMLP 相比，PFMLP 增加了三个部分：(1) 学习客户端的加密和解密操作；(2) 计算服务器的同态操作；(3) 密钥管理中心（KMC）中的密钥对生成和分发。在 PFMLP 中，包含学习客户端、计算服务器和 KMC。学习客户端的算法如下所示：\n算法 2：学习客户端中的 PFMLP\n输入：数据集 x\n输出：模型 θ_f inal\n向 KMC 请求密钥对 初始化模型参数 θ 对于每次迭代 i: 前向传播：$out_i = f_p(x_i, \\theta_i)$ 计算损失：$c_i = loss(f^*(x_i), out_i)$ 如果 $c_i \u0026lt; e$，则 退出 否则： 反向传播：$grad_i = bp(x_i, \\theta_i, c_i)$ 使用客户端 i 的公钥加密梯度：$Enc(grad_i) = EncPaillier(Publickey, grad_i)$ 将加密的梯度 $Enc(grad_i)$ 发送给计算服务器并接收：$Enc(grad_i^{new})$ 使用客户端 i 的私钥解密梯度：$grad_i = DecPaillier(Privatekey, Enc(grad_i))$ 更新：$\\theta_{i+1} = \\theta_i - lr \\cdot grad_{new}$ 结束 返回具有参数 θ_f inal 的模型 学习客户端在每次学习迭代计算完梯度后，并不会立即更新本地模型。它会对梯度数据进行同态加密，并将其传输给计算服务器，随后等待服务器在进行同态操作后返回新的加密梯度数据。在解密阶段，一旦客户端解密了新的加密梯度数据，它就可以使用新的梯度更新每个学习客户端的本地模型。因此，新的梯度隐式地包含了其他客户端的私有数据，从而间接保护了数据隐私。\n由于 PFMLP 对梯度数据执行 Paillier 加密，即使计算服务器被黑客攻破，泄露的数据也仅显示加密后的梯度数据 $Enc(grad)$。因此，可以避免推理攻击的威胁。KMC 的算法如算法 3 所示。此外，KMC 的主要功能是生成和分发密钥对。也就是说，当它收到来自学习客户端的请求时，它会生成一个密钥对并将其分发给客户端。\n算法 3 KMC 中的 PFMLP\n输入：请求\n输出：密钥对\n1: 在监听客户端请求时循环\n2: 如果收到来自客户端的请求，则\n3: 生成一个密钥对；\n4: 将密钥对返回给学习客户端；\n5: 结束条件\n6: 结束循环\n算法 4 计算服务器中的 PFMLP\n输入：请求\n输出：梯度数据\n1: 在监听来自客户端的请求时循环\n2: 初始化梯度数据；\n3: 如果收到请求，则\n4: 将加密数据 Enc(data) 推送到队列；\n5: 如果请求的数量 == 学习客户端的数量，则\n6: 对于每个学习客户端的请求：\n7: 将梯度数据更新为：GradientData = GradientData ⊕ Enc(datai)；\n8: 结束循环\n9: 将梯度数据返回给每个客户端；\n10: 退出循环；\n11: 结束条件\n12: 结束条件\n13: 结束循环\n算法安全性分析 在 PFMLP 中，密钥管理中心仅负责密钥生成，并且不能访问任何数据。对于密钥管理中心，它甚至不知道客户端用密钥加密了哪些数据，因此它无法与其他方串通非法访问数据。计算服务器接收到的数据是客户端加密的密文，所有操作都是同态操作，没有解密过程。这意味着，在计算服务器上，所有数据都是加密格式的，因此即使服务器被攻破，也无法获得明文数据。学习客户端从密钥管理中心获取密钥对，然后将加密的梯度数据发送到计算服务器；计算服务器计算完成后，将仍然是加密格式的结果返回给客户端。在整个过程中，客户端无法访问其他客户端的数据。参与的唯一数据是上传的数据和返回的结果，它们都是加密格式的，这可以确保数据的安全性。如果攻击者想通过攻击计算服务器或通信通道来获取数据，他/她只能得到密文。由于我们可以在每次迭代中更换密钥对，即使攻击者足够幸运能够破解几轮训练结果，他/她也无法获得最终结果。即使攻击者是参与者，由于上述的客户端安全分析，他/她也无法从其他客户端获取数据。\n实验与结果分析 实验数据集与环境 本实验使用了两个数据集进行验证：MNIST数据集和金属疲劳强度数据集。对于MNIST手写数字数据集[50]，它包含60,000个训练样本和10,000个测试样本。此外，神经网络模型包括784个输入层单元、两个默认64个单元的隐藏层和一个包含10个输出单元的输出层。关于金属疲劳数据，它只有437条记录，来自NIMS MatNavi开放数据集[51]。MatNavi是全球最大的材料数据库之一，涵盖了聚合物、陶瓷、合金、超导材料、复合材料和扩散数据库等。这里，我们从MatNavi中选择了437条金属疲劳强度数据，用于建立回归模型，测试不同金属（如碳钢、低合金钢、渗碳钢和弹簧钢）在不同测试条件下（如不同组件、轧制产品特性和后续热处理）下的表现。每条金属疲劳数据包含15维特征和1维标签。根据疲劳数据集，我们将其分为四类，如表2所示。实验中使用的模型结构包括一个15个单元（15维）的输入层、三个64单元的隐藏层和一个包含四个单元的输出层。PFMLP的网络结构如表3所示。\n表2. 用于多分类任务的疲劳数据集。\n数据集 数据范围 数据量 疲劳 [200, 400) 56 [400, 500) 147 [500, 600) 148 [600, ∞) 86 表3. PFMLP的网络结构。\n数据集 输入层 隐藏层 输出层 MNIST 784（单位） 2（层）· 64（单位） 10（单位） 疲劳 15（单位） 3（层）· 64（单位） 4（单位） 这里，$D^{Dataset}_n$ 表示数据集的第 n 个数据。为了评估 PFMLP 算法及其优化方法，设计了几组对比实验，从三个角度进行比较：(1) 联邦多层感知器与单节点多层感知器的预测准确性；(2) 使用不同密钥长度进行模型训练的时间消耗；(3) 使用不同大小的隐藏层单元进行模型训练的时间消耗；(4) 不同数量的学习客户端对模型性能的影响。实验环境为 Windows 10，Python 3.6，scikit-learn 0.21.3 和 phe 1.4.0。我们在局域网中部署了计算服务器、KMC 和多个客户端，并通过 Socket 建立了机器之间的通信。具体的网络部署如图6所示。\n精度比较 为了进行比较，PFMLP 和 MLP 算法在相同的网络结构下进行模型训练，并使用相同的数据集进行学习。假设有两个学习客户端，我们将每个数据集分成两个子集，并将它们分配给两个学习客户端。对于 MNIST 数据集，我们选择前 4000 个数据作为训练集 Dmnist，然后将这 4000 个数据分别分为两个部分：Dmnist I = {Dmnist 1, · · · , Dmnist 200}，Dmnist II = {Dmnist 201, · · · , Dmnist 400}。测试数据使用 MNIST 提供的 10,000 个测试集。同时，我们从金属疲劳强度数据集中选择 400 个数据，并将它们分为两个相等的子集进行模型训练。假设原始数据为 DFatigue，将原始数据随机化，记为 DFatigue′ = random(DFatigue)。两个子数据集为 DFatigue′ I = {DFatigue′ 1, · · · , DFatigue′ 200}，DFatigue′ II = {DFatigue′ 201, · · · , DFatigue′ 400}。此外，我们将 70% 作为训练集，剩余的 30% 作为测试集。实验结果如表 4 所示。\n表 4. MLP 和 PFMLP 在两个数据集上的预测精度比较结果\n数据集 数据子集 算法 精度 MNIST Dmnist 1 MLP 0.8333 Dmnist 2 MLP 0.9033 Dmnist MLP 0.9245 Dmnist 1 PFMLP 0.9252 Dmnist 2 PFMLP 0.9252 Fatigue DFatigue′ 1 MLP 0.9013 DFatigue′ 2 MLP 0.7833 DFatigue′ MLP 0.8583 DFatigue′ 1 PFMLP 0.8833 DFatigue′ 2 PFMLP 0.8167 DFatigue′ PFMLP 0.8500 从表 4 可以看出，PFMLP 训练的模型比本地 MLP 的模型更准确。PFMLP 训练的最终模型几乎等于或甚至优于使用每个客户端所有数据训练的 MLP 模型。在 MNIST 数据集上的实验表明，PFMLP 训练的模型在测试集上的准确率为 0.9252，而使用所有训练集数据的 MLP 训练的模型准确率为 0.9245，PFMLP 的准确率比 MLP 高出 0.007。\n对于金属疲劳强度数据集，由于每个客户端上的模型都是从相同的 PFMLP 中学习的，我们可以根据测试集的数量对两个实验的结果进行加权平均，从而得到最终的预测精度为 0.85。与学习了所有数据的 MLP 模型精度为 0.858 相比，精度仅下降了 0.008。因此，从两个数据集的实验结果来看，PFMLP 算法能够训练出一个与 MLP 在多个客户端上的所有数据上几乎相同精度的模型。详细结果如图 7 所示。\n不同密钥长度下模型训练时间对比 由于成员推断攻击的威胁，明文传输梯度数据可能被恶意用户利用来训练自己的影子模型，从而侵犯其他客户端的隐私数据安全。为此，PFMLP 使用了 Paillier 同态加密。此外，在梯度数据传输过程中执行加密，并且在计算服务器中进行同态操作，确保即使服务器存在安全漏洞，加密的梯度数据也不会泄露。\n在 Paillier 中，密钥长度是影响安全级别的一个重要因素。通常，密钥长度越长，安全性越高。然而，使用较长的密钥也会增加生成密文的时间开销。对于 MNIST 和金属疲劳数据集，进行了三次对比实验。模型结构固定，不同的密钥长度是模型训练时间开销的核心因素。表 5 显示了详细信息。\n从表 5 中可以看出，在两个数据集中，Paillier 密钥长度的影响与时间消耗成正比。同时，如图 8 和图 9 所示，这是 PFMLP 在两个数据集上每轮学习训练时间的折线图。由于在每轮中都需要对梯度数据进行加密和解密，并将加密后的梯度数据传输到计算服务器进行进一步操作，因此，随着密钥长度的增加，每轮训练的时间开销也增加，这一现象是合理的。因此，我们可以选择一个适当的密钥长度，作为安全性和时间性能之间的折衷。此外，为了提高安全级别，我们可以在每轮训练中更新密钥。这样，即使某一轮的密钥被破解，也不会影响整体训练过程的安全性，从而实现更高的数据安全级别。\n从上述实验中可以看出，在相同模型和相同密钥长度下，4000 条数据每轮需要 358.14 秒，8000 条数据需要 733.69 秒，12000 条数据需要 1284.06 秒。因此，时间开销与加密数据的数量呈正相关。我们使用改进的 Paillier 算法在 MNIST 数据集上进行实验，并比较了相同梯度数据在相同轮次中的加密和解密时间开销，结果如表 6 所示。\n表 6. 不同密钥长度对每轮迭代时间的影响\n算法 密钥长度（位） 时间（秒） Paillier 128 1068.38 256 6411.23 512 35,930.83 改进版 Paillier 128 779.37 256 4716.37 512 26,148.56 从表 6 可以看出，与原版 Paillier 算法相比，改进版 Paillier 在加密和解密性能上显著提高，提升幅度接近 25% 至 28%。\n隐藏层大小对训练性能的影响比较 对于神经网络，每一层的大小会影响前向传播和反向传播的时间性能。一般来说，网络大小与训练时间呈正相关。在这里，我们设计了几组关于两个数据集的比较实验，结果如表 7 所示。具体的每轮训练时间开销如图 10 和图 11 所示。\n表 7. 不同隐藏层大小对总训练时间的影响\n数据集 隐藏层大小（单元数） 时间（秒） MNIST 2 · 64 12,033.25 2 · 128 23,981.02 2 · 256 47,702.87 Fatigue 3 · 64 2615.42 3 · 128 6941.04 3 · 256 21,782.07 因此，由于算法需要对梯度矩阵进行加密，并且隐藏层单元数越多，时间开销会成比例增加。此外，随着隐藏层单元数的增加，网络中需要传输的数据量也会增加。为了减少 PFMLP 算法的时间开销，在保证准确性的前提下，应尽可能减少隐藏层的数量和每个隐藏层的单元数。\n不同数量学习客户端对训练准确度和时间开销的影响 PFMLP 支持多方机器学习。此外，理论上，随着客户端数量的增加，学习算法应该保证在模型训练过程中获得相似的准确度，并且减少时间开销。在此，我们设计了一项比较实验，分别在单节点（MLP）、两个客户端（2-Client-PFMLP）和四个客户端（4-Client-PFMLP）上对金属疲劳强度数据集进行实验。实验结果如表 8 所示。这里，本地准确度是指模型在本地测试数据集上的预测准确率；逻辑准确度是指每个客户端上的平均准确率。详细结果见图 12。\n表 8. 不同学习客户端数量下，PFMLP 在金属疲劳数据集上的准确度\n算法 数据集 本地准确度 逻辑准确度 MLP DFatigue′ 0−400 0.858 MLP DFatigue′ 0−200 0.833 MLP DFatigue′ 200−400 0.783 2-Client-PFMLP DFatigue′ 0−200 0.867 0.850 2-Client-PFMLP DFatigue′ 200−400 0.833 0.850 MLP DFatigue′ 0−100 0.767 MLP DFatigue′ 100−200 0.933 MLP DFatigue′ 200−300 0.800 MLP DFatigue′ 300−400 0.600 4-Client-PFMLP DFatigue′ 0−100 0.833 0.850 4-Client-PFMLP DFatigue′ 100−200 0.967 0.850 4-Client-PFMLP DFatigue′ 200−300 0.867 0.850 4-Client-PFMLP DFatigue′ 300−400 0.733 0.850 从图 12 可以看出，多客户端 PFMLP 算法显著提高了预测准确度。两种和四种学习客户端的逻辑准确率几乎相同。与划分数据后的本地训练相比，PFMLP 训练的本地准确率有所提高，尤其在极端情况下，准确度得到了放大。例如，在一个 4-Client-PFMLP 实验中，最后一个学习客户端显然有离群数据，而 PFMLP 的准确率比 MLP 高出 13.3%。第二个学习客户端的本地准确率达到了 93.3%，而使用 PFMLP 后仍提高了 3.4%。\n此外，从表 8 可以看出，客户端数量对 PFMLP 训练模型的性能几乎没有影响。这些模型的性能接近于通过收集所有参与者数据的单一 MLP 训练的模型。同时，由于 N-Client-PFMLP 的核心思想是基于批次扩展，一旦每个客户端的数据量较少，他们每轮学习的批次大小也会更小。因此，训练过程的时间开销将会减少。\n结论与未来工作 本文提出的多方隐私保护机器学习方法，通过结合同态加密和联邦学习，能够帮助多个用户在不泄露自己私密数据的情况下进行机器学习。特别是在隐私数据保护方面，该算法能够在数据孤立的情况下训练通用模型。PFMLP 算法的实验结果表明，使用 PFMLP 训练的模型效果与在单台机器上使用所有数据训练的模型相似。所有参与方只需传输梯度数据，梯度融合则在中央计算服务器上通过同态操作完成。学习模型会根据经过同态操作后的新梯度数据进行更新。然而，同态加密不可避免地会引发一些性能问题，例如加密和解密过程的额外开销，这将大大影响训练效率。此外，网络结构、加密/解密密钥长度和密钥替换频率等因素也会影响最终的性能。\n关于未来的工作，首先，应考虑更加强大和可扩展的联邦学习方法，例如将特征分配到不同客户端的纵向联邦学习算法。其次，高效的同态加密算法将加速学习性能。最后，应该更加关注更加稳健的隐私保护学习算法，包括混合算法、防恶意攻击客户端算法等。\n","date":"2024-11-14T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E4%BD%BF%E7%94%A8%E5%90%8C%E6%80%81%E5%8A%A0%E5%AF%86%E5%92%8C%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/forest-6818683_1280_hu4239700516202494138.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E4%BD%BF%E7%94%A8%E5%90%8C%E6%80%81%E5%8A%A0%E5%AF%86%E5%92%8C%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","title":"使用同态加密和联邦学习的隐私保护机器学习"},{"content":"FEDML-HE：一种基于同态加密的高效隐私保护联邦学习系统 摘要 联邦学习通过聚合本地模型更新而非本地数据，在分布式设备上训练机器学习模型。然而，由于聚合的本地模型可能会被反向攻击泄露敏感的个人信息，因此引入隐私问题。隐私保护方法，如同态加密 (HE)，因此在联邦学习训练中变得必要。尽管HE具有隐私优势，但其应用面临不切实际的开销，尤其是在基础模型中。在本文中，我们提出了FedML-HE，这是第一个实用的、基于高效HE的安全模型聚合联邦学习系统。FedML-HE提出选择性加密敏感参数，显著减少了训练过程中的计算和通信开销，同时提供可定制的隐私保护。我们的优化系统展示了显著的开销减少，尤其是在大型基础模型上（例如，对ResNet-50减少约10倍，对BERT减少至多40倍），展示了基于HE的联邦学习扩展部署的潜力。\n引言 联邦学习 (FL) 因其能够让分布式客户端在不直接共享数据的情况下共同训练全局模型，在当代机器学习实践中越来越受欢迎。在标准联邦学习系统中，隐私保护依赖于分布式训练过程和模型聚合函数，例如 FedAvg (McMahan 等, 2017)、FedSGD (Shokri \u0026amp; Shmatikov, 2015) 和 FedGAN (Rasouli 等, 2020)。在 FL 中，客户端不上传原始数据到中央服务器进行训练，而是本地训练模型并将模型上传到服务器，由服务器根据聚合函数对本地模型进行聚合。尽管 FL 确保了本地原始数据不会离开其原始位置，但它仍然容易受到窃听者和恶意 FL 服务器的攻击，这些攻击可能利用本地模型（或模型更新）的明文来重建敏感训练数据，即文献中的数据重建攻击或梯度反演攻击 (Zhu 等, 2019; Criswell 等, 2014; Bhowmick 等, 2018; Hitaj 等, 2017; Han 等, 2023; Hatamizadeh 等, 2022; Fowl 等, 2022)，如图 1 所示。特别是在本地模型是在小型本地数据集上训练的情况下，这种隐私漏洞尤为明显，这是现实应用中常见的场景，例如用于大型语言模型的智能手机文本数据。这些小数据集训练的本地模型固有地包含了细粒度的信息，使得对手更容易从小型模型更新中提取敏感信息。现有的防御方法包括差分隐私 (DP) (Truex 等, 2019a; Byrd \u0026amp; Polychroniadou, 2020) 和安全聚合 (Bonawitz 等, 2017; So 等, 2022)，用于防止明文本地模型的隐私泄露。DP 在原始模型上添加噪声，但可能由于引入的隐私噪声而导致模型性能下降。另一方面，安全聚合使用零和掩码来保护本地模型更新，确保每次更新的细节保持私密。然而，安全聚合需要额外的交互式同步步骤，并对客户端掉线敏感，在实际 FL 应用中不太实用，因为客户端的不稳定环境会面临诸如不可靠的网络连接和软件崩溃等挑战。\n如表 1 所示，与上述非同态加密 (HE) 联邦学习解决方案相比，同态加密 (HE) (Paillier, 1999; Gentry, 2009; Fan \u0026amp; Vercauteren, 2012; Brakerski 等, 2014; Cheon 等, 2017) 提供了一种稳健的抗量子安全解决方案，能够防止本地模型遭受攻击，并在保持模型聚合的精确梯度的同时提供更强的隐私保证。基于 HE 的联邦学习 (HE-FL) 在客户端加密本地模型，并在服务器上对密文进行模型聚合。这种方法使安全的联邦学习部署能够实现与普通 FL 完全相同的模型性能，且已被多个 FL 系统 (Roth 等, 2022; IBM, 2022; Zhang 等, 2020; Du 等, 2023) 以及一些特定领域应用 (Stripelis 等, 2021; Yao 等, 2023) 采用。\n尽管同态加密具有诸多优势，但 HE 仍然是一种强大却复杂的加密基础，且在大多数实际应用中存在不切实际的开销（如图 2 所示）。先前的 FL-HE 解决方案主要采用现有的通用 HE 方法，而缺乏针对大规模 FL 部署的充分优化 (Roth 等, 2022; IBM, 2022; Zhang 等, 2020; Du 等, 2023)。在联邦训练期间，加密计算和通信的可扩展性因此成为瓶颈，限制了其在实际场景中的可行性。这种 HE 开销限制在跨资源受限设备训练大型基础模型时尤其明显（计算和通信通常增加约 15 倍 (Gouert 等, 2022)），因为大模型的加密计算和通信可能比实际模型训练花费更长时间。众所周知，HE 不可避免地在计算和通信方面引入了大量开销 (Gouert 等, 2022)。为了验证这一点，我们对基础 HE 实现进行了评估，以定位开销瓶颈。\n观察结果：正如图 2 中的评估结果所示，由 HE 引入的计算和通信（包大小）开销为 O(n)，即与输入大小 n（在本案例中为聚合模型的大小）成线性增长。尽管未优化的系统比 Nvidia FLARE 更快，但其执行时间和文件大小依然不切实际，特别是在处理大型模型时。\n为了解决这些挑战，我们提出了FedML-HE，一种基于同态加密的高效隐私保护联邦学习系统，采用选择性参数加密，旨在实现跨分布式边缘设备的实际部署。我们的系统显著减少了通信和计算开销，使得基于同态加密的联邦学习在现实场景中更加可访问和高效（与其他流行的基于同态加密的联邦学习工作对比可见于表2）。\nFEDML-HE 系统设计 在本节中，我们首先在§2.1中提供FedML-HE系统的概述，在§2.2中定义威胁模型，在§2.3中描述FedML-HE的算法设计，在§2.4中通过定位开销瓶颈提出我们高效的优化方法——选择性参数加密，并在§2.5中从软件框架的角度解释我们如何将同态加密集成到联邦学习中。\n系统概述 如图3所示，我们基于同态加密的高效联邦训练过程主要经历三个阶段：\n(1) 加密密钥协议：客户端使用阈值同态加密密钥协议或受信任的密钥授权机构生成同态加密密钥；\n(2) 加密掩码计算：客户端和服务器使用同态加密应用选择性参数加密方法，共同生成选择性加密掩码；\n(3) 加密联邦学习：客户端使用同态加密密钥和加密掩码选择性加密本地模型更新，以实现高效的隐私保护训练。\n威胁模型 我们定义一个半诚实的对手A，该对手可以破坏聚合服务器或任何子集的本地客户端。A遵循协议，但试图尽可能多地获取信息。粗略地说，在这种对手模型下，安全性定义要求，当A破坏一部分客户端时，只有来自被破坏客户端的本地模型中的私人信息会被泄露；当A破坏聚合服务器时，不会泄露任何本地模型或全局模型中的私人信息。 当A同时破坏聚合服务器和多个客户端时，默认设置下私钥与所有客户端共享（包括被破坏的客户端），这将允许A解密来自正常客户端的本地模型（通过结合被破坏的服务器接收到的加密本地模型和任何被破坏客户端接收到的私钥）。这一问题可以通过采用同态加密的阈值或多密钥变体来缓解，其中解密必须由一定数量的客户端共同执行（Aloufi et al., 2021; Ma et al., 2022; Du et al., 2023）。由于多方同态加密问题不是本文的重点，本文其余部分默认使用单密钥同态加密设置，但阈值同态加密联邦学习设置和微基准的详细信息将在附录中提供。\n$$\r[W_{\\text{glob}}] = \\sum_{i=1}^{N} \\alpha_i \\left[ M \\odot W_i \\right] + \\sum_{i=1}^{N} \\alpha_i \\left( (1 - M) \\odot W_i \\right)\r$$ 其中，$[W_{\\text{glob}}]$ 是部分加密的全局模型，$W_i$ 是第i个明文本地模型，$[[\\cdot]]$ 表示模型的加密部分，$\\alpha_i$ 是客户端i的聚合权重，$M$ 是模型加密掩码。\n请注意，聚合权重可以是加密的，也可以是明文的，这取决于聚合服务器是否足够可信，能够获得这些信息。在我们的系统中，默认情况下，我们将聚合权重设置为明文。在我们的算法中，权重计算仅需要一次同态加密乘法深度，这样有助于减少同态加密乘法操作。我们的系统还可以轻松扩展，以支持通过加密并计算这些算法中的新参数来支持更多的联邦学习聚合函数（例如FedProx（Li等，2020））。此外，在算法1中，如果需要差分隐私，可以在本地模型训练完成后轻松添加可选的本地差分隐私噪声。\n算法 1 基于同态加密的联邦聚合\n[[W]]：完全加密的模型 | [W]：部分加密的模型； p：选择性加密的参数比例； b：（可选）差分隐私参数。 $$(pk, sk) \\leftarrow HE.KeyGen(λ);$$$$i \\in [N]$$$$\rW_i \\leftarrow Init(W); \\\\\rS_i \\leftarrow Sensitivity(W, D_i); \\\\\r[[S_i]] \\leftarrow Enc(pk, S_i); \\\\\r$$$$[[S_i]]$$ 发送给服务器;\n结束循环\n$$\r[[M]] \\leftarrow Select\\left(\\sum_{i=1}^N \\alpha_i [[S_i]], p\\right);\r$$$$t = 1, 2, \\dots, T$$$$i \\in [N]$$$$t = 1$$$$[[M]]$$$$M \\leftarrow HE.Dec(sk, [[M]]);$$\n结束条件\n$$t \u003e 1$$$$[W_{glob}]$$$$W_i \\leftarrow HE.Dec(sk, M \\odot [W_{glob}]) + (1 - M) \\odot [W_{glob}];$$\n结束条件\n$$W_i \\leftarrow Train(W_i, D_i);$$$$W_i \\leftarrow W_i + Noise(b);$$\n结束条件\n$$[W_i] \\leftarrow HE.Enc(pk, M \\odot W_i) + (1 - M) \\odot W_i;$$$$[W_i]$$$$S$$;\n结束循环\n$$\r[W_{glob}] \\leftarrow \\sum_{i=1}^N \\alpha_i[[M \\odot W_i]] + \\sum_{i=1}^N \\alpha_i((1 - M) \\odot W_i);\r$$ 结束循环\n$$M$$ 的形式化过程。\n通过选择性参数加密实现高效优化 完全加密模型能够确保对抗者无法访问明文的本地模型，但会带来高开销。然而，隐私泄漏分析的研究表明，“部分透明性”，例如隐藏部分模型（Hatamizadeh等, 2022; Mo等, 2020），可以限制对抗者成功执行攻击（如梯度反演攻击）的能力（Lu等, 2022）。因此，我们提出了选择性参数加密方法，选择性地加密最隐私敏感的参数，以在降低不可行的开销的同时提供可定制的隐私保护；见图4。\n$$W$$$$X$$$$y$$$$K$$$$w_m$$$$\\frac{1}{K} \\sum_{k=1}^{K} \\| J_m(y_k) \\|$$$$J_m(y_k) = \\frac{\\partial}{\\partial y_k} \\frac{\\partial l(X, y, W)}{\\partial w_m} \\in \\mathbb{R}$$$$l(\\cdot)$$$$X$$$$y$$$$W$$$$\\|\\cdot\\|$$$$k$$$$y_k$$$$i$$$$[[S_i]]$$发送至服务器。如图5所示，模型的不同部分通过暴露不均等的信息量对攻击做出贡献。基于此见解，我们建议仅选择和加密模型中更重要且易受攻击的部分，以减少同态加密（HE）开销，同时保持足够的隐私保护。\n$$P = \\sum_{i=1}^N \\alpha_i [[S_i]]$$$$p \\in [0, 1]$$$$M$$，该比率表示为选择最敏感的参数进行加密的比例。最后，将全局加密掩码作为联邦学习配置的一部分共享给客户端。\n软件框架：同态加密在联邦学习中的应用 在本节中，我们将从软件框架的角度说明如何设计基于同态加密（HE）的聚合机制。图 6 展示了我们框架的高层次设计，框架由三个主要层次构成：\n加密基础层。基础层通过 Python 包装器实现同态加密功能，包括密钥生成、加密/解密、安全聚合以及密文序列化，使用的是开源的同态加密库；\n机器学习桥接层。桥接层连接联邦学习系统的编排功能和加密功能。具体来说，我们设计了机器学习处理 API，用于将本地训练过程的输入转化为同态加密功能的输入，并处理相应输出。此外，我们在此层实现了优化模块，以减少同态加密带来的开销；\n联邦学习编排层。在联邦学习系统层，密钥管理服务器负责密钥分发，(服务器/客户端)管理器和任务执行器用于组织参与方的协作。\n我们的分层设计使得同态加密基础层和优化模块具有半独立性，便于在 FedML-HE 中切换不同的同态加密库，并易于将进一步的联邦学习优化技术添加到系统中。\n通过选择性参数加密实现隐私保护 在本节中，我们首先提供证明来分析完全加密联邦学习的隐私性，然后分析选择性参数加密的隐私保障。\n基础协议的证明 在本小节中，我们证明了基础协议的隐私性，其中基于同态加密的联邦学习使用完全模型参数加密（即选择性参数加密率设置为1）。我们在定义 3.1 中定义了攻击者，并在定义 3.3 中定义了隐私性。\n定义 3.1（单密钥攻击者） 一个半诚实的攻击者 A 可以同时破坏任何 n 个学习者和聚合服务器的子集，但不能同时破坏它们。 请注意，证明的参考假设了单密钥设置，并且同态加密联邦学习（HE-FL）阈值变体的隐私性（如定义 3.2 所示）可以通过扩展阈值同态加密的证明（Boneh 等，2006；Laud \u0026amp; Ngo，2008；Asharov 等，2012）来轻松证明。\n定义 3.2（阈值攻击者） 一个半诚实的攻击者 $A_T$ 可以同时破坏任何 n − k 个学习者和聚合服务器的子集。\n定义 3.3（隐私性） 在半诚实攻击者 A 存在的情况下，基于同态加密的联邦学习协议 π 是模拟安全的。即存在一个理想世界中的模拟器 S，该模拟器与 A 破坏的相同参与方交互，并生成一个与 A 在实际世界中输出分布相同的输出。\n理想世界 我们理想世界的功能 F 与学习者和聚合服务器的交互如下：\n每个学习者向 F 发送注册消息以获取联邦训练模型任务 $W_{glob}$。F 确定一个学习者子集 N′ ⊂ N，其数据可以用于计算全局模型 $W_{glob}$。 无论是诚实的还是被破坏的学习者都会将他们的本地模型上传到 F。 如果 N′ 中学习者的本地模型 ⃗W 足以计算 $W_{glob}$，F 会将 $W_{glob} ← \\sum_{i=1}^{N^′}α_iW_i$发送给 N′ 中的所有学习者，否则 F 会发送空消息 ⊥。 实际世界 在实际世界中，F 被我们在算法 1 中描述的协议替代，该协议使用完全模型参数加密。 我们描述一个模拟器 S，它模拟攻击者 A 在我们协议的实际执行中的视图。我们的隐私定义 3.3 和模拟器 S 证明了机密性和正确性。我们省略了模拟攻击者 A 破坏聚合服务器的视图，因为在执行 π 时，学习者不会收到其他学习者的本地模型的密文，因此这种模拟是直接且简单的。\n模拟器 在理想世界中，S 从 F 接收 λ 和 $1^n$，并执行以下步骤：\n$$r$$。\n$$pk$$$$(pk, sk) \\leftarrow HE.\\text{KeyGen}(\\lambda)$$。\n$$i$$$$(c_i) \\leftarrow HE.\\text{Enc}(pk, r^{|W_i|})$$。\n$$\\vec{c}$$$$f$$$$(c_{\\text{glob}}) \\leftarrow HE.\\text{Eval}(\\vec{c}, f)$$。\nS 的执行意味着：\n$$\r\\{(c_i, c_{\\text{glob}})\\} \\overset{s}{\\equiv} \\left\\{ HE.\\text{Enc}(pk, W_i), HE.\\text{Eval}(\\vec{W}, f) \\right\\}\r$$因此，我们可以得出结论，S 在理想世界中的输出在计算上与 A 在实际执行中的视图不可区分：\n$$\r\\{S (1^n, \\lambda)\\} \\overset{s}{\\equiv} \\{\\text{view}^\\pi (\\lambda)\\}\r$$$$\\text{view}$$$$\\pi$$ 实际执行中的视图。\n通过差分隐私理论对加密学习的证明 $$D_1$$$$D_2$$$$|D_1 \\triangle D_2| = 1$$，则它们为相邻数据集。\n$$M$$$$\\varepsilon$$$$D_1$$$$D_2$$$$O \\subseteq \\text{Range}(F)$$，以下不等式成立：\n$$\r\\frac{\\Pr[M(D_1) \\in O]}{\\Pr[M(D_2) \\in O]} \\leq e^{\\varepsilon}\r$$$$\\varepsilon$$ 的较小值意味着更强的隐私保障。\n$$f : D \\to \\mathbb{R}$$$$D$$$$d$$$$f$$$$b$$ 为拉普拉斯分布的尺度参数，其定义如下：\n$$\r\\text{Lap}(x | b) = \\frac{1}{2b} e^{-\\frac{|x|}{b}}\r$$$$D$$$$F$$ 定义为：\n$$\rM(D) = f(D) + \\text{Lap}(0 | b)^d\r$$$$\\varepsilon$$$$b$$$$f$$$$f$$$$\\Delta f$$$$f$$ 应用于任意两个相邻数据集时输出的最大差值：\n$$\r\\Delta f = \\max_{D_1 ,D_2 :|D_1 \\triangle D_2 |=1} \\|f(D_1) - f(D_2)\\|_1\r$$基于定义 3.4、3.5、3.6 和 3.7，我们得出以下引理：\n$$\\varepsilon$$$$b$$ 为：\n$$\rb = \\frac{\\Delta f}{\\varepsilon}\r$$$$b$$$$F$$$$\\varepsilon$$-差分隐私。\n$$\\text{Lap}(0 | b)^d$$$$b = \\frac{\\Delta f}{\\varepsilon}$$$$\\varepsilon$$-差分隐私。随后我们表明同态加密提供了更强的差分隐私保障。\n$$D_1$$$$D_2$$$$M(D)$$ 在计算上不可区分，因此：\n$$\r\\frac{\\Pr[M(D_1) \\in O]}{\\Pr[M(D_2) \\in O]} \\leq e^{\\varepsilon}\r$$$$O$$$$\\varepsilon = 0$$。\n换句话说，攻击者无法从加密的参数中获取敏感信息。\n选择性参数选择的证明 $$M_1(x)$$$$\\varepsilon_1$$$$M_2(x)$$$$\\varepsilon_2$$$$G(x) = (M_1(x), M_2(x))$$$$(\\varepsilon_1 + \\varepsilon_2)$$-差分隐私。\n基于引理 3.8、3.10 和定理 3.9，现在我们可以分析选择性参数加密的隐私性。\n$$S$$$$[N]/S$$$$b$$$$i \\in [N]/S$$$$\\varepsilon_i = \\frac{\\Delta f_i}{b}$$$$\\sum_{i \\in [N]/S} \\frac{\\Delta f_i}{b}$$-差分隐私。\n$$J = \\sum_{i=1}^{N} \\frac{\\Delta f_i}{b}$$$$\\Delta f \\sim U(0,1)$$$$U$$ 表示均匀分布，那么我们可以证明在所有参数上添加拉普拉斯噪声、随机参数加密以及选择性参数加密的隐私代价。\n$$b$$$$J$$-差分隐私。\n$$p$$$$(1 - p)J$$-差分隐私。\n$$p$$$$(1 - p)^2 J$$-差分隐私。\n$$(1 - p)^2$$ 倍。\n评估 在本节中，我们关注评估结果，以展示我们提出的通用优化方案如何在实际应用中大幅减少这些开销，同时仍能有效抵御隐私攻击。此外，有关其他联邦学习系统方面的实验结果已包含在附录中。\n实验设置 模型 我们在不同机器学习领域的模型上测试了我们的框架，包含不同规模的模型，例如 Llama-2（70 亿参数）（附录中提供了更多详细信息）。\n同态加密（HE）库 我们使用 PALISADE 和 TenSEAL 实现了我们的 HE 核心模块。除非另有说明，否则我们的结果显示 PALISADE 版本的评估。\n默认加密参数 除非另有说明，评估中我们选择的默认 HE 加密参数包括：乘法深度为 1，缩放因子位数为 52，HE 批处理大小为 4096，安全等级为 128。\n微基准测试 为了对 HE 开销进行微基准测试，我们使用了一台带有 32 GB 内存和 NVIDIA Tesla T4 GPU 的 Intel 8 核 3.60GHz i7-7700 CPU，在 Ubuntu 18.04.6 上运行。\n优化 为了缓解 HE 开销激增，我们的优化方案选择性参数加密通过选取部分敏感参数进行加密计算，而其余部分按预期的开销和隐私要求保持明文。在本节中，我们首先评估选择性参数加密带来的开销优化，然后使用最先进的隐私攻击来评估我们的选择性防御在联邦学习训练过程中的效果。\n请注意，其他参数效率技术（Tang 等, 2019；Hu 等, 2021）在从头训练和微调场景中都可以在选择性参数加密前应用，并且直接减少共享模型的大小有助于 HE 计算和通信效率（我们在附录中还包含了该部分的初步结果）。\n优化的开销 我们首先检查选择性参数加密带来的开销优化效果。在选择并加密具有较高隐私重要性的参数时，观察开销的变化。图 7 显示了仅加密模型的某些部分所带来的开销减少情况，这种开销与加密模型参数的大小几乎成比例，这符合 HE 开销与输入大小之间的普遍关系。值得注意的是，在按选择性参数加密加密 10% 参数后，开销接近于明文聚合的开销。\n图 8 从开销分布的角度分析了 HE 框架（优化前和优化后）和明文框架在单个 AWS 区域带宽下的训练周期组成。对于中型模型，HE 带来的开销（包括计算和通信）将本地训练过程的一部分转移到聚合相关步骤中，但与非 HE 相比差距相对可接受。尽管一般而言较小的模型需要较短的训练时间，但 HE 聚合的开销也成比例下降。\n选择性防御的效果 为了评估选择性参数加密的防御效果，我们首先使用隐私敏感性生成隐私映射（图 5），然后通过执行梯度反演（DLG (Zhu 等, 2019)）验证选择性的有效性。我们还提供了 BERT 模型在语言模型反演攻击（Fowl 等, 2022）下的防御结果。\n计算机视觉任务的防御效果 我们使用 CIFAR-100 数据集的图像样本来计算模型参数的敏感性。在 DLG 攻击实验中，我们采用多尺度结构相似性指数 (MSSSIM)、视觉信息保真度 (VIF) 和通用质量图像指数 (UQI) 作为指标，以衡量恢复图像与原始训练图像的相似性，从而评估攻击质量和隐私泄露程度。如图 9 所示，相较于随机加密选择（需要加密 42.5% 的参数才能开始防御攻击），通过我们基于模型隐私映射的前 10% 参数加密选择即可抵御攻击，这意味着在相同隐私保护水平下能够实现更低的整体开销。\n自然语言处理任务的防御效果 我们在实验中使用了 wikitext 数据集的语言样本。如图 10 所示，通过我们的敏感性映射确定的前 30% 隐私敏感参数加密掩码能够有效防止反演攻击，其防御效果优于随机加密 75% 模型参数。\n经验选择策略 我们的选择策略通过优先加密更重要的模型参数实现防御效果。根据实验结果，优先加密最敏感的前 30% 参数，以及模型的首层和末层，通常可以有效防止信息泄露 (Hatamizadeh 等, 2022) 和攻击 (如图 5)，这一策略可作为在模型隐私映射基础上的通用指导方针。\n相关工作 现有的联邦学习隐私攻击 近年来，针对联邦学习（FL）领域的隐私威胁和攻击进行了深入研究 (Mothukuri 等, 2021)。FL 隐私攻击通常分为两类：推断攻击 (Nasr 等, 2019; Wang 等, 2019; Truex 等, 2019b) 和数据泄露/重构攻击 (Criswell 等, 2014; Bhowmick 等, 2018; Hitaj 等, 2017)。攻击者通常通过对模型进行攻击，以获取数据提供者的某些特性，甚至重构训练数据集中的数据。在使用较小数据集训练的更精细的本地模型上直接访问 (Wang 等, 2019) 时，攻击成功的几率更高。此外，还可以使用基于生成对抗网络（GAN）的攻击来完全恢复原始数据 (Hitaj 等, 2017)。大多数隐私攻击的根源在于本地模型的明文访问被暴露给其他方（通常是服务器）。\n现有的非同态加密防御机制 局部差分隐私已被采用来保护本地模型更新，通过在客户端在服务器聚合之前添加差分噪声实现 (Truex 等, 2019a; Byrd 和 Polychroniadou, 2020)。然而，为了保证隐私，细粒度的本地更新需要大量的统计噪声，通常会显著降低模型性能。另一方面，也有工作提出应用零和掩码（通常是成对的）来掩盖本地模型更新，使得任何单个本地更新对服务器是不可区分的 (Bonawitz 等, 2017; So 等, 2022)。但这种策略带来了许多挑战，包括密钥/掩码同步要求以及联邦学习参与者的掉线问题。相比这些提供 FL 隐私保护的解决方案，同态加密（HE）是非交互式的，并且对掉线具有鲁棒性（与一般的安全聚合协议 (Bonawitz 等, 2017; So 等, 2022) 相比），且其对模型性能的影响微乎其微（相比基于噪声的差分隐私解决方案 (Truex 等, 2019a; Byrd 和 Polychroniadou, 2020)）。\n现有的基于同态加密的联邦学习工作 目前的基于 HE 的 FL 工作或是应用有限的 HE 方案（如加法方案 Paillier）(Zhang 等, 2020; Fang 和 Qian, 2021; Jiang 等, 2021)，但无法扩展至更复杂的 FL 聚合函数，同时在性能和安全性方面难以保障（受限于 Paillier）；或是提供了通用的 HE 实现用于 FL 聚合 (Roth 等, 2022; IBM, 2022; Jiang 等, 2021; Du 等, 2023; Ma 等, 2022)。然而，之前的工作在 HE 开销增加问题上仍未提供解决方案。在本研究中，我们提出了一种通用的优化方案，在系统和算法层面大幅降低了开销，同时确保了隐私保护，使基于 HE 的 FL 能够在实际部署中具有可行性。\n结论 本文提出了 FedML-HE，这是首个实用的基于同态加密的隐私保护联邦学习（FL）系统，支持加密密钥管理、加密 FL 平台部署，以及通过加密优化来降低系统开销，并设计用于支持高效的大模型联邦训练。我们设计了选择性参数加密（Selective Parameter Encryption），可以有选择性地加密最隐私敏感的参数，从而最小化加密模型更新的大小，同时提供可定制的隐私保护。未来的工作包括对隐私保障、系统开销和模型性能之间的权衡进行定量和理论分析，与其他方法（如差分隐私和安全聚合方法）进行对比，并提升在 FL 场景中门限同态加密的性能，支持去中心化的技术如代理重加密（Proxy Re-Encryption，Ateniese 等，2006）。\n","date":"2024-11-12T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/fedml-he%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E5%90%8C%E6%80%81%E5%8A%A0%E5%AF%86%E7%9A%84%E9%AB%98%E6%95%88%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/tree-3072431_1280_hu12245760729682699742.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/fedml-he%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E5%90%8C%E6%80%81%E5%8A%A0%E5%AF%86%E7%9A%84%E9%AB%98%E6%95%88%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/","title":"FEDML-HE：一种基于同态加密的高效隐私保护联邦学习系统"},{"content":"端到端隐私保护的多机构医学影像深度学习 使用大型、多国数据集来构建高性能医学影像人工智能系统需要隐私保护机器学习的创新，以便模型能够在不需要数据传输的情况下对敏感数据进行训练。我们在此介绍PriMIA（隐私保护的医学影像分析），这是一个免费、开源的软件框架，用于医学影像数据的差分隐私、加密聚合的联邦学习和加密推理。我们在实际案例中测试了PriMIA，通过一个专家级深度卷积神经网络对儿童胸部X光片进行分类，所得到的模型分类性能与在本地非安全环境中训练的模型相当。我们从理论和实证上评估了该框架的性能和隐私保障，并展示了所提供的保护措施如何防止通过基于梯度的模型反演攻击来重构可用数据。最后，我们在端到端的加密远程推理场景中成功应用了训练模型，利用安全多方计算来防止数据和模型的泄露（还好有最后一句，不然差点又不看这个论文了）。\n人工智能（AI）和机器学习（ML）在生物医学数据分析中的快速发展最近带来了令人鼓舞的成果，展示了AI系统能够在多种情境下协助临床医生，例如在医学影像中进行癌症的早期检测。这类系统正逐步超越概念验证阶段，预计将在未来几年实现广泛应用，正如专利申请数量和监管审批不断增加所显示的那样。高性能AI系统的共同特征是需要大量多样的数据集来训练ML模型，这通常通过数据所有者自愿共享数据以及多机构或多国数据集的积累来实现。通常情况下，患者数据会在原始机构进行匿名化或假名化处理，然后传输至分析和模型训练的场所（即集中式数据共享）。然而，匿名化已被证明不足以抵御再识别攻击。因此，大规模的患者数据收集、聚合和传输从法律和伦理角度来说至关重要。此外，控制个人健康数据的存储、传输和使用是患者的一项基本权利。集中式数据共享在实际中几乎剥夺了这种控制权，导致数据主权的丧失。此外，匿名化数据一旦传输，就难以进行追溯性的更正或增强，例如引入后来获得的额外临床信息。\n尽管存在这些顾虑，对数据驱动解决方案的需求不断增加，预计将进一步推动健康相关数据的收集，不仅来自医学影像数据集、临床记录和医院患者数据，还包括例如通过可穿戴健康传感器和移动设备收集的数据。因此，需要创新的解决方案来平衡数据使用与隐私保护。安全且隐私保护的机器学习（PPML）旨在保护数据的安全、隐私和保密性，同时仍然允许从数据中得出有用的结论或将其用于模型开发。实际中，PPML使得即便在本地数据有限的低信任环境中也能进行先进的模型开发。这种环境在医学领域很常见，因为数据所有者无法依赖其他方的隐私和保密合规性。PPML还可以为模型所有者提供保障，确保其模型在使用过程中不会被修改、盗用或滥用，例如通过加密保护。通过缓解资产保护的顾虑，这为可持续的协作模型开发和商业部署奠定了基础。\n先前研究的证据 近期研究显示了PPML在生物医学科学，尤其是医学影像中的实用性。例如，联邦学习（FL）是一种基于将机器学习模型分发给数据所有者（即计算节点）进行分布式训练的去中心化计算技术，而非将数据集集中收集。该方法被提议用于促进跨国协作，同时避免数据传输。在COVID-19疫情背景下，FL被用于保留数据主权并执行数据仓库的本地治理政策。在医学影像领域，最近的研究表明，基于脑肿瘤分割或乳腺密度分类的深度学习模型的联邦训练，其表现与本地训练相当，并且能够更广泛地纳入多样化数据源，提升了模型的泛化能力。\n然而，FL本身并非完全的隐私保护技术。先前研究表明，反演攻击能够通过模型权重或梯度更新重构图像，甚至具有令人惊讶的视觉细节。此外，在“推理即服务”的场景下，模型暴露给不可信的第三方可能导致模型被滥用或直接盗用。因此，FL必须结合其他隐私增强技术才能真正保护隐私。例如，通过安全聚合（SecAgg）模型权重或梯度更新，或差分隐私（DP），可以防止数据集重构攻击，而在模型推理过程中使用安全多方计算（SMPC）协议可以保护正在使用的模型。我们在先前的工作中对这些技术进行了概述。\n目标与贡献 PPML在医学影像中的临床应用需要开发安全与隐私框架，并在复杂的临床任务中进行验证。我们在此介绍PriMIA，这是一个免费、开源的框架，用于医学影像的端到端隐私保护去中心化深度学习。我们的框架结合了差分隐私的联邦模型训练、模型更新的加密聚合以及加密的远程推理。我们的贡献包括以下创新：\n我们展示了在公共互联网环境下，通过PriMIA的隐私增强技术辅助联邦学习（FL），对具有临床挑战性的儿童胸部X光片分类任务进行深度卷积神经网络（CNN）训练的过程。\n我们的框架兼容多种医学影像数据格式，用户配置简单，并在FL训练中引入了功能性改进（加权梯度下降/联邦平均、丰富的数据增强、本地提前停止、联邦范围的超参数优化、DP数据集统计交换），提升了灵活性、可用性、安全性和性能。 我们比较了采用与不采用隐私增强技术训练的模型、在聚合数据集上集中训练的模型、在数据子集上个性化训练的模型以及在未见的真实数据集上与专家放射科医生的分类表现，评估医学影像研究中的各种典型情境。 我们评估了框架的理论和实证隐私与安全保障，并提供了针对多种训练场景下模型的先进梯度反演攻击的应用示例。 最后，我们展示了在安全的“推理即服务”场景中使用训练模型的案例，实现了数据和模型在明文中不被泄露，并展示了我们SMPC协议在推理延迟方面的改进。 库功能 PriMIA是作为PySyft/PyGrid开源PPML工具生态系统的扩展开发的。PySyft（https://github.com/OpenMined/PySyft）是一个Python框架，允许远程执行机器学习任务（例如，张量操作），并通过与常见的机器学习框架（如PyTorch）接口进行加密深度学习。PyGrid提供了服务器/客户端功能，用于在服务器和边缘计算设备上部署此类工作流。有关这些框架所提供的通用功能的详细描述，请参见我们之前的工作。PriMIA基于这些功能，针对医学影像特定应用进行了扩展，原生支持医学影像数据格式，如DICOM，并能够在任意模态和维度的医学数据集上进行操作（例如，计算机断层扫描、X光、超声和磁共振成像）。除了上述PPML技术外，它还提供了解决医学影像分析工作流中常见挑战的方案，例如数据集不平衡、先进的图像增强、全联邦超参数调优功能。此外，它还提供了一个可访问的用户界面，支持从用户机器上的本地实验到远程计算节点上的分布式训练的应用，以促进在医学联盟中应用PPML最佳实践。库的源代码、文档和公开可用的数据可以在https://doi.org/10.5281/zenodo.454559918上找到。\n案例研究、系统设计与威胁模型 我们通过在公共互联网的云计算节点上训练一个包含1110万个参数的ResNet18 CNN模型，展示了PriMIA在临床数据上的应用案例。该模型在由Kermany等人最初提出的儿童肺炎数据集上进行训练，目的是将儿童胸部X光片分类为以下三类之一：正常（无感染迹象）、病毒性肺炎或细菌性肺炎。肺炎是导致儿童死亡的主要原因之一。胸部X光检查常规用于鉴别诊断和治疗选择，但分类儿童胸部X光片具有挑战性。该案例研究的设置基于以下真实场景：\nFL训练阶段 三个医院的联合体希望训练一个深度学习模型，用于胸部X光片分类。由于它们自身既没有足够的数据，也没有足够的专业知识来在这些数据上训练模型，因此它们寻求模型开发者的支持，以在中央服务器上协调训练。在训练阶段，我们将持有患者数据的医院称为数据所有者。在本文中，我们使用“模型”一词来指代深度神经网络的结构和参数。我们假设训练阶段采用先诚实后好奇的威胁模型，如前所述。这里，参与者信任彼此不会主动破坏学习协议，避免有意降低效用，例如主动提供对抗性输入或低质量数据（诚实）。然而，假设个别参与者及其合谋团体会主动尝试从其他参与者的数据中提取私人信息（好奇）。我们的框架的隐私增强技术旨在防止这种行为，我们将在后续部分详细描述。简而言之，差分隐私梯度下降将差分隐私的保证属性扩展到深度神经网络训练中。具体来说，它限制了数据集中单个患者的最坏隐私损失，并提供了针对模型反演/重构攻击的隐私保障，这些攻击可能发生在联邦参与者或在推理时对模型所有者进行攻击。PriMIA为每个FL节点实现了差分隐私（局部差分隐私），以提供患者级的隐私保证。每个节点的隐私预算使用Rényi差分隐私会计师进行。SMPC允许各方在不泄露各自贡献的情况下共同计算函数。在训练过程中，它用于安全地平均网络权重更新（SecAgg）。SecAgg使用基于SPDZ协议的加法秘密共享。训练阶段如图1所示，最终所有参与者都持有完全训练好的最终模型副本。\n远程推理阶段 一旦模型完全训练完成，就可以用于远程推理。在我们的案例研究中，我们假设一个不同的数据所有者——在远程位置的医生，持有一些患者数据，并希望从模型中获得推理结果以协助诊断。推理服务由模型所有者通过互联网提供。数据和模型所有者之间不信任彼此，并希望他们的数据和模型保持私密。PriMIA的SMPC协议保证了推理阶段模型和数据的加密安全性。我们在之前的工作中描述的AriaNN框架被用来适应端到端加密推理。一个常见的SMPC技术是利用提前生成的加密安全随机数（加密原语）（即所谓的离线阶段），以加速某些计算。提供这些原语的可信系统（例如硬件设备）被称为加密提供者，它不参与实际的推理过程（在线阶段），也永远不会接触到任何一方的数据。实际上，加密原语的“储备”可以提前提供给协议参与者，在多个推理过程中使用。加密推理过程总结如图2所示。\n分类性能： 我们训练了不使用SecAgg或DP的FL模型（DP-/SecAgg-），仅使用SecAgg的模型（DP-/SecAgg+）和同时使用这两种技术的模型（DP+/SecAgg+）。此外，我们还在单台机器上对整个数据集进行了训练（集中训练），并对数据所有者的单独数据子集进行了个性化训练。集中训练的模型代表了引言中描述的集中数据共享场景。个性化模型则代表了每个机构仅基于其自身数据进行训练的情况，这是当前医学影像研究工作流程中的典型做法。FL旨在通过使模型训练效果优于个性化训练，并且理想情况下达到与集中训练模型相当的效果。我们在验证集上测试了这些模型的分类性能，并与两名专家放射科医师在测试集1（145张图像）上的分类表现进行了比较，同时还与测试集2（345张图像）上的临床实际数据进行了对比。我们使用了准确度、敏感性/特异性（召回率）、受试者工作特征曲线下的面积（ROC-AUC）和马修斯相关系数（MCC）27作为评估指标。详细信息请参见方法部分。模型和专家在数据集上的分类性能见表1。未使用SecAgg和DP的FL模型表现最好，与集中训练模型之间没有显著的统计差异。加入SecAgg后，模型性能略微下降，但未达到统计显著性。无论是FL模型还是集中训练模型，在性能上均显著优于人工观察者。DP训练过程（ε = 6.0，δ = 1.9 × 10^-4，α值（发散度阶数）为4.4）显著降低了模型性能，但该模型的表现仍与人工观察者相当，并在测试集1和2的样本外数据上保持了稳定的表现。我们注意到，ε值代表训练结束时所消耗的总隐私预算。仅在数据所有者的个别数据子集上训练的个性化模型仅在验证数据上表现相当，但在测试集1和2的样本外数据上表现显著较差，表明其泛化能力较差。这些结果的统计评估以及评分者间/模型间的一致性度量可以在补充材料2部分和补充表格1、2中找到。\n训练和推理性能基准测试： 为了评估PriMIA隐私增强技术对性能的影响，我们在多种场景中对训练和推理性能进行了基准测试，如图3所示。训练时长是以固定批量大小计算的每批次平均时间，以便将其与数据集大小解耦。与本地训练相比，FL由于网络通信的开销会导致性能下降，加入SecAgg和DP后，性能下降更加明显，当同时使用SecAgg和DP时，训练时间是原来的三倍。大型神经网络架构由于需要更多的网络传输时间，因此需要更长的训练时间，这也为我们研究中使用ResNet18架构而非更大ResNet架构提供了依据。增加更多的工作节点会导致时间线性增加，尤其是在使用SecAgg时，因该协议的通信开销。然而，由于每轮操作的数量较少，该协议对多方的扩展性较好：对扩展性的线性回归分析得出t(w) = 0.57w + 2.61，其中t表示时间（秒），w表示工作节点的数量（R² = 0.98，p \u0026lt; 0.001，N = 100个样本，每个工作节点数量测试）。没有SecAgg时，训练时间几乎是常数。对于较大的数据集大小，批次训练时间保持不变，表明训练时间仅依赖于数据集大小，在其他条件相同的情况下。最后，我们基于功能秘密共享（FSS）协议28对加密推理实现进行了基准测试，该协议在执行比较操作、最大池化和批量归一化层时相较于广泛使用的SecureNN29提供了更高的效率。使用FSS进行加密推理显著减少了推理时间，特别是在高延迟设置中，FSS相比SecureNN提供了更好的性能。实现细节可以在方法部分找到，统计评估可以在补充材料3部分找到。\n模型反演攻击： 先前的研究13,30表明，模型反演攻击能够重建特征或整个数据集记录（在我们的案例中是胸部X光图像），因此在FL设置中构成患者隐私的威胁。为了说明使用和不使用PriMIA提供的隐私增强技术训练的模型的易受攻击性，我们利用了改进的深度梯度泄漏攻击31,32，并进行了如方法部分所述的小修改。我们选择这种方法是因为它是第一个被证明对我们的案例研究中使用的ResNet18架构高度有效的技术。图4展示了胸部X光案例研究的示例结果。\n我们使用逐像素均方误差（MSE）、信噪比（SNR）和Fréchet初始距离（FID）指标来量化攻击成功率。实证评估表明，攻击的成功率高度依赖于梯度更新的L2范数和所使用的批量大小。为了生成最佳情况下的基准攻击，我们在训练开始时，以批量大小为1攻击集中训练模型，此时损失幅度（因此梯度范数）最高。对于我们案例研究中使用SecAgg的FL模型，攻击未成功，可能是因为其有效批量大小为600。与DP的隐私保障一致，当使用DP训练时，攻击未能成功。结果表明，即使模型在本地被攻击，或未使用SecAgg时，DP也能抵消攻击，这些结果可见于补充材料5部分和补充图2。为了进一步强调在医疗影像设置中隐私相关攻击的高风险，以及因此在协作模型训练中隐私增强技术的重要性，我们在公开的MedNIST数据集上进行了额外实验，并能够在未使用DP的情况下恢复揭示敏感患者属性的图像。启用DP时无法恢复任何图像（图5）。关于攻击的更多细节和统计评估可以在方法部分和补充材料4、6部分找到。\n讨论：我们介绍了PriMIA，这是一个用于医疗影像隐私保护FL和加密推理的开源框架。我们展示了在儿科胸部X光图像分类这一具有挑战性的临床任务中，如何进行去中心化的协作训练，并训练出了一个专家级深度卷积神经网络。此外，我们还展示了端到端加密推理，可以用于安全的诊断服务，无需披露机密数据或暴露模型。我们的工作是向医疗影像工作流程中实现下一代隐私保护方法的第一步。它适用于多机构研究和企业模型开发环境，能够保护数据治理和患者健康数据的主权。我们的框架可以用于推理即服务场景，在这些场景中，诊断支持可以远程提供，同时提供隐私、机密性和资产保护的理论和实证保障。PriMIA代表了我们以前工作17的针对性进化，面向医疗保健领域的部署。虽然我们在所展示的案例研究中专注于分类任务，但PriMIA高度可适应多种医疗影像分析工作流程，支持不同的网络架构、数据集等。在补充材料7部分和补充图3中，我们展示了一个额外的案例研究，聚焦于腹部计算机断层扫描的语义分割，以展示这一灵活性。\n模型分类性能： 最近的研究评估了数据质量（过于同质/独立同分布的数据与过于异质的数据）和分布式系统拓扑对联邦模型性能的影响，例如模型对样本外数据的泛化能力。在我们的案例研究中，使用FL训练的模型与集中训练模型表现相当，类似于参考文献5，并且优于人类观察者。仅在数据子集上训练的模型（个性化模型）在样本外数据上的表现显著下降。由于个性化模型训练是大多数单中心医疗影像研究中的标准做法，这一发现提醒我们，通过FL包含来自多个来源的大量更具多样性的数据，可以训练出具有更好泛化性能的模型，这也是当前最佳实践所要求的33。DP模型训练能够提供客观的隐私保障，并增强对模型反演攻击的抵抗力30,32。使用DP会降低模型性能，但其表现仍与人类观察者相当。与此同时，所选模型实现的DP保障（ε = 6）仅为中等水平。这一现象（隐私与效用的权衡）是深度学习与DP领域中一个众所周知的观察结果。例如，先前的研究23在CIFAR-10数据集上达到了大约8的ε值，另一项研究报告了ε值在6.9到8.48之间34。这两项研究也报告了最终模型性能的下降。我们认为改进DP模型训练的方法是未来研究的一个有前景的方向。\nFL功能改进： 为了提高框架的可用性、灵活性以及FL模型性能，我们的框架包括以下功能改进。（1）除了采用最近显示出改进收敛结果的Adam优化器形式的自适应客户端优化外，我们还包括了一系列先进的图像增强技术，包括MixUp，已被证明具有增强隐私保护的属性36。（2）我们实施了技术来解决节点之间（本地早停）以及数据集类别之间（类别加权梯度下降和联邦平均37）的数据量不平衡问题。（3）我们提供了在整个联盟中进行集中协调超参数优化的功能，使用的是树状Parzen估计器算法38。展示我们使用超参数选择框架搜索最优FL模型的实验数据可以在补充材料1部分和补充图1中找到。上述所有训练优化都在节点本地实施，并且不会对隐私保障产生负面影响。然而，在使用DP时，超参数调优必须考虑，因为它依赖于多次训练重复。\n关于隐私增强技术的讨论： 在FL过程中引入提供可证明隐私和安全保障的方法是向广泛实施隐私保护AI技术迈出的关键一步8。我们在攻击实验中成功重建未保护模型的图像，强调了此类攻击对患者隐私的风险，这一点也在之前的研究中有所讨论6,39。DP训练在遭遇来自联盟成员或推理过程中的攻击时提供了客观的隐私保障，并不限于我们示例中使用的基于梯度的反演攻击。利用SMPC的SecAgg即使在最多n−1个成员串通的情况下，也仅将聚合模型更新披露给各方。这种我们提出的DP安全聚合数据集统计（均值和标准差）的方法可以保护FL参与者免受数据泄露，特别是在模型构建中包含非影像数据时（例如临床记录，其中如年龄等特征的均值代表敏感信息）。最后，加密推理不会向任何一方泄露数据或模型信息。与完全同态加密协议40（依赖于基于密钥的加密技术）相比，后者在神经网络训练和推理中的实现受到加密过程的计算复杂性和由于函数近似（例如激活函数）带来的性能下降的限制，通信开销传统上一直是SMPC的限制因素。在我们最近的工作中，我们引入了AriaNN26，这是一种利用函数秘密共享（FSS）28并基于SPDZ25构建的SMPC协议。它代表了一种替代协议，如SecureNN29或Falcon41，并通过一次通信回合计算私有比较。这使得FSS比其他SMPC协议在通信上更加高效，特别是在各方地理位置远离且通信延迟高时，例如我们研究中展示的在公共网络上执行推理的情况。通过本案例，我们确认了在其他数据集上获得的结果：在高延迟环境下，安全推理从FSS协议中获得的好处成比例更大。因此，我们建议在诚实但好奇的环境下，当希望减少延迟时，使用FSS而非SecureNN。\n与先前工作的比较： 当前有几项研究旨在将隐私保护机器学习（PPML）技术引入生物医学影像领域：Silva等人42提出了一种面向生物医学的前端FL框架，但未考虑DP、SecAgg或加密推理。Xu及其同事（https://bit.ly/3pl5dD1）提供了一个使用同态加密进行SecAgg的FL框架，但未使用DP或提供加密推理能力。Sheller等人43展示了一个基于分割的FL应用案例，但没有评估DP、SecAgg或加密推理选项。Li等人44也展示了一个FL分割任务。他们的DP实现依赖于一种替代技术（稀疏向量），且框架未提供安全聚合或加密推理功能。Lu及其同事45的研究展示了带有DP的FL，但他们的应用案例集中于病理切片，未使用SecAgg或提供加密推理能力。Li等人46使用了DP，但假设了固定的敏感性并未进行隐私分析，他们的框架不提供SecAgg或加密推理。\n局限性： 我们考虑了以下几点局限性。部署我们的系统的计算要求很高，尽管我们提出了协议改进，但加密推理带来的延迟仍然远高于未加密推理。当前的远程执行环境仅提供实验性的图形处理单元（GPU）支持，计划在即将发布的版本中提供完整支持。FL模型的成功在很大程度上依赖于节点上的高质量数据。数据的审计和整理、量化单个数据集对模型的贡献或检测局部过拟合的方法仍在研究中47。我们的库设计用于诚实但好奇的环境，我们认为这代表了医疗联盟中的标准。因此，尽管我们提供了全面的隐私保护措施，但没有针对恶意贡献低质量或对抗性数据到FL过程中的具体反制措施，也未验证/保证数据所有者推理设置中使用的模型是所承诺的模型。此外，我们指出，关于理论威胁模型的讨论是一种抽象层次，无法完全代表现实生活中的复杂情况。例如，威胁建模通常是在代表整个医院的FL参与者层面进行的，但这不能考虑到为这些医院工作的每个个体及其具体动机。类似地，关于FL中参与者报酬或模型所有权的问题超出了我们当前研究的范围。未来的研究需要进一步阐明这些细节。最后，如上所述，使用DP会导致模型隐私和效用之间的直接权衡。未来的工作需要通过改进隐私分析和训练技术来解决这一权衡，因为当前研究的隐私保障（包括我们研究中大约6.0的ε值）尚不够严格，不能被认为是普遍适用的。\n结论： 我们提出了一个免费的开源软件框架，用于隐私保护的FL和医疗影像数据的端到端加密推理，并在一个具有临床意义的实际案例研究中展示了该框架。进一步的研究和开发将促进我们框架的更大规模部署，验证我们在不同跨机构数据上的发现，并推动PPML技术在医疗保健及其他领域的广泛应用。\n","date":"2024-11-11T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E7%AB%AF%E5%88%B0%E7%AB%AF%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E7%9A%84%E5%A4%9A%E6%9C%BA%E6%9E%84%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/fog-7482180_1280_hu5876590458899171179.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E7%AB%AF%E5%88%B0%E7%AB%AF%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E7%9A%84%E5%A4%9A%E6%9C%BA%E6%9E%84%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"端到端隐私保护的多机构医学影像深度学习"},{"content":"🎵总结 这一周没干啥正经事，周一周二库库读了两天论文，初步了解了一下安全推理的基础知识（就看了一点点），周三图书馆闭馆，晚上上课，所以周三休息一天，周三下午参加了理想汽车AI算法实习生的面试，想找一个这种实习边干活边学习，可惜这并不是啥也不会就能行的，还得学习，沉淀一下。一开始挺紧张的，在脑海里思考了好多遍我该怎么解释，表现自己，结果面试官让我开始自我介绍，他就一只在笑😁（牙很齐很白），不知道他在笑什么，但是看他笑得那么开心我就不紧张了，问了一下transformer为什么要加入位置编码来表示位置信息，LLM微调除了LoRA还有什么，还了解那些大模型，一个也不会，哈哈哈，真不是啥也不会就可以找工作的，不过此行并非失败，而是挺有收获的，面试官一直说，没关系，不会咱就换一个，下去把这个搞清楚就行了，搞得我有点不好意思，在问问题的过程中，面试官还顺带给我解释解释，增长了一些知识，最后我反问到该怎么系统的学习一下才能找到实习（我真是个鬼才🤣，问面试官这问题）\n面试官说：建议我先不要着急找面试，用几个月学习一下，把基础的机器学习和深度学习过一遍，代码复现都能看懂，还问了我研究生方向是大模型安全，看看这方面的论文，把基础知识掌握扎实，上Kaggle上参加或者复现两个项目放到简历里，会更加出色。\n非常感谢面试官的指导，没有想象中的KPI面试，不会有脸色看，更像是前辈教育后辈如何学习🤩。\n周四上午起了个大早直奔图书馆找同学，同学给占了一个之前我没有坐过的位置，有点风，恰好我没有棉袄，差点冻死，于是下午就回了宿舍，周四周五周六就在宿舍待着，写软件工程的文档，哎呦，忒难写，还得画图，周日棉袄到位，在图书馆待了一天，把所有文档完结，做了一个PPT，顺手的事。\n于是这一周就这么过完了\n🎧阅读 Secure and Private Machine Learning: A Survey of Techniques and Applications\nDifferential Privacy: A Survey of Results\n🥁计划 计划就是完成上周未完成的计划，开干！\n\u0026ldquo;Secure and Private Machine Learning: A Survey\u0026rdquo;\n这篇论文综述了在机器学习中实现安全和隐私保护的各种方法，包括使用同态加密和安全多方计算的技术。 \u0026ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption\u0026rdquo;\n该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。 \u0026ldquo;Federated Learning with Encrypted Data\u0026rdquo;\n这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。 \u0026ldquo;Secure Inference with Deep Learning Models\u0026rdquo;\n该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。 \u0026ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning\u0026rdquo;\n本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。 \u0026ldquo;Secure and Private Inference of Deep Learning Models\u0026rdquo;\n这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。 \u0026ldquo;Privacy-Preserving Machine Learning: A Practical Approach\u0026rdquo;\n本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。 \u0026ldquo;Secure Outsourced Computation in Machine Learning\u0026rdquo;\n这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。 \u0026ldquo;Differentially Private Inference in Machine Learning\u0026rdquo;\n本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。 \u0026ldquo;Towards Secure and Private Machine Learning: A Survey\u0026rdquo;\n这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。 \u0026ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning\u0026rdquo;\n该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。 \u0026ldquo;Secure and Private Model Inference: A Survey\u0026rdquo;\n本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。 ","date":"2024-11-10T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC5%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/ai-generated-7926621_1280_hu17162743065600781340.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC5%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/","title":"第5周工作总结"},{"content":"☕安全与隐私机器学习：技术与应用的综述 🥛Abstract 机器学习（ML）中的隐私违规可能导致歧视和身份盗窃等严重后果。随着近年来用于训练模型的敏感数据越来越多，保护隐私的机器学习方法的必要性变得更加重要。本综述研究全面回顾了隐私保护机器学习（PPML）的最新方法，如安全多方计算、同态加密和差分隐私。我们还评估了PPML的缺点，包括可扩展性、计算效率以及隐私与实用性之间的权衡。最后，我们识别了PPML中的开放问题和未来研究方向，包括新兴趋势、挑战和机会。本文旨在为对PPML领域感兴趣的研究人员和从业者提供有价值的资源。\n🍵Introduction 在当今的生活中，计算能力几乎与我们生活的每个方面都有互动。这种对计算机的高度依赖可能导致严重的隐私违规。计算机使用的主要趋势之一是机器学习（ML）。根据（Burkov, 2019），机器学习是“一个研究领域，专注于设计和开发能够从数据中学习和进行预测的算法，而不需要显式编程。换句话说，它涉及开发计算模型，这些模型可以根据从数据中获得的反馈自动改进其在特定任务上的表现。”机器学习正被用于解决医疗、金融等多个领域的许多生活问题。这项技术使用包含特征的数据集来构建所需的知识，以便得到正确的答案。有一种说法认为，训练阶段使用的数据记录越多，测试阶段的准确性就越高（Hey, Tansley, \u0026amp; Tolle, 2009）。从这个角度来看，科学家们开始请求与他们研究领域相关的数据，以便将其应用于机器学习模型。如果以医疗保健为例，根据健康保险流通与问责法案（HIPAA）规定，这些信息应通过建立国家标准来保护个人健康信息的隐私和安全，规范受保护健康信息（PHI）的使用和披露。 当这些数据被共享给科学家用于实验时，许多对策被考虑在内，例如隐藏个人身份信息（PII），然而，主要问题是：这些对策是否足以保护个人身份信息和受保护健康信息？\n有多位研究人员思考过这个问题；（Kim, Kim, \u0026amp; Kim, 2017）发现，在韩国存在通过居民注册号（RRN）去匿名化医疗数据的可能性，RRN是每位韩国居民的独特标识符。作者指出，利用公开可获取的信息，RRN可以被高精度地重新识别。此外，（Sweeney, 2002）声称，个人的出生日期、性别和邮政编码这三项信息可以用于唯一识别美国87%的人口。仅仅使用数据匿名化技术来保护个人身份信息是不够的。从这个角度来看，隐私保护机器学习（PPML）领域应运而生。\n隐私保护机器学习（PPML）是一种开发机器学习算法和模型的技术，旨在在推理和训练阶段保护敏感数据的隐私。在传统的机器学习中，模型通常使用已收集、整合并用于多种应用的数据进行训练。然而，当使用敏感数据（如财务或个人健康记录）时，这种方法可能会引发隐私问题。PPML通过创建允许在不危及基础数据隐私的情况下部署和训练机器学习模型的方法来解决这个问题。\n隐私保护机器学习（PPML）方法大致可以分为两类：\n差分隐私。这种方法通过向数据中引入随机噪声来隐蔽用户身份并阻止私人信息泄露。 安全多方计算（SMC）。多个参与方可以在不泄露任何私人数据的情况下共同训练机器学习模型。数据以一种允许每个参与方对数据进行计算而不直接访问数据的方式进行交换和加密。 在这项工作中，将讨论PPML的一个新颖概述。这项综述可以帮助读者找到研究空白，并提出与该领域相关的未来工作。本文的结构将如下：第二部分将介绍有助于理解将要讨论的主题的背景术语；第三部分将讨论用于PPML的技术；第四部分将讨论与该主题相关的最新论文；第五部分将讨论该主题的挑战；最后第六部分是结论。\n🍾Background 在本节中，将提到所需的基础概念。本节中的术语对于理解我们研究的主题概念将是有用的。\n🍷机器学习模型。 机器学习（ML）被视为人工智能（AI）领域的一部分。它旨在构建一种方法，帮助机器在许多问题中做出决策。机器学习算法可以分为两个主要领域：监督学习和无监督学习算法。这两类之间的主要区别在于是否有来自所研究领域的专家提供的标签。如果算法在学习过程中需要带有标签的数据集，那么该算法被视为监督学习。一般来说，机器学习算法可以检测所用数据集特征中的模式，并基于这些模式对新案例进行预测。\n🍸隐私保护机器学习（PPML）中的加密。 通过促进私密数据的安全交换、存储和处理，密码学在隐私保护机器学习（PPML）中至关重要。为加密数据并对其进行计算而不向未经授权的方披露其内容，使用了同态加密、安全多方计算和秘密共享等密码学技术。同态加密允许对加密数据进行计算，而无需先解密，从而确保在计算过程中不会泄露敏感信息。安全多方计算允许多个参与方共同计算一个函数，而不向彼此披露数据。秘密共享将数据拆分成多个部分，并以一种方式分发到不同的参与方中，只有授权的参与方可以访问和重构原始数据。这些密码学技术使得开发能够保护敏感数据隐私的PPML算法成为可能，同时仍然能够提取有用的见解和知识。图1展示了同态加密在机器学习过程中的概述（Olzak, 2022）。\n🍹差分隐私 差分隐私提供了数据分析中隐私保证的精确规范。它确保在分析结果中不会泄露任何数据集参与者的私人信息。\n$$\rPr [M(D) ∈ S] ≤ exp(ε) Pr [M(D′) ∈ S]\r$$ 在这个方程中，Pr [M(D) ∈ S] 表示应用于数据集 D 的机制 M 输出结果在集合 S 中的概率。ε 参数控制提供的隐私保护程度，其中较小的 ε 值意味着更强的隐私保证。\n为了用一个例子说明这一点，考虑一个公司希望发布其员工平均收入的汇总统计数据，同时保护个人隐私的场景。通过应用差分隐私，公司可以以受控的方式向真实的平均收入添加随机噪声。\n假设员工的真实平均收入为50,000土耳其里拉。如果没有差分隐私，攻击者可能通过将发布的平均收入与外部信息进行比较，来确定某位员工的薪水。然而，通过应用差分隐私，公司向平均收入添加了随机噪声。例如，报告的平均收入可能是50,200土耳其里拉或49,800土耳其里拉，且概率相等。这种额外的随机性确保了攻击者无法根据发布的统计数据可靠地推断任何个别员工的确切收入。\n通过仔细控制添加的噪声量（由 ε 参数确定），差分隐私实现了个人隐私保护与提供准确汇总信息之间的平衡。较小的 ε 值提供更强的隐私保证，但可能引入更多噪声，从而降低发布数据的准确性，而较大的 ε 值可能提供较少的隐私保护，但会产生更准确的结果。如需了解有关差分隐私的更多信息，请参阅 (Dwork, 2006)。\n🍺机器学习的隐私保护技术 鉴于机器学习中使用的模型和算法通常依赖于大量敏感数据，隐私在隐私保护机器学习（PPML）中是一个关键挑战。PPML的目标是创建能够从敏感数据中学习的机器学习模型，而不泄露提供数据的人员的任何敏感信息。在处理应保密的机密信息时，例如财务或个人健康数据，这一点尤其重要。为了确保隐私，机器学习中通常采用多种隐私保护策略。其中一些机制包括数据匿名化、加密和访问限制。数据匿名化是一种在使用数据进行机器学习之前去除个人身份信息的过程。去除姓名、地址和其他可能用于识别特定个人的数据就是一种实现方式。加密则是在数据中进行编码，只有获得适当权限的人才能访问。通过访问控制措施限制谁可以访问数据以及如何访问。这些控制措施对于确保私人信息得到保护以及在整个机器学习过程中保持敏感数据的隐私至关重要。\n除了这些工具，隐私保护机器学习（PPML）还需要在模型推理和训练过程中对数据处理和共享协议进行仔细设计。这些协议的设计方式对防止敏感数据泄露或不当使用至关重要。为了防止重新识别个人，可能会采用差分隐私技术，例如通过对数据进行增强。多个参与方可以通过安全多方计算，在不互相披露个人信息的情况下，共同计算一个函数。数据被分成多个片段，并通过秘密共享在多个参与方之间传输，这样只有授权方才能访问并重新组装原始数据。这些加密方法使得可以创建能够保护敏感数据隐私的机器学习算法，同时仍然能够提取有价值的信息。为了保持对算法的信任，并确保个人的私密信息不被泄露，机器学习中的隐私保护至关重要。\n🥃PPML中的相关工作 在本节中，将讨论学术领域在PPML（隐私保护机器学习）方面的研究工作。我们选择了该领域的重要研究论文，旨在研究它们如何解决研究问题。\n🥤Al-Rubaie, 2019 本文可以被视为开始研究PPML主题的良好资源。作者提供了关于机器学习中隐私面临的威胁和挑战的概述，以及在机器学习模型的训练和推理阶段保护隐私的解决方案和技术。\n首先，他们讨论了在机器学习模型中应用额外对策以增强隐私的重要性。他们展示了几种可能的未经授权访问或滥用共享数据的场景。根据他们的分类，机器学习过程中可以涉及三方：数据拥有者、处理者以及接收结果的人。当这些参与者属于同一方或同一人时，能够实现最高的安全性，然而并非所有情况都是如此。他们提到了五种隐私泄露的威胁等级，这些威胁包括：私密数据的明文存储、重构攻击、模型反演攻击、成员推断攻击和再识别攻击。\n在私密数据的明文存储中，数据在转移到计算部分时以明文形式存储在存储设备中，这可能会影响隐私，尤其是在数据存储被攻破或遭遇不忠诚员工的内部威胁时。专家建议仅发送必要的特征，而不是发送整个数据行，但这种解决方案会导致第二种威胁——重构攻击。获取提取特征的知识或元数据有助于重建原始数据，从而可能导致隐私泄露。\n在模型反演攻击中，攻击者通过使用机器学习模型的输出，生成与用于构建该模型的特征向量相似的向量。这些攻击利用了作为响应返回的置信度数据。知道某个成员或参与者的数据是否被用于训练阶段，会导致第四种威胁——成员推断攻击。为保护参与者在构建数据集过程中的隐私，提出了一种解决方案，即删除与个人身份信息（PII）相关的任何特征或列。然而，根据第五种威胁——再识别攻击，攻击者可以通过将收集到的数据集与其他数据集结合，来恢复这些被遗漏的数据。\n接着，研究讨论了几种保护机器学习隐私的方法和解决方案，包括联邦学习、同态加密、安全多方计算和差分隐私。每种方法都进行了详细阐述，包括其基本原理、优点以及潜在应用。\n研究还涵盖了PPML中的问题和权衡，以及处理敏感数据时出现的伦理和法律问题。这些问题包括对模型准确性、计算时间和通信开销的影响。\n该论文提供了对PPML领域最新进展的全面且易于理解的总结，并强调了该领域需要进一步研究和发展的必要性，以应对机器学习中隐私保护不断变化的风险和需求。\n🧃Liu, Guo, Lam, \u0026amp; Zhao, 2022 本文的作者提出了一种可扩展的隐私保护方案，该方案能够容忍任何参与者在任何时候的掉线。他们在提案中使用了同态加密伪随机生成器（HPRG）和沙密尔秘密共享方案。该提议的解决方案通过避免从客户端构造用于生成掩码的种子，减少了通信开销。此外，在客户端掉线的情况下，也无需像他们在比较中使用的基于SecAgg的方案那样向服务器发送额外的沙密尔共享数据。他们声称，提出的方案在抗掉线恢复能力上比其他解决方案更强。\n$$\rSIM^{U,t,k}_C (x_U, U_1, U_2, U_3) ≡ REAL^{U,t,k}_C (x_U, U_1, U_2, U_3)\r$$$$\rSIM^{U,t,k}_C (x_U, U_1, U_2, U_3) ≡ REAL^{U,t,k}_C (x_U, U_1, U_2, U_3)\r$$ 另外两个理论讨论了防御活跃恶意客户端的安全性。为提供所需的安全性，提出了一种基于 HPRG 的安全聚合协议。第三个定理关注恶意客户端和诚实服务器。对于所有的 U, t, k，其中 |C| \u0026lt; t, $x_{U/C}$ 和 C，使用算法 MC，其中 C ⊆ U，所提出的协议是一个计算 FHSecAgg 的安全协议。最后一个定理涉及包括服务器在内的恶意客户端的安全性。\n为了理解这些理论，我们需要了解其中术语的含义。$U$ 表示一组具有本地训练模型 $x_U$ 的客户端。$S$ 表示模型的服务器端。这两种理论中的等式表示两边分布上的恒等关系。关于所提出协议的概述，私有输入可以总结为每个客户端的本地训练模型或梯度，它们可以表示为向量、用于初始化秘密密钥的密钥、以及签名的秘密密钥。在服务器端，存储了用于验证秘密通道和签名的密钥。另一方面，协议的公共输入包括客户端的数量、丢失客户端的阈值和签名过程的公钥。本地训练模型的聚合结果可以视为协议的输出。图 2 显示了协议的步骤，并对每个步骤进行了简要描述。\n文章中进行了若干测试，证明了他们所提到的假设。\n🧉Gupta \u0026amp; Singh, 2022 在本文中，作者提出了一种基于云的PPML模型，结合了差分隐私和机器学习模型。该模型明确了不可信方之间的通信协议。进行的实验包括朴素贝叶斯（NB）分类器，并应用于多个数据集，最终达到了95%的准确率，超过了已有文献中的结果。图3展示了所提模型，该模型被称为基于数据和分类服务的差分隐私保护机器学习模型（DA-PMLM）。\n从图3中，可以看到该架构中有四个主要参与者：数据所有者（DOid），即云客户端，是数据的创建者，并愿意将其共享到云端；分类器所有者（CO），是负责向云服务提供商（CSP）提供分类服务的一方，CSP 进一步向DOid提供服务；最后是请求用户（RUid），即从CSP请求所有者数据的实体。数据所有者在共享其私人数据之前，通过差分隐私向数据中注入噪声。\n他们证明了以下理论：“在所提出的模型中，分类模型的隐私保护机制满足ε-差分隐私的并行组合。”\n图4展示了所提模型的分类和数据流。\n在接收到带有噪声的数据 $D_{N1}^1, D_{N2}^2, \\dots, D_{Nn}^n$ 来自相应的数据所有者（DOs）后，云服务提供商（CSP）通过应用标准化函数（使用Z-score标准化公式）对其进行预处理。得到的值可以用符号 $D_{\\hat{N_i}}^i$ 表示。与任何机器学习过程类似，给定的数据集被划分为训练部分和测试部分。之后，进行分类模型（CM）的训练和测试，在实验中使用了朴素贝叶斯（NB）分类器，以进行评估测量。\n🧊Lee, et al., 2022 本文的作者声称，当前的PPML解决方案仅限于非标准的机器学习模型。这些解决方案在实际数据集上未能证明取得良好的结果。此外，所使用的激活函数从算术角度来看较为简单，并且替换了非算术激活函数。它们还避免使用自助法（bootstrapping）。当前的解决方案中也无法实现大量层数，他们的提议包括使用标准的ResNet-20模型与RNS-CKKS全同态加密（FHE）结合自助法进行实现。为了验证他们的模型，他们将其应用于一个标准化的数据集——CIFAR-10。所得结果与使用原始ResNet-20模型且未加密时的结果非常接近，最终的准确率约为92.43%。\n🍻PPML中的挑战 PPML面临着若干挑战，这些挑战阻碍了其在实际应用中的广泛采用。主要的挑战之一是隐私与效用之间的权衡。像差分隐私和安全多方计算这样的PPML技术通常会引入噪声或通信开销，这可能显著影响机器学习模型的准确性和效率。另一个挑战是不同PPML技术之间缺乏标准化和互操作性，使得比较和结合不同方法的结果变得困难。此外，PPML需要专门的专业知识和资源，如加密学和安全计算的知识，这些可能并不容易为所有组织所拥有。最后，PPML还涉及法律和伦理问题，如合规性和透明度，这些问题必须仔细处理，以确保敏感数据的负责任和可信赖的使用。\n🥂结论与未来工作 敏感数据的隐私必须得到保护，而隐私保护机器学习（PPML）这一研究领域正迅速发展。随着机器学习在医疗、银行和社交媒体等行业的广泛应用，隐私保护系统的重要性愈加突出。为了确保人们的敏感信息不被泄露，能够在尊重隐私的前提下支持机器学习的隐私保护算法和协议至关重要。该领域的研究进展涉及信息理论、机器学习和密码学等计算机科学多个分支，依赖于跨学科的合作。PPML是一个具有挑战性且复杂的问题，但它是实现机器学习带来益处的前提，而不牺牲个人隐私。在本次综述中，我们概述了PPML的主要难题和未来的研究方向，并回顾了PPML中使用的主要方法和技术。为了克服该领域中的重大问题，我们希望本综述能够成为对PPML感兴趣的研究人员和实践者的重要参考资源。未来的研究工作将包括在真实数据上的PPML研究，并与传统机器学习模型的表现进行比较。\n","date":"2024-11-05T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/leaves-4574860_1280_hu3544453875173869001.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/","title":"安全与隐私机器学习：技术与应用的综述"},{"content":"Differential Privacy: A Survey of Results 差分隐私：研究结果综述\n摘要。 在过去的五年中，一种新的隐私保护数据分析方法取得了成果。这种方法与统计学、数据库、理论和密码学领域的许多相关文献（但并非全部）不同之处在于，它定义了一个形式化的、普遍适用的隐私保障，并且所提出的数据分析技术被严格证明满足这一保障。出现的关键隐私保障是差分隐私。粗略来说，这确保了（几乎可以量化地）通过加入一个统计数据库而不会承担风险。在本次综述中，我们回顾了差分隐私的定义以及实现它的两种基本技术。然后，我们展示了这些技术的一些有趣应用，提出了三个具体任务的算法和关于差分隐私学习的三个一般结果。\n🍦1. Introduction 隐私保护的数据分析也被称为统计披露控制、推断控制、隐私保护数据挖掘和私密数据分析。我们的主要动机场景是统计数据库。统计量是从样本中计算出的数量。假设一个受信任且值得信赖的管理者从大量受访者（样本）中收集敏感信息，目的是学习（并向公众发布）关于潜在群体的统计事实。问题在于如何在不侵犯个别受访者隐私的情况下发布统计信息。有两种设置：在非交互式设置中，管理者计算并发布一些统计信息，数据不会再被进一步使用。隐私问题可能会影响管理者发布的精确答案，甚至影响发布的统计量的集合。请注意，由于数据不会再被使用，管理者可以在统计信息发布后销毁数据（和自己）。在交互式设置中，管理者处于用户和数据库之间。用户提出的查询和/或对这些查询的响应，可能会被管理者修改，以保护受访者的隐私。数据不能被销毁，管理者必须在数据库的整个生命周期中保持在场。当然，任何交互式解决方案都能产生一个非交互式解决方案，前提是查询在此之前已知：管理者可以模拟一个互动，其中这些已知查询被提出，并发布结果记录。关于这个问题的文献丰富，主要来自统计学界（见，例如，[10, 14, 27, 28, 29, 38, 40, 26, 39]以及有关表格数据的受控发布、列联表和单元抑制的文献），也来自计算机科学的各种分支，如算法、数据库理论和密码学，例如在[3, 4, 21, 22, 23, 33, 34, 41, 48]，[1, 24, 25, 31]，以及[6, 9, 11, 12, 13, 18, 7, 19]中；有关1989年前领域总结的文献，见调查[2]。本次综述关于差分隐私。粗略来说，差分隐私确保删除或添加单个数据库项不会（实质性地）影响任何分析的结果。因此，加入数据库不会产生风险，提供了一种在分布信息可能导致泄露的情况下进行严格数学处理的方法。我们将首先描述三种针对特定、不相关的数据分析任务的差分隐私算法。然后，我们将提出三条关于当需要保护个别数据项隐私时的计算学习的一般结果。这在学习理论文献中通常不是一个关注点，标志着一条新研究方向的出现。\n🍧2.Differential Privacy 在后续中，随机化函数 K 是管理者在发布信息时应用的算法。因此，输入是数据集，输出是发布的信息或记录。我们不需要区分交互式和非交互式设置。可以将数据库视为一组行。如果数据库 D1 和 D2 仅在一个元素上有所不同，我们称其为 D1 和 D2 的关系：一个是另一个的真子集，而较大的数据库仅包含一行附加的记录。\n$$\r\\Pr[K(D_1) \\in S] \\leq \\exp(ε) \\times \\Pr[K(D_2) \\in S]\r$$ 所涉及的概率是基于 K 的抛硬币过程。\n满足此定义的机制 K 解决了任何参与者对个人信息泄露的担忧：即使参与者从数据集中删除了她的数据，输出（因此输出的后果）也不会变得显著更可能或更不可能。例如，如果数据库被保险提供者查询，以决定是否为某个特定个体投保，那么该个体数据在数据库中的存在或缺失不会显著影响她获得保险的机会。因此，差分隐私是一种全面的保障。这也是一种非常强的保障，因为它是关于机制行为的统计特性，因此独立于对手/用户的计算能力和附加信息。差分隐私并不是隐私的绝对保障。事实上，Dwork 和 Naor 已经证明，任何具有非平凡效用的统计数据库都会妥协隐私的自然定义。然而，在一个已决定某些数据库的好处大于其成本的社会中，差分隐私确保参与这些社会有益的数据库仅会承担有限的额外风险。\n定义 1 中的参数是公开的。这个参数的选择基本上是一个社会性的问题，超出了本文的讨论范围。也就是说，我们倾向于将其视为，比如，0.01、0.1，或者在某些情况下，ln 2 或 ln 3。如果某个不良事件发生的概率非常小，可能可以接受将其增加 2 或 3 倍；而如果这个概率已经被认为接近不可接受，那么增加 $e^{0.01} ≈ 1.01$ 可能是可以容忍的，而增加 e，甚至仅仅是 $e^{0.1}$，可能就是不可接受的。\n定义 1 讨论了机制 K 的行为，并独立于对手或用户可能拥有的任何附加知识。因此，满足该定义的机制在保护数据库中个别行的隐私时，即使对手知道数据库中的其他所有行，也能做到这一点。\n定义 1 也可以扩展到群体隐私（以及个体对数据库贡献多于一行的情况）。一组 c 个参与者可能担心他们的集体数据泄露信息，即使单个参与者的数据不会。根据这个定义，我们可以将任何概率的扩展限制在最多 exp(c)，对于小的 c，这可能是可以容忍的。当然，统计数据库的目的是披露有关大型群体的汇总信息（同时保护个体隐私），因此我们应该预期随着群体规模的增加，隐私界限会逐渐减弱。\n🍨3.Achieving Differential Privacy in Statistical Databases 在统计数据库中实现差分隐私\n我们将介绍一种由 Dwork、McSherry、Nissim 和 Smith 提出的交互机制 K，适用于连续值查询的情况。在本节中，查询是一个将数据库映射到（向量形式的）实数的函数。例如，查询“计数 P”统计数据库中具有属性 P 的行数。当查询为函数 f，而数据库为 X 时，真实答案是值 f(X)。机制 K 向真实答案添加适当选择的随机噪声，以生成我们所称的响应。通过用真实答案的嘈杂版本进行回应来保护隐私的想法并不新鲜，但这种方法非常微妙。例如，如果噪声是关于原点对称的，并且同样的问题被多次询问，则响应可能会被平均，从而抵消噪声。我们必须考虑这些因素。\n$$\rΔf = \\underset{\\text{ D1, D2}}{\\max} \\| f(D_1) - f(D_2) \\|_1\r$$ 对于所有D1和D2，它们最多只在一个元素上不同。\n特别地，当 k = 1 时，函数 f 的敏感性是两个仅在一个元素上不同的数据库所可能取的值之间的最大差异。对于许多类型的查询，Δf 会非常小。特别是，上述讨论的简单计数查询（‘有多少行具有属性 P?’）的 Δf = 1。我们的技术在 Δf 较小时效果最佳——引入最少的噪声。请注意，敏感性仅是函数的属性，与数据库无关。敏感性本质上捕捉了需要由管理员生成的加性噪声隐藏的差异程度（即在两个仅在一个元素上不同的数据库上，f 的值之间的差异）。\n标准差为$\\sqrt2Δf /ε$的缩放对称指数分布，记作$\\text{Lap}(\\Delta f / \\epsilon)$ ，其在 \\(x\\) 处的质量与 $\\exp(-|x|(\\epsilon/\\Delta f))$成正比。更准确地说，令$b = \\Delta f/\\epsilon$ 。概率密度函数为\n$$\rp(x) = \\frac{1}{2b} \\exp\\left(-\\frac{|x|}{b}\\right)\r$$累积分布函数为\n$$\rD(x) = \\frac{1}{2}\\left(1 + \\text{sgn}(x)\\left(1 - \\exp\\left(\\frac{|x|}{b}\\right)\\right)\\right) $$在查询函数 \\(f\\) 上，隐私机制 \\(K\\) 的响应为\n$$\rf(X) + (\\text{Lap}(\\Delta f/\\epsilon))^k\r$$即独立地向 $f(x)$的每个$k$ 个分量添加分布为 $\\text{Lap}(\\Delta f / \\epsilon)$的噪声。注意，减少已知的公共参数 $\\epsilon$ 会使 $\\text{Lap}(\\Delta f / \\epsilon)$曲线变平，从而产生更大的期望噪声幅度。当 $\\epsilon$ 固定时，具有高敏感性的函数 f 会产生更平坦的曲线，同样导致更高的期望噪声幅度。\n$$\r\\exp(-|f(D_1) - r|\\frac{\\Delta f}{\\epsilon})\r$$成正比，$D_2$亦然。应用三角不等式，我们得到的比值最多为\n$$\r\\exp\\left(-|f(D_1) - f(D_2)|\\frac{\\Delta f}{\\epsilon}\\right)\r$$根据敏感性的定义，$|f(D_1) - f(D_2)| \\leq \\Delta f$，因此比值被限制为\n$$\r\\exp(-\\epsilon)\r$$从而实现了$\\epsilon$差分隐私。显而易见，通过对每个查询运行 $K$，使噪声分布为\n$$\r\\text{Lap}\\left(\\sum_{i} \\Delta f_i/ \\epsilon\\right)\r$$可以为任何（自适应选择的）查询序列 $f_1, \\ldots, f_d$实现 $\\epsilon$差分隐私。换句话说，每个答案的质量随着查询的敏感性总和而下降。有趣的是，有时可以做到比这更好。大致来说，重要的是\n$$\r\\Delta = ||(f_1(D_1), f_2(D_1), \\ldots, f_d(D_1)) - (f_1(D_2), f_2(D_2), \\ldots, f_d(D_2))||_1 $$的最大可能值。由于查询的潜在自适应选择，陈述的精确表述需要一些谨慎。有关全面的处理，请参见 [19]。我们在此对非自适应情况声明定理，视查询序列$f_1, f_2, \\ldots, f_d$（具有各自的元数 $k_1, \\ldots, k_d$）为一个单一的 $k = \\sum_{i=1}^{d} k_i$元查询 $f$，并回顾对于任意$k$的定义 2。\n定理 1 ([19])：对于 $f : D \\to \\mathbb{R}^k$，机制 $K_f$ 向每个输出项独立添加服从分布 $\\text{Lap}\\left(\\frac{\\Delta f}{\\epsilon}\\right)$ 的噪声，具有 $\\epsilon$-差分隐私。\n机制 $K$ 的上述描述在对不敏感查询时具有出色的准确性。特别地，确保差分隐私所需的噪声仅依赖于函数的敏感性和参数 $\\epsilon$。这两者都与数据库的内容和行数无关。因此，如果数据库非常大，由差分隐私机制引入的许多典型查询的错误相对较小。我们可以将 $K$ 看作分析师与数据之间的差分隐私保护接口。这提示了一种隐私保护数据分析的一般方法：寻找需要少量不敏感查询的算法。参见例如 [7, 8, 32]。事实上，甚至计数查询也极其强大，允许对许多标准数据挖掘任务进行准确和差分隐私计算，包括主成分分析、$k$-均值聚类、分离超平面的感知机学习以及生成 ID3 决策树 [7]，以及（邻近的）半空间学习 [8]（见下面的第 4.3 节）。在定理 1 的许多应用中，尤其引人关注的是直方图查询类。直方图查询是将数据库行的域任意划分为不相交的“单元”，其真实答案是描述每个单元中数据库行数的计数集。虽然具有 $k$ 个单元的直方图查询可以视为 $k$ 个独立的计数查询，但添加或删除单个数据库行最多只能影响整个 $k$ 元组计数的一个位置（即与添加（删除）行的单元对应的计数）；此外，该单元的计数最多受到 1 的影响，因此根据定义 2，每个直方图查询的敏感性为 1。许多数据分析实际上就是直方图；因此，令人鼓舞的是，复杂的直方图并不需要每个单元中有大方差，而是需要非常小的方差。\n🍩3.1 当噪声毫无意义时 在某些任务中，添加噪声毫无意义。例如，函数 $f$ 可能将数据库映射到字符串、策略或树。在最近的一篇论文中，McSherry 和 Talwar 解决了在保持 $\\epsilon$-差分隐私的同时优化此类函数输出的问题 [35]。假设策展人持有一个数据库 $X$，目标是生成一个对象 $y$。简而言之，他们的指数机制工作如下：假设存在一个效用函数 $u(X, y)$，用于衡量在数据库为 $X$ 时输出 $y$ 的质量。例如，如果数据库保存了个人在拍卖中为数字商品所评估的价值，那么当价格设为 $y$ 时，$u(X, y)$ 可能是这些评估的收入。拍卖是噪声毫无意义的一个很好的例子，因为稍微定得高一点的价格可能会阻止许多投标者购买。McSherry 和 Talwar 的指数机制以与 $ \\exp(- u(X, y)/2)$ 成比例的概率输出 $y$。这确保了 $\\Delta u$-差分隐私，或在 $\\Delta u \\leq 1$ 时的 $\\epsilon$-差分隐私。这里，$\\Delta u$ 的定义与之前略有不同；它是通过改变单行数据而引起的 $u$ 值的最大可能变化（与添加或删除一行数据不同；这两者的差异最多为二的倍数）；参见 [35]。通过这种方法，McSherry 和 Talwar 得到近似真实的拍卖，并实现几乎最优的售价。粗略来说，这表明参与者不能通过虚报其评估值来大幅降低其支付的价格。有趣的是，他们展示了差分隐私的简单组合可以用于获取拍卖，在这些拍卖中，任何 $c$ 个代理的合作组都不能通过提交除真实评估以外的出价显著提高其效用。这类似于上述备注 1 的情况，在该情况下，组合被用来为 $c$ 个个体的群体获取隐私。\n🍪4 特定任务的算法 在本节中，我们描述了针对三个不相关任务的差分隐私算法。\n🎂4.1 统计数据推断 本节的结果来自 Dwork 和 Nissim [18]。考虑一个场景，其中数据库中的每个元素由一组 $k$ 个布尔属性 $\\alpha_1, \\ldots, \\alpha_k$ 描述，且行是从某个底层分布中独立采样得到的，分布在 ${0, 1}^k$ 上。设 $1 \\leq t \\leq \\frac{k}{2}$ 为整数。这里的目标是利用关于任何属性值组合的发生率的信息，以学习任意两个属性值组合的发生率。尽管我们使用“查询”一词，但这些查询在此之前都是已知的，且机制将是非交互式的。从大约 $\\binom{k}{2}$ 个发布的信息中，可以计算所有 $\\binom{k}{2} \\cdot 2^2$ 个小项的发生率的近似值。这将允许数据分析师近似计算所有 $2^{\\binom{k}{2}}$ 个长度为 2 的二元小项的概率，前提是初始近似值足够准确。我们将概率与发生率等同，因此概率空间是在数据库中的行上。固定任意一组属性。这些属性值的所有可能设置的发生率可以通过具有 $2$ 个单元的直方图来描述，直方图的敏感性为 $1$，因此我们将处理一个整体敏感性与 $O(k)$ 成比例的查询序列（实际上，它会比这个差一个因子 $t$，下面会讨论）。设 $\\alpha$ 和 $\\beta$ 为属性。如果在概率上 $\\alpha$ 蕴含 $\\beta$，则表示在给定 $\\alpha$ 的情况下 $\\beta$ 的条件概率超过 $\\beta$ 的无条件概率。测量概率中的蕴含能力对数据挖掘至关重要。注意，由于 $\\Pr[\\beta]$ 可以通过计数查询简单地估计，因此测量概率中的蕴含问题归结为获得 $\\Pr[\\beta | \\alpha]$ 的良好估计。此外，一旦我们可以估计 $\\Pr[\\beta | \\alpha]$、$\\Pr[\\beta]$ 和 $\\Pr[\\alpha]$，就可以使用贝叶斯规则和德摩根定律来确定任何布尔函数的统计数据。例如，$\\Pr[\\alpha \\land \\beta] = \\Pr[\\alpha] \\Pr[\\beta | \\alpha]$，因此如果我们对两个乘数的估计在加性 $\\eta$ 内，我们就得到了乘积的估计，其准确度在 $3\\eta$ 内。作为朝向非交互式解决方案的第一步，考虑交互式情况，假设我们对 $\\Pr[\\alpha]$ 和 $\\Pr[\\beta]$ 有良好的估计。确定 $\\Pr[\\beta | \\alpha]$ 的关键是找到一个 $\\alpha$ 的重集，即一个集合 $q \\subseteq [n]$，使得 $\\alpha$ 的发生率至少比预期高出一个标准差，然后确定该重集上 $\\beta$ 的发生率是否高于 $\\beta$ 的整体发生率。更具体地，可以测试该条件发生率是否高于给定阈值，然后使用二分搜索找到“正确”的阈值。找到重集很容易，因为随机选择的 $[n]$ 的子集在超过预期的 $\\alpha$ 的发生率至少一个标准差的概率是常数。为了“模拟”交互式情况，策展人选择一些随机子集，并为每个子集发布（带噪声的）关于该子集中 $\\alpha$ 和 $\\beta$ 的发生率的估计。以高概率（取决于 $t$），至少有一个子集对 $\\alpha$ 是重的。将所有部分结合起来：对于 $t$ 个随机子集，策展人发布所有 $m = \\binom{k}{2}$ 个字面联结的发生率的良好近似值。具体而言，我们要求以至少 $1 - \\frac{\\delta}{m^2}$ 的概率计算的条件概率 $\\Pr[\\alpha | \\beta]$ 的准确度在 $\\frac{\\eta}{3m^2}$ 内，其中 $\\alpha$ 和 $\\beta$ 现在是字面的最小项。这确保了以至少 $1 - \\delta$ 的概率，所有计算的条件概率在 $\\frac{\\eta}{3m^2}$ 内准确，因此所有长度为 $2$ 的最小项的估计概率在 $\\frac{\\eta}{m^2}$ 内准确。数字 $t$ 相当大，并且依赖于许多因素，包括差分隐私参数以及 $\\eta$、$\\delta$、$k$ 和 $t$。文献 [18] 的分析表明，当 $\\eta$ 和 $\\delta$ 是常数时，这种方法将查询数量从 $O\\left(\\binom{k}{2}\\right)$（每个 2 元组的一个直方图）减少到 $O(24 k^2 \\log k)$。注意有趣的权衡：我们需要依赖于 $m^2$ 的准确度以避免进行 $m^2$ 次查询。当数据库足够大时，这种权衡是可以实现的。\n🍰4.2 contingency 表的发布 本节的结果来自 Barak、Chaudhuri、Dwork、Kale、McSherry 和 Talwar [5]。 contingency 表是一个计数表。在人口普查或其他调查的背景下，我们将个人的数据视为数据库中的一行。我们不假设行是相互独立的。目前，每一行由 $k$ 位描述 $k$ 个二元属性 $a_1, \\ldots, a_k$ 的值。严格来说，contingency 表是一个 $\\mathbb{R}^{2^k}$ 中的向量，描述对于 $k$ 个属性的每种设置，数据库中具有该属性值设置的行数。换句话说，它是一个具有 $2^k$ 个单元的直方图。通常情况下，contingency 表本身不会被发布，因为当 $k$ 较大时，它可能会很稀疏。相反，对于不同的属性子集，数据策展人会发布 contingency 表在每个子集上的投影，即每个可能设置的计数，这些较小的计数表称为边际，每个边际由属性的子集命名。由 $j$ 个属性（$j \\leq k$）命名的边际称为 $j$-way 边际。数据策展人通常会发布许多低阶边际集合，以揭示许多不同且可能重叠的属性集合之间的关联。由于 contingency 表是一个直方图，我们可以向 contingency 表的每个单元添加与 $-1$ 成比例的独立生成的噪声，以获得一个差分隐私的（非整数且不一定非负的）表。我们稍后将讨论完整性和非负性的问题。目前，我们只需注意，从这个带噪声的表中可以直接计算任何所需的边际，并且不同边际之间的一致性是显而易见的。然而，这种方法的一个缺点是，尽管 contingency 表的每个单元中的噪声相对较小，但计算出的边际中的噪声可能较大。例如，描述属性 $a_1$ 的 1-way 表的方差为 $2^{k-1} - 2$。我们认为这不可接受，尤其是当 $n \\gg 2^k$ 时。边际也是直方图。第二种方法，在低阶边际的（常见）情况下噪声较小，但不提供边际之间的一致性，其工作原理如下。设 $C$ 为要发布的边际集合。我们可以考虑一个函数 $f$，当应用于数据库时，生成所需的边际。现在，应用定理 1，选择这个 $f$（对每个表中的每个单元独立添加噪声），其敏感性 $\\Delta f = |C|$。当 $n$（数据库中的行数）与 $|C|/\\epsilon$ 相比很大时，这也会产生良好的准确性。因此，如果独立随机化每个（表中每个）单元导致的小表与表之间的不一致不成问题，并且用户能接受偶尔出现负值和通常非整数的单元计数，那么我们就可以完成这个过程。我们对这些隐私增强技术的产物——不一致性、负值和非整数——没有哲学或数学上的反对，但在实践中，它们可能会造成问题。例如，单元计数可能作为其他程序（可能是现成的程序）的输入，而这些程序预期正整数，从而导致类型不匹配。不一致性，更不用说负值，可能会让普通用户感到困惑，例如美国 FactFinder 网站的普通用户。接下来，我们概述 Barak 等人的主要工作步骤 [5]。\n移动到傅里叶域。当添加噪声时，两个自然的解决方案出现了：向源表的条目添加噪声（这是我们的第一个提议；当 $k$ 较大时，准确性较差），或向报告的边际添加噪声（我们的第二个提议；不一致性受到影响）。第三种方法是将数据转换到傅里叶域。这只是一个基底的变化。如果我们计算所有 $2^k$ 的傅里叶系数，我们将获得整个一致性表的非冗余编码。如果我们对傅里叶系数进行扰动，然后再转换回 contingency 表域，我们将得到一个（不同的，可能是非整数的，可能是负的）contingency 表，其“距离”（例如，$l_2$ 距离）与原始表的差距由扰动的大小决定。转到傅里叶域的优点是，如果只需要一组边际 $C$，那么我们不需要完整的傅里叶系数集。例如，如果 $C$ 是所有 3-way 边际的集合，那么我们只需要权重最多为 3 的傅里叶系数，数量为 $\\binom{k}{3} + \\binom{k}{2} + k + 1$。这将转化为一个噪声更小的边际集合。用于计算边际 $C$ 的傅里叶系数形成了数据集的一个模型，捕获了从边际集 $C$ 中可以学习的所有信息。按照定理 1 的指示对这些系数添加噪声，然后再转换回 contingency 表域，得到一个生成合成数据集的程序，确保差分隐私，同时在很大程度上（且可测量地）捕获模型中的信息。这是一个生成具有可证明差分隐私的合成数据的具体方法示例。傅里叶系数准确地描述了边际所需的信息。通过准确测量所需的信息，Barak 等人使用 [19] 的技术添加尽可能少的噪声。此外，傅里叶基底由于根据属性值集的自然分解而特别有吸引力。通过注意到计算低阶边际时不需要额外的傅里叶系数，并且使用更少的噪声系数，可以在给定边际的子边际（即，低阶边际）上施加比定理 4 中更紧的界限，从而通过减少方差提高准确性。使用线性规划和舍入。Barak 等人 [5] 使用线性规划来获得一个非负的但可能是非整数的数据集，具有（几乎）给定的傅里叶系数，然后对结果进行舍入以获得整数解。有趣的是，从线性程序获得的边际与噪声测量的边际之间的“距离”并不比原始数据的真实边际更远（在 [5] 中被精确定义）。因此，通过一致性的施加所引入的额外误差不超过隐私机制本身所引入的误差。\n🧁符号和预备知识。 回想一下，设 $k$ 表示（布尔）属性的数量，我们可以将数据集视为一个向量 $x \\in \\mathbb{R}^{2^k}$，以属性元组为索引。对于每个 $\\alpha \\in {0, 1}^k$，数量 $x_\\alpha$ 是具有该属性设置的数据元素的数量。我们令 $n = |x|_1$ 表示数据集中元组或行的总数。对于任意 $\\alpha \\in {0, 1}^k$，我们使用 $|\\alpha|_1$ 表示非零位置的数量。我们写作 $\\beta \\preceq \\alpha$，表示 $\\alpha, \\beta \\in {0, 1}^k$，如果 $\\alpha$ 中的每个零位置在 $\\beta$ 中也是零。\n🥧边际算子 $$\r(C_\\alpha(x))_\\beta = \\sum_{\\gamma : \\gamma \\land \\alpha = \\beta} x_\\gamma \\tag{3}\r$$注意，算子 $C_\\alpha$ 对于所有 $\\alpha$ 是线性的。\n定理 2. ${f_\\alpha}$ 形式构成了 $\\mathbb{R}^{2^k}$ 的正交归一基。因此，可以将任何边际写成与相关的傅里叶系数的小和：\n$$\rC_\\beta x = \\sum_{\\alpha \\preceq \\beta} \\langle f_\\alpha, x \\rangle C_\\beta f_\\alpha. \\tag{4}\r$$系数 $\\langle f_\\alpha, x \\rangle$ 是从 $x$ 计算 $C_\\beta x$ 的必要和充分数据。\n定理 3 ([5]). 设 $B \\subseteq {0, 1}^k$ 描述一组傅里叶基向量。释放集合 $\\phi_\\beta = \\langle f_\\beta, x \\rangle + \\text{Lap}(|B|/2^{k/2})$ 对于 $\\beta \\in B$ 保护 $\\epsilon$-差分隐私。\n证明：每个元组对每个输出坐标贡献正好是 ±$1/2^{k/2}$，因此该 $|B|$ 个输出的 $L_1$ 灵敏度至多为 $|B|/2^{k/2}$。根据定理 1，添加标准差为 $|B|/2^{k/2}$ 的对称指数噪声给出 $\\epsilon$-差分隐私。\n备注：为了理解规模，我们可以通过随机添加或删除 $|B|2^/$ 个数据集中的个体来实现对每个坐标的类似扰动，这个数量可能远小于 $n$。\n将步骤整合在一起。为了计算一组边际分布 A，我们需要所有在 A 的向下闭包下的 Fourier 系数 fβ。边际分布（A ⊆ {0, 1}^k，D）的步骤如下：\n令 B 为 A 在向下闭包下的集合。 对于 β ∈ B，计算 $$ \\phi_\\beta = \\langle f_\\beta, D \\rangle + \\text{Lap}\\left(\\frac{|B|}{2^{k/2}}\\right) $$。 在以下线性规划中求解 wα，并四舍五入到最接近的整数权重 w\u0026rsquo;α： 最小化 $$ b $$ 约束条件： $$ w_\\alpha \\geq 0 \\quad \\forall \\alpha $$ $$ \\phi_\\beta - \\sum_\\alpha w_\\alpha f_\\beta^\\alpha \\leq b \\quad \\forall \\beta \\in B $$ $$ \\phi_\\beta - \\sum_\\alpha w_\\alpha f_\\beta^\\alpha \\geq -b \\quad \\forall \\beta \\in B $$ 使用列联表 w\u0026rsquo;α，计算并返回边际分布 A。 定理 4 ([5])。使用边际分布 $\\text{Marginals}(A) $的记号，以概率 $1 - \\delta $，对于所有 $\\alpha \\in A$ ，有\n$$\r\\|C_\\alpha x - C_\\alpha w'\\|_1 \\leq 2\\|\\alpha\\|_1 \\frac{2|B| \\log(|B|/\\delta)}{ + |B|}. \\quad (5)\r$$当 \\( k \\) 较大时，线性规划的时间复杂度是 $O(2^k) $。当 \\( k \\) 很大时，这并不令人满意。然而，令人惊讶的是，通过在回到数据域之前向第一个 Fourier 系数添加相对较小的量，可以实现非负性（但不是整数性）。不需要线性规划，并且引入的误差非常小。因此，如果在 $O(2^k)$ 上的多项式时间开销不可接受，而可以接受非整数性，那么该方法效果很好。我们注意到，在该工作的初步实现中，非整数性并不是问题，因为计数总是被转换为百分比。\n🍫4.3 学习（邻近）半空间 我们以一个例子结束本节，该例子受到学习理论中问题的启发，出现在Blum、Ligett和Roth即将发表的论文中[8]。其目标是提供一种对半空间查询的非交互式解决方案。从高层次来看，他们的方法是发布信息，以（近似）回答一大组“规范”查询，保证对于任何（可能是非规范的）该类型的查询，都存在一个“邻近”的规范查询。因此，数据分析师可以获得与感兴趣的查询在某种意义上接近的答案。在[8]中，查询是$\\mathbb{R}^d$中的半空间查询，具体定义如下。在整个这一节中，我们采用[8]中的假设，即数据库点已缩放到单位球体上。\n定义 3。给定一个数据库$D \\subset \\mathbb{R}^d$ 和单位长度向量 $y \\in \\mathbb{R}^d$，半空间查询$H_y$ 定义为\n$$\rH_y(D) = \\frac{|\\{x \\in D : \\sum_{i=1}^{d} x_i \\cdot y_i \\geq 0\\}|}{|D|}.\r$$注意，半空间查询可以通过两个计数查询来估计：“$|D|$ 是多少？”和“$|{x \\in D : \\sum_{i=1}^{d} x_i \\cdot y_i \\geq 0}|$是多少？”因此，半空间查询的灵敏度最多为 2。半空间查询$H_{y_1} 和 H_{y_2}$ 之间的距离定义为它们之间角度的正弦，记作$\\sin(y_1, y_2) $ 。考虑到这一点，Blum、Ligett 和 Roth 的算法确保了以下效用概念：\n定义 4 ([8])。数据库机制 \\( A \\) 对于某个度量 \\( d \\) 下的查询类 \\( C \\) 是 )-有用的，如果以概率$1 - \\delta$，对于每个$Q \\in C$和每个数据库 \\( D \\)，都有\n$$\r|Q(A(D)) - Q'(D)| \\leq \\epsilon\r$$对于某个$Q\u0026rsquo; \\in C$ 满足$d(Q, Q\u0026rsquo;) \\leq \\gamma$。注意，接近的是查询，而不是（不一定是）它们的答案。给定一个半空间查询$H_{y_1} $ ，下面的算法将输出一个值 \\( v \\)，使得\n$$\r|v - H_{y_2}(D)| \u003c \\epsilon\r$$对于某个与$H_{y_1}$ $\\gamma$ -接近的$H_{y_2}$。等价地，算法任意计数或不计数满足$\\cos(x, y_1) \\leq \\gamma$的点$ x \\in D$ 。Blum 等人指出，$\\gamma$的作用类似于机器学习中的边际概念，并且即使$H_{y_1}$ 和 $H_{y_2}$是$\\gamma$-接近的，也并不意味着查询$H_{y_1}(D)$和$H_{y_2}(D)$ 的真实答案是接近的，除非大多数数据点位于$H_{y_1}$和 $H_{y_2}$ 的$\\gamma$ 边际之外。\n定义 5 ([8])。一个半空间查询$H_y$是$b$ -离散化的，如果对于每个$i \\in [d]$，$ y_i$ 可以用 \\( b \\) 位来表示。令$C_b$ 为所有 \\( b \\)-离散化半空间在$\\mathbb{R}^d$ 中的集合。\n考虑一个特定的 \\( k \u003c d \\) 维子空间，该子空间由一个随机的 $d \\times k$矩阵 \\( M \\) 定义，其元素独立且均匀地从 \\(\\{-1, 1\\}\\) 中选择。考虑投影\n$$\rP_M(x) = \\frac{1}{\\sqrt{k}} x \\cdot M,\r$$它将数据库点投影到子空间，并将它们重新缩放到单位球体。对于半空间查询$H_y$ ，投影$P_M(H_y)$ 只是由投影$P_M(y)$ 定义的 \\( k \\) 维半空间查询。关键的事实是，对于随机选择的 \\( M \\)，将数据库点 \\( x \\) 和半空间查询指定符 \\( y \\) 投影到一起，极不可能显著改变它们之间的角度：\n定理 5（约翰逊-林登斯特劳斯定理）。考虑将点 \\( x \\) 和半空间$H_y$ 投影到由投影矩阵 \\( M \\) 定义的随机 \\( k \\) 维子空间中。则有\n$$\r\\Pr\\left[| \\cos(x, H_y) - \\cos(P_M(x), H_{P_M(y)})| \\geq \\frac{\\gamma}{4}\\right] \\leq 2e^{-\\left(\\frac{\\gamma}{16}\\right)^2 - \\left(\\frac{\\gamma}{16}\\right)^3} \\frac{k}{4}.\r$$子空间的维度 \\( k \\) 被选择为使得投影一个点和一个半空间的角度变化超过$ \\frac{\\gamma}{4}$的概率最多为$\\frac{1}{4}$ 。这导致\n$$\rk \\geq \\frac{4 \\ln(8/\\delta)}{\\left(\\frac{\\gamma}{16}\\right)^2 - \\left(\\frac{\\gamma}{16}\\right)^3}.\r$$因此，查询$H_y$ \\( \\) 的答案可以通过对投影半空间查询的隐私保护估计来估计，整体准确性可以通过选择 \\( m \\) 个投影矩阵来提高； \\( x \\) 和 \\( y \\) 之间的角度将通过 \\( m \\) 组投影结果的中位数来估计。当然，如果目标只是响应少量半空间查询，经过投影过程就没有意义，更不用说进行多次投影了。但文献[8]的目标更为雄心勃勃：为（非离散化的）半空间查询提供一个$(\\epsilon, \\delta, \\gamma)$-有用的非交互机制；这正是低维度发挥作用的地方。算法选择 \\( m \\) 个投影矩阵，其中 \\( m \\) 取决于离散化参数 \\( b \\)、维度 \\( d \\) 和失败概率 $\\delta$（更具体地说，$m \\in O(\\ln(1/\\delta) + \\ln(bd))$。对于每个随机子空间（由投影矩阵 \\( M \\) 定义），算法选择一个“标准”半空间的网$N_M$ （由子空间中的标准向量定义），使得对于每个向量$y \\in \\mathbb{R}^k$ ，都有一个邻近的标准向量，具体而言，距离（诱导的正弦）最多为$\\frac{3}{4}\\gamma$ 。所需的标准向量数量为$O\\left(\\frac{1}{\\gamma^{k-1}}\\right)$。对于每个标准向量，策展人发布投影半空间查询的隐私保护估计。该机制是非交互式的，策展人将不再发挥进一步的作用。为了处理任意查询 \\( y \\)，分析师从一个空的多重集合开始。对于每个 \\( m \\) 个投影 \\( M \\)，分析师找到与$P_M(y)$最近的向量$\\hat{y} \\in N_M $，并将该半空间查询的答案添加到多重集合中。算法输出这些 \\( m \\) 个值的中位数。\n定理 6 ([8])。设\n$$\rn \\geq \\log(1/\\delta) + \\log m + (k - 1) \\log(1/\\gamma) + m O\\left(\\frac{1}{\\gamma}\\right) k - 1 2\\alpha.\r$$$$ \\alpha $$$$ (\\epsilon, \\gamma, \\delta) $$$$ n $$ 的数据库。该算法的运行时间为多项式级别，具体为\n$$\r\\text{poly}(\\log(1/\\delta), 1/\\epsilon, 1/\\alpha, b, d)\r$$$$ \\gamma $$。\n🍬5 一般学习理论结果 我们简要概述三个关于在交互模型中可以私密学习的常规结果。首先，我们介绍Blum、Dwork、McSherry和Nissim的结果，表明在统计查询学习模型中可学习的任何内容也可以通过交互方式有效地私密学习[7]。然后，我们转向忽略计算问题的结果，表明McSherry和Talwar的指数机制[35]可以用于：\n私密学习任何可PAC学习的内容[32]； 为任何具有多项式VC维度的函数类 $$ C $$ 生成一个差分隐私的“合成数据库”，该数据库对 $$ C $$ 中的任何查询给出“良好”的答案[8]。 在这种情况下使用指数机制的工作归功于Kasiviswanathan、Lee、Nissim、Raskhodnikova和Smith[32]。\n🍭5.1 模拟统计查询模型 $$ f_1, \\ldots, f_k $$$$ \\tau_1, \\ldots, \\tau_k $$$$ 1 \\leq i \\leq k $$$$ f_i $$$$ \\tau_i $$$$ f_i $$$$ \\delta $$$$ c $$$$ 1 - \\delta $$。\n$$ k $$$$ f_1, \\ldots, f_k $$$$ \\tau = \\min\\{\\tau_1, \\ldots, \\tau_k\\} $$$$ n $$$$ f_i $$$$ p_i $$$$ f_i $$$$ n $$$$ k $$$$ b = k/\\epsilon $$$$ \\tau = \\rho/n $$$$ \\rho $$$$ p_i $$$$ \\rho $$$$ \\tau $$$$ \\rho $$$$ \\text{Lap}(k/\\epsilon) $$$$ \\rho $$$$ \\delta/k $$$$ x \u003c 0 $$$$ x $$$$ \\delta/2k $$：\n$$\r\\frac{1}{2} e^{-|x|/b} \u003c \\frac{\\delta}{2k} $$$$ |x| \u003e b \\ln(k/\\delta) $$$$ |x| \u003e (k/\\epsilon) \\ln(k/\\delta) $$$$ \\rho \u003e (k/\\epsilon) \\ln(k/\\delta) $$$$ \\rho/n \u003c \\tau $$$$ n \u003e \\rho/\\tau $$$$ K $$ 引入的噪声；即，它假设\n$$\r\\frac{1}{n} \\sum_{i=1}^{n} f_j(d_i) = \\Pr_{x \\in D}[f_j(x)], \\quad 1 \\leq j \\leq k, $$$$ D $$$$ D $$$$ f_j $$$$ 1 \\leq j \\leq k $$$$ \\tau $$$$ 1 - \\delta $$$$ n \u003e \\tau^{-2} \\log(k/\\delta) $$$$ \\tau $$$$ \\tau/2 $$$$ n $$ 的最大下界，可以处理这两种类型的错误。\n🍮5.2 私密PAC学习 $$ t: X \\rightarrow Y $$$$ X $$$$ Y $$$$ D $$$$ h: X \\rightarrow Y $$，使得错误率较小，定义为：\n$$\r\\text{error}(h) = \\Pr_{x \\in R^D}[t(x) = h(x)].\r$$$$ \\alpha $$$$ \\beta $$$$ C $$$$ 1 - \\beta $$$$ \\alpha $$$$ C $$$$ C $$$$ \\text{OPT} = \\min_{c \\in C}\\{\\text{error}(c)\\} $$，我们希望\n$$\r\\Pr[\\text{error}(h) \\leq \\text{OPT} + \\alpha] \\geq 1 - \\beta,\r$$其中概率是关于学习者看到的样本和学习者的随机性的。这个过程称为不可知学习。\n$$ d $$$$ t: X_d \\rightarrow Y_d $$$$ X_d $$$$ D $$$$ Z \\in D^n $$$$ D $$$$ n $$$$ Z $$$$ n $$$$ (x_i, y_i) $$$$ y_i = t(x_i) $$$$ 1 \\leq i \\leq n $$$$ C $$$$ \\epsilon $$-差分隐私。\n$$ C $$$$ \\epsilon $$$$ H = C $$$$ n \\in O(\\log |C|d + \\log(1/\\beta) \\cdot \\max\\{1/\\alpha, 1/\\alpha^2\\}) $$。学习者可能不是高效的。\n该定理是通过McSherry和Talwar的指数机制证明的，效用函数为\n$$\ru(Z, h) = -| \\{ i : t(x_i) = h(x_i) \\} | $$$$ Z \\in (X \\times Y)^n $$$$ h \\in H_d $$$$ u $$$$ \\exp(u(Z, c)/2) $$$$ c \\in H_d $$$$ u $$ 的小敏感度。准确性（以高概率低错误）稍微难以论证；证明直观上源于输出的概率随着错误分类数量的增加而呈指数下降。\n🍯5.3 差分隐私的多项式 VC 维度类查询 $$ C $$ 中所有查询的“合理”准确回答。读者应该对概念类的 Vapnick-Chervonenkis (VC) 维度有所了解。粗略来说，VC 维度是衡量类中概念复杂度的一种度量。\n$$ A $$$$ C $$$$ Q \\in C $$$$ D $$$$ \\hat{D} = A(D) $$，有\n$$\r|\\ Q(\\hat{D}) - Q(D)| \\leq \\gamma.\r$$$$ C $$$$ n $$$$ D \\in ({0, 1}^d)^n $$$$ n $$$$ C $$$$ \\epsilon $$$$ \\delta $$$$ \\hat{D} $$$$ C $$$$ \\epsilon $$$$ m = O\\left(\\frac{\\text{VC dim}(C)}{\\gamma^2}\\right) $$$$ d $$ 元组。它是根据指数机制选择的，使用的效用函数为\n$$\ru(D, \\hat{D}) = - \\max_{h \\in C} \\left| h(D) - \\frac{n}{m} h(\\hat{D}) \\right| .\r$$$$ C $$$$ D \\subset \\{0, 1\\}^d $$$$ |D| \\geq O\\left( \\frac{d \\cdot \\text{VC dim}(C)}{\\gamma^3} + \\frac{\\log(1/\\delta)}{\\gamma} \\right) $$$$ \\hat{D} $$$$ \\epsilon $$$$ C $$$$ \\gamma $$$$ |\\hat{D}| \u003c |D| $$ 的事实，即匹配任何查询的数据库条目的数量会成比例地较小。\n🍼6 总结 在本节中，我们探讨了隐私机制如何根据数据库上应用的查询序列的复杂性增加噪声。尽管使用高斯噪声可以在一定程度上缓解这一问题，但Dinur和Nissim [13]（参见[17, 20]）所开始的研究表明，这种增加是必不可少的。Dinur和Nissim的结果在很大程度上推动了机制K的开发以及本综述中倡导的整个交互方法。为了更精确地分析现实攻击，并更好地理解未能提供$\\epsilon$-差分隐私在实践中可能意味着什么，我们需要进一步研究以改善这些结果，或者确认这种改进是不可能的，从而理解如何在除非常大、“互联网规模”的数据集之外有效利用这些技术。\n","date":"2024-11-04T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/differential-privacy-a-survey-of-results/astronomy-1867616_1920_hu7843088620757822539.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/differential-privacy-a-survey-of-results/","title":"差分隐私：研究结果综述"},{"content":"总结 这一周高强度阅读把书都读完了，但是没看代码，后果是有点记不住，等有需要时再去看吧。\n这周看了大模型内容比较多，主要是为了快点看完相关内容找ZD哥确定毕设方向，早点开始，做的好一点。\n周五下午找了ZD哥确定方向，看完方向人有点蒙，可能是大模型看得多，安全那方面看得少，导致我不知道这个方向在干什么，于是开始沉思，思考了半天感觉确实不会，找gpt老师询问，没有得到结果，故有些惆怅，周日又坐在实验室上了一天的软件工程实训，还好马上就结束了，太烦了。今天是周一，先把上周周报做了\n我又重新换了问法，问了gpt老师，这次得到了满意的结果，找到了几篇论文，这周就读这几篇。\n感觉这才是周报，之前的都是书的摘抄。\n阅读 LLM预训练数据准备\ntransformer模型架构（不知道是不是我本，这个我从不同的地方看了听了好多遍，现在才刚有点感觉）\nInstruction Tuning\n提示学习（实在不行，没有研究天赋就去干点轻松的(bushi)）\n解码与部署\n后面就是补了两篇差分隐私的文献，读完感觉对要研究的方向还是不清楚，故又找了几篇：\n计划 \u0026ldquo;Secure and Private Machine Learning: A Survey\u0026rdquo;\n这篇论文综述了在机器学习中实现安全和隐私保护的各种方法，包括使用同态加密和安全多方计算的技术。 \u0026ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption\u0026rdquo;\n该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。 \u0026ldquo;Federated Learning with Encrypted Data\u0026rdquo;\n这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。 \u0026ldquo;Secure Inference with Deep Learning Models\u0026rdquo;\n该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。 \u0026ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning\u0026rdquo;\n本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。 \u0026ldquo;Secure and Private Inference of Deep Learning Models\u0026rdquo;\n这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。 \u0026ldquo;Privacy-Preserving Machine Learning: A Practical Approach\u0026rdquo;\n本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。 \u0026ldquo;Secure Outsourced Computation in Machine Learning\u0026rdquo;\n这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。 \u0026ldquo;Differentially Private Inference in Machine Learning\u0026rdquo;\n本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。 \u0026ldquo;Towards Secure and Private Machine Learning: A Survey\u0026rdquo;\n这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。 \u0026ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning\u0026rdquo;\n该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。 \u0026ldquo;Secure and Private Model Inference: A Survey\u0026rdquo;\n本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。 ","date":"2024-11-03T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/colorful-2609978_1280_hu7268197494349331923.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/","title":"第4周工作总结"},{"content":"❤️Differential Privacy – A Primer for the Perplexed 差分隐私：给困惑者的入门指南论文笔记\n差分隐私是一种针对隐私保护数据分析的隐私目标定义。自其提出以来，差分隐私数据分析一直是激烈的算法和理论研究的主题。不幸的是，部分文献存在对这一定义的基本误解。在这篇简短的邀请性说明中，我们阐明并回应了这些常见错误中的最重要的几种。\n💖错误 1：将定义与特定算法混淆 原文 差分隐私是一个定义。它是一个数学保证，可以通过一个发布数据集统计信息的算法来满足。许多不同的算法都可以满足这一定义。接下来，我们将假设有一个包含 n 行的数据库 X，每一行的数据属于单个个体。如果每个个体的数据被描述为集合 U 的一个元素，那么我们可以将数据集视为 U 中的一个多重集（即每个元素可以出现多次的集合）。设 D 表示可能的数据集空间。存在一个算法 M，它以数据库和（可选）查询作为输入，并返回响应。我们将这种算法称为机制。我们将在后面陈述差分隐私的定义。不过，现在我们注意到，如果一个随机机制 M 在所有数据库上具有相同的输出分布，那么它的输出就不会泄露任何关于其输入的信息。实际上，这等同于以下陈述，其中 D 是所有可能数据库的集合，Q 是所有可能查询的集合：\n$$\r\\frac{\\Pr[M(X, q) \\in S]}{\\Pr[M(X', q) \\in S]} = e^0 = 1\r$$ 在分子和分母中，概率空间都是基于机制所做的随机选择。需要注意的是，确定性机制如果忽略其输入，也可以满足这一属性。从方程式1可以推导出，如果产生了输出y，这并不会透露关于数据库的信息。方程式1是绝对的：如果一个算法具有这个属性，那么无论观察者——即看到输出y的数据分析师知道或能访问什么，这个属性都成立。如果观察者已经知道数据库是X，或者数据库包含了本文作者的医疗数据，甚至观察者知道数据库中所有镰状细胞状态位的模2总和，这些都不能改变机制的行为保证。与满足方程式1的机制进行交互不会泄露关于数据库的任何进一步信息。\n解读 作者引入了“机制”的概念，指的是一个接受数据库和查询作为输入的算法，并返回结果。重要的是，作者提到如果一个随机机制在所有数据库上都有相同的输出分布，那么该机制不会泄露关于输入的任何信息，这是一种非常强的隐私保护保证。这为后续对差分隐私定义的详细阐述奠定了基础，并强调了实现隐私保护的机制设计的重要性。\n符号解释：\nX 和 X′：代表两个可能的数据库。 D：所有可能的数据库的集合。 q：表示某个查询。 S：表示机制 M 的输出范围中的一个子集。 Pr⁡[M(X,q)∈S]：表示在数据库 X 上执行查询 q 时，输出结果落在集合 S 中的概率。 条件意义：\n该条件表明，对于任意两个数据库 X 和 X′，它们的输出落在集合 S 中的概率比是 e0=1，这意味着两者的概率是相等的。 这个特性确保了任何一个个体的存在或缺失对查询结果的影响是不可察觉的，从而有效保护个体的隐私。 原文 这是香农（Shannon）在加密上下文中提出的完美保密的定义。这不是一个算法，而是对随机算法概率分布的条件；任何满足这一条件的算法在信息论意义上都不会泄露关于数据的信息。两个数据库X和X′之间的对称差（symmetric difference），记作X⊕X′，是指出现在X或X′中但不在它们交集中的元素或行的集合。例如，如果X′是X的一个子集，且X中仅包含一行不在X′中，那么对称差的基数为1。（由于X和X′可以是多重集合，对称差实际上要复杂一些：如果一个项在一个数据库中比另一个数据库多出现k次，那么它在对称差中出现k次）。我们现在准备阐述差分隐私的定义。它的形式与方程1完全相同，只是现在我们允许释放一些信息：\n$$\r\\frac{\\Pr[M(X, q) \\in S]}{\\Pr[M(X', q) \\in S]} = e^{ε|X Θ X'|}\r$$ 在分子和分母中，概率空间是基于机制所做的随机选择。\n这个定义与完美保密的定义（方程1）有一个显著的不同：在概率比率的界限中，e的指数是ε|XX′|而不是0。\n这里的符号长这样，但我敲不出来，不知道这是啥东西：\n如果一个算法满足定义1，那么这一点不受观察者（看到输出y的数据分析师）所知道或能够访问的内容的影响。如果观察者已经知道数据库是X，或者数据库包含了本文作者的医疗数据，甚至如果观察者知道数据库中所有镰形细胞状态位的模2和，这都不会改变机制的行为保证。对这个定义的一个常见解释是，无论观察者（或攻击者）事先知道什么，差分隐私算法的输出几乎不会指示任何特定用户的数据是否被包含在数据集中。我们稍后会再讨论这一点。现在我们总结：这个定义不是一个算法，而是许多不同算法所满足的条件。需要注意的是，以这些术语来表述隐私，作为可以通过多种方式满足的要求，提供了一个框架，使得人们可以研究算法，比较它们的隐私保证，并理解它们对隐私的联合影响。我们相信，这是隐私科学方法中的必要一步。”\n解读 这段话讨论了差分隐私的定义与完美保密之间的差异，强调了差分隐私算法在不同背景下的行为保证，即使观察者掌握了一些先验知识，也无法推断出特定用户数据的包含情况。作者指出，这种定义不仅可以适用于多种算法，还提供了一个研究框架，使得对隐私保护的算法进行比较和理解成为可能。这种方法被认为是科学研究隐私的必要步骤，显示了隐私保护的复杂性和重要性。\n💗错误2：混淆输出与生成它们的过程 原文 考虑著名的1位一次性密码机制，它提供完美的保密性。该随机机制的输入是一个位b ∈ {0, 1}。该机制选择一个新的随机位p ∈R {0, 1}，并输出b + p mod 2。”\n$$\r\\forall X, X' \\in D, \\forall q \\in Q, \\forall S \\subseteq \\text{Range}(M) :\\frac{\\Pr[M(X, q) \\in S]}{\\Pr[M(X', q) \\in S]} = 1\r$$ 请注意，尽管机制实际上是使用其输入进行计算的，因此在某种正式的操作意义上它并不忽略其输入，但效果是相同的：输出分布是相同的，与数据库无关。如上所述，这意味着机制的输出不提供关于数据库的任何信息。这在以下四种情况下都是正确的：输出为0且数据库为0，输出为1且数据库为0，输出为0且数据库为1，或者输出为1且数据库为1。在所有四种情况下，输出对数据库没有任何可学习的信息。确保这一点的是生成输出的过程。因此，我们不说“安全”或“保护隐私”的输出；相反，我们认为这些输出是根据保护隐私的过程生成的。1位一次性密码是一种提供完美隐私的过程。所有完美私密的机制都是差分隐私的，但差分隐私机制的类别中包含一些不会提供完美隐私的机制；相反，它们提供关于数据库的一些非平凡信息，这在我们希望进行保护隐私的数据分析时是合理的。随机响应就是这样一种机制[War65]。考虑以下1位随机响应机制。与1位一次性密码机制一样，可能的数据库集是D = {0, 1}。设X ∈ {0, 1}为实际数据库。然而，该机制的步骤如下：如果p = 1，则翻转第二个硬币c ∈R {0, 1}，并公布结果。如果p = 0，则发布X。\n声明2：1位随机响应机制是(ln 3)/2-差分隐私的。\n证明：固定d ∈ {0, 1}。没有损失的一般性，设X = d，X′ = 1 − d。在输入X时，以概率3/4输出d。另一方面，在输入X′时，以概率1/4输出d。两者的比率是3。对称差为2，得出((ln 3)/2)-差分隐私。请注意，对于给定数据库并不存在“好输出”或“坏输出”的概念。尽管如此，发布的值确实给出了两个1位数据库哪个更可能的一点线索，但随机性提供了不确定性。与往常一样，我们指出，1位随机响应机制保持其差分隐私属性，独立于任何看到输出的人对数据库或其与其他数据库的连接所知的内容。\n解读 这段文字首先声称1位随机响应机制具有特定的差分隐私级别，即(ln 3)/2。接下来，作者通过一个简单的证明来支持这一声明，设定输入数据库X和X\u0026rsquo;，并计算在这两种情况下输出d的概率。通过比较输出概率，可以得出比率为3，这意味着在一定条件下，一个数据库比另一个数据库更可能生成特定输出。这里提到的对称差反映了两个数据库之间的差异。作者进一步解释，尽管随机响应机制的输出可能提供了关于数据库的信息，但并不存在绝对的“好”或“坏”输出，随机性引入了不确定性，从而保护了隐私。此外，强调了该机制的差分隐私属性与观察者所知的信息无关，确保了其隐私保护的强度。\n💟错误3：将特定差分隐私算法的低效用与差分隐私本身的失败混淆。 低效用指的是某个具体的差分隐私算法在提供隐私保护的同时，可能导致输出结果的质量或信息量下降。然而，这并不意味着差分隐私本身是一种失败的概念或方法。相反，某个特定算法的效用差可能源于设计、实现或数据特性等因素，而不是差分隐私的理论基础。\n原文 正如我们所指出的，1位完美一次性密码是一种差分隐私算法。更具体地说，对于所有ε ≥ 0，1位一次性密码算法是ε-差分隐私的。它也不提供关于数据库的信息。这并不意味着‘差分隐私不能起作用’或没有差分隐私算法能够产生有用的输出。实际上，我们也看到1位随机响应是(ln 3)/2-差分隐私的。从效用的角度来看，它是一个比一次性密码更好的(ln 3)/2-差分隐私算法。特别是，众所周知，假设随机响应调查中的所有n个受访者都正确遵循指示，人们可以从他们的回答中推导出承认特定行为的受访者数量的估计，其预期的不准确性（或扭曲）在√n的数量级上。实际上，存在一些(ln 3)/2-差分隐私算法，其准确性更高，即能在预期扭曲独立于n的情况下给出估计计数。当ε = (ln 3)/2时，拉普拉斯机制[DMNS06]发布的统计数据具有正确的均值和标准差√2(√3/2) = √3/2，而不是随机响应中标准差为Θ(√n)。因此，特定差分隐私算法的局限性不一定适用于所有差分隐私算法。任何声称差分隐私无法实现给定准确性目标的说法，必须附有证明，说明差分隐私——关于概率分布比率的具体要求——与该目标不一致。也就是说，必须表明没有任何差分隐私算法能够实现给定的准确性水平。这是相当棘手的，我们对于什么是可能的、什么是不可能的直觉往往是错误的。例如，自然会猜测上述提到的拉普拉斯机制基本上是在差分隐私下回答“统计”类型问题的最佳选择。然而，在需要同时计算多个统计数据的情况下，存在更复杂的算法（例如[BLR08]）表现得更好。尽管存在这些困难，还是有一些已发表的结果证明了差分隐私算法的非平凡局限性（见[GRS08，HT09]以获取特别优雅的例子）。\n解读 这段话探讨了差分隐私算法的有效性和局限性。作者首先确认了1位完美一次性密码是差分隐私的，并强调这种算法提供的信息量为零。接着，作者指出这并不意味着差分隐私本身无效，实际上，1位随机响应机制在效用上表现更好，且具有可接受的差分隐私级别。通过提供具体的例子，作者说明在一些情况下，差分隐私算法可以有效地估计特定行为的发生数量，并且存在一些算法可以在准确性方面表现得更好。\n作者进一步指出，对于声称差分隐私无法达到特定准确性目标的说法，必须提供充分的证明，表明差分隐私的定义与该目标之间的不一致性。强调了解决差分隐私算法的局限性并非易事，且我们的直觉可能会误导我们。作者还提到，即使拉普拉斯机制在某些情况下看似最佳，仍可能存在更复杂且效果更好的算法。最后，作者提到一些研究已证明了差分隐私算法的非平凡局限性，表明在这一领域还有很多需要探索的内容。\n💝“差分隐私的杂项‘做’和‘不做’” 这部分内容通常会列出在实施和理解差分隐私时应遵循的一些基本原则和误区。以下是一些可能的“做”和“不做”的示例：\n做（Does）： 确保输入的隐私：在设计差分隐私机制时，要确保能够有效保护用户数据的隐私。 选择合适的噪声机制：使用合适的噪声添加方法（如拉普拉斯噪声或高斯噪声）来达到所需的差分隐私级别。 进行严格的分析：对算法进行严格的数学分析，以确认其满足差分隐私的要求。 理解隐私损失：在实际应用中，清楚了解不同参数（如ε）的选择对隐私保护和数据效用的影响。 不做（Does Not）： 不要忽视算法的效用：在追求差分隐私的同时，不能完全忽略算法的效用，确保输出仍然有用。 不要混淆差分隐私和其他隐私保护技术：差分隐私是一个特定的框架，与其他隐私保护方法（如匿名化）有所不同。 不要低估数据特性：在实施差分隐私时，忽视数据本身的特性可能会影响隐私保护效果。 不要期望完美的隐私保护：差分隐私提供的是概率性保证，而非绝对安全，因此要有合理的期望。 这些“做”和“不做”可以帮助研究人员和工程师更好地理解和实施差分隐私，避免常见的误区和误解。\n原文 我们重申迄今为止的主要观点：\n差分隐私是一个定义，而不是一个算法。 对于任何特定的数据库，没有输出是‘好’或‘坏’的。隐私（完美隐私或差分隐私）是通过生成输出的过程获得的。 某个特定的ε-差分隐私机制未能提供足够准确的答案，并不意味着差分隐私与该目标不兼容。 在最后一节中，我们将对此进行进一步阐述，并进一步解释差分隐私，解决文献中关于差分隐私所提供或不提供的若干误解。差分隐私通常是在提供给数据分析的敏感记录的背景下进行解释的。隐私保证是，当个体选择加入或退出输入数据库时，输出的分布，因此从这些输出得出的结论的分布，最多只会变化一个（通常较小的）乘法因子。因此，参与其中几乎没有任何危害；关于您、您的数据或更广泛的世界得出的任何结论，几乎与不使用您的数据时得出的结论一样可能。” 事实：差分隐私并不保证您认为的秘密保持秘密。差分隐私所保证的是，您参与调查不会泄露您在调查中贡献的具体信息。得出的结论可能会反映出关于您的统计信息。一个旨在发现某种疾病早期指示的健康调查可能会产生强有力甚至结论性的结果；这些结论适用于您并不证明差分隐私的违反。事实上，您甚至可能没有参与调查（再次强调，差分隐私确保无论您是否参与调查，这些结论性结果几乎以相同的概率得出）。特别是，如果调查显示特定私人属性与公共属性之间有很强的相关性，这并不构成差分隐私的违反，因为这种相关性几乎同样的概率会在有无任何受访者的情况下被观察到。我们最喜欢的寓言是：假设一个统计数据库教我们吸烟导致癌症。已知吸烟者艾丽斯受到了伤害，因为她的保险费上涨。无论艾丽斯是否在数据库中，她受到伤害的可能性基本上是相同的。艾丽斯也得到了帮助：因为这项研究，她参加了戒烟项目。差分隐私的设计旨在确保艾丽斯和其他人都有动力参与这些社会有益的研究。\n解读 这段文字强调了差分隐私的局限性，并澄清了一些常见误解。作者指出，尽管差分隐私保护了个人在参与调查时的具体贡献不被泄露，但这并不意味着与个人相关的秘密会保持秘密。调查得出的统计结论可能仍然会反映出个人的某些特征，即使这些特征并没有直接在调查中披露。\n接着，作者用一个例子来说明这一点：即使吸烟导致癌症的结论在某个数据库中被证实，个体（如艾丽斯）因其吸烟习惯而遭受的后果并不违反差分隐私，因为这一结论可以在几乎相同的概率下得出，而与个体是否参与调查无关。作者强调，差分隐私的设计目的是鼓励个人参与对社会有益的研究，尽管参与者可能面临一些与其私人生活相关的后果。\n整体而言，这段文字意在提醒读者，差分隐私虽能保护个体数据的具体性，但并不消除由统计结果引发的潜在影响，反而要让人们更积极地参与能够产生社会价值的研究。\n原文 在讨论差分隐私时，有一种观点认为差分隐私的定义在涉及多个参与者或多个数据记录时变得不再有效或合理。这种说法的误解主要源于对差分隐私如何处理多个数据库或多个记录的理解不足。\n解读 差分隐私的扩展性：差分隐私的定义本质上是针对单个记录的，但它可以扩展到多个记录和多个参与者的情况。在这种情况下，算法仍然可以通过添加适当的噪声来保证输出的隐私性。 统计特性：在处理多个记录时，差分隐私并不失去其意义，因为差分隐私关注的是如何在统计分析中保持个体数据的隐私。当多个人的数据被聚合时，差分隐私确保了即使某个特定个体的记录被包含或排除，输出的变化仍然受到限制。 应用实例：在实际应用中，差分隐私已经被成功地应用于大规模数据集和多个参与者的场景中，如大型调查和机器学习模型的训练。在这些情况下，差分隐私提供了一种框架，使得即使存在多个数据记录，仍然可以有效地保护个体隐私。 误解来源：认为“差分隐私对于多于一个记录没有意义”的观点，往往源于对其基本原理和统计保证的误解。实际上，差分隐私的设计目的就是为了解决在涉及多个数据记录时的隐私问题。 原文 （奇怪的是）人们常常声称，由于概率的最大值为1，因此ε值大于1的情况几乎没有意义。具体选择这个值的来源并不清楚，因为exp(1) = e与概率的最大值并没有特别的关系，尽管大ε值在根本上是没有意义的这种观点是可以理解的。确实，较大的ε值会导致隐私保障的意义远不如较小的ε值。然而，它们仍然提供了数学上有意义的保证。考虑AOL发布的搜索日志事件，其中一名个人通过她的搜索记录被识别。假设在未包含她的数据的情况下，纽约时报记者准确描述西尔玛·阿诺德的私密担忧和关切的事件概率是十亿分之一。在这种情况下，形式为“允许您的数据被纳入将使您遭遇尴尬的风险增加e^12 \u0026lt; 164,000倍”的保证仍然使得披露的可能性非常小。\n解读： 对ε的理解：作者指出，虽然有些人认为ε值大于1没有意义，但这并不准确。尽管较大的ε值确实会导致隐私保障的有效性降低，数学上它们仍然提供了某种保证。 例子说明：通过AOL搜索日志的例子，作者强调了大概率事件和隐私泄露之间的关系。即使在极小的概率下，允许数据被使用仍然可能导致一定程度的风险增加，然而这种增加并不一定会导致信息泄露。 概率与风险：作者指出，虽然较大的ε值可能在直观上让人觉得隐私保护效果不佳，但其数学意义仍然存在。在某些情况下，即使是大ε值的风险增加也可能非常小，意味着泄露的可能性仍然是微乎其微的。 对风险的理解：最后，作者提到即使风险增加的倍数看起来很大，但在具体情境中，泄露事件的概率可能依然很小，这意味着参与者仍然可以相对安全地共享数据。 原文 错误：差分隐私不能工作，因为没有人能够解释如何设置ε的值。差分隐私方法的一大优点是能够保持个体在特定数据库中所遭受的累积隐私损失的量化衡量。这进一步强调了差分隐私研究的强大之处：我们很好地理解那些数据在多个独立操作数据库中的个体所遭受的累积隐私损失，即使对手可以访问所有这些数据库。此外，差分隐私研究发展出技术来显著减少在可以协调回答的情况下的隐私损失，这也是其强大之处。再者，差分隐私研究者已经确立这种协调对启用高复杂度查询至关重要，且无法替代。这种研究方向——在差分隐私中，而非其他私人数据分析方法——使我们能够通过ε来理解和减轻隐私损失。是的，这项研究尚不完整。是的，以下形式的定理似乎令人感到限制重重：\n如果一个个体参与了10,000个对抗性选择的数据库，并且我们希望确保她的累积隐私损失在概率至少为1 - e^(-32)的情况下被限制在e^1以内，那么每个数据库的ε值设定为1/801差分隐私是足够的。\n但我们还有什么其他方法可以找到理解如何放松对最坏情况对手保护的起点？我们还能如何测量这样做的影响？还有什么其他技术可以证明这样的主张？从更哲学的层面考虑，可以将其比作时间：一个人的一生中只有那么多小时，一旦这些时间耗尽，你就会死亡。（这有时比某人得知关于你私密信息的情况更糟。）然而，我们作为一个社会找到了一些方法来为个体的时间确定价值，而这一过程的基本部分就是能够量化地衡量时间。对于隐私的价值，确实需要新的思考，但我们相信，现在能够做到这一点是一件非常好的事情；每一个有意义的隐私保障都应该是量化的。\n解读 放松隐私保护的思考：作者提到在理解如何放宽对最坏情况对手的保护时，探讨不同的方法和工具是必要的。这表明在设计隐私保护机制时，需要权衡保护程度和实际应用的灵活性。\n量化的必要性：通过量化隐私保护的效果，作者认为可以更好地理解和测量隐私保护的影响。这种量化方法有助于设计出既能有效保护隐私又能满足实际需求的解决方案。\n时间的比喻：作者通过将隐私的价值与时间的价值进行比较，强调了对隐私进行量化的复杂性。就像时间的价值在社会中得到认可并进行量化一样，隐私的价值也应该被理解和衡量。\n隐私价值的新思考：最后，作者提到在隐私价值方面需要新的思考方式，指出这种新思考不仅是必要的，而且是积极的。这种思考方式可以推动隐私保护的理论和实践发展，使其更加科学和有效。\n","date":"2024-11-02T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/differential_privacy/cosmos-6680031_1280_hu14348253898639341674.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/differential_privacy/","title":"Differential Privacy"},{"content":"💐解码与部署 大语言模型是通过文本生成的方式进行工作的。在自回归架构中，模型针对输入内容逐个单词生成输出内容的文本。这个过程一般被称为 解码。\n🌸解码策略 大语言模型的生成方式本质上是一个概率采样过程，需要合适的解码策略来生成合适的输出内容。\n🌹语言模型解码的背景知识 模型M每次根据当前上下文词元序列𝒖=[𝑢1,𝑢2,··· ,𝑢𝑡]建模下一个词的概率分布𝑃（即公式$O = softmax(W^LY_L)$的输出），然后根据一定的解码策略选择下一个词𝑢′，之后再将𝒖和𝑢′作为新的上下文重复上述步骤，直到生成结束词元或者达到长度上限为止。在这个流程中，解码策略将主要关注如何基于概率分布𝑃选择合适的下一个词𝑢′。自回归的解码策略并不局限于特定架构，常见的编码器-解码器、因果解码器和前缀解码器均可适用。\n🌺贪心搜索 目前常见的大语言模型主要是通过语言建模任务进行预训练的。基于这种训练方式，一种直观的解码策略是贪心搜索（Greedy Search）。具体来说，贪心搜索在每个生成步骤中都选择概率最高的词元， 其可以描述为以下形式:\n贪心搜索示意图\r🌻概率采样 该方法根据模型建模的概率分布采样得到下一个词元，旨在增强生成过程中的随机性和结果的多样性：\n在图中展示了下一个词元的概率分布，虽然单词“coffee”被选中的概率较高，但基于采样的策略同时也为选择其他单词（如“water”、“tea”等）留有一定的可能性，从而增加了生成文本的多样性和随机性。\n🌼贪心搜索的改进 贪心搜索在每个生成步骤中均选择最高概率的词元，这可能会由于忽略在某些步骤中概率不是最高、但是整体生成概率更高的句子而造成局部最优。\n束搜索. 在解码过程中，束搜索（BeamSearch）[1]会保留前𝑛个具有最高概率的句子，并最终选取整体概率最高的生成回复。这里的𝑛被称为束大小（Beam Size）。当 𝑛 = 1，束搜索就退化为贪心搜索。如图9.3所示（𝑛=2），第一步保留了概率最高的两个词“coffee” 和 “water” 作为候选；第二步基于 “coffee” 和 “water” 均进行扩展，得到模型在两个上下文内容下的概率分布，最后选择联合概率最高的两个句子 “coffee then” 和 “coffee and” 作为候选。在下面的生成步骤中，将会继续基于这两个候选去进行扩展，每次都选择联合概率最高的两个句子。最后，当两个束的句子均生成结束后，选择整体生成概率最高的候选句子作为最终的输出。在实践中，束的数量通常设定在3到6的范围内，设置过大的束会显著增加运算开销，并可能会导致性能下降。\n长度惩罚. 由于束搜索中需要比较不同长度候选句子的概率，往往需要引入长度惩罚（LengthPenalty）（亦称为长度归一化）技术。如果没有长度惩罚，传统的束搜索会倾向于生成较短的句子，因为每生成一个单词，都会乘以一个小于1的概率，使得句子的生成概率逐渐变小。因此，可以在生成概率的计算中引入长度惩罚，通过将句子概率除以其长度的指数幂𝛼，对于句子概率进行归一化处理，从而鼓励模型生成更长的句子。在实践中，𝛼通常设置为0.6到0.7之间的数值。\n重复惩罚.为了缓解贪心搜索重复生成的问题，可以使用𝑛-元惩罚（𝑛-gram Penalty）来强制避免生成重复的连续𝑛个词元，实践中𝑛通常设置为3到5之间的整数。进一步地，研究人员还提出了相对“温和”的惩罚机制来降低生成重复词元的概率，而不是“一刀切”地完全避免某些短语的生成，如出现惩罚（Presence Penalty）和频率惩罚（FrequencyPenalty）。具体地，出现惩罚在生成过程中会将已经生成词元的logits减去惩罚项𝛼来降低该词元之后生成的概率。频率惩罚相较于出现惩罚，会记录每个词元生成的数目，然后减去出现次数乘以惩罚项𝛼，因此如果一个词元生成得越多，惩罚也就越大。在实践中，𝛼的取值范围通常在0.1到1之间。这些重复惩罚方法不止适用于贪心搜索，对于随机采样也均适用。\n🌷随机采样的改进策略 基于概率采样的方法会在整个词表中选择词元，这可能会导致生成不相干的词元。为了进一步提高生成质量，可以进一步使用一些改进的采样策略，减少具有极低概率词汇对于生成结果的影响。\n$$\rP(u_j | \\mathbf{u}_i) = \\frac{\\exp(l_j / t)}{\\sum_{j'} \\exp(l_{j'} / t)}\r$$ 其中，$l_{j^′ }$表示每个候选词元的logit，𝑡是温度系数。具体来说，降低温度系数𝑡会使得概率分布更加集中，从而增加了高概率词元的采样可能性，同时降低了低概率词元的采样可能；当温度系数𝑡设置为1时，该公式退化为标准的随机采样方法；而当𝑡趋近于0时，实际上等同于贪心搜索，即总是选择概率最高的词。此外，当𝑡趋近于无穷大时，温度采样会退化为均匀采样。\n🌱低资源部署策略 由于大模型的参数量巨大，在解码阶段需要占用大量的显存资源，因而在实际应用中的部署代价非常高。这里介绍一种常用的模型压缩方法，即模型量化（Model Quantization），来减少大模型的显存占用，从而使得能够在资源有限的环境下使用大模型.\n🌲量化基础知识 在神经网络压缩中，量化通常是指从浮点数到整数的映射过程[2]，目前比较常用的是8比特整数量化，即INT8量化。针对神经网络模型，通常有两种类型的数据需要进行量化，分别为权重量化（也称为模型参数量化）和激活（值）量化，它们都以浮点数形式进行表示与存储。\n🌳量化的数学表述 $$\rx_q = R(x/S)-Z.\r$$ 通过上述数学变换，量化算法将浮点数向量𝒙转化为量化值𝒙𝒒。其中，𝑆表示缩放因子，用于确定裁剪范围，𝑍表示零点因子，用于确定对称或非对称量化，𝑅(·) 表示将缩放后的浮点值映射为近似整数的取整操作。一般来说，裁剪范围对于量化性能有很大影响，通常需要根据实际数据分布进行校准，可以通过静态（离线）或动态（运行时）方式。\n$$\r\\tilde{\\mathbf{x}} = \\mathbf{S} \\cdot (\\mathbf{x}_q + \\mathbf{Z})\r$$ 进一步，可以定义量化误差是原始值𝒙和恢复值$\\tilde{\\mathbf{x}}$之间的数值差异：$Δ=∥𝒙−\\tilde{\\mathbf{x}}∥^2_2$。\n🌴量化的策略 基于上述量化函数的定义形式，下面介绍一些对于量化函数常见的分类与实现策略。\n均匀量化和非均匀量化.根据映射函数的数值范围是否均匀，可以将量化分为两类：均匀量化和非均匀量化。均匀量化是指在量化过程中，量化函数产生的量化值之间的间距是均匀分布的。这种方法通常用于将连续的数据转换为离散的表示，以便在计算机中进行高效处理。与此不同，在非均匀量化方法中，它的量化值不一定均匀分布，可以根据输入数据的分布范围而进行调整。其中，均匀量化方法因其简单和高效的特点而在实际中被广泛使用。\n对称量化和非对称量化. 根据零点因子𝑍是否为零，均匀量化可以进一步分为两类：对称量化（𝑍=0）和非对称量化（𝑍≠0）。对称量化与非对称量化的一个关键区别在于整数区间零点的映射，对称量化需要确保原始输入数据中的零点（𝑥=0）在量化后仍然对应到整数区间的零点。而非对称量化则不同，根据前面的公式可以看出此时整数区间的零点对应到输入数值的𝑆·𝑍。为了方便讨论，这里以一个常见的8比特量化为例进行介绍。如图9.6(a)和图9.6(b)所示，对称量化将输入数值𝒙通过一个映射公式转换为八比特整数的表示范围内，如果是有符号整数，则该范围可以设置为[−128,127]，适用于𝒙的数值大致分布在零点两侧的情况，如果是无符号整数，则设置为[0,255]，适用于输入数值基本都分布在零点一侧的情况。\n量化粒度的选择. 量化算法通常针对一个批次的数据进行处理，其中批次的规模大小就反应了量化粒度，可以由算法设计人员进行选择。在神经网络模型中，输入数据和模型权重通常是以张量的形式进行表示与组织的。首先，如果每个张量只定义一组量化参数（即𝑆和𝑍），这称为按张量量化。为了进一步提高量化的精度，可以针对每个张量定义多组量化参数，例如可以为权重矩阵的列维度（也称为“通道”）设置特定的量化参数，称为按通道量化。还有一些研究工作采用了更细粒度的量化方案，对一个通道的数值细分为多个组，即按组的方式进行量化。在神经网络量化中，从按张量到按组，量化粒度越来越小，且使用较小的粒度通常可以提高量化的准确性，有效保持原始模型的性能。但是由于引入了更多的量化参数，在使用时会带来额外的计算开销。相反，按张量量化的粒度较粗，可能会引入较大的误差，但由于在硬件实现上更加简单，也是一种常见的量化粒度选择策略。在实践中，量化粒度需要根据具体任务和模型进行选择，应该采用可以平衡量化准确性以及额外计算开销的合适粒度。\n🌵大模型训练后量化方法 基于上述的量化基础知识，本部分将主要介绍大语言模型相关的量化方法。通常来说，模型量化方法可以分为两大类，即量化感知训练（Quantization-Aware Training,QAT）和训练后量化（Post-Training Quantization,PTQ）。从方法的名字可以看出，量化感知训练方法需要更新权重进而完成模型量化，而训练后量化方法则无需更新模型权重。与小型语言模型相比，在为大语言模型设计或选择量化方法时需要侧重关注两个主要因素。首先，大语言模型由大规模神经网络参数组成，在设计量化算法时需要考虑到所需要花费的计算开销。一般来说，训练后量化方法需要更少的算力开销，实践中应用更为广泛。其次，大语言模型中具有不同的数值分布特征（如激活值中存在较大的数值），这使得对于大语言模型进行低比特量化变得非常困难，特别是对于激活值。\n🌿权重量化 首先介绍主要面向模型权重的量化方法。其中，主流的权重量化方法通常是基于逐层量化的方法进行设计的，旨在通过最小化逐层的重构损失来优化模型的量化权重，可以刻画为：$\\text{argmin}_{\\mathbf{W}} | \\mathbf{X} \\mathbf{W} - \\mathbf{X} \\mathbf{W} |_2^2$，其中𝑾，𝑾 分别表示原始权重和量化后的权重，𝑿为输入。\n为了有效地优化该目标函数，GPTQ[3]的基本想法是在逐层量化的基础上，进一步将权重矩阵按照列维度分组（例如128个列为一组），对一个组内逐列进行量化，每列参数量化后，需要适当调整组内其他未量化的参数，以弥补当前量化造成的精度损失。在具体的实现中，GPTQ还进一步采用了特殊设计的优化方法来加速整个过程，如延迟批次更新、Cholesky重构等。GPTQ可以在3比特或4比特精度下实现对于大语言模型的有效权重量化。\n进一步，AWQ[4]发现在逐层和逐组权重量化过程中，对于模型性能重要的权重只占全部参数的一小部分（0.1%∼1%），并且应该更加关注那些与较大激活值维度相对应的关键权重，因为它们对应着更为重要的特征。为了加强对于这部分关键权重的关注，AWQ方法提出引入针对权重的激活感知缩放策略。具体来说，AWQ的优化目标将逐层的重构损失$| \\mathbf{X} \\mathbf{W} - \\mathbf{X} \\mathbf{W} |_2^2$修改为：$\\left| \\left( \\text{diag}(\\mathbf{s})^{-1} \\cdot \\mathbf{X} \\right) \\cdot \\mathbf{Q}(\\mathbf{W} \\cdot \\text{diag}(\\mathbf{s})) - \\mathbf{X} \\mathbf{W} \\right|_2^2$，其中𝑄 为量化函数。通过引入缩放因子𝒔，AWQ算法可以使得量化方法更为针对性地处理关键权重所对应的权重维度。\n☘️模型蒸馏 模型蒸馏（ModelDistillation）的目标是将复杂模型（称为教师模型）包含的知识迁移到简单模型（称为学生模型）中，从而实现复杂模型的压缩。一般来说，通常会使用教师模型的输出来训练学生模型，以此来传递模型知识。以分类问题为例，教师模型和学生模型在中间每一层会输出特征表示（特指神经网络模型），在最后一层会输出针对标签集合的概率分布。模型蒸馏的核心思想是，引入额外的损失函数（称为蒸馏损失函数），训练学生模型的输出尽可能接近教师模型的输出。在实际应用中，蒸馏损失函数通常与分类损失函数（交叉熵损失函数）联合用于训练学生模型。下面首先介绍传统的知识蒸馏方法，再介绍其在大语言模型中的应用。\n🍀参考文献 [1]. CARNEGIE-MELLON UNIV PITTSBURGH PA DEPT OF COMPUTER SCIENCE. Speech Understanding Systems. Summary of Results of the Five-Year Research Effort at Carnegie-Mellon University. 1977.\n[2]. Amir Gholami et al. “A Survey of Quantization Methods for Efficient Neural Network Inference”. In: arXiv preprint arXiv:2103.13630 (2021).\n[3]. EliasFrantaretal.“GPTQ:AccuratePost-TrainingQuantizationforGenerativePre-trained Transformers”. In: arXiv preprint arXiv:2210.17323 (2022).\n[4]. Ji Lin et al. “AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration”. In: arXiv preprint arXiv:2306.00978 (2023).\n","date":"2024-11-01T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/decoderbs/roses-7314623_1280_hu10924616842157365117.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/decoderbs/","title":"解码与部署"},{"content":"🎄提示学习 经过预训练、指令微调和人类对齐后，我们接下来讨论如何通过提示学习方法来有效地使用大语言模型解决实际任务。目前常用的方法是设计合适的提示（Prompting），通过自然语言接口与大模型进行交互。在现有研究中，任务提示的设计主要依靠人工设计和自动优化两种策略来实现。为了更好地 解决未见过的任务，一种典型的提示方法是上下文学习（In-contextLearning,ICL），它将任务描述与示例以自然语言文本形式加入到提示中。此外，思维链提示（Chain-of-Thought, CoT）作为一种增强技术，将一系列中间推理步骤加入到提示中，以增强复杂推理任务的解决效果。\n🎆基础提示 因为大语言模型的微调代价较高，基于自然语言的提示方法已经成为了使用大语言模型解决下游任务的主要途径。由于提示的质量在很大程度上会影响大语言模型在特定任务中的表现，因此一系列工作深入研究了通过人工设计或自动优化的方法来生成合适的任务提示。\n🎇人工提示设计 针对特定任务设计合适的任务提示，这一过程被称为“提示工程”（Prompt Engineering）。\n🧨关键要素 一般而言，针对大语言模型的提示设计需要考虑四个关键要素，即任务描述、输入数据、上下文信息和提示策略。\n任务描述.任务描述部分展示了大语言模型应当遵循的具体指令。一般来说，用户应该使用清晰的、具体的表达来描述任务目标。进一步，某些特定任务还需要对于输入或输出的格式进行更详细的说明，可以使用关键词或者特殊符号来强 调特殊设置以指导大语言模型更好地完成任务。 输入数据. 通常情况下，用户可以直接使用自然语言描述输入数据的内容。对于特殊形式的输入数据，则需要采用合适的方法使其能够被大语言模型读取与理解。例如，对于结构化数据（如知识图谱、表格等），通常使用线性化方法将其转换为易于处理的文本序列[264]。此外，由于结构化数据具有较好的组织形式，可以使用编程代码中的数据结构进行存储表示，将结构化数据中的属性表示为数据结构中的变量。基于代码表示的结构化数据可以使用外部工具（如程序执行器）进行准确地读取。 上下文信息. 除了任务描述和输入数据外，上下文信息对某些特定任务也非常重要。例如，搜索引擎可以为开放问答任务提供参考文档，可以通过将检索到的参考文档以上下文信息的形式引入提示作为大语言模型的输入。在引入外部信息时，需要对于这些信息进行合适的格式化，以加强大语言模型对它们的利用。此外，上下文学习中的任务示例数据也有助于提升大语言模型处理复杂任务的能力，大模型可以通过这些示例数据学习任务目标、输出格式以及输入和输出之间的映射关系。 提示策略. 针对不同的大语言模型设计合适的提示策略对于激发模型解决特定任务的能力非常重要。在某些情况下，添加特定的前缀或后缀有助于引导大语言模型解决复杂任务。例如，使用前缀“让我们一步一步地思考”可以激发大语言模型的逐步推理能力，而使用前缀“你是这项任务（或这个领域）的专家”可以提高大语言模型在某些特定任务（或领域）中的表现。此外，对于对话式的大语言模型（例如ChatGPT），由于其使用了大量对话数据进行训练，因此更合适的做法是将提示拆分为多个子任务提示，以多轮对话的方法逐步输入给大语言模型。 ✨自动提示优化 人工设计提示虽然比较直接，但是需要耗费较多的人工成本，同时要求设计人员具有丰富的提示工程经验。此外，大语言模型对于提示设计比较敏感，人工设计的提示有时很难获得最优的效果，还可能导致任务性能的下降。需要注意的是，由于大语言模型参数量巨大，并且很多工作机制已经与传统预训练模型有着较大的差异，许多提示优化方法已经不再适用于大语言模型。\n🎉离散提示优化 离散提示通常是由一系列自然语言词元组成的完整句子表达（如“请根据提供的检索信息回答下列问题”）。然而，在离散的词元空间中进行组合搜索，不仅时间复杂度高，而且可能导致非最优的搜索结果。下面将介绍四种常见的离散提示优化方法，能够提升离散任务提示的有效性与搜索效率。\n基于梯度的方法. 这类方法通过梯度更新技术以最大化模型的似然分数来优化离散提示的搜索过程。一种早期的代表性方法[1]使用梯度引导技术，首先将提示初始化为一系列“[MASK]”标记，然后迭代地将提示中的词元替换为词典中的其他词元，通过词元替换产生的对数似然变化来近似估计梯度，进而为提示的每个位置贪心搜索出最佳的词元。由于该方法对提示的每个位置都进行所有候选词元的替换和梯度评估，因此需要模型进行多次前向和后向计算，导致搜索过程的效率较低。为了改进搜索效率，可以将离散词元转化为连续嵌入表示（又称为“软词元”），使用梯度直接对连续嵌入参数进行优化，最后将每个连续嵌入映射为词典中最邻近的离散词元。 基于强化学习的方法. 为了实现更有效的离散词元选择，另一种解决方法是将离散提示优化问题转换为强化学习问题，并使用强化学习算法进行求解。具体来说，可以将预训练语言模型作为强化学习中的策略网络并依次生成提示中的词元。在提示生成结束之后，策略网络可以获得任务特定的奖励信号，该奖励信号可通过强化学习算法用于策略网络参数的训练。在实践中，可以设计不同类型的奖励信号，比如真实标签与基于提示的预测标签是否一致、生成文本与给定条件的匹配程度。在最后的测试阶段，基于训练好的策略网络，可以采用贪心搜索策略来生成任务提示中的每个词元。 基于编辑的方法. 这类方法主要关注如何通过编辑现有的提示来改进模型的性能，通常是基于模型在目标任务上的表现来判断提示的好坏。它特别适用于无法直接访问模型内部状态（如梯度）的情况，例如只能通过API调用的模型。在这类方法中，通常需要事先定义好编辑操作，然后迭代地对提示进行修改，直至达到最大迭代轮次或者模型最佳性能。提示的关键要素包括任务描述、输入数据、上下文信息和提示策略。因此，常用的提示编辑操作有修改任务描述、添加或删除上下文任务示例、调整输入到输出的标签映射器（例如可以使用“positive/negative”或者“正/负”表示二分类）等。此外，提示编辑操作也可以根据不同的场景或者需求进行设计，以适配下游具体任务。整体流程可以概述如下：基于预定义的编辑操作，在现有提示的基础上修改得到新提示，并输入至模型得到目标任务上的表现，根据表现筛选出合适的提示。由于上述过程可能需要迭代进行，可以只选择少量测试样例来评估模型表现，以减少计算开销。 基于大语言模型的方法. 由于大语言模型具有通用的任务能力，因此可以将提示优化看作一个待求解的任务，进而直接使用大语言模型作为提示生成器来生成或改进提示[2,3]。基于大语言模型的自动提示生成框架将提示优化过程看作是一个由大语言模型指导的黑盒优化问题。该框架首先利用提示生成模型（用于生成提示指令的大语言模型）基于少量上下文示例生成一批候选的任务指令。随后，使用“目标模型”（用于下游测试的大语言模型）对这些候选指令在目标任务上的表现进行逐一评估。在评估过程中，可以采用模型困惑度或任务准确率作为衡量指令质量的指标。上述过程可以通过基于蒙特卡洛搜索的多轮优化策略进行扩展。在每一轮迭代中，根据模型表现对候选指令进行筛选得到高评分指令，并利用大语言模型生成与高评分指令相似的新指令，从而扩展候选指令集。迭代完成后，选择模型表现最佳的候选指令作为最终使用的提示。然而，上述方法没有充分考虑提示的整个历史改进轨迹，因此可能在提示搜索过程中陷入局部最优或者产生效果震荡，无法生成更好的提示。为了解决这一问题，可以将所有改进的历史提示及其分数纳入提示优化过程，以指导大语言模型逐步生成更好的新提示。 🎊连续提示优化 与离散提示不同，连续提示由一组连续空间中的嵌入向量组成，可以根据下游任务的损失直接通过梯度更新进行优化。值得注意的是，已有连续提示优化的工作主要是基于预训练语言模型开展的，由于大语言模型参数量巨大，连续提示受到的关注较为有限。已有的连续提示优化研究通常依赖于有监督学习方法。当数据稀缺的情况下，还可以采用迁移学习方法来缓解目标任务标注数据不足的问题。\n监督学习方法. 这类方法将连续提示向量视为可训练的模型参数，基于下游任务数据，通过最小化交叉熵损失来优化连续提示。Prefix-tuning [4] 会在语言模型的每个Transformer 层预置一串前缀（即一组可训练的连续向量），而Prompt-tuning[5]只会在输入层加入可训练的提示向量。 通过固定语言模型的大规模参数而只微调这些连续的提示向量，可以有效节省训练所需要的参数量。然而，这些提示优化方法通常与输入无关，缺乏对于输入语义的充分考虑。 迁移学习方法. 有监督学习方法通常需要充足的训练数据来学习最优的任务提示，很难在数据稀缺场景下获得较好的模型性能。为了解决这个问题，基于提示的迁移学习方法首先为若干个具有代表性的源任务学习一个所有任务共享的连续提示，然后使用该提示初始化目标任务的提示，这可以为下游任务的提示优化提供良好的初始点。然而，这种方法存在一定的局限性，它在解决目标任务的所有实例时都使用了相同提示，而即使是一个精心优化过的提示也未必适合所有的任务实例。为了解决这一问题，可以为每个源任务独自学习任务特定的连续提示（而不是所有源任务共享），在解决目标任务的实例时，可以采用注意力机制等方式学习目标实例与每个源任务提示的相关性权重系数，对若干个源任务的提示向量进行加权组合，将组合后的新提示（为连续向量形式）用于帮助模型解决当前任务实例。 🎋上下文学习 在GPT-3的论文[6]中，OpenAI研究团队首次提出上下文学习（In-context learning,ICL）这种特殊的提示形式。目前，上下文学习已经成为使用大语言模型解决下游任务的一种主流途径。下面将详细介绍这一提示方法。\n🎍上下文学习的形式化定义 根据GPT-3论文中所给出的描述[6]，上下文学习使用由任务描述和（或）示例所组成的自然语言文本作为提示。图10.1展示了上下文学习的提示构建过程。首先，通过自然语言描述任务，并从任务数据集中选择一些样本作为示例。其次，根据特定的模板，将这些示例按照特定顺序组合成提示内容。最后，将测试样本添加到提示后面，整体输入到大语言模型以生成输出。基于任务描述以及示例信息，大语言模型无需显式的梯度更新即可识别和执行新的任务。\n形式上，我们使用𝐷𝑘={𝑓(𝑥1,𝑦1),\u0026hellip;, 𝑓(𝑥𝑘,𝑦𝑘)}来表示由𝑘个样本构成的一组示例数据，其中𝑓(𝑥𝑘,𝑦𝑘)是一个函数，负责将第𝑘个任务样本转换为自然语言提示。给定任务描述𝐼、示例𝐷𝑘以及新的输入𝑥𝑘+1，大语言模型生成答案ˆ 𝑦𝑘+1的过程可以通过下面的公式来表述：\n值得一提的是，上下文学习与指令微调之间存在着紧密的联系，因为它们都涉及将任务或样本转化为自然语言形式供大语言模型进行处理。在原始的GPT-3论文中，作者将上下文学习的提示定义为任务描述和示例的组合，这两部分均为可选。按照这个定义，如果大语言模型仅通过任务描述（即任务指令）来解决未见过的任务，也可以被看作是上下文学习的一种特例。两者的主要区别是，指令微调需要对大语言模型进行微调，而上下文学习仅通过提示的方式来调用大语言模型解决任务。此外，指令微调还可以有效提升大语言模型在执行目标任务时的上下文学习能力，尤其是在零样本场景下（即仅依赖任务描述而无需额外的示例）。\n🎁思维链提示 思维链提示[7,8]是一种高级提示策略，旨在增强大语言模型在各类复杂推理任务上的表现。常见的推理任务包括算术推理[9]、常识推理[9]以及符号推理[7]等多种任务。与上下文学习方法仅使用⟨输入，输出⟩二元组来构造提示不同，思维链提示进一步融合了中间的推理步骤来指导从输入到输出的推理过程。下图展示了一个思维链提示的具体例子。\n思维链提示技术的演化过程\r🎑参考文献 [1]. Taylor Shin et al. “AutoPrompt: Eliciting Knowledge from Language Models with Auto matically Generated Prompts”. In: EMNLP. 2020.\n[2]. Yongchao Zhou et al. “Large Language Models are Human-Level Prompt Engineers”. In: ICLR. 2023.\n[3]. Chengrun Yang et al. “Large Language Models as Optimizers”. In: arXiv preprint arXiv: 2309.03409 (2023).\n[4]. XiangLisa Li and Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Gen eration”. In: ACL. 2021.\n[5]. Brian Lester, Rami Al-Rfou, and Noah Constant. “The Power of Scale for Parameter Efficient Prompt Tuning”. In: EMNLP. 2021.\n[6]. TomB.Brown et al. “Language Models are Few-Shot Learners”. In: NeurIPS. 2020.\n[7]. JasonWeietal. “Chain of Thought Prompting Elicits Reasoning in Large Language Mod els”. In: arXiv preprint arXiv:2201.11903 (2022).\n[8]. Zheng Chu et al. “A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future”. In: arXiv preprint arXiv:2309.15402 (2023).\n[9]. Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. “A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers”. In: ACL. 2020.\n[10]. AlonTalmoret al. “CommonsenseQA: A Question Answering Challenge Targeting Com monsense Knowledge”. In: NAACL-HLT. 2019.\n","date":"2024-11-01T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/prompt_learning/birch-trees-8345812_1280_hu1913448854319438606.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/prompt_learning/","title":"提示学习"},{"content":"🍁人类对齐 在大语言模型的学习过程中，如何确保大语言模型的行为与人类价值观、人类真实意图和社会伦理相一致成为了一个关键研究问题，通常称这一研究问题为人类对齐（HumanAlignment）。\n🍂人类对齐的背景与标准 🌰背景 尽管大语言模型在下游任务中表现出优秀的性能，这些模型有时会出现错误或具有危害性的行为，例如无法正确遵循指令、生成虚假信息、以及产生有害、有误导性以及带有偏见的表达。在大语言模型的预训练和有监督微调的过程中，主要训练目标是根据上下文内容来预测下一个词元。但是，这一过程并未充分考虑人类的价值观或偏好，可能导致大语言模型从数据中学习到不符合人类期望的生成模式。为了规避这些潜在风险，研究人员提出了“人类对齐”这一关键概念，旨在保证大语言模型的行为与人类期望和价值观相一致[1,2]。\n与预训练和指令微调不同，人类对齐需引入全新的评估标准，如有用性、诚实性和无害性。 为了更直观地理解人类对齐对于大语言模型的重要性，例1对比了同一个语言模型在对齐前后对于相同输入的不同输出。在这个例子当中，输入的问题刻意包含了具有误导性的逻辑关系，即“土地价格”和“有污染的产业”是有直接关系的。因此，在经过人类价值观对齐之前的大语言模型会被输入中的错误逻辑所引导，产生了带有偏见的建议“农村地区更适合发展污染较严重的产业”。在经济生产中，发展有污染的产业需要综合考虑多方面的因素，不能仅仅因为土地价格更为便宜就认为适合发展相关产业。对齐前的大语言模型给出了一个错误的观点，不符合人类价值观，违背了无害性的原则。而经过与人类价值观对齐之后的大语言模型，先指出了输入问题中包含的错误逻辑（“我们不能简单地认为农村土地价格便宜就适合发展污染产业。”），并且给出了正确且合理的做法。对齐后的大语言模型的回复符合有用性和无害性，与人类价值观和偏好相符。\n例1 大语言模型（YuLan）对于相同输入在对齐前后的不同输出\r思考：\n之前我就遇到过类似的情况，在前面的文章中我也提到过，LLM似乎就是倾向于认同人类的提问（就算那是错误的），到这里明白了，原来是没有经过对齐。那么我就有新的问题了，对齐也是要训练的吧，而且得要人来准备数据，这就又是一个耗费人力的工程了。\n🍠对齐标准 人类对齐是一个较为抽象的概念，难以直接进行形式化建模，关于对齐的定义和标准也存在不同的观点。这里主要围绕三个具有代表性的对齐标准展开讨论，分别是有用性（Helpfulness）、诚实性（Honesty）和无害性（Harmlessness），这三种对齐标准已被现有的大语言模型对齐研究广泛使用[1,3]。下面具体介绍这三个代表性的对齐标准。\n有用性. 在实际应用中，大语言模型需要提供有用的信息，能够准确完成任务，正确理解上下文，并展现出一定的创造性与多样性。模型应尽量以简洁、高效的方式协助用户完成任务。当任务描述存在歧义或涉及背景信息时，模型应具备主动询问并获取任务相关信息的能力，同时具有一定的敏感度、洞察力和审慎态度。由于用户意图的多样性，有用性这一对齐标准仍然难以进行统一的定义与刻画，需要根据不同的用户进行确定。 诚实性. 模型的输出应具备真实性和客观性，不应夸大或歪曲事实，避免产生误导性陈述，并能够应对输入的多样性和复杂性。在人机交互过程中，大语言模型应向用户提供准确内容，还应适当表达对于输出信息的不确定性程度，以避免任何形式的误导。本质上，这要求模型了解自身的能力和知识水平。与有用性和无害性相比，诚实性是一个更为客观的标准，对人类标注的依赖相对较少。 无害性. 大语言模型应避免生成可能引发潜在负面影响或危害的内容。在处理敏感主题时，模型应遵循道德标准和社会价值观，从而消除冒犯性与歧视性。此外，模型需要能够检测到具有恶意目的的查询请求。当模型被诱导执行危险行为（如犯罪行为）时，应直接予以拒绝。然而，何种行为被视为有害，很大程度上取决于大语言模型的使用者、用户问题类型以及使用大语言模型的背景。 上述三种通用的对齐标准较为宽泛，因此许多研究针对性地给出了一些更为细化的对齐标准，以更全面地规范大语言模型的输出。例如，行为对齐要求人工智能系统能够做出符合人类期望的行为；在此基础上，意图对齐则进一步要求大语言模型在意图和行为上都要与人类期望保持一致，这涉及到哲学、心理学以及技术细节上的多重挑战；道德对齐要求语言模型应避免涉及非法、不道德或有害的话题，在回应中优先考虑用户安全、道德准绳和行为边界。这些对齐标准在本质上与前述三个标准是相似的，研究人员可以根据任务的特定需求进行调整。\n在实践中，红队攻击（RedTeaming）技术也被广泛运用，通过人工或自动 化的手段，以对抗方式探测大语言模型，诱导其生成有害输出，并据此针对性地 调整大语言模型，以避免产生此类有害输出[4]。\n🍊基于人类反馈的强化学习 由于对齐标准难以通过形式化的优化目标进行建模，因此研究人员提出了基于人类反馈的强化学习（Reinforcement Learning from Human Feedback,RLHF），引入人类反馈对大语言模型的行为进行指导。\n🥮RLHF 概述 RLHF首先需要收集人类对于不同模型输出的偏好，然后使用收集到的人类反馈数据训练奖励模型，最后基于奖励模型使用强化学习算法（例如Proximal Policy Optimization,PPO[5]）微调大语言模型。这种将人类反馈纳入大语言模型训练过程的方法已成为实现人类对齐的主要技术途径之一。\n🎑RLHF算法系统 RLHF 算法系统主要包括三个关键组成部分：需要与人类价值观对齐的模型、基于人类反馈数据学习的奖励模型以及用于训练大语言模型的强化学习算法。\n具体来说，待对齐模型一般指的是经过预训练、具备一定通用能力的大语言模型。然而，这些模型并没有与人类价值观对齐，在下游任务中可能表现出不合适甚至有害的行为。\n奖励模型的作用是为强化学习过程提供指导信号，反映了人类对于语言模型生成文本的偏好，通常以标量值的形式呈现。奖励模型既可以采用人类偏好数据对已有的语言模型继续微调，也可以基于人类偏好数据重新训练一个新的语言模型。\n在训练过程中，基于奖励模型提供的反馈信号，RLHF使用特定的强化学习算法进行大语言模型的训练。目前，PPO算法[5] 是一种被广泛用于人类对齐的强化学习算法。\n🥧RLHF的关键步骤 监督微调. 为了让待对齐语言模型具有较好的指令遵循能力，通常需要收集高质量的指令数据进行监督微调。指令数据一般包括任务描述和示例输出，可以由人类标注员针对特定任务编写，也可以由大语言模型自动生成。\n奖励模型训练.第二步是使用人类反馈数据训练奖励模型。具体来说，首先使用语言模型针对任务指令生成一定数量的候选输出。随后，邀请标注员对于输出文本进行偏好标注，这个标注过程可以采用多种形式，其中最常用的是对候选文本进行排序标注，这样可以有效减少多个标注员之间的不一致情况。进一步，使用人工标注的偏好数据进行奖励模型的训练，使其能够建模人类偏好。\n强化学习训练. 在这一步骤中，语言模型对齐被转化为一个强化学习问题。具体来说，待对齐语言模型担任策略实施者的角色（称为策略模型），它接收提示作为输入并返回输出文本，其动作空间是词汇表中的所有词元，状态指的是当前已生成的词元序列。\n🔥人类反馈数据的收集 在预训练阶段，大语言模型通过语言建模目标在大规模无标注语料库上进行训练。然而，这一过程无法直接反映人类对于大语言模型输出的主观和定性偏好（本书中称为“人类反馈”）。为了实现有效的人类对齐，需要使用高质量的人类反馈数据（不是高质量人类，哈哈哈）对大语言模型进行针对性的微调。\n🧡人类反馈形式 基于评分的人类反馈.最直接的标注方式是根据预设的标准邀请标注人员对于大语言模型的输出进行打分，从而作为模型输出质量的判断。 基于排序的人类反馈.排序是一种比较典型的人类偏好标注形式。最简单的方式是标注人员根据自身偏好对于大语言模型的输出进行全排序。 🏮奖励模型的训练 由于RLHF的训练过程中需要依赖大量的人类偏好数据进行学习，因此很难在训练过程中要求人类标注者实时提供偏好反馈。为此，我们需要训练一个模型来替代人类在RLHF训练过程中实时提供反馈，这个模型被称为奖励模型。\n思考：\n这里倒是解决我的疑问了，可以训练一个奖励模型干这个事。\n🀨非强化学习的对齐方法 尽管RLHF已被证明是一种较为有效的语言模型对齐技术，但是它也存在一些局限性。首先，在RLHF的训练过程中，需要同时维护和更新多个模型，这些模型包括策略模型、奖励模型、参考模型以及评价模型。这不仅会占用大量的内存资源，而且整个算法的执行过程也相对复杂。此外，RLHF中常用的近端策略优化算法在优化过程中的稳定性欠佳，对超参数的取值较为敏感，这进一步增加了模型训练的难度和不确定性。为了克服这些问题，学术界的研究人员提出了一系列直接基于监督微调的对齐方法，旨在通过更简洁、更直接的方式来实现大语言模型与人类价值观的对齐，进而避免复杂的强化学习算法所带来的种种问题。\n☕代表性监督对齐算法DPO 直接偏好优化（DirectPreference Optimization, DPO）是一种不需要强化学习的对齐算法。由于去除了复杂的强化学习算法，DPO可以通过与有监督微调相似的复杂度实现模型对齐，不再需要在训练过程中针对大语言模型进行采样，同时超参数的选择更加容易。\n🌆参考文献 [1]. Long Ouyang et al. “Training language models to follow instructions with human feed back”. In: arXiv preprint arXiv:2203.02155 (2022).\n[2]. Daniel M. Ziegler et al. “Fine-Tuning Language Models from Human Preferences”. In: arXiv preprint arXiv:1909.08593 (2019).\n[3]. Amelia Glaese et al. “Improving alignment of dialogue agents via targeted human judge ments”. In: arXiv preprint arXiv:2209.14375 (2022).\n[4]. Ethan Perez et al. “Red Teaming Language Models with Language Models”. In: EMNLP. 2022.\n[5]. John Schulman et al. “Proximal policy optimization algorithms”. In: arXiv preprint arXi v:1707.06347 (2017).\n","date":"2024-10-31T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/humanalignment/flowers-7382926_1920_hu9801049349447409777.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/humanalignment/","title":"HumanAlignment"},{"content":"指令微调（Instruction Tuning） 指令微调（Instruction Tuning）是指使用自然语言形式的数据对预训练后的大语言模型进行参数微调，这一术语由谷歌研究员在2022年的一篇ICLR论文中正式提出[1]。在另外一些参考文献中，指令微调也被称为有监督微调（Supervised Fine-tuning）[2] 或多任务提示训练（MultitaskPromptedTraining）[3]。指令微调过程需要首先收集或构建指令化的实例，然后通过有监督的方式对大语言模型的参数进行微调。经过指令微调后，大语言模型能够展现出较强的指令遵循能力，可以通过零样本学习的方式解决多种下游任务。\n🏔️指令数据的构建 一般来说，一个经过指令格式化的数据实例包括任务描述（也称为指令）、任务输入-任务输出以及可选的示例。\n⛰️基于现有的NLP任务数据集构建 学术界围绕传统NLP任务（如机器翻译、文本摘要和文本分类等）发布了大量的开源数据集合，这些数据是非常重要的监督学习数据资源，可以用于指令数据集的构造。通常来说，这些NLP数据集都包括输入和输出两个主要部分。\n例如，在中英翻译任务中，输入是“大语言模型已经成为机器学习的一个重要研究方向”，而相应的输出则是“Large language models have become one important research direction for machine learning”。\n为了生成指令化的训练数据，一个非常关键的步骤就是为上述的“输入-输出”对数据添加任务描述信息，用于指导模型去理解任务目标以及相关信息。在上述的例子中，可以向中译英的翻译数据集中添加指令， 例如“请把这个中文句子翻译成英文”。通过上述操作，就可以将一个NLP任务的数据实例全部通过自然语言形式进行表达，进而数据实例可以被用于大语言模型的指令微调。\n现有NLP数据集的指令格式化示意图\r为了更好地标注NLP指令微调数据，研究 人员开发了众包平台PromptSourcehttps://github.com/bigscience-workshop/promptsource，它能够高效地支持标注人员为不同数据集创建、共享及验证任务描述。\n🌋基于日常对话数据构建 尽管通过指令格式化已有的NLP数据集能够获得大量的指令数据实例，但是这些数据的多样性比较局限，与人类的真实需求也不能很好匹配。为此，研究人员开始使用用户在日常对话中的实际需求作为任务描述。例如，InstructGPT[2]将用户提交给OpenAI API的查询作为任务描述。由于这些用户查询源自于真实应用场景，均采用自然语言形式进行表达，因此特别适合大模型学习指令跟随能力。为了进一步增加任务的多样性，OpenAI还雇佣标注者创作更多的真实生活任务，包括开放式生成、开放式问答、头脑风暴等任务，然后由另一组标注者来回答这些问题作为输出。OpenAI最终将指令（用户真实查询或者人工标注的任务）和期望的输出（人工编写的答案）配对作为一个训练实例。但是，OpenAI没有对外开放所使用的指令数据。\n🗻基于合成数据构建 Self-Instruct \u0026mdash;\u0026mdash;-\u0026gt; Yizhong Wang et al. “Self-Instruct: Aligning Language Model with Self Generated In structions”. In: arXiv preprint arXiv:2212.10560 (2022).\nEvol-Instruct \u0026mdash;\u0026mdash;-\u0026gt; Can Xu et al. “WizardLM: Empowering Large Language Models to Follow Complex In structions”. In: arXiv preprint arXiv:2304.12244 (2023).\n🏖️指令微调的训练策略 🏜️优化设置 指令微调中的优化器设置（AdamW或Adafactor）、稳定训练技巧（权重衰减和梯度裁剪）和训练技术（3D并行、ZeRO和混合精度训练）都与预训练保持阶段一致，可以完全沿用。\n不同之处：\n目标函数. 预训练阶段通常采用语言建模损失，优化模型在每一个词元上的损失。而指令微调可以被视为一个有监督的训练过程，通常采用的目标函数为序列到序列损失，仅在输出部分计算损失，而不计算输入部分的损失。 批次大小和学习率.考虑到预训练阶段已经学习到了能够展现较好性能的模型参数，指令微调阶段通常只需要使用较小的批次大小和学习率对模型进行小幅度的调整。例如InstructGPT(175B)微调的批次大小为8，学习率恒定为5.03×10−6；Alpaca (7B) 微调的批次大小为128，学习率预热到2×10−5，然后采用余弦衰减策略。 多轮对话数据的高效训练.对于一个多轮对话数据，通常的训练算法是将其拆分成多个不同的对话数据进行单独训练。为了提升训练效率，可以采用特殊的掩码机制来实现多轮对话数据的高效训练。在因果解码器架构中，由于输入输出没有明显的分界，可以将所有一个对话的多轮内容一次性输入模型，通过设计损失掩码来实现仅针对每轮对话的模型输出部分进行损失计算，从而显著减少重复前缀计算的开销。 🏝️数据组织策略 平衡数据分布：最常见的方法是样本比例混合策略，即把所有数据集进行合并，然后从混合数据集中等概率采样每个实例。例如，研究者建议混合使用NLP任务数据（如FLANv2）、对话数据（如ShareGPT）和合成数据（如GPT4-Alpaca），来进行大模型的指令微调。\n多阶段指令数据微调：首先使用大规模NLP任务指令数据对模型进行微调，然后再使用相对多样的日常对话指令和合成指令进一步微调。为了避免能力遗忘问题，可以在第二阶段中添加一些NLP指令数据。\n结合预训练数据与指令微调数据：为了使得微调过程更加有效和稳定，可以在指令微调期间引入预训练数据和任务，这可以看作是对于指令微调的正则化。\n🏛️参数高效的模型微调 通过指令微调，大语言模型能够更好地学习遵循和执行人类指令。然而，由于大语言模型的参数量巨大， 进行全参数微调（需要较多的算力资源开销）在本节中，我们将讨论如何针对大语言模型进行参数高效微调（Parameter-efficient Fine-tuning），也称为轻量化微调 （Lightweight Fine-tuning）。在现有文献中，参数高效微调[4,5,6] 是一个重要的 研究方向，旨在减少需要训练的模型参数量，同时保证微调后的模型性能能够与全量微调的表现相媲美。下面将首先介绍常用于Transformer架构的参数高效微调方法，然后以LoRA微调方法为例介绍参数高效微调的代码实现。\n🏘️低秩适配微调方法 LoRA微调示意图\r🏬LoRA基础 $$\rW = W_0 +ΔW\r$$$$\rh = W_0·x + A·B^T·x\r$$ 在训练完成后，进一步将原始参数矩阵$W_0$和训练得到的权重𝑨和𝑩进行合并：$W = W_0 + A·B^T$，得到更新后的参数矩阵。因此，LoRA微调得到的模型在解码过程中不会增加额外的开销。\n🏦LoRA变种 在原始的LoRA实现中，每个低秩矩阵的低秩参数𝑅都被设置为固定且相同的数值，并且在训练过程中无法进行调整，这种设定忽略了不同的秩在微调任务中可能产生的差异化影响。因此，通过这种方式训练得到的低秩矩阵往往并非最优解。AdaLoRA[7]讨论了如何更好地进行秩的设置。它引入了一种动态低秩适应技术，在训练过程中动态调整每个参数矩阵需要训练的秩同时控制训练的参数总量。具体来说，模型在微调过程中通过损失来衡量每个参数矩阵对训练结果的重要性，重要性较高的参数矩阵被赋予比较高的秩，进而能够更好地学习到有助于任务的信息。相对而言，不太重要的参数矩阵被赋予比较低的秩，来防止过拟合并节省计算资源。尽管LoRA能够有效地节省显存，但对于参数规模达到上百亿级别的模型而言，其微调所需的成本仍然相当高昂。QLoRA[8]将原始的参数矩阵量化为4比特，而低秩参数部分仍使用16比特进行训练，在保持微调效果的同时进一步节省了显存开销。根据上一小节的分析，对于给定参数量为𝑃的模型，QLoRA微调所需要的显存由LoRA微调所需要的2𝑃进一步下降为0.5𝑃。因此通过QLoRA技术，可以在一张A6000(48GB)的GPU上微调65B的模型，接近16比特模型微调的性能。\n🏡其他高效微调方法 适配器微调、前缀微调、提示微调\n参考文献 [1]. Jason Wei et al. “Finetuned Language Models are Zero-Shot Learners”. In: ICLR. 2022.\n[2]. Long Ouyang et al. “Training language models to follow instructions with human feed back”. In: arXiv preprint arXiv:2203.02155 (2022).\n[3]. VictorSanhetal. “Multitask Prompted Training Enables Zero-Shot Task Generalization”. In: ICLR. 2022.\n[4]. XiangLisa Li and Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Gen eration”. In: ACL. 2021.\n[5]. Brian Lester, Rami Al-Rfou, and Noah Constant. “The Power of Scale for Parameter Efficient Prompt Tuning”. In: EMNLP. 2021.\n[6]. EdwardJ.Huetal. “LoRA:Low-RankAdaptation of Large Language Models”. In: ICLR. 2022.\n[7]. Qingru Zhang et al. “Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning”. In: arXiv preprint arXiv:2303.10512 (2023).\n[8]. TimDettmersetal.“QLoRA:EfficientFinetuningofQuantizedLLMs”.In:arXivpreprint arXiv:2305.14314 (2023).\n","date":"2024-10-31T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/instructiontuning/apples-1776744_1280_hu10159539994416574822.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/instructiontuning/","title":"Instruction Tuning"},{"content":"模型架构 大语言模型架构配置表\u003e\r🍈Transformer 模型 当前主流的大语言模型都是基于Transformer模型进行设计的。Transformer是由多层的多头自注意力（Multi-headSelf-attention）模块堆叠而成的神经网络模型。原始的Transformer 模型由编码器和解码器两个部分构成，而这两个部分实际上可以独立使用，例如基于编码器架构的BERT模型[1]和解码器架构的GPT模型[2]。与BERT等早期的预训练语言模型相比，大语言模型的特点是使用了更长的向量维度、更深的层数，进而包含了更大规模的模型参数，并主要使用解码器架构，对于Transformer 本身的结构与配置改变并不大。\n🍉输入编码 在Transformer 模型中，输入的词元序列(𝒖 = [𝑢1,𝑢2,\u0026hellip;,𝑢𝑇]) 首先经过一个输入嵌入模块（InputEmbeddingModule）转化成词向量序列。具体来说，为了捕获词汇本身的语义信息，每个词元在输入嵌入模块中被映射成为一个可学习的、具有固定维度的词向量$v_t$ ∈$R^H$。由于Transformer的编码器结构本身无法识别序列中元素的顺序，位置编码（PositionEmbedding,PE）被引入来表示序列中的位置信息。给定一个词元$u_t$，位置编码根据其在输入中的绝对位置分配一个固定长度的嵌入向量$p_t$ ∈$R^H$。然后，每个词元对应的词向量和位置向量将直接相加，生成了最终的输入嵌入序列𝑿=[𝒙1,\u0026hellip;,𝒙𝑇]，并且被传入到后续层中：$x_t=v_t+p_t$.\n通过这种建模方法的表示，Transformer 模型可以利用位置编码 𝒑𝑡 建模不同词元的位置信息。由于不同词元的位置编码仅由其位置唯一决定，因此这种位置建模方式被称为绝对位置编码。尽管绝对位置编码能够一定程度上建模位置信息，然而它只能局限于建模训练样本中出现的位置，无法建模训练数据中未出现过的位置，因此极大地限制了它们处理长文本的能力。\n🍋多头自注意力机制 多头自注意力是Transformer模型的核心创新技术。相比于循环神经网络（Recurrent Neural Network, RNN）和卷积神经网络（ConvolutionalNeuralNetwork,CNN）等传统神经网络，多头自注意力机制能够直接建模任意距离的词元之间的交互关系。作为对比，循环神经网络迭代地利用前一个时刻的状态更新当前时刻的状态，因此在处理较长序列的时候，常常会出现梯度爆炸或者梯度消失的问题。而在卷积神经网络中，只有位于同一个卷积核的窗口中的词元可以直接进行交互，通过堆叠层数来实现远距离词元间信息的交换。\n$$\rQ = XW^Q,\r$$$$\rK = XW^K,\r$$$$\rV = XW^V,\r$$$$\rAttention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{D}})V.\r$$$$\rhead_n = Attention(XW^Q_n,XW^K_n,XW^V_n)\r$$$$\rMHA = Concat(head_1,...,head_N)W^O\r$$由上述内容可见，自注意力机制能够直接建模序列中任意两个位置之间的关系，进而有效捕获长程依赖关系，具有更强的序列建模能力。另一个主要的优势是，自注意力的计算过程对于基于硬件的并行优化（如GPU、TPU等）非常友好，因此能够支持大规模参数的高效优化。\n🍏前馈网络层 $$\rFFN(X) = σ(XW^U + b_1)W^D + b_2\r$$ 其中$W^U$ ∈ $R^{H \\times H}$ 和$W^D$ ∈ $R^{H \\times H}$ 分别是第一层和第二层的线性变换权重矩阵，$b_1$ ∈ $R^{𝐻^′}$ 和 $b_2$ ∈ $R^H$ 是偏置项，𝜎是激活函数（在原始的Transformer中，采用 ReLU 作为激活函数）。前馈网络层通过激活函数引入了非线性映射变换，提升了 模型的表达能力，从而更好地捕获复杂的交互关系。\nTransformer 架构图\r🍐编码器 $$\rX^′_l = LayerNorm(MHA(X_{l-1})+X_{l-1})\r$$$$\rX_l = LayerNorm(FFN(X^′_l)+X^′_l)\r$$其中，$X^′_l$ 和 $X_l$ 分别是该Transformer层的输入和输出，$X^′_l$是该层中输入经过多头注意力模块后的中间表示，LayerNorm表示层归一化。\n🥥解码器 $$\rY^′_l = LayerNorm(MaskedMHA(Y_{l-1})+Y_{l-1})\r$$$$\rY^\"_l = LayerNorm(CrossMHA(Y^′_l,X_L)+Y^′_l)\r$$$$\rY_l = LayerNorm(FFN(Y^\"_l)+Y^\"_l)\r$$$$\rO = softmax(W^LY_L)\r$$ 其中，𝑶 ∈$R^{H \\times V}$ 是模型最终的输出，代表下一个词在词表上的概率分布；$W^L$ ∈ $R^{H \\times V}$ 是将输入表示映射到词汇表维度的参数矩阵，而$W^LY_L$是概率化前的中间值，通常被称为logits。\n[1]. JacobDevlin et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. In: NAACL-HLT. 2019.\n[2]. Alec Radford et al. “Improving language understanding by generative pre-training”. In: OpenAI Blog (2018).\n","date":"2024-10-29T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/transformermodelbase/house-7497002_1280_hu17957794167668257846.png","permalink":"https://JiangZhiyu-1024.github.io/p/transformermodelbase/","title":"transformer模型架构"},{"content":"大模型预训练数据准备 ✅数据来源 根据来源不同，预训练数据主要分为两种类型：通用文本数据和专用文本数据\n🔥通用文本数据 网页：随着互联网的普及与发展，网页的数据规模持续扩大，覆盖的内容类型也变得丰富多样。使用大规模网页文本数据进行预训练，有助于大语言模型获取多样化的语言知识，并增强其自然语言理解和生成的能力[1,2]。为了便于使用网页数据进行预训练或相关研究，相关机构已经爬取并发布了多个大规模的网页数据集，包括C4[2]、RefinedWeb[3]、CC-Stories [4] 等。然而，这些网页数据集中既包含了维基百科这种高质量文本，也不可避免地引入了广告网页等低质量文本。因此，在进行预训练之前，对网页进行筛选和处理显得尤为重要，这直接关系到最终数据的质量与预训练效果。 书籍：相较于其他语料，书籍中的文本内容往往更为正式与详实，篇幅也相对较长。这些书籍文本在大语言模型的学习过程中，发挥着非常重要的作用，它们不仅能够帮助模型积累丰富的语言知识，还可以加强其长程语义关系的建模。现有的研究工作通常使用Books3和Bookcorpus2等开源书籍数据集。这些数据可以在Pile 数据集中获得[5]。 💯专用文本数据 专用数据集有助于提升大语言模型解决特定下游任务的能力。\n多语文本. 在预训练语料中，加入多语言的文本数据可以增强模型的多语理解与生成能力。BLOOM[6]模型和PaLM[7]模型在其预训练语料中分别使用了涵盖46种和122种语言的多语数据，进而使得这两个模型在翻译、跨语言摘要和问答等多语言任务中性能表现优异。相比于仅针对单一目标语言进行微调的模型，在多语言语料库上训练过的大语言模型能够更好地建立多语言间的语义关联，为跨语言理解与对话任务提供支持。不仅如此，多语言数据还能有效增加数据的多样性，从而有助于提升模型的综合性能。 科学文本. 随着科学研究的不断发展，相关出版物的数量不断增加。为了增强大语言模型对科学知识的理解，可以将科学文本数据加入到模型的预训练语料中。通过在大规模的科学文本语料上进行预训练，大语言模型可以在自然科学以及工程技术方面建立坚实的知识基础，从而在科学问答与推理等任务上取得出色的表现[8]。构建科学文本语料的常用方法是收集arXiv论文、科学教材、数学网页等科学资源。然而，由于科学文本数据中包含数学公式、蛋白质序列等特殊符号，通常需要采用特定的分词和预处理技术，将这些不同格式的数据转化为大语言模型能够处理的统一格式。 代码. 代码能力目前已经成为大语言模型备受关注的一种能力。为了提高模型的代码能力，需要在大量代码语料上进行预训练，进而提高其所生成的程序质量。这些由大语言模型编写的程序甚至可以成功通过专家设计的单元测试用例[9] 或解决具有挑战性的算法竞赛问题[10]。一般来说，常用于大语言模型预训练的代码语料有两种来源，第一种是StackExchange等编程问答社区的数据，第二种是 GitHub 等开源项目仓库。这两种来源包含了代码以及对应的注释和文档。与自然语言文本相比，代码主要以结构化的编程语言形式呈现。在代码数据上训练能够提升模型的结构化语义理解与逻辑推理能力[11]。同时，代码中的函数调用关系还有助于增强模型的工具使用与学习能力[12]。此外，将推理任务格式化为代码 可以帮助大语言模型生成更准确的结果[13,14]。 🗡数据预处理 当收集了丰富的文本数据之后，为了确保数据的质量和效用，还需要对数据进行预处理，从而消除低质量、冗余、无关甚可能有害的数据。\n🌈质量过滤 基于启发式规则的方法（P74） 设计规则来针对地识别和剔除低质量的文本数据[6,15]。\n基于语种的过滤.\n基于简单统计指标的过滤.\n基于关键词的过滤.\n思考：启发式规则过滤需要自己设计并决定使用什么规则，很考验想法的合理性和这方面的经验。 基于分类器的方法 训练用于判别数据质量的文本分类器，进行预训练语料的清洗。\n可以将维基百科等高质量数据作为正样本，同时从网页中筛选出含有不良内容或低质量数据的样本作为负样本。利用这个训练好的文本分类器，能够精准地识别和过滤低质量数据，从而显著提升整个语料库的质量。\n注意：基于分类器的方法也可能无意中删除一些低资源但高质量的文本，如文言文数据等，数据清洗人员需要意识到这种情况，并且建立合理的数据召回与保留机制。\n目前常用来实现分类器的方法包括：\n轻量级模型（如FastText等） 可微调的预训练语言模型（如BERT、BART或者LLaMA等） 闭源大语言模型API（如 GPT-4、Claude 3） 这三个方法各自具有不同的优缺点：轻量级模型效率较高，但是分类的准确率和精度可能受限于模型能力；预训练语言模型可以针对性微调， 但是分类性能的通用性和泛化性仍然有一定的限制；闭源大语言模型的能力较强， 但是无法灵活针对任务进行适配，而且用于预训练数据清洗需要花费较高的成本。 对于后两种方法来说，除了简单地进行数据过滤，还可以针对性进行数据的改写，从而使得一些整体质量还不错、但存在局部数据问题的文本仍然可以被保留下来使用。\n效率问题：\n启发式规则写得好，过滤起来快又好，分类器更精准但是慢，可以结合一下，首先利用启发式规则进行初步筛选，以快速排除不符合要求的文档，随后再采用分类器方法进一步精细过滤，确保最终筛选出的语料具有较好的文本质量。在这一过程中，还可以同时应用多种分类器，可以先使用轻量级分类器进行数据过滤，进而使用更为有效但是资源消耗更高的分类器在粗滤后的数据上再次进行选择。\n这看起来怎么似曾相识呢？果然计算机里面好多知识都是互通的。\n📚敏感内容过滤 与质量过滤类似，不同类型的数据内容往往需要采用特定的过滤规则。\n过滤有毒内容：为了精确过滤含有有毒内容的文本，可以采用基于分类器的过滤方法。Jigsaw评论数据集[16]提供了用于训练毒性分类器的数据。该数据集收集了近160K条论坛评论数据，每条评论都经过细致的标注，包括“有毒”、“严重有毒”、“有威胁”、“侮辱性”、“暴力”以及“身份仇恨”等六个类别。利用这一数据集进行训练，可以构建出高效的毒性文本分类器。通过设置合理的阈值，训练完成的分类器将能够有效识别并过滤掉含有有毒内容的信息。在进行分类阈值设置时，需要在精确度和召回率之间寻求平衡，避免过多或者过少去除候选数据。Dolma的技术报告[17]指出，使用高阈值时去除的数据会过少，语料中未过滤掉的有毒内容会导致模型在下游任务上的性能下降；而低阈值则会过滤更多的有毒内容，但同时也会造成大量数据的浪费。考虑到后续的预处理操作（如质量筛选、去重等）同样能够有效剔除有害内容，Dolma选择为分类器设定了一个相对较高的阈值（0.4），从而保留更多的候选数据。最终，Dolma在这一阶段仅过滤了CommonCrawl中30%左右的数据。 过滤隐私内容：预训练文本数据大多来自互联网，其中可能包括用户生成的敏感信息或可识别的个人信息（Personally Identifiable Information, PII），如姓名、地址和电话号码等。这些信息如果不加处理，将增加隐私泄露的潜在风险。例如，在2023年11月有用户发现，反复要求ChatGPT重复某个单词可能会使其无意间泄露训练数据中的个人隐私信息，这个漏洞现在已经修复。因此，在预处理阶段，需要去除这些可识别的个人信息。一种直接且有效的方法是使用启发式方法，如关键字识别，来检测和删除这些私人信息[18]。Dolma采用了基于规则的方法来过滤数据集中的隐私内容，主要标注了三类敏感信息：邮箱地址、IP地址以及电话号码。在文本收集过程中，一旦检测到这些隐私信息，Dolma会根据其出现的频率采取不同的处理策略。具体来说，如果某个文档中的隐私信息少于五条，Dolma 会使用特定的词元（如“|||EMAIL_ADDRESS|||”、“|||PHONE_NUMBER|||” 和“|||IP_ADDRESS|||”）来替换这些信息，以保护用户的隐私。然而，如果文档中的隐私信息达到六条或更多，Dolma会选择直接删除整个文档。这是因为当文档中频繁出现隐私信息时，很可能还隐藏着其他未标注的敏感内容。 📈数据去重 由于大语言模型具有较强的数据拟合与记忆能力，很容易习得训练数据中的重复模式，可能导致对于这些模式的过度学习。总体来说，去重算法的设计可以基于不同的计算粒度以及匹配方法。\n计算粒度.去重可以在句子级别、文档级别和数据集级别等多种粒度上进行。现有方法主要依靠单词或 n 元词组的重叠这类表层特征，来衡量文档的重叠比率，进而检测和删除包含相似 内容的重复文档。现有的数据集往往采用多阶段、多粒度的方式来实现高效的去重。首先针对数据集和文档级别进行去重，旨在去除那些具有高度相似甚至完全一致内容的文档，例如多个URL可能具有相同的网页内容，或者网页数据集和新闻数据集中包含相同的新闻文档。随后，可以进一步在句子级别实现更为精细的去重。例如，可以计算两个句子之间公共子串的长度（公共子串长度，好熟悉，算法无处不在。），当其长度过长时直接删除某一个句子。\n用于去重的匹配方法.在去重过程中，可以使用精确匹配算法（即每个字符完全相同）或近似匹配算法（基于某种相似性度量）[19]。对于精确匹配来说，通常使用后缀数组来匹配最小长度的完全相同子串[20]。对于近似匹配来说，可以采用局部敏感哈希（Locality-SensitiveHashing,LSH）算法，如最小哈希（MinHash） 来实现。考虑到预训练数据集合的规模非常大，实现中可以综合考虑去重效率和去重效果之间的权衡。例如，RefinedWeb在文档层面采用了开销较小的近似匹配技术来实现去重，而在句子层面则采用了精确匹配算法来确保去重的准确性。\n⭐词元化 词元化（Tokenization）是数据预处理中的一个关键步骤，旨在将原始文本分割成模型可识别和建模的词元序列，作为大语言模型的输入数据。\n🌼BPE分词 BPE算法从一组基本符号（例如字母和边界字符）开始，迭代地寻找语料库中的两个相邻词元，并将它们替换为新的词元，这一过程被称为合并。合并的选择标准是计算两个连续词元的共现频率，也就是每次迭代中，最频繁出现的一对词元会被选择与合并。合并过程将一直持续达到预定义的词表大小。\nBPE算法的代码如下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import re from collectionsimportdefaultdict def extract_frequencies(sequence): #给定一个字符串，计算字符串中的单词出现的频率，并返回词表（一个词到频率的映射字典）。 token_counter = Counter() for item insequence: tokens= \u0026#39;\u0026#39;.join(list(item)) + \u0026#39;\u0026lt;/w\u0026gt;\u0026#39; token_counter[tokens] += 1 return token_counter def frequency_of_pairs(frequencies): #给定一个词频字典，返回一个从字符对到频率的映射字典。 pairs_count = Counter() for token , count in frequencies.items(): chars = token.split() for i inrange(len(chars) - 1): pair = (chars[i],chars[i+1]) pairs_count[pair] += count return pairs_count def merge_vocab(merge_pair, vocab): #给定一对相邻词元和一个词频字典，将相邻词元合并为新的词元，并返回新的词表。 re_pattern = re.escape(\u0026#39;\u0026#39;.join(merge_pair)) pattern = re.compile(r\u0026#39;(?\u0026lt;!\\S)\u0026#39; +re_pattern + r\u0026#39;(?!\\S)\u0026#39;) updated_tokens ={pattern.sub(\u0026#39;\u0026#39;.join(merge_pair),token):freq for token,freq invocab.items()} return updated_tokens def encode_with_bpe(texts,iterations): #给定待分词的数据以及最大合并次数，返回合并后的词表。 vocab_map = extract_frequencies(texts) for _ in range(iterations): pair_freqs = frequency_of_pairs(vocab_map) if not pair_freqs: break most_common_pair = pair_freqs.most_common(1)[0][0] vocab_map = merge_vocab(most_common_pair,vocab_map) return vocab_map num_merges = 1000 bpe_pairs =encode_with_bpe(data,num_merges) 字节级别的BPE（Byte-levelBPE,B-BPE）是BPE算法的一种拓展。它将字节视为合并操作的基本符号，从而可以实现更细粒度的分割，且解决了未登录词问题。采用这种词元化方法的代表性语言模型包括GPT-2、BART和LLaMA。具体来说，如果将所有Unicode字符都视为基本字符，那么包含所有可能基本字符的基本词表会非常庞大（例如将中文的每个汉字当作一个基本字符）。而将字节作为基本词表可以设置基本词库的大小为256，同时确保每个基本字符都包含在词汇中。例如，GPT-2的词表大小为50,257，包括256个字节的基本词元、一个特殊的文末词元以及通过50,000次合并学习到的词元。\nBPE算法的具体流程示例 假设语料中包含了五个英文单词： “loop”，“pool”，“loot”，“tool”，“loots”\n在这种情况下，BPE假设的初始词汇表即为： [“l”,“o”,“p”,“t”,“s”]\n在实践中，基础词汇表可以包含所有ASCII字符，也可能包含一些Unicode字符 （比如中文的汉字）。如果正在进行分词的文本中包含了训练语料库中没有的字 符，则该字符将被转换为未知词元（如“\u0026lt;UNK\u0026gt;”）。 假设单词在语料库中的频率如下： （“loop”，15），（“pool”，10），（“loot”，10），（“tool”，5），（“loots”，8）\n其中，出现频率最高的是“oo”，出现了48次，因此，学习到的第一条合并规则 是（“o”,“o”）→“oo”，这意味着“oo”将被添加到词汇表中，并且应用这一 合并规则到语料库的所有词汇。在这一阶段结束时，词汇和语料库如下所示： 词汇：[“l”,“o”,“p”,“t”,“s”,“oo”] 语料库：（“l” “oo” “p”，15），（“p” “oo” “l”，10），（“l” “oo” “t”， 10），（“t” “oo” “l”，5），（“l” “oo” “t” “s”，8）\n此时，出现频率最高的配对是（“l”，“oo”），在语料库中出现了33次，因此学习 到的第二条合并规则是（“l”，“oo”）→“loo”。将其添加到词汇表中并应用到所 有现有的单词，可以得到： 词汇：[“l”,“o”,“p”,“t”,“s”,“oo”,“loo”] 语料库：（“loo” “p”，15），（“p” “oo” “l”，10），（“loo” “t”，10），（“t” “oo” “l”， 5），（“loo” “t” “s”，8）\n现在，最常出现的词对是（“loo”,“t”），因此可以学习合并规则（“loo”,“t”） →“loot”，这样就得到了第一个三个字母的词元： 词汇：[“l”,“o”,“p”,“t”,“s”,“oo”,“loo”,“loot”] 语料库：（“loo” “p”，15），（“p” “oo” “l”，10），（“loot”，10），（“t” “oo” “l”， 5），（“loot” “s”，8） 可以重复上述过程，直到达到所设置的终止词汇量。\n🎵WordPiece 分词 WordPiece分词和BPE分词的想法非常相似，都是通过迭代合并连续的词元，但是合并的选择标准略有不同。在合并前，WordPiece分词算法会首先训练一个语言模型，并用这个语言模型对所有可能的词元对进行评分。然后，在每次合并时，它都会选择使得训练数据的似然性增加最多的词元对。\n谷歌并未发布WordPiece分词算法的官方实现\n$$\r得分 = \\frac{\\text{词对出现的频率}}{\\text{第一个词出现的频率} \\times \\text{第二个词出现的频率}}\r$$🐉Unigram 分词 与BPE分词和WordPiece 分词不同，Unigram 分词方法 [21] 从语料库的一组足够大的字符串或词元初始集合开始，迭代地删除其中的词元，直到达到预期的词表大小。它假设从当前词表中删除某个词元，并计算训练语料的似然增加情况，以此来作为选择标准。这个步骤是基于一个训练好的一元语言模型来进行的。为估计一元语言模型，它采用期望最大化（Expectation–Maximization,EM）算法：在每次迭代中，首先基于旧的语言模型找到当前最优的分词方式，然后重新估计一元概率从而更新语言模型。这个过程中一般使用动态规划算法（即维特比算法，Viterbi Algorithm）来高效地找到语言模型对词汇的最优分词方式。采用这种分词方法的代表性模型包括T5和mBART。\n🍌数据调度 数据调度（DataScheduling）主要关注两个方面：各个数据源的混合比例以及各数据源用于训练的顺序（称为数据课程，Data Curriculum）。\n预训练大语言模型时数据调度的示意图\r🎈数据混合 在预训练期间，将根据混合比例从不同数据源中采样数据：数据源的权重越大，从中选择的数据就越多。进一步，可能会对每个数据源的全部数据进行上采样或下采样，以创建特定的数据混合集合作为预训练数据。\n🎉数据课程 除了设置有效的数据混合配比外，在训练过程中对于预训练数据的顺序进行合适的安排也比较重要。具体来说，数据课程是指按照特定的顺序安排预训练数据进行模型的训练。例如，从简单/通用的数据开始，逐渐引入更具挑战性/专业化的数据。更广泛地说，它可以指训练期间在不同阶段使用不同的数据源混合配比。为了设定合适的数据课程，一种实用方法是基于专门构建的评测基准监控大语言模型的关键能力的学习过程，然后在预训练期间动态调整数据的混合配比。\n🧨参考文献 [1]. Alec Radford et al. “Language models are unsupervised multitask learners”. In: OpenAI Blog (2019).\n[2]. Colin Raffel et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”. In: J. Mach. Learn. Res. (2020).\n[3]. JiantaoQiuetal. “WanJuan-CC:ASafeandHigh-QualityOpen-sourcedEnglish Webtext Dataset”. In: arXiv preprint arXiv:2402.19282 (2024).\n[4]. Trieu H. Trinh and Quoc V. Le. “A Simple Method for Commonsense Reasoning”. In: arXiv preprint arXiv:1806.02847 (2018).\n[5]. LeoGaoetal.“ThePile: An 800GBDataset of Diverse Text for Language Modeling”. In: arXiv preprint arXiv:2101.00027 (2021).\n[6]. Teven Le Scao et al. “BLOOM: A 176B-Parameter Open-Access Multilingual Language Model”. In: arXiv preprint arXiv:2211.05100 (2022).\n[7]. Aakanksha Chowdhery et al. “PaLM: Scaling Language Modeling with Pathways”. In: arXiv preprint arXiv:2204.02311 (2022).\n[8]. Tarek Saier, Johan Krause, and Michael Färber. “unarXive 2022: All arXiv Publications Pre-Processed for NLP, Including Structured Full-Text and Citation Network”. In: arXiv preprint arXiv:2303.14957 (2023).\n[9]. MarkChenetal.“EvaluatingLargeLanguageModelsTrainedonCode”.In:arXivpreprint arXiv:2107.03374 (2021).\n[10]. YujiaLietal.“Competition-LevelCodeGenerationwithAlphaCode”.In:Science(2022).\n[11]. Yingwei Ma et al. “At Which Training Stage Does Code Data Help LLMs Reasoning?” In: arXiv preprint arXiv:2309.16298 (2023).\n[12]. Ke Yang et al. “If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents”. In: arXiv preprint ar Xiv:2401.00812 (2024).\n[13]. AmanMadaan et al. “Language Models of Code are Few-Shot Commonsense Learners”. In: EMNLP. 2022.\n[14]. Yuhuai Wu et al. “Autoformalization with Large Language Models”. In: arXiv preprint a rXiv:2205.12615 (2022).\n[15]. JackW.Raeetal.“ScalingLanguageModels:Methods,Analysis\u0026amp;InsightsfromTraining Gopher”. In: arXiv preprint arXiv:2112.11446 (2021).\n[16]. CJ Adams et al. Toxic comment classification challenge, 2017. https://kaggle.com/ competitions/jigsaw-toxic-comment-classification-challenge. 2017.\n[17]. Luca Soldaini et al. “Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research”. In: arXiv preprint arXiv:2402.00159 (2024).\n[18]. Hugo Laurençon et al. “The bigscience roots corpus: A 1.6 tb composite multilingual dataset”. In: Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022.\n[19]. Guilherme Penedo et al. “The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only”. In: arXiv preprint arXiv:2306.01116 (2023).\n[20]. Udi Manber and Eugene W. Myers. “Suffix Arrays: A New Method for On-Line String Searches”. In: SIAM J. Comput. (1993).\n[21]. Taku Kudo. “Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates”. In: ACL. 2018.\n","date":"2024-10-28T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/pretraining-data-preparation-for-large-models/road-5710320_1280_hu738609299032606297.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/pretraining-data-preparation-for-large-models/","title":"LLM预训练数据准备"},{"content":"文献阅读 每周都完不成上次的任务，杂事太多了，尽快完成： Detecting formal thought disorder by deep contextualized word representations 竟然要收费，不看了，让gpt给讲一下\n这篇论文《通过深度上下文化词表示检测形式思维障碍》研究了如何使用深度学习中的上下文词嵌入模型（如ELMo）来检测精神疾病患者的形式思维障碍（Formal Thought Disorder, FTD）。FTD是一种影响语言表达和思维逻辑的精神疾病症状，常见于精神分裂症患者。\n论文主要利用深度上下文化词嵌入模型，如ELMo，通过生成词在句子中的嵌入表示，捕捉语境中词的多义性和丰富的语义信息。研究团队通过分析患者的语言数据，构建模型来区分正常语言与存在思维障碍的语言表达，从而实现自动化的FTD检测。\n研究的核心贡献是使用更复杂的语义表示（如上下文化词嵌入）来解决传统方法无法有效处理的词义歧义问题。\nDistributed Representations of Words and Phrases and their Compositionality 《Distributed Representations of Words and Phrases and their Compositionality》是由Mikolov等人在2013年提出的一篇重要论文，介绍了Word2Vec模型的改进和创新。这篇文章主要讨论了如何通过分布式词表示（distributed word representations）更好地捕捉单词和短语的语义信息，并且提出了两种核心模型：CBOW（Continuous Bag of Words） 和 Skip-gram。\n这篇论文的主要贡献包括：\nSkip-gram和CBOW模型：论文提出了两种用于生成词向量的架构。Skip-gram模型的目标是根据一个词预测其上下文，而CBOW模型则根据上下文词来预测目标词。这两种模型能够有效地捕捉单词之间的语义关系。 层次Softmax和负采样：为了提高训练大规模语料库的效率，作者引入了层次Softmax和**负采样（negative sampling）**技术，这大幅降低了模型的计算复杂度，使得大规模训练变得可行。 词和短语的组合性：论文不仅处理了单词的表示，还处理了短语的表示，通过数据驱动的方式将多词短语映射为词向量，捕捉更复杂的语义结构。 向量运算反映语义关系：Word2Vec生成的词向量能够通过简单的向量运算表达词语之间的关系。例如，向量运算 vec(\u0026quot;King\u0026quot;) - vec(\u0026quot;Man\u0026quot;) + vec(\u0026quot;Woman\u0026quot;) ≈ vec(\u0026quot;Queen\u0026quot;) 说明了这种分布式表示在捕捉语义关系上的强大能力。 LLM BOOK 找到一本书，准确的来说是一个github仓库，组织者把近些年的LLM相关的东西都整进去了，好全，写的真好，库库看吧那就\n先把资源放在这里：RUCAIBox/LLMSurvey: The official GitHub page for the survey paper \u0026ldquo;A Survey of Large Language Models\u0026rdquo;.\n第一部分 背景与基础知识 语言模型的发展历程（P16） LLM大语言模型那肯定是从LM语言模型来的，语言模型的发展又经历了：\n统计语言模型（StatisticalLanguageModel, SLM） 神经语言模型（NeuralLanguageModel,NLM） 预训练语言模型（Pre-trainedLanguageModel,PLM） 大语言模型（LargeLanguageModel, LLM） LLM的特点（P19） 具有较为丰富的世界知识 具有较强的通用任务解决能力 具有较好的复杂任务推理能力 具有较强的人类指令遵循能力 具有较好的人类对齐能力 具有可拓展的工具使用能力 大语言模型对于产业应用带来了变革性的技术影响，将会催生一个基于大语言模型的应用生态系统。\n第二部分 预训练 大语言模型的第一个训练阶段是预训练，预训练语料的规模和质量对于提升大语言模型的能力至关重要。\n数据来源 通用文本数据和专用文本数据。通用文本数据涵盖了网页、书籍和对话文本等。由于通用文本数据规模较大、多样性强且易于获取，大多数大语言模型都会收集大量的通用文本数据，以增强其语言建模能力。此外，为了进一步提升大语言模型在特定专业任务上的表现，人们还将预训练语料的范围扩展至更专业的数据集，如多语数据、科学数据和代码数据等。\n通用文本数据 现有大语言模型预训练数据中各种数据来源的比例分布图\u003e\r专用文本数据 多语文本、 科学文本、代码。\n典型的预训练数据预处理流程图\u003e\r数据预处理 构建并使用系统化的数据处理框架（如开源库Data-Juicer）\n具体操作（P74）\n附录 softmax是啥？ Softmax是一种常用的函数，特别是在多分类任务中，用来将一个向量中的元素转换为0到1之间的概率分布。其核心作用是： 将实数映射为概率分布：Softmax 接受一个实数向量，输出一个与输入维度相同的向量，其中每个元素的值表示该元素作为某个类别的概率，且这些概率的和为1。这样就可以根据这些概率来进行分类预测。\n$$\r\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\r$$ 其中，\\( z_i \\) 是输入向量中的某个元素，\\( e^{z_i} \\) 是该元素的指数值，分母是所有输入元素的指数值的和。这个函数将每个 \\( z_i \\) 映射为一个在 \\( (0,1) \\) 之间的概率，并且所有输出的概率之和为1。\n作用机制：\n指数放大效果：Softmax 函数通过对输入值取指数（\\( e^x \\)），会将较大的输入值放大更多，而较小的值则被进一步压缩，突出更大的概率。 归一化：Softmax 保证所有输出的概率之和为1，符合概率的定义。 应用场景：在神经网络的最后一层，特别是分类任务中，Softmax 常用于多类分类问题，比如在自然语言处理（NLP）中的词分类、图像识别中的对象分类等。它将网络的输出转化为易于理解的概率值，帮助模型决定输入属于哪个类别。\n举例 假设我们有一个模型输出三个值 \\( [2.0, 1.0, 0.1] \\)，通过 Softmax 转换后得到的结果是大约 \\( [0.71, 0.26, 0.03] \\)，其中最大的值0.71表示模型认为输入属于第一个类别的概率最大。\n简言之，Softmax 是一种将输出转换为概率分布的工具，特别适合多分类问题的场景。\n看了半天论文，感觉没有一个时间线很难受，不清楚那些论文之间的关系，问问gpt： 基础概念与理论 先掌握大模型背后的基本理论，这些知识是理解后续论文的基础：\n语言模型的基础： 经典论文：《Attention Is All You Need》（Transformer） 相关概念：自注意力机制、序列到序列模型、词嵌入（如Word2Vec、GloVe） 自回归模型与自编码器： GPT 系列（GPT, GPT-2, GPT-3）的原理和应用 BERT 及其衍生模型的预训练与微调方式 扩展学习与生成式任务： 《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》 生成式预训练模型的设计与任务应用（如文本生成、机器翻译） 模型的训练与优化 学习如何高效地训练大模型，并且了解模型的优化技术：\n微调与参数高效训练： Efficient Fine-Tuning Techniques for Large Models: Adapter, LoRA 等 探索模型压缩技术（知识蒸馏、量化、剪枝等） 对比学习与自监督学习： 《SimCLR》、《MoCo》 等代表性论文，理解对比学习在大模型中的应用 模型对齐与安全 在大模型发展过程中，模型对齐（alignment）和伦理问题变得越来越重要：\n模型对齐与价值观嵌入： 《InstructGPT: Training language models to follow instructions with human feedback》 RLHF（强化学习通过人类反馈）等对齐策略 模型安全与公平性： 探讨模型偏见问题以及在实际应用中的安全隐患 视觉-语言模型与多模态模型 学习大模型在跨模态任务中的应用，比如视觉-语言任务：\nCLIP（Contrastive Language-Image Pretraining）：理解图像和文本共同表征的训练方法 DALL-E、Stable Diffusion：生成式视觉任务的模型设计与应用 视觉语言对话系统：如《Visual ChatGPT》 前沿研究与新趋势 在深入理解基本模型后，可以关注大模型领域的前沿研究方向：\n大模型与知识图谱的结合：如何将知识图谱融入语言模型 领域自适应模型：如何通过特定领域数据进行模型的进一步微调 检索增强生成（RAG）：如《RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》 实战应用与开源框架 阅读开源项目中的技术文档，参与开源项目（如Hugging Face的模型库） 学习如何应用大模型解决实际问题，并尝试训练和微调自己的模型 这样，你可以从基础概念、模型训练到前沿研究和实战应用，逐步深入，既能掌握理论又能结合实践。这个过程可以帮助你筛选出重要的论文和研究方向。\n对齐是什么意思？ 在大型语言模型（LLM）中，“对齐”通常指的是模型的输出与人类意图或价值观的匹配程度。具体来说，它涉及确保模型在生成文本时能够产生符合人类期望的、有益的和安全的内容。对齐的目标是减少模型生成有害、误导或不准确的信息。\n对齐可以通过多种方法实现，包括：\n训练数据的选择和清洗：使用高质量的数据集进行训练，确保模型接触到的内容符合人类的价值观和期望。\n强化学习：利用人类反馈来微调模型，使其在生成内容时更符合人类的意图。\n安全性和伦理性评估：在模型发布之前，对其输出进行评估，以识别和修正潜在的偏见或有害内容。\n通过这些方法，可以提高模型在特定任务或应用场景中的可靠性和安全性。\narXiv 论文是什么？ arXiv 是一个开放获取的电子预印本存档，主要用于科学研究领域，特别是在物理学、数学、计算机科学、统计学、金融学、生物学和其他相关学科。研究者可以在 arXiv 上免费上传和分享他们的研究论文，通常是在正式出版之前的版本。\narXiv 的特点包括：\n快速发布：研究者可以迅速分享他们的研究成果，而不必等待同行评审和正式出版的过程。\n开放获取：任何人都可以访问和下载论文，无需订阅或支付费用。\n广泛的学科覆盖：涵盖多个领域，促进跨学科的交流和合作。\n社区反馈：研究者可以在正式发表之前收到来自社区的反馈，帮助他们改进论文。\narXiv 的论文通常以“预印本”的形式存在，因此它们可能尚未经过同行评审。这意味着在引用这些论文时，研究者需要谨慎，了解其可能的局限性。\n在arXiv上发还未出版的论文不怕被别人剽窃成果吗？ 在 arXiv 上发布未出版的论文确实存在被剽窃的风险，但研究者通常认为在 arXiv 上共享研究成果是有益的，原因如下：\n时间戳和公开记录：一旦论文被上传到 arXiv，上传的时间戳可以作为知识产权的证明。这在某种程度上可以保护研究者的原创性。\n学术社区的支持：arXiv 是一个活跃的学术社区，研究者通常会关注相关领域的论文，并且对剽窃行为持有负面态度。学术界普遍重视诚信，剽窃通常会受到严厉的谴责。\n早期反馈：通过发布预印本，研究者可以获得来自同行的反馈，从而改进自己的工作。这也有助于在正式出版之前建立声誉。\n增加曝光率：在 arXiv 上发布可以提高研究的可见性，吸引潜在合作者或资助者。\n通用人工智能系统是什么？ 通用人工智能系统（AGI, Artificial General Intelligence）指的是一种能够理解、学习和应用知识，完成任何人类所能完成的智力任务的人工智能系统。与当前的狭义人工智能（Narrow AI）相比，后者在特定任务或领域表现出色（如图像识别、自然语言处理等），AGI 的目标是实现一种更广泛的智能，具备跨领域的学习和适应能力。\nAGI 的特点包括：\n自主学习：能够从有限的数据中学习并推理，具备自我改进的能力。\n多任务处理：能够在多个不同的领域和任务中表现出色，而不局限于特定的应用。\n常识推理：具备理解和应用常识的能力，能够处理复杂的、不确定的环境。\n灵活性和适应性：能够适应新情况和挑战，灵活地应对不同的环境和需求。\nAGI 的发展仍然是一个前沿研究领域，目前尚未实现。许多研究者在探索如何构建这样的系统，同时也讨论其潜在的伦理和社会影响，例如对就业、隐私和安全的影响。\n思考 随着看的东西多了，和使用chatgpt，我隐隐感觉到，由于chatgpt的语言模型主要是通过预测下一个最可能的词来生成文本，这一过程称为自回归生成（autoregressive generation）。导致gpt经常会顺着你的话去说，即有更小的概论去反对你的话，尽量让你说的话看起来正确，当然，如果你说的是：“太阳从西边升起”，那么他会义无反顾的反驳你。\n","date":"2024-10-26T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/third-week/cards-8594729_640_hu5982296781466901225.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/third-week/","title":"第3周工作总结"},{"content":"第一次成功创建个人博客 回忆 想当年其实我自己创建过，跟着一个教程做的，但当时才大一，很多东西都不懂，哼哧哼哧做了半天，最后不知道哪里不行，没有成功，于是放弃了，改用CSDN写，但现在我又有时间了，保研结束，该拾掇拾掇自己了，我觉得个人博客就像自己的一个小花园，写文章就好像在里面种花，精心打理后很有成就感，所以这次花了一天时间重新建起了我的花园，圆了大一时的梦，顺便做个记录，好记性不如烂笔头，忘了回来查查。\n资料 这次是跟着B站一个up主建立的博客，讲的非常清楚，把他的视频链接放在这里。\nhttps://www.bilibili.com/video/BV1bovfeaEtQ?vd_source=9cfc0ca3d84e1ed0064763e32a183f15\n记录 文章存放 content/post放的是文章\n文章名 文章都叫index.md\n日期问题 每个文章前面要有插入一个yaml块，里面放一些信息，日期这里一开始有一个非常搞心态的问题，因为我发现我随便测试日期，显示的日期都是10-10，09-09这种，日期和月份一样，后来检查了半天才发现，视频中up主在修改配置文件hugo.yaml时，把日期格式修改为了2006-01-02，而我顺手就写了个2006-01-01，我发现他把我这个格式理解成了月份和日期要一样，于是产生了那个令人哭笑不得的现象。\n深刻理解配置文件 今天才知道仓库的actions存放的是一些自动执行的脚本，一开始新建文章的开头配置文件没有写好，导致脚本运行一直出错，现在改好之后，看不到错的记录了，一片绿很养眼\nslug 这个头配置文件里的slug: firstblog是给链接起个名字，我这个内容是从我另一个博客复制过来的，于是他俩冲突了，显示的内容都杂在一起了，坏了，刚交上去就出bug，正好记上\n收获 对github仓库加深了理解\n对一些配置文件的存在有了新的认识\n致谢 感谢up主：Letere-莱特雷\n对，因为对这个博主的英文名字很感兴趣，于是给自己起了一个名字Zion Blaze,我感觉very古德，比我的损友老马起的foolish fishy强一w倍，明天联系我的律师起诉他。\n感谢chatgpt同学，今天依旧稳定发力，明天开始继续研究LLM，争取有朝一日让gpt同学感谢感谢我。\n","date":"2024-10-25T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/firstblog/liuying5_hu10731212018240133723.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/firstblog/","title":"记第一次创建我的个人博客"},{"content":"文献阅读 看了知乎的科普文章，先从经典的论文开始看：\nAttention Is All You Need 《Attention is All You Need》是由Vaswani等人在2017年提出的一篇重要论文，首次引入了Transformer模型。以下是其主要内容的总结：\n背景：传统的序列到序列模型（如RNN和LSTM）在处理长序列时存在瓶颈。Transformer模型通过自注意力机制克服了这些限制，使得模型可以并行处理数据，提升了训练效率。\n模型架构：\nEncoder-Decoder结构：Transformer由两个主要部分组成：编码器和解码器。编码器将输入序列转化为上下文向量，解码器则根据上下文生成输出序列。 自注意力机制：每个输入的表示都通过自注意力机制与其他输入进行交互，从而捕捉序列中的依赖关系。每个位置的表示通过加权求和得到，权重由输入的相似度计算得到。 多头注意力：模型使用多个注意力头并行计算不同的表示，能够更全面地捕捉信息。 位置编码：由于Transformer没有循环结构，论文引入了位置编码，提供序列中词语的位置信息，以保留词序信息。\n训练与优化：Transformer使用了残差连接和层归一化，以提高训练的稳定性和收敛速度。论文中还介绍了基于Adam优化器的训练过程。\n实验结果：Transformer在机器翻译任务上取得了显著的性能提升，并且在其他NLP任务中也展示了强大的通用性。\n影响：该论文奠定了现代NLP研究的基础，推动了基于Transformer的预训练语言模型（如BERT、GPT等）的发展。\n差分隐私深度学习(Deep Learning with Differential Privacy) 《Deep Learning with Differential Privacy》是由Martin Abadi等人在2016年提出的一篇重要论文，探讨了在深度学习中实现差分隐私的策略。以下是其主要内容的总结：\n背景：\n差分隐私：一种保护用户数据隐私的技术，旨在确保模型在训练时无法推断出任何单个用户的敏感信息。即使攻击者知道模型和数据集的其余部分，也无法有效地获取关于某个特定用户的信息。 深度学习与隐私问题：深度学习模型通常依赖大量数据，这使得保护用户隐私变得尤为重要。 模型与训练：\n噪声注入：通过在模型的梯度更新中添加噪声，可以达到差分隐私的效果。噪声的大小与隐私预算（ε）相关，预算越小，隐私保护越强，但模型的性能可能会受到影响。 Privacy Accounting：论文提出了一种计算隐私损失的框架，确保在每次训练步骤中监控和控制隐私损失。 算法设计：\nDP-SGD（Differentially Private Stochastic Gradient Descent）：论文提出了一种改进的随机梯度下降算法，结合了梯度裁剪和噪声注入，以实现差分隐私。这种方法确保在训练过程中，个别数据对模型的影响是被限制的。 实验结果：\n论文通过实验展示了DP-SGD在图像分类任务上的有效性，同时比较了不同噪声级别对模型性能的影响，证明了在保持合理准确性的同时实现差分隐私是可行的。 应用与展望：\n论文强调了在各类机器学习应用中（如医疗、金融等）保护隐私的重要性，提出将差分隐私技术与深度学习结合是一个有效的解决方案，并鼓励未来的研究在这一领域的进一步探索。 1 2 3 感觉偏向数学一点，全是公式和证明，好难看懂。 差分隐私顾名思义就是用来防范差分攻击的 加入噪声，改变原来的概率分布 Deep Reinforcement Learning from Human Preferences 《Deep Reinforcement Learning from Human Preferences》是由Christiano等人在2017年提出的一篇重要论文，探讨了如何利用人类偏好来训练强化学习模型。以下是其主要内容的总结：\n背景：\n传统的强化学习方法通常依赖于奖励信号来指导学习，但设计有效的奖励函数在复杂任务中往往困难且耗时。 人类可以提供更直观的偏好信息，这为训练强化学习代理提供了一种新途径。 方法概述：\n人类偏好收集：论文提出了一种方法，通过收集人类对不同行为的偏好来生成奖励信号。人类评估多个策略的表现，从中选择更好的策略，以此来建立奖励模型。 奖励模型：利用收集到的人类偏好数据，训练一个深度神经网络来预测代理在给定状态下的奖励。这种奖励模型可以用来指导强化学习算法。 训练过程：\n深度强化学习：在获得奖励模型后，使用深度强化学习算法（如Proximal Policy Optimization，PPO）来训练代理。代理通过与环境交互，不断优化其策略。 迭代学习：通过不断收集新的偏好数据并更新奖励模型，实现迭代学习，使得代理能够逐步提升性能。 实验结果：\n论文在多个复杂任务（如Atari游戏和模拟环境）中展示了该方法的有效性，代理能够通过学习人类的偏好，超越基于手动设计的奖励函数的表现。 结果表明，利用人类偏好进行训练能够加速学习过程，并提升代理的最终性能。 意义与展望：\n该方法展示了人类偏好在强化学习中的潜力，提出了一种新的思路来解决奖励设计问题。 论文强调了未来研究中结合人类反馈和偏好的重要性，特别是在涉及复杂决策和安全性问题的领域。 附录 残差连接（residual connection）和层归一化（layer normalization）是什么？ 残差连接（Residual Connection）和 层归一化（Layer Normalization）是深度学习中常用的两种技术，主要用于提高神经网络的训练效率和性能。下面是它们的定义和作用：\n残差连接（Residual Connection） $$\r\\text{Output} = \\text{Layer}(x) + x\r$$ 其中 \\(x\\) 是输入，\\(\\text{Layer}(x)\\) 是通过某个层（如卷积层或全连接层）处理后的输出。\n作用：\n缓解梯度消失问题：在非常深的网络中，梯度可能会随着层数增加而消失，导致网络难以训练。残差连接提供了一条捷径，使得梯度可以更容易地通过网络传播。 加速收敛：通过引入直接路径，残差连接有助于提高网络的收敛速度。 提高模型性能：它允许模型学习恒等映射，这使得训练更深的网络成为可能，提升了模型的表现。 层归一化（Layer Normalization） $$\r\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta\r$$ 其中：\n\\(x\\) 是输入。 \\(\\mu\\) 是输入的均值。 \\(\\sigma\\) 是输入的标准差。 \\(\\epsilon\\) 是一个小常数，避免除以零。 \\(\\gamma\\) 和 \\(\\beta\\) 是可学习的参数，用于缩放和平移归一化的结果。 作用：\n减少内部协变量偏移：层归一化帮助减少模型训练过程中每层输入的变化，使得网络在训练时更加稳定。 提高训练速度：通过标准化输入，层归一化可以使得训练过程更加平滑，从而加快收敛速度。 适用于变长序列：与批量归一化（Batch Normalization）不同，层归一化可以直接应用于变长的输入序列，适合于RNN和Transformer等模型。 总结 残差连接主要通过提供捷径来缓解深层网络中的梯度消失问题，并加速训练过程。 层归一化则通过标准化激活值来提高训练的稳定性和速度。这两者结合使用，能够显著提升深度学习模型的表现。 ","date":"2024-10-20T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/second-week/leaves-6729056_640_hu5688234115053010817.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/second-week/","title":"第2周工作总结"},{"content":"工作总结： 文献阅读 综述类： 2023年国内大模型发展综述与趋势研判_赵子忠\nAIGC大模型测评综述：使能技术、安全隐患和应对_许志伟\nAI大模型发展综述_张乾君\n深入： 《A Neural Probabilistic Language Model》是Yoshua Bengio等人在2003年发表的一篇重要论文，提出了一种基于神经网络的语言模型方法。这篇论文的主要目标是通过神经网络来改进传统的语言模型。\n传统的语言模型的任务是估计一个词序列的概率： P(w1,w2,…,wT) 在这种情况下，模型需要根据上下文词来预测当前词的概率。传统的做法一般是通过统计方法（如n元语法模型）来估计这些概率，但这类方法有一个很大的问题：它们无法处理高维数据，尤其是在词汇量非常大的情况下，计算复杂度极高，且泛化能力有限。\nBengio等人提出了一种新的神经网络语言模型，目的是克服这个问题。这个模型的核心思想是使用一个多层感知器来学习词的分布式表示（即词向量），并基于这些词向量来估计概率。具体过程大致如下：\n词嵌入：每个词通过一个嵌入矩阵被映射为一个连续向量，这种向量的维度比词汇表要小得多。这一步的目的是将离散的词映射到一个连续的低维空间，减少计算复杂度。\n神经网络建模：利用一个前馈神经网络（通常是多层感知器），根据前面的若干个词的词向量，来预测下一个词的条件概率。通过这个网络，模型可以学习到词与词之间的相似性和上下文关系。\n训练：模型通过最大化训练数据的似然估计来进行训练。具体来说，模型会调整嵌入矩阵和网络的参数，使得预测的概率尽可能接近真实的词序列概率。\n这篇论文的创新之处在于提出了“词向量”的概念，并展示了使用神经网络来处理语言建模问题的潜力。这为后来的深度学习在自然语言处理（NLP）中的应用奠定了基础。\n附录 什么是语言模型 语言模型（Language Model, LM）是自然语言处理中的一个核心概念，其主要任务是对一个词序列的概率分布进行建模。简单来说，语言模型的目标是估计一段文本或句子出现的概率，或者根据前面的词来预测下一个词。\n语言模型的定义 $$\rP( w_1, w_2, \\dots, w_T )\r$$$$\rP(w_1, w_2, \\dots, w_T)\r$$$$\rP(w_1, w_2, \\dots, w_T) = P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_1, w_2) \\cdot \\dots \\cdot P(w_T | w_1, w_2, \\dots, w_{T-1})\r$$ 这意味着每个词的概率取决于它前面的词。一个好的语言模型能够通过学习语料库中的模式，准确地捕捉这些条件概率。\n语言模型的用途 语言模型有广泛的应用场景，包括但不限于：\n文本生成：语言模型可以用来自动生成自然语言文本。例如，给定一部分句子，模型可以预测并生成合理的下一个词，从而逐步生成完整的句子或段落。\n机器翻译：在机器翻译中，语言模型帮助翻译系统生成符合目标语言语法的翻译结果。\n语音识别：语言模型能够帮助语音识别系统，在可能的识别结果中选择最合适的词序列。\n拼写纠正：在文本输入或搜索引擎中，语言模型可以用于纠正拼写错误，提供更符合语言习惯的结果。\n传统语言模型 $$\rP(w_T | w_1, w_2, \\dots, w_{T-1}) \\approx P(w_T | w_{T-2}, w_{T-1})\r$$这种简化降低了计算复杂度，但也限制了模型的表达能力，因为它只能捕捉到短距离的上下文信息，无法充分考虑长距离的依赖关系。\n神经网络语言模型 传统语言模型的一个主要问题是词汇量增大时，模型的计算复杂度随之增加。为了解决这个问题，Yoshua Bengio等人在《A Neural Probabilistic Language Model》论文中提出了使用神经网络进行语言建模的想法。这种神经网络模型通过将词表示为词向量（word embeddings），并通过深度学习模型来捕捉更复杂的语境和词间关系，从而解决了传统语言模型的许多不足之处。\n总之，语言模型的核心任务就是对自然语言的概率进行建模，并通过学习语言中的统计模式来预测文本。\n什么是纬度灾难？ 维度灾难（Curse of Dimensionality）是指在处理高维数据时，随着数据维度增加，问题的复杂性呈指数级增长，导致很多传统方法在高维空间中变得低效或不可行的现象。\n其影响包括：\n数据稀疏性：在高维空间中，数据点变得非常稀疏，因为数据的总量相对于空间的体积变得极其小，导致模型难以从有限的数据中捕捉到模式和关系。简单来说，随着维度的增加，需要的样本量也随之爆炸性增长，否则模型可能会过拟合或无法泛化。\n计算复杂度增加：在高维空间中，计算距离、相似性等操作的复杂度大大增加，使得很多算法在高维情况下效率低下。\n模型的泛化能力下降：由于数据点过于稀疏，模型很难学习到有用的特征，从而可能导致泛化能力下降。\n具体来说，维度灾难对机器学习、统计学、最优化等领域带来了极大的挑战，尤其是当模型需要处理很多特征或变量时。为应对这个问题，研究人员提出了许多方法，如降维技术（如主成分分析PCA）和分布式表示（如词嵌入），以减少有效维度或通过分布式方法来表示复杂的数据结构。\n有概率为什么要引入似然的概念，他们有什么不同？ 概率和似然虽然看起来相似，但在统计学中它们有不同的定义和用途：\n概率（Probability） 概率是用来描述随机事件发生的可能性。在给定模型参数的情况下，概率告诉我们某个事件或观测结果发生的机会有多大。\n假设我们有一个随机变量 \\( X \\)，它取某个值 \\( x \\) 的概率可以表示为 \\( P(X = x|\\theta) \\)，其中 \\( \\theta \\) 是模型的参数。例如，如果我们有一个硬币掷出的模型，参数 \\( \\theta \\) 可以是硬币正面朝上的概率（比如 \\( \\theta = 0.5 \\)）。\n核心问题：给定模型的参数 \\( \\theta \\)，事件 \\( X = x \\) 发生的概率是多少？ 似然（Likelihood） 似然是从反方向来考虑的。它描述的是在已知观测数据的前提下，模型参数的可能性。具体来说，似然是给定观测数据后，模型参数如何使观测数据变得最可能的一个度量。\n假设我们已经观察到数据 \\( X = x \\)，现在我们想知道，在不同的模型参数 \\( \\theta \\) 下，这个数据出现的可能性有多大。似然可以表示为 \\( L(\\theta|X = x) \\)，或者更直观地写作 \\( P(X = x|\\theta) \\)，虽然数学表达式相同，但意义不同——在这里我们关注的是参数 \\( \\theta \\) 的值使观测到的数据最可能。\n核心问题：给定观测到的数据 \\( X = x \\)，模型参数 \\( \\theta \\) 有多大可能是正确的？ 概率和似然的区别 概率：我们知道参数 \\( \\theta \\)，希望知道某个事件 \\( X \\) 发生的概率。\n问题：给定参数，事件的概率是多少？ 似然：我们知道事件（观测数据），希望推断出最有可能的参数 \\( \\theta \\)。\n问题：给定观测数据，哪个参数最可能是正确的？ 例子 抛硬币的例子 概率：假设你有一枚硬币，已知它是公平的，即 \\( \\theta = 0.5 \\)，那么掷硬币得到正面的概率是 \\( P(\\text{正面}) = 0.5 \\)。\n似然：现在，你掷硬币10次，得到8次正面和2次反面。你不知道硬币的公平性（即不知道 \\( \\theta \\) 的值）。你想根据这次实验结果估计硬币正面朝上的概率 \\( \\theta \\)。此时，你需要用似然来衡量在不同 \\( \\theta \\) 值下，观测结果（8次正面，2次反面）出现的可能性有多大，从而找到最有可能的 \\( \\theta \\)。\n似然函数 \\( L(\\theta|X) \\) 可能在某个 \\( \\theta \\) 值处达到最大值，这个 \\( \\theta \\) 就是最能解释观测数据的参数值。\n总结 概率用于给定模型参数时预测事件的发生可能性。 似然用于在已知观测数据时，推断哪个参数最能解释这些数据。 ","date":"2024-10-13T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/first-week/banlan_hu6307248181568134095.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/first-week/","title":"第1周工作总结"}]