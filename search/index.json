[{"content":"文献阅读 每周都完不成上次的任务，杂事太多了，尽快完成： Detecting formal thought disorder by deep contextualized word representations 竟然要收费，不看了，让gpt给讲一下\n这篇论文《通过深度上下文化词表示检测形式思维障碍》研究了如何使用深度学习中的上下文词嵌入模型（如ELMo）来检测精神疾病患者的形式思维障碍（Formal Thought Disorder, FTD）。FTD是一种影响语言表达和思维逻辑的精神疾病症状，常见于精神分裂症患者。\n论文主要利用深度上下文化词嵌入模型，如ELMo，通过生成词在句子中的嵌入表示，捕捉语境中词的多义性和丰富的语义信息。研究团队通过分析患者的语言数据，构建模型来区分正常语言与存在思维障碍的语言表达，从而实现自动化的FTD检测。\n研究的核心贡献是使用更复杂的语义表示（如上下文化词嵌入）来解决传统方法无法有效处理的词义歧义问题。\nDistributed Representations of Words and Phrases and their Compositionality 《Distributed Representations of Words and Phrases and their Compositionality》是由Mikolov等人在2013年提出的一篇重要论文，介绍了Word2Vec模型的改进和创新。这篇文章主要讨论了如何通过分布式词表示（distributed word representations）更好地捕捉单词和短语的语义信息，并且提出了两种核心模型：CBOW（Continuous Bag of Words） 和 Skip-gram。\n这篇论文的主要贡献包括：\nSkip-gram和CBOW模型：论文提出了两种用于生成词向量的架构。Skip-gram模型的目标是根据一个词预测其上下文，而CBOW模型则根据上下文词来预测目标词。这两种模型能够有效地捕捉单词之间的语义关系。 层次Softmax和负采样：为了提高训练大规模语料库的效率，作者引入了层次Softmax和**负采样（negative sampling）**技术，这大幅降低了模型的计算复杂度，使得大规模训练变得可行。 词和短语的组合性：论文不仅处理了单词的表示，还处理了短语的表示，通过数据驱动的方式将多词短语映射为词向量，捕捉更复杂的语义结构。 向量运算反映语义关系：Word2Vec生成的词向量能够通过简单的向量运算表达词语之间的关系。例如，向量运算 vec(\u0026quot;King\u0026quot;) - vec(\u0026quot;Man\u0026quot;) + vec(\u0026quot;Woman\u0026quot;) ≈ vec(\u0026quot;Queen\u0026quot;) 说明了这种分布式表示在捕捉语义关系上的强大能力。 LLM BOOK 找到一本书，准确的来说是一个github仓库，组织者把近些年的LLM相关的东西都整进去了，好全，写的真好，库库看吧那就\n先把资源放在这里：RUCAIBox/LLMSurvey: The official GitHub page for the survey paper \u0026ldquo;A Survey of Large Language Models\u0026rdquo;.\n第一部分 背景与基础知识 语言模型的发展历程（P16） LLM大语言模型那肯定是从LM语言模型来的，语言模型的发展又经历了：\n统计语言模型（StatisticalLanguageModel, SLM） 神经语言模型（NeuralLanguageModel,NLM） 预训练语言模型（Pre-trainedLanguageModel,PLM） 大语言模型（LargeLanguageModel, LLM） LLM的特点（P19） 具有较为丰富的世界知识 具有较强的通用任务解决能力 具有较好的复杂任务推理能力 具有较强的人类指令遵循能力 具有较好的人类对齐能力 具有可拓展的工具使用能力 大语言模型对于产业应用带来了变革性的技术影响，将会催生一个基于大语言模型的应用生态系统。\n第二部分 预训练 大语言模型的第一个训练阶段是预训练，预训练语料的规模和质量对于提升大语言模型的能力至关重要。\n数据来源 通用文本数据和专用文本数据。通用文本数据涵盖了网页、书籍和对话文本等。由于通用文本数据规模较大、多样性强且易于获取，大多数大语言模型都会收集大量的通用文本数据，以增强其语言建模能力。此外，为了进一步提升大语言模型在特定专业任务上的表现，人们还将预训练语料的范围扩展至更专业的数据集，如多语数据、科学数据和代码数据等。\n通用文本数据 现有大语言模型预训练数据中各种数据来源的比例分布图\u003e\r专用文本数据 多语文本、 科学文本、代码。\n典型的预训练数据预处理流程图\u003e\r数据预处理 构建并使用系统化的数据处理框架（如开源库Data-Juicer）\n具体操作（P74）\n附录 softmax是啥？ Softmax是一种常用的函数，特别是在多分类任务中，用来将一个向量中的元素转换为0到1之间的概率分布。其核心作用是： 将实数映射为概率分布：Softmax 接受一个实数向量，输出一个与输入维度相同的向量，其中每个元素的值表示该元素作为某个类别的概率，且这些概率的和为1。这样就可以根据这些概率来进行分类预测。\n公式：Softmax 函数的定义如下： $$\r\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\r$$ 其中，\\( z_i \\) 是输入向量中的某个元素，\\( e^{z_i} \\) 是该元素的指数值，分母是所有输入元素的指数值的和。这个函数将每个 \\( z_i \\) 映射为一个在 \\( (0,1) \\) 之间的概率，并且所有输出的概率之和为1。\n作用机制：\n指数放大效果：Softmax 函数通过对输入值取指数（\\( e^x \\)），会将较大的输入值放大更多，而较小的值则被进一步压缩，突出更大的概率。 归一化：Softmax 保证所有输出的概率之和为1，符合概率的定义。 应用场景：在神经网络的最后一层，特别是分类任务中，Softmax 常用于多类分类问题，比如在自然语言处理（NLP）中的词分类、图像识别中的对象分类等。它将网络的输出转化为易于理解的概率值，帮助模型决定输入属于哪个类别。\n举例 假设我们有一个模型输出三个值 \\( [2.0, 1.0, 0.1] \\)，通过 Softmax 转换后得到的结果是大约 \\( [0.71, 0.26, 0.03] \\)，其中最大的值0.71表示模型认为输入属于第一个类别的概率最大。\n简言之，Softmax 是一种将输出转换为概率分布的工具，特别适合多分类问题的场景。\n看了半天论文，感觉没有一个时间线很难受，不清楚那些论文之间的关系，问问gpt： 基础概念与理论 先掌握大模型背后的基本理论，这些知识是理解后续论文的基础：\n语言模型的基础： 经典论文：《Attention Is All You Need》（Transformer） 相关概念：自注意力机制、序列到序列模型、词嵌入（如Word2Vec、GloVe） 自回归模型与自编码器： GPT 系列（GPT, GPT-2, GPT-3）的原理和应用 BERT 及其衍生模型的预训练与微调方式 扩展学习与生成式任务： 《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》 生成式预训练模型的设计与任务应用（如文本生成、机器翻译） 模型的训练与优化 学习如何高效地训练大模型，并且了解模型的优化技术：\n微调与参数高效训练： Efficient Fine-Tuning Techniques for Large Models: Adapter, LoRA 等 探索模型压缩技术（知识蒸馏、量化、剪枝等） 对比学习与自监督学习： 《SimCLR》、《MoCo》 等代表性论文，理解对比学习在大模型中的应用 模型对齐与安全 在大模型发展过程中，模型对齐（alignment）和伦理问题变得越来越重要：\n模型对齐与价值观嵌入： 《InstructGPT: Training language models to follow instructions with human feedback》 RLHF（强化学习通过人类反馈）等对齐策略 模型安全与公平性： 探讨模型偏见问题以及在实际应用中的安全隐患 视觉-语言模型与多模态模型 学习大模型在跨模态任务中的应用，比如视觉-语言任务：\nCLIP（Contrastive Language-Image Pretraining）：理解图像和文本共同表征的训练方法 DALL-E、Stable Diffusion：生成式视觉任务的模型设计与应用 视觉语言对话系统：如《Visual ChatGPT》 前沿研究与新趋势 在深入理解基本模型后，可以关注大模型领域的前沿研究方向：\n大模型与知识图谱的结合：如何将知识图谱融入语言模型 领域自适应模型：如何通过特定领域数据进行模型的进一步微调 检索增强生成（RAG）：如《RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》 实战应用与开源框架 阅读开源项目中的技术文档，参与开源项目（如Hugging Face的模型库） 学习如何应用大模型解决实际问题，并尝试训练和微调自己的模型 这样，你可以从基础概念、模型训练到前沿研究和实战应用，逐步深入，既能掌握理论又能结合实践。这个过程可以帮助你筛选出重要的论文和研究方向。\n对齐是什么意思？ 在大型语言模型（LLM）中，“对齐”通常指的是模型的输出与人类意图或价值观的匹配程度。具体来说，它涉及确保模型在生成文本时能够产生符合人类期望的、有益的和安全的内容。对齐的目标是减少模型生成有害、误导或不准确的信息。\n对齐可以通过多种方法实现，包括：\n训练数据的选择和清洗：使用高质量的数据集进行训练，确保模型接触到的内容符合人类的价值观和期望。\n强化学习：利用人类反馈来微调模型，使其在生成内容时更符合人类的意图。\n安全性和伦理性评估：在模型发布之前，对其输出进行评估，以识别和修正潜在的偏见或有害内容。\n通过这些方法，可以提高模型在特定任务或应用场景中的可靠性和安全性。\narXiv 论文是什么？ arXiv 是一个开放获取的电子预印本存档，主要用于科学研究领域，特别是在物理学、数学、计算机科学、统计学、金融学、生物学和其他相关学科。研究者可以在 arXiv 上免费上传和分享他们的研究论文，通常是在正式出版之前的版本。\narXiv 的特点包括：\n快速发布：研究者可以迅速分享他们的研究成果，而不必等待同行评审和正式出版的过程。\n开放获取：任何人都可以访问和下载论文，无需订阅或支付费用。\n广泛的学科覆盖：涵盖多个领域，促进跨学科的交流和合作。\n社区反馈：研究者可以在正式发表之前收到来自社区的反馈，帮助他们改进论文。\narXiv 的论文通常以“预印本”的形式存在，因此它们可能尚未经过同行评审。这意味着在引用这些论文时，研究者需要谨慎，了解其可能的局限性。\n在arXiv上发还未出版的论文不怕被别人剽窃成果吗？ 在 arXiv 上发布未出版的论文确实存在被剽窃的风险，但研究者通常认为在 arXiv 上共享研究成果是有益的，原因如下：\n时间戳和公开记录：一旦论文被上传到 arXiv，上传的时间戳可以作为知识产权的证明。这在某种程度上可以保护研究者的原创性。\n学术社区的支持：arXiv 是一个活跃的学术社区，研究者通常会关注相关领域的论文，并且对剽窃行为持有负面态度。学术界普遍重视诚信，剽窃通常会受到严厉的谴责。\n早期反馈：通过发布预印本，研究者可以获得来自同行的反馈，从而改进自己的工作。这也有助于在正式出版之前建立声誉。\n增加曝光率：在 arXiv 上发布可以提高研究的可见性，吸引潜在合作者或资助者。\n通用人工智能系统是什么？ 通用人工智能系统（AGI, Artificial General Intelligence）指的是一种能够理解、学习和应用知识，完成任何人类所能完成的智力任务的人工智能系统。与当前的狭义人工智能（Narrow AI）相比，后者在特定任务或领域表现出色（如图像识别、自然语言处理等），AGI 的目标是实现一种更广泛的智能，具备跨领域的学习和适应能力。\nAGI 的特点包括：\n自主学习：能够从有限的数据中学习并推理，具备自我改进的能力。\n多任务处理：能够在多个不同的领域和任务中表现出色，而不局限于特定的应用。\n常识推理：具备理解和应用常识的能力，能够处理复杂的、不确定的环境。\n灵活性和适应性：能够适应新情况和挑战，灵活地应对不同的环境和需求。\nAGI 的发展仍然是一个前沿研究领域，目前尚未实现。许多研究者在探索如何构建这样的系统，同时也讨论其潜在的伦理和社会影响，例如对就业、隐私和安全的影响。\n思考 随着看的东西多了，和使用chatgpt，我隐隐感觉到，由于chatgpt的语言模型主要是通过预测下一个最可能的词来生成文本，这一过程称为自回归生成（autoregressive generation）。导致gpt经常会顺着你的话去说，即有更小的概论去反对你的话，尽量让你说的话看起来正确，当然，如果你说的是：“太阳从西边升起”，那么他会义无反顾的反驳你。\n","date":"2024-10-26T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/third-week/cards-8594729_640_hu5982296781466901225.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/third-week/","title":"2024.10.21至27第三周工作总结"},{"content":"第一次成功创建个人博客 回忆 想当年其实我自己创建过，跟着一个教程做的，但当时才大一，很多东西都不懂，哼哧哼哧做了半天，最后不知道哪里不行，没有成功，于是放弃了，改用CSDN写，但现在我又有时间了，保研结束，该拾掇拾掇自己了，我觉得个人博客就像自己的一个小花园，写文章就好像在里面种花，精心打理后很有成就感，所以这次花了一天时间重新建起了我的花园，圆了大一时的梦，顺便做个记录，好记性不如烂笔头，忘了回来查查。\n资料 这次是跟着B站一个up主建立的博客，讲的非常清楚，把他的视频链接放在这里。\nhttps://www.bilibili.com/video/BV1bovfeaEtQ?vd_source=9cfc0ca3d84e1ed0064763e32a183f15\n记录 文章存放 content/post放的是文章\n文章名 文章都叫index.md\n日期问题 每个文章前面要有插入一个yaml块，里面放一些信息，日期这里一开始有一个非常搞心态的问题，因为我发现我随便测试日期，显示的日期都是10-10，09-09这种，日期和月份一样，后来检查了半天才发现，视频中up主在修改配置文件hugo.yaml时，把日期格式修改为了2006-01-02，而我顺手就写了个2006-01-01，我发现他把我这个格式理解成了月份和日期要一样，于是产生了那个令人哭笑不得的现象。\n深刻理解配置文件 今天才知道仓库的actions存放的是一些自动执行的脚本，一开始新建文章的开头配置文件没有写好，导致脚本运行一直出错，现在改好之后，看不到错的记录了，一片绿很养眼\nslug 这个头配置文件里的slug: firstblog是给链接起个名字，我这个内容是从我另一个博客复制过来的，于是他俩冲突了，显示的内容都杂在一起了，坏了，刚交上去就出bug，正好记上\n收获 对github仓库加深了理解\n对一些配置文件的存在有了新的认识\n致谢 感谢up主：Letere-莱特雷\n对，因为对这个博主的英文名字很感兴趣，于是给自己起了一个名字Zion Blaze,我感觉very古德，比我的损友老马起的foolish fishy强一w倍，明天联系我的律师起诉他。\n感谢chatgpt同学，今天依旧稳定发力，明天开始继续研究LLM，争取有朝一日让gpt同学感谢感谢我。\n","date":"2024-10-25T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/firstblog/liuying5_hu10731212018240133723.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/firstblog/","title":"记第一次创建我的个人博客"},{"content":"文献阅读 看了知乎的科普文章，先从经典的论文开始看：\nAttention Is All You Need 《Attention is All You Need》是由Vaswani等人在2017年提出的一篇重要论文，首次引入了Transformer模型。以下是其主要内容的总结：\n背景：传统的序列到序列模型（如RNN和LSTM）在处理长序列时存在瓶颈。Transformer模型通过自注意力机制克服了这些限制，使得模型可以并行处理数据，提升了训练效率。\n模型架构：\nEncoder-Decoder结构：Transformer由两个主要部分组成：编码器和解码器。编码器将输入序列转化为上下文向量，解码器则根据上下文生成输出序列。 自注意力机制：每个输入的表示都通过自注意力机制与其他输入进行交互，从而捕捉序列中的依赖关系。每个位置的表示通过加权求和得到，权重由输入的相似度计算得到。 多头注意力：模型使用多个注意力头并行计算不同的表示，能够更全面地捕捉信息。 位置编码：由于Transformer没有循环结构，论文引入了位置编码，提供序列中词语的位置信息，以保留词序信息。\n训练与优化：Transformer使用了残差连接和层归一化，以提高训练的稳定性和收敛速度。论文中还介绍了基于Adam优化器的训练过程。\n实验结果：Transformer在机器翻译任务上取得了显著的性能提升，并且在其他NLP任务中也展示了强大的通用性。\n影响：该论文奠定了现代NLP研究的基础，推动了基于Transformer的预训练语言模型（如BERT、GPT等）的发展。\n差分隐私深度学习(Deep Learning with Differential Privacy) 《Deep Learning with Differential Privacy》是由Martin Abadi等人在2016年提出的一篇重要论文，探讨了在深度学习中实现差分隐私的策略。以下是其主要内容的总结：\n背景：\n差分隐私：一种保护用户数据隐私的技术，旨在确保模型在训练时无法推断出任何单个用户的敏感信息。即使攻击者知道模型和数据集的其余部分，也无法有效地获取关于某个特定用户的信息。 深度学习与隐私问题：深度学习模型通常依赖大量数据，这使得保护用户隐私变得尤为重要。 模型与训练：\n噪声注入：通过在模型的梯度更新中添加噪声，可以达到差分隐私的效果。噪声的大小与隐私预算（ε）相关，预算越小，隐私保护越强，但模型的性能可能会受到影响。 Privacy Accounting：论文提出了一种计算隐私损失的框架，确保在每次训练步骤中监控和控制隐私损失。 算法设计：\nDP-SGD（Differentially Private Stochastic Gradient Descent）：论文提出了一种改进的随机梯度下降算法，结合了梯度裁剪和噪声注入，以实现差分隐私。这种方法确保在训练过程中，个别数据对模型的影响是被限制的。 实验结果：\n论文通过实验展示了DP-SGD在图像分类任务上的有效性，同时比较了不同噪声级别对模型性能的影响，证明了在保持合理准确性的同时实现差分隐私是可行的。 应用与展望：\n论文强调了在各类机器学习应用中（如医疗、金融等）保护隐私的重要性，提出将差分隐私技术与深度学习结合是一个有效的解决方案，并鼓励未来的研究在这一领域的进一步探索。 1 2 3 感觉偏向数学一点，全是公式和证明，好难看懂。 差分隐私顾名思义就是用来防范差分攻击的 加入噪声，改变原来的概率分布 Deep Reinforcement Learning from Human Preferences 《Deep Reinforcement Learning from Human Preferences》是由Christiano等人在2017年提出的一篇重要论文，探讨了如何利用人类偏好来训练强化学习模型。以下是其主要内容的总结：\n背景：\n传统的强化学习方法通常依赖于奖励信号来指导学习，但设计有效的奖励函数在复杂任务中往往困难且耗时。 人类可以提供更直观的偏好信息，这为训练强化学习代理提供了一种新途径。 方法概述：\n人类偏好收集：论文提出了一种方法，通过收集人类对不同行为的偏好来生成奖励信号。人类评估多个策略的表现，从中选择更好的策略，以此来建立奖励模型。 奖励模型：利用收集到的人类偏好数据，训练一个深度神经网络来预测代理在给定状态下的奖励。这种奖励模型可以用来指导强化学习算法。 训练过程：\n深度强化学习：在获得奖励模型后，使用深度强化学习算法（如Proximal Policy Optimization，PPO）来训练代理。代理通过与环境交互，不断优化其策略。 迭代学习：通过不断收集新的偏好数据并更新奖励模型，实现迭代学习，使得代理能够逐步提升性能。 实验结果：\n论文在多个复杂任务（如Atari游戏和模拟环境）中展示了该方法的有效性，代理能够通过学习人类的偏好，超越基于手动设计的奖励函数的表现。 结果表明，利用人类偏好进行训练能够加速学习过程，并提升代理的最终性能。 意义与展望：\n该方法展示了人类偏好在强化学习中的潜力，提出了一种新的思路来解决奖励设计问题。 论文强调了未来研究中结合人类反馈和偏好的重要性，特别是在涉及复杂决策和安全性问题的领域。 附录 残差连接（residual connection）和层归一化（layer normalization）是什么？ 残差连接（Residual Connection）和 层归一化（Layer Normalization）是深度学习中常用的两种技术，主要用于提高神经网络的训练效率和性能。下面是它们的定义和作用：\n残差连接（Residual Connection） 定义：残差连接是一种将输入直接与输出相加的结构，通常在深度神经网络中使用。公式表示为： $$\r\\text{Output} = \\text{Layer}(x) + x\r$$ 其中 \\(x\\) 是输入，\\(\\text{Layer}(x)\\) 是通过某个层（如卷积层或全连接层）处理后的输出。\n作用：\n缓解梯度消失问题：在非常深的网络中，梯度可能会随着层数增加而消失，导致网络难以训练。残差连接提供了一条捷径，使得梯度可以更容易地通过网络传播。 加速收敛：通过引入直接路径，残差连接有助于提高网络的收敛速度。 提高模型性能：它允许模型学习恒等映射，这使得训练更深的网络成为可能，提升了模型的表现。 层归一化（Layer Normalization） 定义：层归一化是一种对网络中每一层的激活进行归一化的技术，它通过计算每个样本的均值和标准差来调整激活值。层归一化的公式为： $$\r\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta\r$$ 其中：\n\\(x\\) 是输入。 \\(\\mu\\) 是输入的均值。 \\(\\sigma\\) 是输入的标准差。 \\(\\epsilon\\) 是一个小常数，避免除以零。 \\(\\gamma\\) 和 \\(\\beta\\) 是可学习的参数，用于缩放和平移归一化的结果。 作用：\n减少内部协变量偏移：层归一化帮助减少模型训练过程中每层输入的变化，使得网络在训练时更加稳定。 提高训练速度：通过标准化输入，层归一化可以使得训练过程更加平滑，从而加快收敛速度。 适用于变长序列：与批量归一化（Batch Normalization）不同，层归一化可以直接应用于变长的输入序列，适合于RNN和Transformer等模型。 总结 残差连接主要通过提供捷径来缓解深层网络中的梯度消失问题，并加速训练过程。 层归一化则通过标准化激活值来提高训练的稳定性和速度。这两者结合使用，能够显著提升深度学习模型的表现。 ","date":"2024-10-20T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/second-week/leaves-6729056_640_hu5688234115053010817.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/second-week/","title":"2024.10.14至20第二周工作总结"},{"content":"工作总结： 文献阅读 综述类： 2023年国内大模型发展综述与趋势研判_赵子忠\nAIGC大模型测评综述：使能技术、安全隐患和应对_许志伟\nAI大模型发展综述_张乾君\n深入： 《A Neural Probabilistic Language Model》是Yoshua Bengio等人在2003年发表的一篇重要论文，提出了一种基于神经网络的语言模型方法。这篇论文的主要目标是通过神经网络来改进传统的语言模型。\n传统的语言模型的任务是估计一个词序列的概率： P(w1,w2,…,wT) 在这种情况下，模型需要根据上下文词来预测当前词的概率。传统的做法一般是通过统计方法（如n元语法模型）来估计这些概率，但这类方法有一个很大的问题：它们无法处理高维数据，尤其是在词汇量非常大的情况下，计算复杂度极高，且泛化能力有限。\nBengio等人提出了一种新的神经网络语言模型，目的是克服这个问题。这个模型的核心思想是使用一个多层感知器来学习词的分布式表示（即词向量），并基于这些词向量来估计概率。具体过程大致如下：\n词嵌入：每个词通过一个嵌入矩阵被映射为一个连续向量，这种向量的维度比词汇表要小得多。这一步的目的是将离散的词映射到一个连续的低维空间，减少计算复杂度。\n神经网络建模：利用一个前馈神经网络（通常是多层感知器），根据前面的若干个词的词向量，来预测下一个词的条件概率。通过这个网络，模型可以学习到词与词之间的相似性和上下文关系。\n训练：模型通过最大化训练数据的似然估计来进行训练。具体来说，模型会调整嵌入矩阵和网络的参数，使得预测的概率尽可能接近真实的词序列概率。\n这篇论文的创新之处在于提出了“词向量”的概念，并展示了使用神经网络来处理语言建模问题的潜力。这为后来的深度学习在自然语言处理（NLP）中的应用奠定了基础。\n附录 什么是语言模型 语言模型（Language Model, LM）是自然语言处理中的一个核心概念，其主要任务是对一个词序列的概率分布进行建模。简单来说，语言模型的目标是估计一段文本或句子出现的概率，或者根据前面的词来预测下一个词。\n语言模型的定义 给定一个词序列 $$\rP( w_1, w_2, \\dots, w_T )\r$$ ，语言模型的目标是计算这个序列的联合概率： $$\rP(w_1, w_2, \\dots, w_T)\r$$ 这通常可以分解为条件概率的乘积： $$\rP(w_1, w_2, \\dots, w_T) = P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_1, w_2) \\cdot \\dots \\cdot P(w_T | w_1, w_2, \\dots, w_{T-1})\r$$ 这意味着每个词的概率取决于它前面的词。一个好的语言模型能够通过学习语料库中的模式，准确地捕捉这些条件概率。\n语言模型的用途 语言模型有广泛的应用场景，包括但不限于：\n文本生成：语言模型可以用来自动生成自然语言文本。例如，给定一部分句子，模型可以预测并生成合理的下一个词，从而逐步生成完整的句子或段落。\n机器翻译：在机器翻译中，语言模型帮助翻译系统生成符合目标语言语法的翻译结果。\n语音识别：语言模型能够帮助语音识别系统，在可能的识别结果中选择最合适的词序列。\n拼写纠正：在文本输入或搜索引擎中，语言模型可以用于纠正拼写错误，提供更符合语言习惯的结果。\n传统语言模型 在神经网络语言模型之前，最常用的语言模型是n元语法模型（n-gram Model）。n元语法模型通过假设一个词的概率只与它前面的n-1个词有关来简化问题。举个例子，三元语法模型（trigram model）认为： $$\rP(w_T | w_1, w_2, \\dots, w_{T-1}) \\approx P(w_T | w_{T-2}, w_{T-1})\r$$这种简化降低了计算复杂度，但也限制了模型的表达能力，因为它只能捕捉到短距离的上下文信息，无法充分考虑长距离的依赖关系。\n神经网络语言模型 传统语言模型的一个主要问题是词汇量增大时，模型的计算复杂度随之增加。为了解决这个问题，Yoshua Bengio等人在《A Neural Probabilistic Language Model》论文中提出了使用神经网络进行语言建模的想法。这种神经网络模型通过将词表示为词向量（word embeddings），并通过深度学习模型来捕捉更复杂的语境和词间关系，从而解决了传统语言模型的许多不足之处。\n总之，语言模型的核心任务就是对自然语言的概率进行建模，并通过学习语言中的统计模式来预测文本。\n什么是纬度灾难？ 维度灾难（Curse of Dimensionality）是指在处理高维数据时，随着数据维度增加，问题的复杂性呈指数级增长，导致很多传统方法在高维空间中变得低效或不可行的现象。\n其影响包括：\n数据稀疏性：在高维空间中，数据点变得非常稀疏，因为数据的总量相对于空间的体积变得极其小，导致模型难以从有限的数据中捕捉到模式和关系。简单来说，随着维度的增加，需要的样本量也随之爆炸性增长，否则模型可能会过拟合或无法泛化。\n计算复杂度增加：在高维空间中，计算距离、相似性等操作的复杂度大大增加，使得很多算法在高维情况下效率低下。\n模型的泛化能力下降：由于数据点过于稀疏，模型很难学习到有用的特征，从而可能导致泛化能力下降。\n具体来说，维度灾难对机器学习、统计学、最优化等领域带来了极大的挑战，尤其是当模型需要处理很多特征或变量时。为应对这个问题，研究人员提出了许多方法，如降维技术（如主成分分析PCA）和分布式表示（如词嵌入），以减少有效维度或通过分布式方法来表示复杂的数据结构。\n有概率为什么要引入似然的概念，他们有什么不同？ 概率和似然虽然看起来相似，但在统计学中它们有不同的定义和用途：\n概率（Probability） 概率是用来描述随机事件发生的可能性。在给定模型参数的情况下，概率告诉我们某个事件或观测结果发生的机会有多大。\n假设我们有一个随机变量 \\( X \\)，它取某个值 \\( x \\) 的概率可以表示为 \\( P(X = x|\\theta) \\)，其中 \\( \\theta \\) 是模型的参数。例如，如果我们有一个硬币掷出的模型，参数 \\( \\theta \\) 可以是硬币正面朝上的概率（比如 \\( \\theta = 0.5 \\)）。\n核心问题：给定模型的参数 \\( \\theta \\)，事件 \\( X = x \\) 发生的概率是多少？ 似然（Likelihood） 似然是从反方向来考虑的。它描述的是在已知观测数据的前提下，模型参数的可能性。具体来说，似然是给定观测数据后，模型参数如何使观测数据变得最可能的一个度量。\n假设我们已经观察到数据 \\( X = x \\)，现在我们想知道，在不同的模型参数 \\( \\theta \\) 下，这个数据出现的可能性有多大。似然可以表示为 \\( L(\\theta|X = x) \\)，或者更直观地写作 \\( P(X = x|\\theta) \\)，虽然数学表达式相同，但意义不同——在这里我们关注的是参数 \\( \\theta \\) 的值使观测到的数据最可能。\n核心问题：给定观测到的数据 \\( X = x \\)，模型参数 \\( \\theta \\) 有多大可能是正确的？ 概率和似然的区别 概率：我们知道参数 \\( \\theta \\)，希望知道某个事件 \\( X \\) 发生的概率。\n问题：给定参数，事件的概率是多少？ 似然：我们知道事件（观测数据），希望推断出最有可能的参数 \\( \\theta \\)。\n问题：给定观测数据，哪个参数最可能是正确的？ 例子 抛硬币的例子 概率：假设你有一枚硬币，已知它是公平的，即 \\( \\theta = 0.5 \\)，那么掷硬币得到正面的概率是 \\( P(\\text{正面}) = 0.5 \\)。\n似然：现在，你掷硬币10次，得到8次正面和2次反面。你不知道硬币的公平性（即不知道 \\( \\theta \\) 的值）。你想根据这次实验结果估计硬币正面朝上的概率 \\( \\theta \\)。此时，你需要用似然来衡量在不同 \\( \\theta \\) 值下，观测结果（8次正面，2次反面）出现的可能性有多大，从而找到最有可能的 \\( \\theta \\)。\n似然函数 \\( L(\\theta|X) \\) 可能在某个 \\( \\theta \\) 值处达到最大值，这个 \\( \\theta \\) 就是最能解释观测数据的参数值。\n总结 概率用于给定模型参数时预测事件的发生可能性。 似然用于在已知观测数据时，推断哪个参数最能解释这些数据。 ","date":"2024-10-13T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/first-week/banlan_hu6307248181568134095.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/first-week/","title":"2024.10.7至13第一周工作总结"},{"content":"正文测试 而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用 思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n图片 1 2 3 ![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2021-01-18T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu4699868770670889127.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/test-chinese/","title":"Chinese Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu6307248181568134095.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\nExierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\nComas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt The Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/placeholder-text/matt-le-SJSpo9hQf7s-unsplash_hu10664154974910995856.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/placeholder-text/","title":"Placeholder Text"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\nBlock math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","date":"2019-03-08T00:00:00Z","permalink":"https://JiangZhiyu-1024.github.io/p/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3 .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","date":"2019-03-05T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_hu5876398126655421130.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/emoji-support/","title":"Emoji Support"}]