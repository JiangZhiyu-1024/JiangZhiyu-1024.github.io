[{"content":"💐解码与部署 大语言模型是通过文本生成的方式进行工作的。在自回归架构中，模型针对输入内容逐个单词生成输出内容的文本。这个过程一般被称为 解码。\n🌸解码策略 大语言模型的生成方式本质上是一个概率采样过程，需要合适的解码策略来生成合适的输出内容。\n🌹语言模型解码的背景知识 模型M每次根据当前上下文词元序列𝒖=[𝑢1,𝑢2,··· ,𝑢𝑡]建模下一个词的概率分布𝑃（即公式$O = softmax(W^LY_L)$的输出），然后根据一定的解码策略选择下一个词𝑢′，之后再将𝒖和𝑢′作为新的上下文重复上述步骤，直到生成结束词元或者达到长度上限为止。在这个流程中，解码策略将主要关注如何基于概率分布𝑃选择合适的下一个词𝑢′。自回归的解码策略并不局限于特定架构，常见的编码器-解码器、因果解码器和前缀解码器均可适用。\n🌺贪心搜索 目前常见的大语言模型主要是通过语言建模任务进行预训练的。基于这种训练方式，一种直观的解码策略是贪心搜索（Greedy Search）。具体来说，贪心搜索在每个生成步骤中都选择概率最高的词元， 其可以描述为以下形式: $$\ru_i = \\text{argmax}_{u}P(u|U_{","date":"2024-11-01T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/decoderbs/roses-7314623_1280_hu10924616842157365117.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/decoderbs/","title":"解码与部署"},{"content":"🎄提示学习 经过预训练、指令微调和人类对齐后，我们接下来讨论如何通过提示学习方法来有效地使用大语言模型解决实际任务。目前常用的方法是设计合适的提示（Prompting），通过自然语言接口与大模型进行交互。在现有研究中，任务提示的设计主要依靠人工设计和自动优化两种策略来实现。为了更好地 解决未见过的任务，一种典型的提示方法是上下文学习（In-contextLearning,ICL），它将任务描述与示例以自然语言文本形式加入到提示中。此外，思维链提示（Chain-of-Thought, CoT）作为一种增强技术，将一系列中间推理步骤加入到提示中，以增强复杂推理任务的解决效果。\n🎆基础提示 因为大语言模型的微调代价较高，基于自然语言的提示方法已经成为了使用大语言模型解决下游任务的主要途径。由于提示的质量在很大程度上会影响大语言模型在特定任务中的表现，因此一系列工作深入研究了通过人工设计或自动优化的方法来生成合适的任务提示。\n🎇人工提示设计 针对特定任务设计合适的任务提示，这一过程被称为“提示工程”（Prompt Engineering）。\n🧨关键要素 一般而言，针对大语言模型的提示设计需要考虑四个关键要素，即任务描述、输入数据、上下文信息和提示策略。\n任务描述.任务描述部分展示了大语言模型应当遵循的具体指令。一般来说，用户应该使用清晰的、具体的表达来描述任务目标。进一步，某些特定任务还需要对于输入或输出的格式进行更详细的说明，可以使用关键词或者特殊符号来强 调特殊设置以指导大语言模型更好地完成任务。 输入数据. 通常情况下，用户可以直接使用自然语言描述输入数据的内容。对于特殊形式的输入数据，则需要采用合适的方法使其能够被大语言模型读取与理解。例如，对于结构化数据（如知识图谱、表格等），通常使用线性化方法将其转换为易于处理的文本序列[264]。此外，由于结构化数据具有较好的组织形式，可以使用编程代码中的数据结构进行存储表示，将结构化数据中的属性表示为数据结构中的变量。基于代码表示的结构化数据可以使用外部工具（如程序执行器）进行准确地读取。 上下文信息. 除了任务描述和输入数据外，上下文信息对某些特定任务也非常重要。例如，搜索引擎可以为开放问答任务提供参考文档，可以通过将检索到的参考文档以上下文信息的形式引入提示作为大语言模型的输入。在引入外部信息时，需要对于这些信息进行合适的格式化，以加强大语言模型对它们的利用。此外，上下文学习中的任务示例数据也有助于提升大语言模型处理复杂任务的能力，大模型可以通过这些示例数据学习任务目标、输出格式以及输入和输出之间的映射关系。 提示策略. 针对不同的大语言模型设计合适的提示策略对于激发模型解决特定任务的能力非常重要。在某些情况下，添加特定的前缀或后缀有助于引导大语言模型解决复杂任务。例如，使用前缀“让我们一步一步地思考”可以激发大语言模型的逐步推理能力，而使用前缀“你是这项任务（或这个领域）的专家”可以提高大语言模型在某些特定任务（或领域）中的表现。此外，对于对话式的大语言模型（例如ChatGPT），由于其使用了大量对话数据进行训练，因此更合适的做法是将提示拆分为多个子任务提示，以多轮对话的方法逐步输入给大语言模型。 ✨自动提示优化 人工设计提示虽然比较直接，但是需要耗费较多的人工成本，同时要求设计人员具有丰富的提示工程经验。此外，大语言模型对于提示设计比较敏感，人工设计的提示有时很难获得最优的效果，还可能导致任务性能的下降。需要注意的是，由于大语言模型参数量巨大，并且很多工作机制已经与传统预训练模型有着较大的差异，许多提示优化方法已经不再适用于大语言模型。\n🎉离散提示优化 离散提示通常是由一系列自然语言词元组成的完整句子表达（如“请根据提供的检索信息回答下列问题”）。然而，在离散的词元空间中进行组合搜索，不仅时间复杂度高，而且可能导致非最优的搜索结果。下面将介绍四种常见的离散提示优化方法，能够提升离散任务提示的有效性与搜索效率。\n基于梯度的方法. 这类方法通过梯度更新技术以最大化模型的似然分数来优化离散提示的搜索过程。一种早期的代表性方法[1]使用梯度引导技术，首先将提示初始化为一系列“[MASK]”标记，然后迭代地将提示中的词元替换为词典中的其他词元，通过词元替换产生的对数似然变化来近似估计梯度，进而为提示的每个位置贪心搜索出最佳的词元。由于该方法对提示的每个位置都进行所有候选词元的替换和梯度评估，因此需要模型进行多次前向和后向计算，导致搜索过程的效率较低。为了改进搜索效率，可以将离散词元转化为连续嵌入表示（又称为“软词元”），使用梯度直接对连续嵌入参数进行优化，最后将每个连续嵌入映射为词典中最邻近的离散词元。 基于强化学习的方法. 为了实现更有效的离散词元选择，另一种解决方法是将离散提示优化问题转换为强化学习问题，并使用强化学习算法进行求解。具体来说，可以将预训练语言模型作为强化学习中的策略网络并依次生成提示中的词元。在提示生成结束之后，策略网络可以获得任务特定的奖励信号，该奖励信号可通过强化学习算法用于策略网络参数的训练。在实践中，可以设计不同类型的奖励信号，比如真实标签与基于提示的预测标签是否一致、生成文本与给定条件的匹配程度。在最后的测试阶段，基于训练好的策略网络，可以采用贪心搜索策略来生成任务提示中的每个词元。 基于编辑的方法. 这类方法主要关注如何通过编辑现有的提示来改进模型的性能，通常是基于模型在目标任务上的表现来判断提示的好坏。它特别适用于无法直接访问模型内部状态（如梯度）的情况，例如只能通过API调用的模型。在这类方法中，通常需要事先定义好编辑操作，然后迭代地对提示进行修改，直至达到最大迭代轮次或者模型最佳性能。提示的关键要素包括任务描述、输入数据、上下文信息和提示策略。因此，常用的提示编辑操作有修改任务描述、添加或删除上下文任务示例、调整输入到输出的标签映射器（例如可以使用“positive/negative”或者“正/负”表示二分类）等。此外，提示编辑操作也可以根据不同的场景或者需求进行设计，以适配下游具体任务。整体流程可以概述如下：基于预定义的编辑操作，在现有提示的基础上修改得到新提示，并输入至模型得到目标任务上的表现，根据表现筛选出合适的提示。由于上述过程可能需要迭代进行，可以只选择少量测试样例来评估模型表现，以减少计算开销。 基于大语言模型的方法. 由于大语言模型具有通用的任务能力，因此可以将提示优化看作一个待求解的任务，进而直接使用大语言模型作为提示生成器来生成或改进提示[2,3]。基于大语言模型的自动提示生成框架将提示优化过程看作是一个由大语言模型指导的黑盒优化问题。该框架首先利用提示生成模型（用于生成提示指令的大语言模型）基于少量上下文示例生成一批候选的任务指令。随后，使用“目标模型”（用于下游测试的大语言模型）对这些候选指令在目标任务上的表现进行逐一评估。在评估过程中，可以采用模型困惑度或任务准确率作为衡量指令质量的指标。上述过程可以通过基于蒙特卡洛搜索的多轮优化策略进行扩展。在每一轮迭代中，根据模型表现对候选指令进行筛选得到高评分指令，并利用大语言模型生成与高评分指令相似的新指令，从而扩展候选指令集。迭代完成后，选择模型表现最佳的候选指令作为最终使用的提示。然而，上述方法没有充分考虑提示的整个历史改进轨迹，因此可能在提示搜索过程中陷入局部最优或者产生效果震荡，无法生成更好的提示。为了解决这一问题，可以将所有改进的历史提示及其分数纳入提示优化过程，以指导大语言模型逐步生成更好的新提示。 🎊连续提示优化 与离散提示不同，连续提示由一组连续空间中的嵌入向量组成，可以根据下游任务的损失直接通过梯度更新进行优化。值得注意的是，已有连续提示优化的工作主要是基于预训练语言模型开展的，由于大语言模型参数量巨大，连续提示受到的关注较为有限。已有的连续提示优化研究通常依赖于有监督学习方法。当数据稀缺的情况下，还可以采用迁移学习方法来缓解目标任务标注数据不足的问题。\n监督学习方法. 这类方法将连续提示向量视为可训练的模型参数，基于下游任务数据，通过最小化交叉熵损失来优化连续提示。Prefix-tuning [4] 会在语言模型的每个Transformer 层预置一串前缀（即一组可训练的连续向量），而Prompt-tuning[5]只会在输入层加入可训练的提示向量。 通过固定语言模型的大规模参数而只微调这些连续的提示向量，可以有效节省训练所需要的参数量。然而，这些提示优化方法通常与输入无关，缺乏对于输入语义的充分考虑。 迁移学习方法. 有监督学习方法通常需要充足的训练数据来学习最优的任务提示，很难在数据稀缺场景下获得较好的模型性能。为了解决这个问题，基于提示的迁移学习方法首先为若干个具有代表性的源任务学习一个所有任务共享的连续提示，然后使用该提示初始化目标任务的提示，这可以为下游任务的提示优化提供良好的初始点。然而，这种方法存在一定的局限性，它在解决目标任务的所有实例时都使用了相同提示，而即使是一个精心优化过的提示也未必适合所有的任务实例。为了解决这一问题，可以为每个源任务独自学习任务特定的连续提示（而不是所有源任务共享），在解决目标任务的实例时，可以采用注意力机制等方式学习目标实例与每个源任务提示的相关性权重系数，对若干个源任务的提示向量进行加权组合，将组合后的新提示（为连续向量形式）用于帮助模型解决当前任务实例。 🎋上下文学习 在GPT-3的论文[6]中，OpenAI研究团队首次提出上下文学习（In-context learning,ICL）这种特殊的提示形式。目前，上下文学习已经成为使用大语言模型解决下游任务的一种主流途径。下面将详细介绍这一提示方法。\n🎍上下文学习的形式化定义 根据GPT-3论文中所给出的描述[6]，上下文学习使用由任务描述和（或）示例所组成的自然语言文本作为提示。图10.1展示了上下文学习的提示构建过程。首先，通过自然语言描述任务，并从任务数据集中选择一些样本作为示例。其次，根据特定的模板，将这些示例按照特定顺序组合成提示内容。最后，将测试样本添加到提示后面，整体输入到大语言模型以生成输出。基于任务描述以及示例信息，大语言模型无需显式的梯度更新即可识别和执行新的任务。\n形式上，我们使用𝐷𝑘={𝑓(𝑥1,𝑦1),\u0026hellip;, 𝑓(𝑥𝑘,𝑦𝑘)}来表示由𝑘个样本构成的一组示例数据，其中𝑓(𝑥𝑘,𝑦𝑘)是一个函数，负责将第𝑘个任务样本转换为自然语言提示。给定任务描述𝐼、示例𝐷𝑘以及新的输入𝑥𝑘+1，大语言模型生成答案ˆ 𝑦𝑘+1的过程可以通过下面的公式来表述：\n值得一提的是，上下文学习与指令微调之间存在着紧密的联系，因为它们都涉及将任务或样本转化为自然语言形式供大语言模型进行处理。在原始的GPT-3论文中，作者将上下文学习的提示定义为任务描述和示例的组合，这两部分均为可选。按照这个定义，如果大语言模型仅通过任务描述（即任务指令）来解决未见过的任务，也可以被看作是上下文学习的一种特例。两者的主要区别是，指令微调需要对大语言模型进行微调，而上下文学习仅通过提示的方式来调用大语言模型解决任务。此外，指令微调还可以有效提升大语言模型在执行目标任务时的上下文学习能力，尤其是在零样本场景下（即仅依赖任务描述而无需额外的示例）。\n🎁思维链提示 思维链提示[7,8]是一种高级提示策略，旨在增强大语言模型在各类复杂推理任务上的表现。常见的推理任务包括算术推理[9]、常识推理[9]以及符号推理[7]等多种任务。与上下文学习方法仅使用⟨输入，输出⟩二元组来构造提示不同，思维链提示进一步融合了中间的推理步骤来指导从输入到输出的推理过程。下图展示了一个思维链提示的具体例子。\n思维链提示技术的演化过程\r🎑参考文献 [1]. Taylor Shin et al. “AutoPrompt: Eliciting Knowledge from Language Models with Auto matically Generated Prompts”. In: EMNLP. 2020.\n[2]. Yongchao Zhou et al. “Large Language Models are Human-Level Prompt Engineers”. In: ICLR. 2023.\n[3]. Chengrun Yang et al. “Large Language Models as Optimizers”. In: arXiv preprint arXiv: 2309.03409 (2023).\n[4]. XiangLisa Li and Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Gen eration”. In: ACL. 2021.\n[5]. Brian Lester, Rami Al-Rfou, and Noah Constant. “The Power of Scale for Parameter Efficient Prompt Tuning”. In: EMNLP. 2021.\n[6]. TomB.Brown et al. “Language Models are Few-Shot Learners”. In: NeurIPS. 2020.\n[7]. JasonWeietal. “Chain of Thought Prompting Elicits Reasoning in Large Language Mod els”. In: arXiv preprint arXiv:2201.11903 (2022).\n[8]. Zheng Chu et al. “A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future”. In: arXiv preprint arXiv:2309.15402 (2023).\n[9]. Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. “A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers”. In: ACL. 2020.\n[10]. AlonTalmoret al. “CommonsenseQA: A Question Answering Challenge Targeting Com monsense Knowledge”. In: NAACL-HLT. 2019.\n","date":"2024-11-01T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/prompt_learning/birch-trees-8345812_1280_hu1913448854319438606.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/prompt_learning/","title":"提示学习"},{"content":"🍁人类对齐 在大语言模型的学习过程中，如何确保大语言模型的行为与人类价值观、人类真实意图和社会伦理相一致成为了一个关键研究问题，通常称这一研究问题为人类对齐（HumanAlignment）。\n🍂人类对齐的背景与标准 🌰背景 尽管大语言模型在下游任务中表现出优秀的性能，这些模型有时会出现错误或具有危害性的行为，例如无法正确遵循指令、生成虚假信息、以及产生有害、有误导性以及带有偏见的表达。在大语言模型的预训练和有监督微调的过程中，主要训练目标是根据上下文内容来预测下一个词元。但是，这一过程并未充分考虑人类的价值观或偏好，可能导致大语言模型从数据中学习到不符合人类期望的生成模式。为了规避这些潜在风险，研究人员提出了“人类对齐”这一关键概念，旨在保证大语言模型的行为与人类期望和价值观相一致[1,2]。\n与预训练和指令微调不同，人类对齐需引入全新的评估标准，如有用性、诚实性和无害性。 为了更直观地理解人类对齐对于大语言模型的重要性，例1对比了同一个语言模型在对齐前后对于相同输入的不同输出。在这个例子当中，输入的问题刻意包含了具有误导性的逻辑关系，即“土地价格”和“有污染的产业”是有直接关系的。因此，在经过人类价值观对齐之前的大语言模型会被输入中的错误逻辑所引导，产生了带有偏见的建议“农村地区更适合发展污染较严重的产业”。在经济生产中，发展有污染的产业需要综合考虑多方面的因素，不能仅仅因为土地价格更为便宜就认为适合发展相关产业。对齐前的大语言模型给出了一个错误的观点，不符合人类价值观，违背了无害性的原则。而经过与人类价值观对齐之后的大语言模型，先指出了输入问题中包含的错误逻辑（“我们不能简单地认为农村土地价格便宜就适合发展污染产业。”），并且给出了正确且合理的做法。对齐后的大语言模型的回复符合有用性和无害性，与人类价值观和偏好相符。\n例1 大语言模型（YuLan）对于相同输入在对齐前后的不同输出\r思考：\n之前我就遇到过类似的情况，在前面的文章中我也提到过，LLM似乎就是倾向于认同人类的提问（就算那是错误的），到这里明白了，原来是没有经过对齐。那么我就有新的问题了，对齐也是要训练的吧，而且得要人来准备数据，这就又是一个耗费人力的工程了。\n🍠对齐标准 人类对齐是一个较为抽象的概念，难以直接进行形式化建模，关于对齐的定义和标准也存在不同的观点。这里主要围绕三个具有代表性的对齐标准展开讨论，分别是有用性（Helpfulness）、诚实性（Honesty）和无害性（Harmlessness），这三种对齐标准已被现有的大语言模型对齐研究广泛使用[1,3]。下面具体介绍这三个代表性的对齐标准。\n有用性. 在实际应用中，大语言模型需要提供有用的信息，能够准确完成任务，正确理解上下文，并展现出一定的创造性与多样性。模型应尽量以简洁、高效的方式协助用户完成任务。当任务描述存在歧义或涉及背景信息时，模型应具备主动询问并获取任务相关信息的能力，同时具有一定的敏感度、洞察力和审慎态度。由于用户意图的多样性，有用性这一对齐标准仍然难以进行统一的定义与刻画，需要根据不同的用户进行确定。 诚实性. 模型的输出应具备真实性和客观性，不应夸大或歪曲事实，避免产生误导性陈述，并能够应对输入的多样性和复杂性。在人机交互过程中，大语言模型应向用户提供准确内容，还应适当表达对于输出信息的不确定性程度，以避免任何形式的误导。本质上，这要求模型了解自身的能力和知识水平。与有用性和无害性相比，诚实性是一个更为客观的标准，对人类标注的依赖相对较少。 无害性. 大语言模型应避免生成可能引发潜在负面影响或危害的内容。在处理敏感主题时，模型应遵循道德标准和社会价值观，从而消除冒犯性与歧视性。此外，模型需要能够检测到具有恶意目的的查询请求。当模型被诱导执行危险行为（如犯罪行为）时，应直接予以拒绝。然而，何种行为被视为有害，很大程度上取决于大语言模型的使用者、用户问题类型以及使用大语言模型的背景。 上述三种通用的对齐标准较为宽泛，因此许多研究针对性地给出了一些更为细化的对齐标准，以更全面地规范大语言模型的输出。例如，行为对齐要求人工智能系统能够做出符合人类期望的行为；在此基础上，意图对齐则进一步要求大语言模型在意图和行为上都要与人类期望保持一致，这涉及到哲学、心理学以及技术细节上的多重挑战；道德对齐要求语言模型应避免涉及非法、不道德或有害的话题，在回应中优先考虑用户安全、道德准绳和行为边界。这些对齐标准在本质上与前述三个标准是相似的，研究人员可以根据任务的特定需求进行调整。\n在实践中，红队攻击（RedTeaming）技术也被广泛运用，通过人工或自动 化的手段，以对抗方式探测大语言模型，诱导其生成有害输出，并据此针对性地 调整大语言模型，以避免产生此类有害输出[4]。\n🍊基于人类反馈的强化学习 由于对齐标准难以通过形式化的优化目标进行建模，因此研究人员提出了基于人类反馈的强化学习（Reinforcement Learning from Human Feedback,RLHF），引入人类反馈对大语言模型的行为进行指导。\n🥮RLHF 概述 RLHF首先需要收集人类对于不同模型输出的偏好，然后使用收集到的人类反馈数据训练奖励模型，最后基于奖励模型使用强化学习算法（例如Proximal Policy Optimization,PPO[5]）微调大语言模型。这种将人类反馈纳入大语言模型训练过程的方法已成为实现人类对齐的主要技术途径之一。\n🎑RLHF算法系统 RLHF 算法系统主要包括三个关键组成部分：需要与人类价值观对齐的模型、基于人类反馈数据学习的奖励模型以及用于训练大语言模型的强化学习算法。\n具体来说，待对齐模型一般指的是经过预训练、具备一定通用能力的大语言模型。然而，这些模型并没有与人类价值观对齐，在下游任务中可能表现出不合适甚至有害的行为。\n奖励模型的作用是为强化学习过程提供指导信号，反映了人类对于语言模型生成文本的偏好，通常以标量值的形式呈现。奖励模型既可以采用人类偏好数据对已有的语言模型继续微调，也可以基于人类偏好数据重新训练一个新的语言模型。\n在训练过程中，基于奖励模型提供的反馈信号，RLHF使用特定的强化学习算法进行大语言模型的训练。目前，PPO算法[5] 是一种被广泛用于人类对齐的强化学习算法。\n🥧RLHF的关键步骤 监督微调. 为了让待对齐语言模型具有较好的指令遵循能力，通常需要收集高质量的指令数据进行监督微调。指令数据一般包括任务描述和示例输出，可以由人类标注员针对特定任务编写，也可以由大语言模型自动生成。\n奖励模型训练.第二步是使用人类反馈数据训练奖励模型。具体来说，首先使用语言模型针对任务指令生成一定数量的候选输出。随后，邀请标注员对于输出文本进行偏好标注，这个标注过程可以采用多种形式，其中最常用的是对候选文本进行排序标注，这样可以有效减少多个标注员之间的不一致情况。进一步，使用人工标注的偏好数据进行奖励模型的训练，使其能够建模人类偏好。\n强化学习训练. 在这一步骤中，语言模型对齐被转化为一个强化学习问题。具体来说，待对齐语言模型担任策略实施者的角色（称为策略模型），它接收提示作为输入并返回输出文本，其动作空间是词汇表中的所有词元，状态指的是当前已生成的词元序列。\n🔥人类反馈数据的收集 在预训练阶段，大语言模型通过语言建模目标在大规模无标注语料库上进行训练。然而，这一过程无法直接反映人类对于大语言模型输出的主观和定性偏好（本书中称为“人类反馈”）。为了实现有效的人类对齐，需要使用高质量的人类反馈数据（不是高质量人类，哈哈哈）对大语言模型进行针对性的微调。\n🧡人类反馈形式 基于评分的人类反馈.最直接的标注方式是根据预设的标准邀请标注人员对于大语言模型的输出进行打分，从而作为模型输出质量的判断。 基于排序的人类反馈.排序是一种比较典型的人类偏好标注形式。最简单的方式是标注人员根据自身偏好对于大语言模型的输出进行全排序。 🏮奖励模型的训练 由于RLHF的训练过程中需要依赖大量的人类偏好数据进行学习，因此很难在训练过程中要求人类标注者实时提供偏好反馈。为此，我们需要训练一个模型来替代人类在RLHF训练过程中实时提供反馈，这个模型被称为奖励模型。\n思考：\n这里倒是解决我的疑问了，可以训练一个奖励模型干这个事。\n🀨非强化学习的对齐方法 尽管RLHF已被证明是一种较为有效的语言模型对齐技术，但是它也存在一些局限性。首先，在RLHF的训练过程中，需要同时维护和更新多个模型，这些模型包括策略模型、奖励模型、参考模型以及评价模型。这不仅会占用大量的内存资源，而且整个算法的执行过程也相对复杂。此外，RLHF中常用的近端策略优化算法在优化过程中的稳定性欠佳，对超参数的取值较为敏感，这进一步增加了模型训练的难度和不确定性。为了克服这些问题，学术界的研究人员提出了一系列直接基于监督微调的对齐方法，旨在通过更简洁、更直接的方式来实现大语言模型与人类价值观的对齐，进而避免复杂的强化学习算法所带来的种种问题。\n☕代表性监督对齐算法DPO 直接偏好优化（DirectPreference Optimization, DPO）是一种不需要强化学习的对齐算法。由于去除了复杂的强化学习算法，DPO可以通过与有监督微调相似的复杂度实现模型对齐，不再需要在训练过程中针对大语言模型进行采样，同时超参数的选择更加容易。\n🌆参考文献 [1]. Long Ouyang et al. “Training language models to follow instructions with human feed back”. In: arXiv preprint arXiv:2203.02155 (2022).\n[2]. Daniel M. Ziegler et al. “Fine-Tuning Language Models from Human Preferences”. In: arXiv preprint arXiv:1909.08593 (2019).\n[3]. Amelia Glaese et al. “Improving alignment of dialogue agents via targeted human judge ments”. In: arXiv preprint arXiv:2209.14375 (2022).\n[4]. Ethan Perez et al. “Red Teaming Language Models with Language Models”. In: EMNLP. 2022.\n[5]. John Schulman et al. “Proximal policy optimization algorithms”. In: arXiv preprint arXi v:1707.06347 (2017).\n","date":"2024-10-31T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/humanalignment/flowers-7382926_1920_hu9801049349447409777.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/humanalignment/","title":"HumanAlignment"},{"content":"指令微调（Instruction Tuning） 指令微调（Instruction Tuning）是指使用自然语言形式的数据对预训练后的大语言模型进行参数微调，这一术语由谷歌研究员在2022年的一篇ICLR论文中正式提出[1]。在另外一些参考文献中，指令微调也被称为有监督微调（Supervised Fine-tuning）[2] 或多任务提示训练（MultitaskPromptedTraining）[3]。指令微调过程需要首先收集或构建指令化的实例，然后通过有监督的方式对大语言模型的参数进行微调。经过指令微调后，大语言模型能够展现出较强的指令遵循能力，可以通过零样本学习的方式解决多种下游任务。\n🏔️指令数据的构建 一般来说，一个经过指令格式化的数据实例包括任务描述（也称为指令）、任务输入-任务输出以及可选的示例。\n⛰️基于现有的NLP任务数据集构建 学术界围绕传统NLP任务（如机器翻译、文本摘要和文本分类等）发布了大量的开源数据集合，这些数据是非常重要的监督学习数据资源，可以用于指令数据集的构造。通常来说，这些NLP数据集都包括输入和输出两个主要部分。\n例如，在中英翻译任务中，输入是“大语言模型已经成为机器学习的一个重要研究方向”，而相应的输出则是“Large language models have become one important research direction for machine learning”。\n为了生成指令化的训练数据，一个非常关键的步骤就是为上述的“输入-输出”对数据添加任务描述信息，用于指导模型去理解任务目标以及相关信息。在上述的例子中，可以向中译英的翻译数据集中添加指令， 例如“请把这个中文句子翻译成英文”。通过上述操作，就可以将一个NLP任务的数据实例全部通过自然语言形式进行表达，进而数据实例可以被用于大语言模型的指令微调。\n现有NLP数据集的指令格式化示意图\r为了更好地标注NLP指令微调数据，研究 人员开发了众包平台PromptSourcehttps://github.com/bigscience-workshop/promptsource，它能够高效地支持标注人员为不同数据集创建、共享及验证任务描述。\n🌋基于日常对话数据构建 尽管通过指令格式化已有的NLP数据集能够获得大量的指令数据实例，但是这些数据的多样性比较局限，与人类的真实需求也不能很好匹配。为此，研究人员开始使用用户在日常对话中的实际需求作为任务描述。例如，InstructGPT[2]将用户提交给OpenAI API的查询作为任务描述。由于这些用户查询源自于真实应用场景，均采用自然语言形式进行表达，因此特别适合大模型学习指令跟随能力。为了进一步增加任务的多样性，OpenAI还雇佣标注者创作更多的真实生活任务，包括开放式生成、开放式问答、头脑风暴等任务，然后由另一组标注者来回答这些问题作为输出。OpenAI最终将指令（用户真实查询或者人工标注的任务）和期望的输出（人工编写的答案）配对作为一个训练实例。但是，OpenAI没有对外开放所使用的指令数据。\n🗻基于合成数据构建 Self-Instruct \u0026mdash;\u0026mdash;-\u0026gt; Yizhong Wang et al. “Self-Instruct: Aligning Language Model with Self Generated In structions”. In: arXiv preprint arXiv:2212.10560 (2022).\nEvol-Instruct \u0026mdash;\u0026mdash;-\u0026gt; Can Xu et al. “WizardLM: Empowering Large Language Models to Follow Complex In structions”. In: arXiv preprint arXiv:2304.12244 (2023).\n🏖️指令微调的训练策略 🏜️优化设置 指令微调中的优化器设置（AdamW或Adafactor）、稳定训练技巧（权重衰减和梯度裁剪）和训练技术（3D并行、ZeRO和混合精度训练）都与预训练保持阶段一致，可以完全沿用。\n不同之处：\n目标函数. 预训练阶段通常采用语言建模损失，优化模型在每一个词元上的损失。而指令微调可以被视为一个有监督的训练过程，通常采用的目标函数为序列到序列损失，仅在输出部分计算损失，而不计算输入部分的损失。 批次大小和学习率.考虑到预训练阶段已经学习到了能够展现较好性能的模型参数，指令微调阶段通常只需要使用较小的批次大小和学习率对模型进行小幅度的调整。例如InstructGPT(175B)微调的批次大小为8，学习率恒定为5.03×10−6；Alpaca (7B) 微调的批次大小为128，学习率预热到2×10−5，然后采用余弦衰减策略。 多轮对话数据的高效训练.对于一个多轮对话数据，通常的训练算法是将其拆分成多个不同的对话数据进行单独训练。为了提升训练效率，可以采用特殊的掩码机制来实现多轮对话数据的高效训练。在因果解码器架构中，由于输入输出没有明显的分界，可以将所有一个对话的多轮内容一次性输入模型，通过设计损失掩码来实现仅针对每轮对话的模型输出部分进行损失计算，从而显著减少重复前缀计算的开销。 🏝️数据组织策略 平衡数据分布：最常见的方法是样本比例混合策略，即把所有数据集进行合并，然后从混合数据集中等概率采样每个实例。例如，研究者建议混合使用NLP任务数据（如FLANv2）、对话数据（如ShareGPT）和合成数据（如GPT4-Alpaca），来进行大模型的指令微调。\n多阶段指令数据微调：首先使用大规模NLP任务指令数据对模型进行微调，然后再使用相对多样的日常对话指令和合成指令进一步微调。为了避免能力遗忘问题，可以在第二阶段中添加一些NLP指令数据。\n结合预训练数据与指令微调数据：为了使得微调过程更加有效和稳定，可以在指令微调期间引入预训练数据和任务，这可以看作是对于指令微调的正则化。\n🏛️参数高效的模型微调 通过指令微调，大语言模型能够更好地学习遵循和执行人类指令。然而，由于大语言模型的参数量巨大， 进行全参数微调（需要较多的算力资源开销）在本节中，我们将讨论如何针对大语言模型进行参数高效微调（Parameter-efficient Fine-tuning），也称为轻量化微调 （Lightweight Fine-tuning）。在现有文献中，参数高效微调[4,5,6] 是一个重要的 研究方向，旨在减少需要训练的模型参数量，同时保证微调后的模型性能能够与全量微调的表现相媲美。下面将首先介绍常用于Transformer架构的参数高效微调方法，然后以LoRA微调方法为例介绍参数高效微调的代码实现。\n🏘️低秩适配微调方法 LoRA微调示意图\r🏬LoRA基础 大语言模型中包含大量的线性变换层，其中参数矩阵的维度通常很高。研究人员[6]发现模型在针对特定任务进行适配时，参数矩阵往往是过参数化（Over-parametrized）的，其存在一个较低的内在秩。为了解决这一问题，LoRA[6] 提出在预训练模型的参数矩阵上添加低秩分解矩阵来近似每层的参数更新，从而减少适配下游任务所需要训练的参数。给定一个参数矩阵𝑾，其更新过程可以一般性地表达为以下形式： $$\rW = W_0 +ΔW\r$$ 其中，$W_0$是原始参数矩阵，Δ𝑾是更新的梯度矩阵。LoRA的基本思想是冻结原始矩阵$W_0 ∈ R^{H \\times H}$，通过低秩分解矩阵$A ∈ R^{H \\times H}$和$B ∈ R^{H \\times H}$来近似参数更新矩阵$Δ𝑾 = A·B^T$，其中𝑅≪𝐻是减小后的秩。在微调期间，原始的矩阵参数$W_0$不会被更新，低秩分解矩阵𝑨和𝑩则是可训练参数用于适配下游任务。在前向传播过程中，原始计算中间状态$h=W_0·x$的公式修改为： $$\rh = W_0·x + A·B^T·x\r$$ 在训练完成后，进一步将原始参数矩阵$W_0$和训练得到的权重𝑨和𝑩进行合并：$W = W_0 + A·B^T$，得到更新后的参数矩阵。因此，LoRA微调得到的模型在解码过程中不会增加额外的开销。\n🏦LoRA变种 在原始的LoRA实现中，每个低秩矩阵的低秩参数𝑅都被设置为固定且相同的数值，并且在训练过程中无法进行调整，这种设定忽略了不同的秩在微调任务中可能产生的差异化影响。因此，通过这种方式训练得到的低秩矩阵往往并非最优解。AdaLoRA[7]讨论了如何更好地进行秩的设置。它引入了一种动态低秩适应技术，在训练过程中动态调整每个参数矩阵需要训练的秩同时控制训练的参数总量。具体来说，模型在微调过程中通过损失来衡量每个参数矩阵对训练结果的重要性，重要性较高的参数矩阵被赋予比较高的秩，进而能够更好地学习到有助于任务的信息。相对而言，不太重要的参数矩阵被赋予比较低的秩，来防止过拟合并节省计算资源。尽管LoRA能够有效地节省显存，但对于参数规模达到上百亿级别的模型而言，其微调所需的成本仍然相当高昂。QLoRA[8]将原始的参数矩阵量化为4比特，而低秩参数部分仍使用16比特进行训练，在保持微调效果的同时进一步节省了显存开销。根据上一小节的分析，对于给定参数量为𝑃的模型，QLoRA微调所需要的显存由LoRA微调所需要的2𝑃进一步下降为0.5𝑃。因此通过QLoRA技术，可以在一张A6000(48GB)的GPU上微调65B的模型，接近16比特模型微调的性能。\n🏡其他高效微调方法 适配器微调、前缀微调、提示微调\n参考文献 [1]. Jason Wei et al. “Finetuned Language Models are Zero-Shot Learners”. In: ICLR. 2022.\n[2]. Long Ouyang et al. “Training language models to follow instructions with human feed back”. In: arXiv preprint arXiv:2203.02155 (2022).\n[3]. VictorSanhetal. “Multitask Prompted Training Enables Zero-Shot Task Generalization”. In: ICLR. 2022.\n[4]. XiangLisa Li and Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Gen eration”. In: ACL. 2021.\n[5]. Brian Lester, Rami Al-Rfou, and Noah Constant. “The Power of Scale for Parameter Efficient Prompt Tuning”. In: EMNLP. 2021.\n[6]. EdwardJ.Huetal. “LoRA:Low-RankAdaptation of Large Language Models”. In: ICLR. 2022.\n[7]. Qingru Zhang et al. “Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning”. In: arXiv preprint arXiv:2303.10512 (2023).\n[8]. TimDettmersetal.“QLoRA:EfficientFinetuningofQuantizedLLMs”.In:arXivpreprint arXiv:2305.14314 (2023).\n","date":"2024-10-31T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/instructiontuning/apples-1776744_1280_hu10159539994416574822.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/instructiontuning/","title":"Instruction Tuning"},{"content":"模型架构 大语言模型架构配置表\u003e\r🍈Transformer 模型 当前主流的大语言模型都是基于Transformer模型进行设计的。Transformer是由多层的多头自注意力（Multi-headSelf-attention）模块堆叠而成的神经网络模型。原始的Transformer 模型由编码器和解码器两个部分构成，而这两个部分实际上可以独立使用，例如基于编码器架构的BERT模型[1]和解码器架构的GPT模型[2]。与BERT等早期的预训练语言模型相比，大语言模型的特点是使用了更长的向量维度、更深的层数，进而包含了更大规模的模型参数，并主要使用解码器架构，对于Transformer 本身的结构与配置改变并不大。\n🍉输入编码 在Transformer 模型中，输入的词元序列(𝒖 = [𝑢1,𝑢2,\u0026hellip;,𝑢𝑇]) 首先经过一个输入嵌入模块（InputEmbeddingModule）转化成词向量序列。具体来说，为了捕获词汇本身的语义信息，每个词元在输入嵌入模块中被映射成为一个可学习的、具有固定维度的词向量$v_t$ ∈$R^H$。由于Transformer的编码器结构本身无法识别序列中元素的顺序，位置编码（PositionEmbedding,PE）被引入来表示序列中的位置信息。给定一个词元$u_t$，位置编码根据其在输入中的绝对位置分配一个固定长度的嵌入向量$p_t$ ∈$R^H$。然后，每个词元对应的词向量和位置向量将直接相加，生成了最终的输入嵌入序列𝑿=[𝒙1,\u0026hellip;,𝒙𝑇]，并且被传入到后续层中：$x_t=v_t+p_t$.\n通过这种建模方法的表示，Transformer 模型可以利用位置编码 𝒑𝑡 建模不同词元的位置信息。由于不同词元的位置编码仅由其位置唯一决定，因此这种位置建模方式被称为绝对位置编码。尽管绝对位置编码能够一定程度上建模位置信息，然而它只能局限于建模训练样本中出现的位置，无法建模训练数据中未出现过的位置，因此极大地限制了它们处理长文本的能力。\n🍋多头自注意力机制 多头自注意力是Transformer模型的核心创新技术。相比于循环神经网络（Recurrent Neural Network, RNN）和卷积神经网络（ConvolutionalNeuralNetwork,CNN）等传统神经网络，多头自注意力机制能够直接建模任意距离的词元之间的交互关系。作为对比，循环神经网络迭代地利用前一个时刻的状态更新当前时刻的状态，因此在处理较长序列的时候，常常会出现梯度爆炸或者梯度消失的问题。而在卷积神经网络中，只有位于同一个卷积核的窗口中的词元可以直接进行交互，通过堆叠层数来实现远距离词元间信息的交换。\n多头自注意力机制通常由多个自注意力模块组成。在每个自注意力模块中，对于输入的词元序列，将其映射为相应的查询（Query,𝑸）、键（Key,𝑲）和值（Value,𝑽）三个矩阵。然后，对于每个查询，将和所有没有被掩盖的键之间计算点积。这些点积值进一步除以$\\sqrt{D}$进行缩放（𝐷是键对应的向量维度），被传入到softmax函数中用于权重的计算。进一步，这些权重将作用于与键相关联的值，通过加权和的形式计算得到最终的输出。在数学上，上述过程可以表示为： $$\rQ = XW^Q,\r$$$$\rK = XW^K,\r$$$$\rV = XW^V,\r$$$$\rAttention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{D}})V.\r$$与单头注意力相比，多头注意力机制的主要区别在于它使用了𝐻组结构相同，但映射参数不同的自注意力模块。输入序列首先通过不同的权重矩阵被映射为一组查询、键和值。每组查询、键和值的映射构成一个“头”，并独立地计算自注意力的输出。最后，不同头的输出被拼接在一起，并通过一个权重矩阵$W^O$∈$R^{H \\times H}$ 进行映射，产生最终的输出。如下面的公式所示： $$\rhead_n = Attention(XW^Q_n,XW^K_n,XW^V_n)\r$$$$\rMHA = Concat(head_1,...,head_N)W^O\r$$由上述内容可见，自注意力机制能够直接建模序列中任意两个位置之间的关系，进而有效捕获长程依赖关系，具有更强的序列建模能力。另一个主要的优势是，自注意力的计算过程对于基于硬件的并行优化（如GPU、TPU等）非常友好，因此能够支持大规模参数的高效优化。\n🍏前馈网络层 为了学习复杂的函数关系和特征，Transformer 模型引入了一个前馈网络层（Feed Forward Netwok, FFN），对于每个位置的隐藏状态进行非线性变换和特征提取。具体来说，给定输入𝒙，Transformer中的前馈神经网络由两个线性变换和一个非线性激活函数组成： $$\rFFN(X) = σ(XW^U + b_1)W^D + b_2\r$$ 其中$W^U$ ∈ $R^{H \\times H}$ 和$W^D$ ∈ $R^{H \\times H}$ 分别是第一层和第二层的线性变换权重矩阵，$b_1$ ∈ $R^{𝐻^′}$ 和 $b_2$ ∈ $R^H$ 是偏置项，𝜎是激活函数（在原始的Transformer中，采用 ReLU 作为激活函数）。前馈网络层通过激活函数引入了非线性映射变换，提升了 模型的表达能力，从而更好地捕获复杂的交互关系。\nTransformer 架构图\r🍐编码器 在Transformer 模型中，编码器（Encoder）的作用是将每个输入词元都编码成一个上下文语义相关的表示向量。编码器结构由多个相同的层堆叠而成，其中每一层都包含多头自注意力模块和前馈网络模块。在注意力和前馈网络后，模型使用层归一化和残差连接来加强模型的训练稳定度。其中，残差连接（Residual Connection）将输入与该层的输出相加，实现了信息在不同层的跳跃传递，从而缓解梯度爆炸和消失的问题。而LayerNorm则对数据进行重新放缩，提升模型的训练稳定性。编码器接受经过位置编码层的词嵌入序列𝑿作为输入，通过多个堆叠的编码器层来建模上下文信息，进而对于整个输入序列进行编码表示。由于输入数据是完全可见的，编码器中的自注意力模块通常采用双向注意力，每个位置的词元表示能够有效融合上下文的语义关系。在编码器-解码器架构中，编码器的输出将作为解码器（Decoder）的输入，进行后续计算。形式化来说，第𝑙层（𝑙∈{1,\u0026hellip;,𝐿}）的编码器的数据处理过程如下所示： $$\rX^′_l = LayerNorm(MHA(X_{l-1})+X_{l-1})\r$$$$\rX_l = LayerNorm(FFN(X^′_l)+X^′_l)\r$$其中，$X^′_l$ 和 $X_l$ 分别是该Transformer层的输入和输出，$X^′_l$是该层中输入经过多头注意力模块后的中间表示，LayerNorm表示层归一化。\n🥥解码器 Transformer 架构中的解码器基于来自编码器编码后的最后一层的输出表示以及已经由模型生成的词元序列，执行后续的序列生成任务。与编码器不同，解码器需要引入掩码自注意力（MaskedSelf-attention）模块，用来在计算注意力分数的时候掩盖当前位置之后的词，以保证生成目标序列时不依赖于未来的信息。除了建模目标序列的内部关系，解码器还引入了与编码器相关联的多头注意力层，从而关注编码器输出的上下文信息$X_L$。同编码器类似，在每个模块之后，Transformer 解码器也采用了层归一化和残差连接。在经过解码器之后，模型会通过一个全连接层将输出映射到大小为𝑉的目标词汇表的概率分布，并基于某种解码策略生成对应的词元。在训练过程中，解码器可以通过一次前向传播，让每个词元的输出用于预测下一个词元。而在解码过程，解码器需要经过一个逐步的生成过程，将自回归地生成完整的目标序列。解码器的数据流程如下所示： $$\rY^′_l = LayerNorm(MaskedMHA(Y_{l-1})+Y_{l-1})\r$$$$\rY^\"_l = LayerNorm(CrossMHA(Y^′_l,X_L)+Y^′_l)\r$$$$\rY_l = LayerNorm(FFN(Y^\"_l)+Y^\"_l)\r$$其中，$Y_{l-1}$ 和 $Y_l$ 分别是该Transformer 层的输入和输出，$Y^′_l$ 和 $Y^\u0026quot;_l$ 是该层中输入经过掩码多头注意力MaskedMHA和交叉多头注意力CrossMHA模块后的中间表示，LayerNorm 表示层归一化。然后将最后一层的输入𝒀𝐿映射到词表的维度上： $$\rO = softmax(W^LY_L)\r$$ 其中，𝑶 ∈$R^{H \\times V}$ 是模型最终的输出，代表下一个词在词表上的概率分布；$W^L$ ∈ $R^{H \\times V}$ 是将输入表示映射到词汇表维度的参数矩阵，而$W^LY_L$是概率化前的中间值，通常被称为logits。\n[1]. JacobDevlin et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. In: NAACL-HLT. 2019.\n[2]. Alec Radford et al. “Improving language understanding by generative pre-training”. In: OpenAI Blog (2018).\n","date":"2024-10-29T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/transformermodelbase/house-7497002_1280_hu17957794167668257846.png","permalink":"https://JiangZhiyu-1024.github.io/p/transformermodelbase/","title":"transformer模型架构"},{"content":"大模型预训练数据准备 ✅数据来源 根据来源不同，预训练数据主要分为两种类型：通用文本数据和专用文本数据\n🔥通用文本数据 网页：随着互联网的普及与发展，网页的数据规模持续扩大，覆盖的内容类型也变得丰富多样。使用大规模网页文本数据进行预训练，有助于大语言模型获取多样化的语言知识，并增强其自然语言理解和生成的能力[1,2]。为了便于使用网页数据进行预训练或相关研究，相关机构已经爬取并发布了多个大规模的网页数据集，包括C4[2]、RefinedWeb[3]、CC-Stories [4] 等。然而，这些网页数据集中既包含了维基百科这种高质量文本，也不可避免地引入了广告网页等低质量文本。因此，在进行预训练之前，对网页进行筛选和处理显得尤为重要，这直接关系到最终数据的质量与预训练效果。 书籍：相较于其他语料，书籍中的文本内容往往更为正式与详实，篇幅也相对较长。这些书籍文本在大语言模型的学习过程中，发挥着非常重要的作用，它们不仅能够帮助模型积累丰富的语言知识，还可以加强其长程语义关系的建模。现有的研究工作通常使用Books3和Bookcorpus2等开源书籍数据集。这些数据可以在Pile 数据集中获得[5]。 💯专用文本数据 专用数据集有助于提升大语言模型解决特定下游任务的能力。\n多语文本. 在预训练语料中，加入多语言的文本数据可以增强模型的多语理解与生成能力。BLOOM[6]模型和PaLM[7]模型在其预训练语料中分别使用了涵盖46种和122种语言的多语数据，进而使得这两个模型在翻译、跨语言摘要和问答等多语言任务中性能表现优异。相比于仅针对单一目标语言进行微调的模型，在多语言语料库上训练过的大语言模型能够更好地建立多语言间的语义关联，为跨语言理解与对话任务提供支持。不仅如此，多语言数据还能有效增加数据的多样性，从而有助于提升模型的综合性能。 科学文本. 随着科学研究的不断发展，相关出版物的数量不断增加。为了增强大语言模型对科学知识的理解，可以将科学文本数据加入到模型的预训练语料中。通过在大规模的科学文本语料上进行预训练，大语言模型可以在自然科学以及工程技术方面建立坚实的知识基础，从而在科学问答与推理等任务上取得出色的表现[8]。构建科学文本语料的常用方法是收集arXiv论文、科学教材、数学网页等科学资源。然而，由于科学文本数据中包含数学公式、蛋白质序列等特殊符号，通常需要采用特定的分词和预处理技术，将这些不同格式的数据转化为大语言模型能够处理的统一格式。 代码. 代码能力目前已经成为大语言模型备受关注的一种能力。为了提高模型的代码能力，需要在大量代码语料上进行预训练，进而提高其所生成的程序质量。这些由大语言模型编写的程序甚至可以成功通过专家设计的单元测试用例[9] 或解决具有挑战性的算法竞赛问题[10]。一般来说，常用于大语言模型预训练的代码语料有两种来源，第一种是StackExchange等编程问答社区的数据，第二种是 GitHub 等开源项目仓库。这两种来源包含了代码以及对应的注释和文档。与自然语言文本相比，代码主要以结构化的编程语言形式呈现。在代码数据上训练能够提升模型的结构化语义理解与逻辑推理能力[11]。同时，代码中的函数调用关系还有助于增强模型的工具使用与学习能力[12]。此外，将推理任务格式化为代码 可以帮助大语言模型生成更准确的结果[13,14]。 🗡数据预处理 当收集了丰富的文本数据之后，为了确保数据的质量和效用，还需要对数据进行预处理，从而消除低质量、冗余、无关甚可能有害的数据。\n🌈质量过滤 基于启发式规则的方法（P74） 设计规则来针对地识别和剔除低质量的文本数据[6,15]。\n基于语种的过滤.\n基于简单统计指标的过滤.\n基于关键词的过滤.\n思考：启发式规则过滤需要自己设计并决定使用什么规则，很考验想法的合理性和这方面的经验。 基于分类器的方法 训练用于判别数据质量的文本分类器，进行预训练语料的清洗。\n可以将维基百科等高质量数据作为正样本，同时从网页中筛选出含有不良内容或低质量数据的样本作为负样本。利用这个训练好的文本分类器，能够精准地识别和过滤低质量数据，从而显著提升整个语料库的质量。\n注意：基于分类器的方法也可能无意中删除一些低资源但高质量的文本，如文言文数据等，数据清洗人员需要意识到这种情况，并且建立合理的数据召回与保留机制。\n目前常用来实现分类器的方法包括：\n轻量级模型（如FastText等） 可微调的预训练语言模型（如BERT、BART或者LLaMA等） 闭源大语言模型API（如 GPT-4、Claude 3） 这三个方法各自具有不同的优缺点：轻量级模型效率较高，但是分类的准确率和精度可能受限于模型能力；预训练语言模型可以针对性微调， 但是分类性能的通用性和泛化性仍然有一定的限制；闭源大语言模型的能力较强， 但是无法灵活针对任务进行适配，而且用于预训练数据清洗需要花费较高的成本。 对于后两种方法来说，除了简单地进行数据过滤，还可以针对性进行数据的改写，从而使得一些整体质量还不错、但存在局部数据问题的文本仍然可以被保留下来使用。\n效率问题：\n启发式规则写得好，过滤起来快又好，分类器更精准但是慢，可以结合一下，首先利用启发式规则进行初步筛选，以快速排除不符合要求的文档，随后再采用分类器方法进一步精细过滤，确保最终筛选出的语料具有较好的文本质量。在这一过程中，还可以同时应用多种分类器，可以先使用轻量级分类器进行数据过滤，进而使用更为有效但是资源消耗更高的分类器在粗滤后的数据上再次进行选择。\n这看起来怎么似曾相识呢？果然计算机里面好多知识都是互通的。\n📚敏感内容过滤 与质量过滤类似，不同类型的数据内容往往需要采用特定的过滤规则。\n过滤有毒内容：为了精确过滤含有有毒内容的文本，可以采用基于分类器的过滤方法。Jigsaw评论数据集[16]提供了用于训练毒性分类器的数据。该数据集收集了近160K条论坛评论数据，每条评论都经过细致的标注，包括“有毒”、“严重有毒”、“有威胁”、“侮辱性”、“暴力”以及“身份仇恨”等六个类别。利用这一数据集进行训练，可以构建出高效的毒性文本分类器。通过设置合理的阈值，训练完成的分类器将能够有效识别并过滤掉含有有毒内容的信息。在进行分类阈值设置时，需要在精确度和召回率之间寻求平衡，避免过多或者过少去除候选数据。Dolma的技术报告[17]指出，使用高阈值时去除的数据会过少，语料中未过滤掉的有毒内容会导致模型在下游任务上的性能下降；而低阈值则会过滤更多的有毒内容，但同时也会造成大量数据的浪费。考虑到后续的预处理操作（如质量筛选、去重等）同样能够有效剔除有害内容，Dolma选择为分类器设定了一个相对较高的阈值（0.4），从而保留更多的候选数据。最终，Dolma在这一阶段仅过滤了CommonCrawl中30%左右的数据。 过滤隐私内容：预训练文本数据大多来自互联网，其中可能包括用户生成的敏感信息或可识别的个人信息（Personally Identifiable Information, PII），如姓名、地址和电话号码等。这些信息如果不加处理，将增加隐私泄露的潜在风险。例如，在2023年11月有用户发现，反复要求ChatGPT重复某个单词可能会使其无意间泄露训练数据中的个人隐私信息，这个漏洞现在已经修复。因此，在预处理阶段，需要去除这些可识别的个人信息。一种直接且有效的方法是使用启发式方法，如关键字识别，来检测和删除这些私人信息[18]。Dolma采用了基于规则的方法来过滤数据集中的隐私内容，主要标注了三类敏感信息：邮箱地址、IP地址以及电话号码。在文本收集过程中，一旦检测到这些隐私信息，Dolma会根据其出现的频率采取不同的处理策略。具体来说，如果某个文档中的隐私信息少于五条，Dolma 会使用特定的词元（如“|||EMAIL_ADDRESS|||”、“|||PHONE_NUMBER|||” 和“|||IP_ADDRESS|||”）来替换这些信息，以保护用户的隐私。然而，如果文档中的隐私信息达到六条或更多，Dolma会选择直接删除整个文档。这是因为当文档中频繁出现隐私信息时，很可能还隐藏着其他未标注的敏感内容。 📈数据去重 由于大语言模型具有较强的数据拟合与记忆能力，很容易习得训练数据中的重复模式，可能导致对于这些模式的过度学习。总体来说，去重算法的设计可以基于不同的计算粒度以及匹配方法。\n计算粒度.去重可以在句子级别、文档级别和数据集级别等多种粒度上进行。现有方法主要依靠单词或 n 元词组的重叠这类表层特征，来衡量文档的重叠比率，进而检测和删除包含相似 内容的重复文档。现有的数据集往往采用多阶段、多粒度的方式来实现高效的去重。首先针对数据集和文档级别进行去重，旨在去除那些具有高度相似甚至完全一致内容的文档，例如多个URL可能具有相同的网页内容，或者网页数据集和新闻数据集中包含相同的新闻文档。随后，可以进一步在句子级别实现更为精细的去重。例如，可以计算两个句子之间公共子串的长度（公共子串长度，好熟悉，算法无处不在。），当其长度过长时直接删除某一个句子。\n用于去重的匹配方法.在去重过程中，可以使用精确匹配算法（即每个字符完全相同）或近似匹配算法（基于某种相似性度量）[19]。对于精确匹配来说，通常使用后缀数组来匹配最小长度的完全相同子串[20]。对于近似匹配来说，可以采用局部敏感哈希（Locality-SensitiveHashing,LSH）算法，如最小哈希（MinHash） 来实现。考虑到预训练数据集合的规模非常大，实现中可以综合考虑去重效率和去重效果之间的权衡。例如，RefinedWeb在文档层面采用了开销较小的近似匹配技术来实现去重，而在句子层面则采用了精确匹配算法来确保去重的准确性。\n⭐词元化 词元化（Tokenization）是数据预处理中的一个关键步骤，旨在将原始文本分割成模型可识别和建模的词元序列，作为大语言模型的输入数据。\n🌼BPE分词 BPE算法从一组基本符号（例如字母和边界字符）开始，迭代地寻找语料库中的两个相邻词元，并将它们替换为新的词元，这一过程被称为合并。合并的选择标准是计算两个连续词元的共现频率，也就是每次迭代中，最频繁出现的一对词元会被选择与合并。合并过程将一直持续达到预定义的词表大小。\nBPE算法的代码如下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import re from collectionsimportdefaultdict def extract_frequencies(sequence): #给定一个字符串，计算字符串中的单词出现的频率，并返回词表（一个词到频率的映射字典）。 token_counter = Counter() for item insequence: tokens= \u0026#39;\u0026#39;.join(list(item)) + \u0026#39;\u0026lt;/w\u0026gt;\u0026#39; token_counter[tokens] += 1 return token_counter def frequency_of_pairs(frequencies): #给定一个词频字典，返回一个从字符对到频率的映射字典。 pairs_count = Counter() for token , count in frequencies.items(): chars = token.split() for i inrange(len(chars) - 1): pair = (chars[i],chars[i+1]) pairs_count[pair] += count return pairs_count def merge_vocab(merge_pair, vocab): #给定一对相邻词元和一个词频字典，将相邻词元合并为新的词元，并返回新的词表。 re_pattern = re.escape(\u0026#39;\u0026#39;.join(merge_pair)) pattern = re.compile(r\u0026#39;(?\u0026lt;!\\S)\u0026#39; +re_pattern + r\u0026#39;(?!\\S)\u0026#39;) updated_tokens ={pattern.sub(\u0026#39;\u0026#39;.join(merge_pair),token):freq for token,freq invocab.items()} return updated_tokens def encode_with_bpe(texts,iterations): #给定待分词的数据以及最大合并次数，返回合并后的词表。 vocab_map = extract_frequencies(texts) for _ in range(iterations): pair_freqs = frequency_of_pairs(vocab_map) if not pair_freqs: break most_common_pair = pair_freqs.most_common(1)[0][0] vocab_map = merge_vocab(most_common_pair,vocab_map) return vocab_map num_merges = 1000 bpe_pairs =encode_with_bpe(data,num_merges) 字节级别的BPE（Byte-levelBPE,B-BPE）是BPE算法的一种拓展。它将字节视为合并操作的基本符号，从而可以实现更细粒度的分割，且解决了未登录词问题。采用这种词元化方法的代表性语言模型包括GPT-2、BART和LLaMA。具体来说，如果将所有Unicode字符都视为基本字符，那么包含所有可能基本字符的基本词表会非常庞大（例如将中文的每个汉字当作一个基本字符）。而将字节作为基本词表可以设置基本词库的大小为256，同时确保每个基本字符都包含在词汇中。例如，GPT-2的词表大小为50,257，包括256个字节的基本词元、一个特殊的文末词元以及通过50,000次合并学习到的词元。\nBPE算法的具体流程示例 假设语料中包含了五个英文单词： “loop”，“pool”，“loot”，“tool”，“loots”\n在这种情况下，BPE假设的初始词汇表即为： [“l”,“o”,“p”,“t”,“s”]\n在实践中，基础词汇表可以包含所有ASCII字符，也可能包含一些Unicode字符 （比如中文的汉字）。如果正在进行分词的文本中包含了训练语料库中没有的字 符，则该字符将被转换为未知词元（如“\u0026lt;UNK\u0026gt;”）。 假设单词在语料库中的频率如下： （“loop”，15），（“pool”，10），（“loot”，10），（“tool”，5），（“loots”，8）\n其中，出现频率最高的是“oo”，出现了48次，因此，学习到的第一条合并规则 是（“o”,“o”）→“oo”，这意味着“oo”将被添加到词汇表中，并且应用这一 合并规则到语料库的所有词汇。在这一阶段结束时，词汇和语料库如下所示： 词汇：[“l”,“o”,“p”,“t”,“s”,“oo”] 语料库：（“l” “oo” “p”，15），（“p” “oo” “l”，10），（“l” “oo” “t”， 10），（“t” “oo” “l”，5），（“l” “oo” “t” “s”，8）\n此时，出现频率最高的配对是（“l”，“oo”），在语料库中出现了33次，因此学习 到的第二条合并规则是（“l”，“oo”）→“loo”。将其添加到词汇表中并应用到所 有现有的单词，可以得到： 词汇：[“l”,“o”,“p”,“t”,“s”,“oo”,“loo”] 语料库：（“loo” “p”，15），（“p” “oo” “l”，10），（“loo” “t”，10），（“t” “oo” “l”， 5），（“loo” “t” “s”，8）\n现在，最常出现的词对是（“loo”,“t”），因此可以学习合并规则（“loo”,“t”） →“loot”，这样就得到了第一个三个字母的词元： 词汇：[“l”,“o”,“p”,“t”,“s”,“oo”,“loo”,“loot”] 语料库：（“loo” “p”，15），（“p” “oo” “l”，10），（“loot”，10），（“t” “oo” “l”， 5），（“loot” “s”，8） 可以重复上述过程，直到达到所设置的终止词汇量。\n🎵WordPiece 分词 WordPiece分词和BPE分词的想法非常相似，都是通过迭代合并连续的词元，但是合并的选择标准略有不同。在合并前，WordPiece分词算法会首先训练一个语言模型，并用这个语言模型对所有可能的词元对进行评分。然后，在每次合并时，它都会选择使得训练数据的似然性增加最多的词元对。\n谷歌并未发布WordPiece分词算法的官方实现\n与BPE类似，WordPiece 分词算法也是从一个小的词汇表开始，其中包括模型使用的特殊词元和初始词汇表。由于它是通过添加前缀（如BERT的##）来识别子词的，因此每个词的初始拆分都是将前缀添加到词内的所有字符上。举例来说，“word”会被拆分为：“w##o ##r ##d”。与BPE方法的另一个不同点在于，WordPiece分词算法并不选择最频繁的词对，而是使用下面的公式为每个词对计算分数： $$\r得分 = \\frac{\\text{词对出现的频率}}{\\text{第一个词出现的频率} \\times \\text{第二个词出现的频率}}\r$$🐉Unigram 分词 与BPE分词和WordPiece 分词不同，Unigram 分词方法 [21] 从语料库的一组足够大的字符串或词元初始集合开始，迭代地删除其中的词元，直到达到预期的词表大小。它假设从当前词表中删除某个词元，并计算训练语料的似然增加情况，以此来作为选择标准。这个步骤是基于一个训练好的一元语言模型来进行的。为估计一元语言模型，它采用期望最大化（Expectation–Maximization,EM）算法：在每次迭代中，首先基于旧的语言模型找到当前最优的分词方式，然后重新估计一元概率从而更新语言模型。这个过程中一般使用动态规划算法（即维特比算法，Viterbi Algorithm）来高效地找到语言模型对词汇的最优分词方式。采用这种分词方法的代表性模型包括T5和mBART。\n🍌数据调度 数据调度（DataScheduling）主要关注两个方面：各个数据源的混合比例以及各数据源用于训练的顺序（称为数据课程，Data Curriculum）。\n预训练大语言模型时数据调度的示意图\r🎈数据混合 在预训练期间，将根据混合比例从不同数据源中采样数据：数据源的权重越大，从中选择的数据就越多。进一步，可能会对每个数据源的全部数据进行上采样或下采样，以创建特定的数据混合集合作为预训练数据。\n🎉数据课程 除了设置有效的数据混合配比外，在训练过程中对于预训练数据的顺序进行合适的安排也比较重要。具体来说，数据课程是指按照特定的顺序安排预训练数据进行模型的训练。例如，从简单/通用的数据开始，逐渐引入更具挑战性/专业化的数据。更广泛地说，它可以指训练期间在不同阶段使用不同的数据源混合配比。为了设定合适的数据课程，一种实用方法是基于专门构建的评测基准监控大语言模型的关键能力的学习过程，然后在预训练期间动态调整数据的混合配比。\n🧨参考文献 [1]. Alec Radford et al. “Language models are unsupervised multitask learners”. In: OpenAI Blog (2019).\n[2]. Colin Raffel et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”. In: J. Mach. Learn. Res. (2020).\n[3]. JiantaoQiuetal. “WanJuan-CC:ASafeandHigh-QualityOpen-sourcedEnglish Webtext Dataset”. In: arXiv preprint arXiv:2402.19282 (2024).\n[4]. Trieu H. Trinh and Quoc V. Le. “A Simple Method for Commonsense Reasoning”. In: arXiv preprint arXiv:1806.02847 (2018).\n[5]. LeoGaoetal.“ThePile: An 800GBDataset of Diverse Text for Language Modeling”. In: arXiv preprint arXiv:2101.00027 (2021).\n[6]. Teven Le Scao et al. “BLOOM: A 176B-Parameter Open-Access Multilingual Language Model”. In: arXiv preprint arXiv:2211.05100 (2022).\n[7]. Aakanksha Chowdhery et al. “PaLM: Scaling Language Modeling with Pathways”. In: arXiv preprint arXiv:2204.02311 (2022).\n[8]. Tarek Saier, Johan Krause, and Michael Färber. “unarXive 2022: All arXiv Publications Pre-Processed for NLP, Including Structured Full-Text and Citation Network”. In: arXiv preprint arXiv:2303.14957 (2023).\n[9]. MarkChenetal.“EvaluatingLargeLanguageModelsTrainedonCode”.In:arXivpreprint arXiv:2107.03374 (2021).\n[10]. YujiaLietal.“Competition-LevelCodeGenerationwithAlphaCode”.In:Science(2022).\n[11]. Yingwei Ma et al. “At Which Training Stage Does Code Data Help LLMs Reasoning?” In: arXiv preprint arXiv:2309.16298 (2023).\n[12]. Ke Yang et al. “If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents”. In: arXiv preprint ar Xiv:2401.00812 (2024).\n[13]. AmanMadaan et al. “Language Models of Code are Few-Shot Commonsense Learners”. In: EMNLP. 2022.\n[14]. Yuhuai Wu et al. “Autoformalization with Large Language Models”. In: arXiv preprint a rXiv:2205.12615 (2022).\n[15]. JackW.Raeetal.“ScalingLanguageModels:Methods,Analysis\u0026amp;InsightsfromTraining Gopher”. In: arXiv preprint arXiv:2112.11446 (2021).\n[16]. CJ Adams et al. Toxic comment classification challenge, 2017. https://kaggle.com/ competitions/jigsaw-toxic-comment-classification-challenge. 2017.\n[17]. Luca Soldaini et al. “Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research”. In: arXiv preprint arXiv:2402.00159 (2024).\n[18]. Hugo Laurençon et al. “The bigscience roots corpus: A 1.6 tb composite multilingual dataset”. In: Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022.\n[19]. Guilherme Penedo et al. “The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only”. In: arXiv preprint arXiv:2306.01116 (2023).\n[20]. Udi Manber and Eugene W. Myers. “Suffix Arrays: A New Method for On-Line String Searches”. In: SIAM J. Comput. (1993).\n[21]. Taku Kudo. “Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates”. In: ACL. 2018.\n","date":"2024-10-28T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/pretraining-data-preparation-for-large-models/road-5710320_1280_hu738609299032606297.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/pretraining-data-preparation-for-large-models/","title":"LLM预训练数据准备"},{"content":"文献阅读 每周都完不成上次的任务，杂事太多了，尽快完成： Detecting formal thought disorder by deep contextualized word representations 竟然要收费，不看了，让gpt给讲一下\n这篇论文《通过深度上下文化词表示检测形式思维障碍》研究了如何使用深度学习中的上下文词嵌入模型（如ELMo）来检测精神疾病患者的形式思维障碍（Formal Thought Disorder, FTD）。FTD是一种影响语言表达和思维逻辑的精神疾病症状，常见于精神分裂症患者。\n论文主要利用深度上下文化词嵌入模型，如ELMo，通过生成词在句子中的嵌入表示，捕捉语境中词的多义性和丰富的语义信息。研究团队通过分析患者的语言数据，构建模型来区分正常语言与存在思维障碍的语言表达，从而实现自动化的FTD检测。\n研究的核心贡献是使用更复杂的语义表示（如上下文化词嵌入）来解决传统方法无法有效处理的词义歧义问题。\nDistributed Representations of Words and Phrases and their Compositionality 《Distributed Representations of Words and Phrases and their Compositionality》是由Mikolov等人在2013年提出的一篇重要论文，介绍了Word2Vec模型的改进和创新。这篇文章主要讨论了如何通过分布式词表示（distributed word representations）更好地捕捉单词和短语的语义信息，并且提出了两种核心模型：CBOW（Continuous Bag of Words） 和 Skip-gram。\n这篇论文的主要贡献包括：\nSkip-gram和CBOW模型：论文提出了两种用于生成词向量的架构。Skip-gram模型的目标是根据一个词预测其上下文，而CBOW模型则根据上下文词来预测目标词。这两种模型能够有效地捕捉单词之间的语义关系。 层次Softmax和负采样：为了提高训练大规模语料库的效率，作者引入了层次Softmax和**负采样（negative sampling）**技术，这大幅降低了模型的计算复杂度，使得大规模训练变得可行。 词和短语的组合性：论文不仅处理了单词的表示，还处理了短语的表示，通过数据驱动的方式将多词短语映射为词向量，捕捉更复杂的语义结构。 向量运算反映语义关系：Word2Vec生成的词向量能够通过简单的向量运算表达词语之间的关系。例如，向量运算 vec(\u0026quot;King\u0026quot;) - vec(\u0026quot;Man\u0026quot;) + vec(\u0026quot;Woman\u0026quot;) ≈ vec(\u0026quot;Queen\u0026quot;) 说明了这种分布式表示在捕捉语义关系上的强大能力。 LLM BOOK 找到一本书，准确的来说是一个github仓库，组织者把近些年的LLM相关的东西都整进去了，好全，写的真好，库库看吧那就\n先把资源放在这里：RUCAIBox/LLMSurvey: The official GitHub page for the survey paper \u0026ldquo;A Survey of Large Language Models\u0026rdquo;.\n第一部分 背景与基础知识 语言模型的发展历程（P16） LLM大语言模型那肯定是从LM语言模型来的，语言模型的发展又经历了：\n统计语言模型（StatisticalLanguageModel, SLM） 神经语言模型（NeuralLanguageModel,NLM） 预训练语言模型（Pre-trainedLanguageModel,PLM） 大语言模型（LargeLanguageModel, LLM） LLM的特点（P19） 具有较为丰富的世界知识 具有较强的通用任务解决能力 具有较好的复杂任务推理能力 具有较强的人类指令遵循能力 具有较好的人类对齐能力 具有可拓展的工具使用能力 大语言模型对于产业应用带来了变革性的技术影响，将会催生一个基于大语言模型的应用生态系统。\n第二部分 预训练 大语言模型的第一个训练阶段是预训练，预训练语料的规模和质量对于提升大语言模型的能力至关重要。\n数据来源 通用文本数据和专用文本数据。通用文本数据涵盖了网页、书籍和对话文本等。由于通用文本数据规模较大、多样性强且易于获取，大多数大语言模型都会收集大量的通用文本数据，以增强其语言建模能力。此外，为了进一步提升大语言模型在特定专业任务上的表现，人们还将预训练语料的范围扩展至更专业的数据集，如多语数据、科学数据和代码数据等。\n通用文本数据 现有大语言模型预训练数据中各种数据来源的比例分布图\u003e\r专用文本数据 多语文本、 科学文本、代码。\n典型的预训练数据预处理流程图\u003e\r数据预处理 构建并使用系统化的数据处理框架（如开源库Data-Juicer）\n具体操作（P74）\n附录 softmax是啥？ Softmax是一种常用的函数，特别是在多分类任务中，用来将一个向量中的元素转换为0到1之间的概率分布。其核心作用是： 将实数映射为概率分布：Softmax 接受一个实数向量，输出一个与输入维度相同的向量，其中每个元素的值表示该元素作为某个类别的概率，且这些概率的和为1。这样就可以根据这些概率来进行分类预测。\n公式：Softmax 函数的定义如下： $$\r\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\r$$ 其中，\\( z_i \\) 是输入向量中的某个元素，\\( e^{z_i} \\) 是该元素的指数值，分母是所有输入元素的指数值的和。这个函数将每个 \\( z_i \\) 映射为一个在 \\( (0,1) \\) 之间的概率，并且所有输出的概率之和为1。\n作用机制：\n指数放大效果：Softmax 函数通过对输入值取指数（\\( e^x \\)），会将较大的输入值放大更多，而较小的值则被进一步压缩，突出更大的概率。 归一化：Softmax 保证所有输出的概率之和为1，符合概率的定义。 应用场景：在神经网络的最后一层，特别是分类任务中，Softmax 常用于多类分类问题，比如在自然语言处理（NLP）中的词分类、图像识别中的对象分类等。它将网络的输出转化为易于理解的概率值，帮助模型决定输入属于哪个类别。\n举例 假设我们有一个模型输出三个值 \\( [2.0, 1.0, 0.1] \\)，通过 Softmax 转换后得到的结果是大约 \\( [0.71, 0.26, 0.03] \\)，其中最大的值0.71表示模型认为输入属于第一个类别的概率最大。\n简言之，Softmax 是一种将输出转换为概率分布的工具，特别适合多分类问题的场景。\n看了半天论文，感觉没有一个时间线很难受，不清楚那些论文之间的关系，问问gpt： 基础概念与理论 先掌握大模型背后的基本理论，这些知识是理解后续论文的基础：\n语言模型的基础： 经典论文：《Attention Is All You Need》（Transformer） 相关概念：自注意力机制、序列到序列模型、词嵌入（如Word2Vec、GloVe） 自回归模型与自编码器： GPT 系列（GPT, GPT-2, GPT-3）的原理和应用 BERT 及其衍生模型的预训练与微调方式 扩展学习与生成式任务： 《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》 生成式预训练模型的设计与任务应用（如文本生成、机器翻译） 模型的训练与优化 学习如何高效地训练大模型，并且了解模型的优化技术：\n微调与参数高效训练： Efficient Fine-Tuning Techniques for Large Models: Adapter, LoRA 等 探索模型压缩技术（知识蒸馏、量化、剪枝等） 对比学习与自监督学习： 《SimCLR》、《MoCo》 等代表性论文，理解对比学习在大模型中的应用 模型对齐与安全 在大模型发展过程中，模型对齐（alignment）和伦理问题变得越来越重要：\n模型对齐与价值观嵌入： 《InstructGPT: Training language models to follow instructions with human feedback》 RLHF（强化学习通过人类反馈）等对齐策略 模型安全与公平性： 探讨模型偏见问题以及在实际应用中的安全隐患 视觉-语言模型与多模态模型 学习大模型在跨模态任务中的应用，比如视觉-语言任务：\nCLIP（Contrastive Language-Image Pretraining）：理解图像和文本共同表征的训练方法 DALL-E、Stable Diffusion：生成式视觉任务的模型设计与应用 视觉语言对话系统：如《Visual ChatGPT》 前沿研究与新趋势 在深入理解基本模型后，可以关注大模型领域的前沿研究方向：\n大模型与知识图谱的结合：如何将知识图谱融入语言模型 领域自适应模型：如何通过特定领域数据进行模型的进一步微调 检索增强生成（RAG）：如《RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》 实战应用与开源框架 阅读开源项目中的技术文档，参与开源项目（如Hugging Face的模型库） 学习如何应用大模型解决实际问题，并尝试训练和微调自己的模型 这样，你可以从基础概念、模型训练到前沿研究和实战应用，逐步深入，既能掌握理论又能结合实践。这个过程可以帮助你筛选出重要的论文和研究方向。\n对齐是什么意思？ 在大型语言模型（LLM）中，“对齐”通常指的是模型的输出与人类意图或价值观的匹配程度。具体来说，它涉及确保模型在生成文本时能够产生符合人类期望的、有益的和安全的内容。对齐的目标是减少模型生成有害、误导或不准确的信息。\n对齐可以通过多种方法实现，包括：\n训练数据的选择和清洗：使用高质量的数据集进行训练，确保模型接触到的内容符合人类的价值观和期望。\n强化学习：利用人类反馈来微调模型，使其在生成内容时更符合人类的意图。\n安全性和伦理性评估：在模型发布之前，对其输出进行评估，以识别和修正潜在的偏见或有害内容。\n通过这些方法，可以提高模型在特定任务或应用场景中的可靠性和安全性。\narXiv 论文是什么？ arXiv 是一个开放获取的电子预印本存档，主要用于科学研究领域，特别是在物理学、数学、计算机科学、统计学、金融学、生物学和其他相关学科。研究者可以在 arXiv 上免费上传和分享他们的研究论文，通常是在正式出版之前的版本。\narXiv 的特点包括：\n快速发布：研究者可以迅速分享他们的研究成果，而不必等待同行评审和正式出版的过程。\n开放获取：任何人都可以访问和下载论文，无需订阅或支付费用。\n广泛的学科覆盖：涵盖多个领域，促进跨学科的交流和合作。\n社区反馈：研究者可以在正式发表之前收到来自社区的反馈，帮助他们改进论文。\narXiv 的论文通常以“预印本”的形式存在，因此它们可能尚未经过同行评审。这意味着在引用这些论文时，研究者需要谨慎，了解其可能的局限性。\n在arXiv上发还未出版的论文不怕被别人剽窃成果吗？ 在 arXiv 上发布未出版的论文确实存在被剽窃的风险，但研究者通常认为在 arXiv 上共享研究成果是有益的，原因如下：\n时间戳和公开记录：一旦论文被上传到 arXiv，上传的时间戳可以作为知识产权的证明。这在某种程度上可以保护研究者的原创性。\n学术社区的支持：arXiv 是一个活跃的学术社区，研究者通常会关注相关领域的论文，并且对剽窃行为持有负面态度。学术界普遍重视诚信，剽窃通常会受到严厉的谴责。\n早期反馈：通过发布预印本，研究者可以获得来自同行的反馈，从而改进自己的工作。这也有助于在正式出版之前建立声誉。\n增加曝光率：在 arXiv 上发布可以提高研究的可见性，吸引潜在合作者或资助者。\n通用人工智能系统是什么？ 通用人工智能系统（AGI, Artificial General Intelligence）指的是一种能够理解、学习和应用知识，完成任何人类所能完成的智力任务的人工智能系统。与当前的狭义人工智能（Narrow AI）相比，后者在特定任务或领域表现出色（如图像识别、自然语言处理等），AGI 的目标是实现一种更广泛的智能，具备跨领域的学习和适应能力。\nAGI 的特点包括：\n自主学习：能够从有限的数据中学习并推理，具备自我改进的能力。\n多任务处理：能够在多个不同的领域和任务中表现出色，而不局限于特定的应用。\n常识推理：具备理解和应用常识的能力，能够处理复杂的、不确定的环境。\n灵活性和适应性：能够适应新情况和挑战，灵活地应对不同的环境和需求。\nAGI 的发展仍然是一个前沿研究领域，目前尚未实现。许多研究者在探索如何构建这样的系统，同时也讨论其潜在的伦理和社会影响，例如对就业、隐私和安全的影响。\n思考 随着看的东西多了，和使用chatgpt，我隐隐感觉到，由于chatgpt的语言模型主要是通过预测下一个最可能的词来生成文本，这一过程称为自回归生成（autoregressive generation）。导致gpt经常会顺着你的话去说，即有更小的概论去反对你的话，尽量让你说的话看起来正确，当然，如果你说的是：“太阳从西边升起”，那么他会义无反顾的反驳你。\n","date":"2024-10-26T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/third-week/cards-8594729_640_hu5982296781466901225.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/third-week/","title":"2024.10.21至27第三周工作总结"},{"content":"第一次成功创建个人博客 回忆 想当年其实我自己创建过，跟着一个教程做的，但当时才大一，很多东西都不懂，哼哧哼哧做了半天，最后不知道哪里不行，没有成功，于是放弃了，改用CSDN写，但现在我又有时间了，保研结束，该拾掇拾掇自己了，我觉得个人博客就像自己的一个小花园，写文章就好像在里面种花，精心打理后很有成就感，所以这次花了一天时间重新建起了我的花园，圆了大一时的梦，顺便做个记录，好记性不如烂笔头，忘了回来查查。\n资料 这次是跟着B站一个up主建立的博客，讲的非常清楚，把他的视频链接放在这里。\nhttps://www.bilibili.com/video/BV1bovfeaEtQ?vd_source=9cfc0ca3d84e1ed0064763e32a183f15\n记录 文章存放 content/post放的是文章\n文章名 文章都叫index.md\n日期问题 每个文章前面要有插入一个yaml块，里面放一些信息，日期这里一开始有一个非常搞心态的问题，因为我发现我随便测试日期，显示的日期都是10-10，09-09这种，日期和月份一样，后来检查了半天才发现，视频中up主在修改配置文件hugo.yaml时，把日期格式修改为了2006-01-02，而我顺手就写了个2006-01-01，我发现他把我这个格式理解成了月份和日期要一样，于是产生了那个令人哭笑不得的现象。\n深刻理解配置文件 今天才知道仓库的actions存放的是一些自动执行的脚本，一开始新建文章的开头配置文件没有写好，导致脚本运行一直出错，现在改好之后，看不到错的记录了，一片绿很养眼\nslug 这个头配置文件里的slug: firstblog是给链接起个名字，我这个内容是从我另一个博客复制过来的，于是他俩冲突了，显示的内容都杂在一起了，坏了，刚交上去就出bug，正好记上\n收获 对github仓库加深了理解\n对一些配置文件的存在有了新的认识\n致谢 感谢up主：Letere-莱特雷\n对，因为对这个博主的英文名字很感兴趣，于是给自己起了一个名字Zion Blaze,我感觉very古德，比我的损友老马起的foolish fishy强一w倍，明天联系我的律师起诉他。\n感谢chatgpt同学，今天依旧稳定发力，明天开始继续研究LLM，争取有朝一日让gpt同学感谢感谢我。\n","date":"2024-10-25T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/firstblog/liuying5_hu10731212018240133723.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/firstblog/","title":"记第一次创建我的个人博客"},{"content":"文献阅读 看了知乎的科普文章，先从经典的论文开始看：\nAttention Is All You Need 《Attention is All You Need》是由Vaswani等人在2017年提出的一篇重要论文，首次引入了Transformer模型。以下是其主要内容的总结：\n背景：传统的序列到序列模型（如RNN和LSTM）在处理长序列时存在瓶颈。Transformer模型通过自注意力机制克服了这些限制，使得模型可以并行处理数据，提升了训练效率。\n模型架构：\nEncoder-Decoder结构：Transformer由两个主要部分组成：编码器和解码器。编码器将输入序列转化为上下文向量，解码器则根据上下文生成输出序列。 自注意力机制：每个输入的表示都通过自注意力机制与其他输入进行交互，从而捕捉序列中的依赖关系。每个位置的表示通过加权求和得到，权重由输入的相似度计算得到。 多头注意力：模型使用多个注意力头并行计算不同的表示，能够更全面地捕捉信息。 位置编码：由于Transformer没有循环结构，论文引入了位置编码，提供序列中词语的位置信息，以保留词序信息。\n训练与优化：Transformer使用了残差连接和层归一化，以提高训练的稳定性和收敛速度。论文中还介绍了基于Adam优化器的训练过程。\n实验结果：Transformer在机器翻译任务上取得了显著的性能提升，并且在其他NLP任务中也展示了强大的通用性。\n影响：该论文奠定了现代NLP研究的基础，推动了基于Transformer的预训练语言模型（如BERT、GPT等）的发展。\n差分隐私深度学习(Deep Learning with Differential Privacy) 《Deep Learning with Differential Privacy》是由Martin Abadi等人在2016年提出的一篇重要论文，探讨了在深度学习中实现差分隐私的策略。以下是其主要内容的总结：\n背景：\n差分隐私：一种保护用户数据隐私的技术，旨在确保模型在训练时无法推断出任何单个用户的敏感信息。即使攻击者知道模型和数据集的其余部分，也无法有效地获取关于某个特定用户的信息。 深度学习与隐私问题：深度学习模型通常依赖大量数据，这使得保护用户隐私变得尤为重要。 模型与训练：\n噪声注入：通过在模型的梯度更新中添加噪声，可以达到差分隐私的效果。噪声的大小与隐私预算（ε）相关，预算越小，隐私保护越强，但模型的性能可能会受到影响。 Privacy Accounting：论文提出了一种计算隐私损失的框架，确保在每次训练步骤中监控和控制隐私损失。 算法设计：\nDP-SGD（Differentially Private Stochastic Gradient Descent）：论文提出了一种改进的随机梯度下降算法，结合了梯度裁剪和噪声注入，以实现差分隐私。这种方法确保在训练过程中，个别数据对模型的影响是被限制的。 实验结果：\n论文通过实验展示了DP-SGD在图像分类任务上的有效性，同时比较了不同噪声级别对模型性能的影响，证明了在保持合理准确性的同时实现差分隐私是可行的。 应用与展望：\n论文强调了在各类机器学习应用中（如医疗、金融等）保护隐私的重要性，提出将差分隐私技术与深度学习结合是一个有效的解决方案，并鼓励未来的研究在这一领域的进一步探索。 1 2 3 感觉偏向数学一点，全是公式和证明，好难看懂。 差分隐私顾名思义就是用来防范差分攻击的 加入噪声，改变原来的概率分布 Deep Reinforcement Learning from Human Preferences 《Deep Reinforcement Learning from Human Preferences》是由Christiano等人在2017年提出的一篇重要论文，探讨了如何利用人类偏好来训练强化学习模型。以下是其主要内容的总结：\n背景：\n传统的强化学习方法通常依赖于奖励信号来指导学习，但设计有效的奖励函数在复杂任务中往往困难且耗时。 人类可以提供更直观的偏好信息，这为训练强化学习代理提供了一种新途径。 方法概述：\n人类偏好收集：论文提出了一种方法，通过收集人类对不同行为的偏好来生成奖励信号。人类评估多个策略的表现，从中选择更好的策略，以此来建立奖励模型。 奖励模型：利用收集到的人类偏好数据，训练一个深度神经网络来预测代理在给定状态下的奖励。这种奖励模型可以用来指导强化学习算法。 训练过程：\n深度强化学习：在获得奖励模型后，使用深度强化学习算法（如Proximal Policy Optimization，PPO）来训练代理。代理通过与环境交互，不断优化其策略。 迭代学习：通过不断收集新的偏好数据并更新奖励模型，实现迭代学习，使得代理能够逐步提升性能。 实验结果：\n论文在多个复杂任务（如Atari游戏和模拟环境）中展示了该方法的有效性，代理能够通过学习人类的偏好，超越基于手动设计的奖励函数的表现。 结果表明，利用人类偏好进行训练能够加速学习过程，并提升代理的最终性能。 意义与展望：\n该方法展示了人类偏好在强化学习中的潜力，提出了一种新的思路来解决奖励设计问题。 论文强调了未来研究中结合人类反馈和偏好的重要性，特别是在涉及复杂决策和安全性问题的领域。 附录 残差连接（residual connection）和层归一化（layer normalization）是什么？ 残差连接（Residual Connection）和 层归一化（Layer Normalization）是深度学习中常用的两种技术，主要用于提高神经网络的训练效率和性能。下面是它们的定义和作用：\n残差连接（Residual Connection） 定义：残差连接是一种将输入直接与输出相加的结构，通常在深度神经网络中使用。公式表示为： $$\r\\text{Output} = \\text{Layer}(x) + x\r$$ 其中 \\(x\\) 是输入，\\(\\text{Layer}(x)\\) 是通过某个层（如卷积层或全连接层）处理后的输出。\n作用：\n缓解梯度消失问题：在非常深的网络中，梯度可能会随着层数增加而消失，导致网络难以训练。残差连接提供了一条捷径，使得梯度可以更容易地通过网络传播。 加速收敛：通过引入直接路径，残差连接有助于提高网络的收敛速度。 提高模型性能：它允许模型学习恒等映射，这使得训练更深的网络成为可能，提升了模型的表现。 层归一化（Layer Normalization） 定义：层归一化是一种对网络中每一层的激活进行归一化的技术，它通过计算每个样本的均值和标准差来调整激活值。层归一化的公式为： $$\r\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta\r$$ 其中：\n\\(x\\) 是输入。 \\(\\mu\\) 是输入的均值。 \\(\\sigma\\) 是输入的标准差。 \\(\\epsilon\\) 是一个小常数，避免除以零。 \\(\\gamma\\) 和 \\(\\beta\\) 是可学习的参数，用于缩放和平移归一化的结果。 作用：\n减少内部协变量偏移：层归一化帮助减少模型训练过程中每层输入的变化，使得网络在训练时更加稳定。 提高训练速度：通过标准化输入，层归一化可以使得训练过程更加平滑，从而加快收敛速度。 适用于变长序列：与批量归一化（Batch Normalization）不同，层归一化可以直接应用于变长的输入序列，适合于RNN和Transformer等模型。 总结 残差连接主要通过提供捷径来缓解深层网络中的梯度消失问题，并加速训练过程。 层归一化则通过标准化激活值来提高训练的稳定性和速度。这两者结合使用，能够显著提升深度学习模型的表现。 ","date":"2024-10-20T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/second-week/leaves-6729056_640_hu5688234115053010817.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/second-week/","title":"2024.10.14至20第二周工作总结"},{"content":"工作总结： 文献阅读 综述类： 2023年国内大模型发展综述与趋势研判_赵子忠\nAIGC大模型测评综述：使能技术、安全隐患和应对_许志伟\nAI大模型发展综述_张乾君\n深入： 《A Neural Probabilistic Language Model》是Yoshua Bengio等人在2003年发表的一篇重要论文，提出了一种基于神经网络的语言模型方法。这篇论文的主要目标是通过神经网络来改进传统的语言模型。\n传统的语言模型的任务是估计一个词序列的概率： P(w1,w2,…,wT) 在这种情况下，模型需要根据上下文词来预测当前词的概率。传统的做法一般是通过统计方法（如n元语法模型）来估计这些概率，但这类方法有一个很大的问题：它们无法处理高维数据，尤其是在词汇量非常大的情况下，计算复杂度极高，且泛化能力有限。\nBengio等人提出了一种新的神经网络语言模型，目的是克服这个问题。这个模型的核心思想是使用一个多层感知器来学习词的分布式表示（即词向量），并基于这些词向量来估计概率。具体过程大致如下：\n词嵌入：每个词通过一个嵌入矩阵被映射为一个连续向量，这种向量的维度比词汇表要小得多。这一步的目的是将离散的词映射到一个连续的低维空间，减少计算复杂度。\n神经网络建模：利用一个前馈神经网络（通常是多层感知器），根据前面的若干个词的词向量，来预测下一个词的条件概率。通过这个网络，模型可以学习到词与词之间的相似性和上下文关系。\n训练：模型通过最大化训练数据的似然估计来进行训练。具体来说，模型会调整嵌入矩阵和网络的参数，使得预测的概率尽可能接近真实的词序列概率。\n这篇论文的创新之处在于提出了“词向量”的概念，并展示了使用神经网络来处理语言建模问题的潜力。这为后来的深度学习在自然语言处理（NLP）中的应用奠定了基础。\n附录 什么是语言模型 语言模型（Language Model, LM）是自然语言处理中的一个核心概念，其主要任务是对一个词序列的概率分布进行建模。简单来说，语言模型的目标是估计一段文本或句子出现的概率，或者根据前面的词来预测下一个词。\n语言模型的定义 给定一个词序列 $$\rP( w_1, w_2, \\dots, w_T )\r$$ ，语言模型的目标是计算这个序列的联合概率： $$\rP(w_1, w_2, \\dots, w_T)\r$$ 这通常可以分解为条件概率的乘积： $$\rP(w_1, w_2, \\dots, w_T) = P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_1, w_2) \\cdot \\dots \\cdot P(w_T | w_1, w_2, \\dots, w_{T-1})\r$$ 这意味着每个词的概率取决于它前面的词。一个好的语言模型能够通过学习语料库中的模式，准确地捕捉这些条件概率。\n语言模型的用途 语言模型有广泛的应用场景，包括但不限于：\n文本生成：语言模型可以用来自动生成自然语言文本。例如，给定一部分句子，模型可以预测并生成合理的下一个词，从而逐步生成完整的句子或段落。\n机器翻译：在机器翻译中，语言模型帮助翻译系统生成符合目标语言语法的翻译结果。\n语音识别：语言模型能够帮助语音识别系统，在可能的识别结果中选择最合适的词序列。\n拼写纠正：在文本输入或搜索引擎中，语言模型可以用于纠正拼写错误，提供更符合语言习惯的结果。\n传统语言模型 在神经网络语言模型之前，最常用的语言模型是n元语法模型（n-gram Model）。n元语法模型通过假设一个词的概率只与它前面的n-1个词有关来简化问题。举个例子，三元语法模型（trigram model）认为： $$\rP(w_T | w_1, w_2, \\dots, w_{T-1}) \\approx P(w_T | w_{T-2}, w_{T-1})\r$$这种简化降低了计算复杂度，但也限制了模型的表达能力，因为它只能捕捉到短距离的上下文信息，无法充分考虑长距离的依赖关系。\n神经网络语言模型 传统语言模型的一个主要问题是词汇量增大时，模型的计算复杂度随之增加。为了解决这个问题，Yoshua Bengio等人在《A Neural Probabilistic Language Model》论文中提出了使用神经网络进行语言建模的想法。这种神经网络模型通过将词表示为词向量（word embeddings），并通过深度学习模型来捕捉更复杂的语境和词间关系，从而解决了传统语言模型的许多不足之处。\n总之，语言模型的核心任务就是对自然语言的概率进行建模，并通过学习语言中的统计模式来预测文本。\n什么是纬度灾难？ 维度灾难（Curse of Dimensionality）是指在处理高维数据时，随着数据维度增加，问题的复杂性呈指数级增长，导致很多传统方法在高维空间中变得低效或不可行的现象。\n其影响包括：\n数据稀疏性：在高维空间中，数据点变得非常稀疏，因为数据的总量相对于空间的体积变得极其小，导致模型难以从有限的数据中捕捉到模式和关系。简单来说，随着维度的增加，需要的样本量也随之爆炸性增长，否则模型可能会过拟合或无法泛化。\n计算复杂度增加：在高维空间中，计算距离、相似性等操作的复杂度大大增加，使得很多算法在高维情况下效率低下。\n模型的泛化能力下降：由于数据点过于稀疏，模型很难学习到有用的特征，从而可能导致泛化能力下降。\n具体来说，维度灾难对机器学习、统计学、最优化等领域带来了极大的挑战，尤其是当模型需要处理很多特征或变量时。为应对这个问题，研究人员提出了许多方法，如降维技术（如主成分分析PCA）和分布式表示（如词嵌入），以减少有效维度或通过分布式方法来表示复杂的数据结构。\n有概率为什么要引入似然的概念，他们有什么不同？ 概率和似然虽然看起来相似，但在统计学中它们有不同的定义和用途：\n概率（Probability） 概率是用来描述随机事件发生的可能性。在给定模型参数的情况下，概率告诉我们某个事件或观测结果发生的机会有多大。\n假设我们有一个随机变量 \\( X \\)，它取某个值 \\( x \\) 的概率可以表示为 \\( P(X = x|\\theta) \\)，其中 \\( \\theta \\) 是模型的参数。例如，如果我们有一个硬币掷出的模型，参数 \\( \\theta \\) 可以是硬币正面朝上的概率（比如 \\( \\theta = 0.5 \\)）。\n核心问题：给定模型的参数 \\( \\theta \\)，事件 \\( X = x \\) 发生的概率是多少？ 似然（Likelihood） 似然是从反方向来考虑的。它描述的是在已知观测数据的前提下，模型参数的可能性。具体来说，似然是给定观测数据后，模型参数如何使观测数据变得最可能的一个度量。\n假设我们已经观察到数据 \\( X = x \\)，现在我们想知道，在不同的模型参数 \\( \\theta \\) 下，这个数据出现的可能性有多大。似然可以表示为 \\( L(\\theta|X = x) \\)，或者更直观地写作 \\( P(X = x|\\theta) \\)，虽然数学表达式相同，但意义不同——在这里我们关注的是参数 \\( \\theta \\) 的值使观测到的数据最可能。\n核心问题：给定观测到的数据 \\( X = x \\)，模型参数 \\( \\theta \\) 有多大可能是正确的？ 概率和似然的区别 概率：我们知道参数 \\( \\theta \\)，希望知道某个事件 \\( X \\) 发生的概率。\n问题：给定参数，事件的概率是多少？ 似然：我们知道事件（观测数据），希望推断出最有可能的参数 \\( \\theta \\)。\n问题：给定观测数据，哪个参数最可能是正确的？ 例子 抛硬币的例子 概率：假设你有一枚硬币，已知它是公平的，即 \\( \\theta = 0.5 \\)，那么掷硬币得到正面的概率是 \\( P(\\text{正面}) = 0.5 \\)。\n似然：现在，你掷硬币10次，得到8次正面和2次反面。你不知道硬币的公平性（即不知道 \\( \\theta \\) 的值）。你想根据这次实验结果估计硬币正面朝上的概率 \\( \\theta \\)。此时，你需要用似然来衡量在不同 \\( \\theta \\) 值下，观测结果（8次正面，2次反面）出现的可能性有多大，从而找到最有可能的 \\( \\theta \\)。\n似然函数 \\( L(\\theta|X) \\) 可能在某个 \\( \\theta \\) 值处达到最大值，这个 \\( \\theta \\) 就是最能解释观测数据的参数值。\n总结 概率用于给定模型参数时预测事件的发生可能性。 似然用于在已知观测数据时，推断哪个参数最能解释这些数据。 ","date":"2024-10-13T00:00:00Z","image":"https://JiangZhiyu-1024.github.io/p/first-week/banlan_hu6307248181568134095.jpg","permalink":"https://JiangZhiyu-1024.github.io/p/first-week/","title":"2024.10.7至13第一周工作总结"}]