<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Homomorphic Encryption on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/categories/homomorphic-encryption/</link>
        <description>Recent content in Homomorphic Encryption on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Tue, 05 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/categories/homomorphic-encryption/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>安全与隐私机器学习：技术与应用的综述</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/</link>
        <pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/leaves-4574860_1280.jpg" alt="Featured image of post 安全与隐私机器学习：技术与应用的综述" /&gt;&lt;h1 id=&#34;安全与隐私机器学习技术与应用的综述&#34;&gt;☕安全与隐私机器学习：技术与应用的综述
&lt;/h1&gt;&lt;h2 id=&#34;abstract&#34;&gt;🥛Abstract
&lt;/h2&gt;&lt;p&gt;机器学习（ML）中的隐私违规可能导致歧视和身份盗窃等严重后果。随着近年来用于训练模型的敏感数据越来越多，保护隐私的机器学习方法的必要性变得更加重要。本综述研究全面回顾了隐私保护机器学习（PPML）的最新方法，如安全多方计算、同态加密和差分隐私。我们还评估了PPML的缺点，包括可扩展性、计算效率以及隐私与实用性之间的权衡。最后，我们识别了PPML中的开放问题和未来研究方向，包括新兴趋势、挑战和机会。本文旨在为对PPML领域感兴趣的研究人员和从业者提供有价值的资源。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;🍵Introduction
&lt;/h2&gt;&lt;p&gt;在当今的生活中，计算能力几乎与我们生活的每个方面都有互动。这种对计算机的高度依赖可能导致严重的隐私违规。计算机使用的主要趋势之一是机器学习（ML）。根据（Burkov, 2019），机器学习是“一个研究领域，专注于设计和开发能够从数据中学习和进行预测的算法，而不需要显式编程。换句话说，它涉及开发计算模型，这些模型可以根据从数据中获得的反馈自动改进其在特定任务上的表现。”机器学习正被用于解决医疗、金融等多个领域的许多生活问题。这项技术使用包含特征的数据集来构建所需的知识，以便得到正确的答案。有一种说法认为，训练阶段使用的数据记录越多，测试阶段的准确性就越高（Hey, Tansley, &amp;amp; Tolle, 2009）。从这个角度来看，科学家们开始请求与他们研究领域相关的数据，以便将其应用于机器学习模型。如果以医疗保健为例，根据健康保险流通与问责法案（HIPAA）规定，这些信息应通过建立国家标准来保护个人健康信息的隐私和安全，规范受保护健康信息（PHI）的使用和披露。 当这些数据被共享给科学家用于实验时，许多对策被考虑在内，例如隐藏个人身份信息（PII），然而，主要问题是：这些对策是否足以保护个人身份信息和受保护健康信息？&lt;/p&gt;
&lt;p&gt;有多位研究人员思考过这个问题；（Kim, Kim, &amp;amp; Kim, 2017）发现，在韩国存在通过居民注册号（RRN）去匿名化医疗数据的可能性，RRN是每位韩国居民的独特标识符。作者指出，利用公开可获取的信息，RRN可以被高精度地重新识别。此外，（Sweeney, 2002）声称，个人的出生日期、性别和邮政编码这三项信息可以用于唯一识别美国87%的人口。仅仅使用数据匿名化技术来保护个人身份信息是不够的。从这个角度来看，隐私保护机器学习（PPML）领域应运而生。&lt;/p&gt;
&lt;p&gt;隐私保护机器学习（PPML）是一种开发机器学习算法和模型的技术，旨在在推理和训练阶段保护敏感数据的隐私。在传统的机器学习中，模型通常使用已收集、整合并用于多种应用的数据进行训练。然而，当使用敏感数据（如财务或个人健康记录）时，这种方法可能会引发隐私问题。PPML通过创建允许在不危及基础数据隐私的情况下部署和训练机器学习模型的方法来解决这个问题。&lt;/p&gt;
&lt;p&gt;隐私保护机器学习（PPML）方法大致可以分为两类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;差分隐私。这种方法通过向数据中引入随机噪声来隐蔽用户身份并阻止私人信息泄露。&lt;/li&gt;
&lt;li&gt;安全多方计算（SMC）。多个参与方可以在不泄露任何私人数据的情况下共同训练机器学习模型。数据以一种允许每个参与方对数据进行计算而不直接访问数据的方式进行交换和加密。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在这项工作中，将讨论PPML的一个新颖概述。这项综述可以帮助读者找到研究空白，并提出与该领域相关的未来工作。本文的结构将如下：第二部分将介绍有助于理解将要讨论的主题的背景术语；第三部分将讨论用于PPML的技术；第四部分将讨论与该主题相关的最新论文；第五部分将讨论该主题的挑战；最后第六部分是结论。&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;🍾Background
&lt;/h2&gt;&lt;p&gt;在本节中，将提到所需的基础概念。本节中的术语对于理解我们研究的主题概念将是有用的。&lt;/p&gt;
&lt;h3 id=&#34;机器学习模型&#34;&gt;🍷机器学习模型。
&lt;/h3&gt;&lt;p&gt;机器学习（ML）被视为人工智能（AI）领域的一部分。它旨在构建一种方法，帮助机器在许多问题中做出决策。机器学习算法可以分为两个主要领域：监督学习和无监督学习算法。这两类之间的主要区别在于是否有来自所研究领域的专家提供的标签。如果算法在学习过程中需要带有标签的数据集，那么该算法被视为监督学习。一般来说，机器学习算法可以检测所用数据集特征中的模式，并基于这些模式对新案例进行预测。&lt;/p&gt;
&lt;h3 id=&#34;隐私保护机器学习ppml中的加密&#34;&gt;🍸隐私保护机器学习（PPML）中的加密。
&lt;/h3&gt;&lt;p&gt;通过促进私密数据的安全交换、存储和处理，密码学在隐私保护机器学习（PPML）中至关重要。为加密数据并对其进行计算而不向未经授权的方披露其内容，使用了同态加密、安全多方计算和秘密共享等密码学技术。同态加密允许对加密数据进行计算，而无需先解密，从而确保在计算过程中不会泄露敏感信息。安全多方计算允许多个参与方共同计算一个函数，而不向彼此披露数据。秘密共享将数据拆分成多个部分，并以一种方式分发到不同的参与方中，只有授权的参与方可以访问和重构原始数据。这些密码学技术使得开发能够保护敏感数据隐私的PPML算法成为可能，同时仍然能够提取有用的见解和知识。图1展示了同态加密在机器学习过程中的概述（Olzak, 2022）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241104204332300.png&#34;
	width=&#34;1049&#34;
	height=&#34;612&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241104204332300_hu1550586218636423810.png 480w, https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241104204332300_hu13796497834075906721.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241104204332300&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;171&#34;
		data-flex-basis=&#34;411px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;差分隐私&#34;&gt;🍹差分隐私
&lt;/h3&gt;&lt;p&gt;差分隐私提供了数据分析中隐私保证的精确规范。它确保在分析结果中不会泄露任何数据集参与者的私人信息。&lt;/p&gt;
$$
Pr [M(D) ∈ S] ≤ exp(ε) Pr [M(D′) ∈ S]
$$&lt;p&gt;
在这个方程中，Pr [M(D) ∈ S] 表示应用于数据集 D 的机制 M 输出结果在集合 S 中的概率。ε 参数控制提供的隐私保护程度，其中较小的 ε 值意味着更强的隐私保证。&lt;/p&gt;
&lt;p&gt;为了用一个例子说明这一点，考虑一个公司希望发布其员工平均收入的汇总统计数据，同时保护个人隐私的场景。通过应用差分隐私，公司可以以受控的方式向真实的平均收入添加随机噪声。&lt;/p&gt;
&lt;p&gt;假设员工的真实平均收入为50,000土耳其里拉。如果没有差分隐私，攻击者可能通过将发布的平均收入与外部信息进行比较，来确定某位员工的薪水。然而，通过应用差分隐私，公司向平均收入添加了随机噪声。例如，报告的平均收入可能是50,200土耳其里拉或49,800土耳其里拉，且概率相等。这种额外的随机性确保了攻击者无法根据发布的统计数据可靠地推断任何个别员工的确切收入。&lt;/p&gt;
&lt;p&gt;通过仔细控制添加的噪声量（由 ε 参数确定），差分隐私实现了个人隐私保护与提供准确汇总信息之间的平衡。较小的 ε 值提供更强的隐私保证，但可能引入更多噪声，从而降低发布数据的准确性，而较大的 ε 值可能提供较少的隐私保护，但会产生更准确的结果。如需了解有关差分隐私的更多信息，请参阅 (Dwork, 2006)。&lt;/p&gt;
&lt;h3 id=&#34;机器学习的隐私保护技术&#34;&gt;🍺机器学习的隐私保护技术
&lt;/h3&gt;&lt;p&gt;鉴于机器学习中使用的模型和算法通常依赖于大量敏感数据，隐私在隐私保护机器学习（PPML）中是一个关键挑战。PPML的目标是创建能够从敏感数据中学习的机器学习模型，而不泄露提供数据的人员的任何敏感信息。在处理应保密的机密信息时，例如财务或个人健康数据，这一点尤其重要。为了确保隐私，机器学习中通常采用多种隐私保护策略。其中一些机制包括数据匿名化、加密和访问限制。数据匿名化是一种在使用数据进行机器学习之前去除个人身份信息的过程。去除姓名、地址和其他可能用于识别特定个人的数据就是一种实现方式。加密则是在数据中进行编码，只有获得适当权限的人才能访问。通过访问控制措施限制谁可以访问数据以及如何访问。这些控制措施对于确保私人信息得到保护以及在整个机器学习过程中保持敏感数据的隐私至关重要。&lt;/p&gt;
&lt;p&gt;除了这些工具，隐私保护机器学习（PPML）还需要在模型推理和训练过程中对数据处理和共享协议进行仔细设计。这些协议的设计方式对防止敏感数据泄露或不当使用至关重要。为了防止重新识别个人，可能会采用差分隐私技术，例如通过对数据进行增强。多个参与方可以通过安全多方计算，在不互相披露个人信息的情况下，共同计算一个函数。数据被分成多个片段，并通过秘密共享在多个参与方之间传输，这样只有授权方才能访问并重新组装原始数据。这些加密方法使得可以创建能够保护敏感数据隐私的机器学习算法，同时仍然能够提取有价值的信息。为了保持对算法的信任，并确保个人的私密信息不被泄露，机器学习中的隐私保护至关重要。&lt;/p&gt;
&lt;h2 id=&#34;ppml中的相关工作&#34;&gt;🥃PPML中的相关工作
&lt;/h2&gt;&lt;p&gt;在本节中，将讨论学术领域在PPML（隐私保护机器学习）方面的研究工作。我们选择了该领域的重要研究论文，旨在研究它们如何解决研究问题。&lt;/p&gt;
&lt;h4 id=&#34;al-rubaie-2019&#34;&gt;🥤Al-Rubaie, 2019
&lt;/h4&gt;&lt;p&gt;本文可以被视为开始研究PPML主题的良好资源。作者提供了关于机器学习中隐私面临的威胁和挑战的概述，以及在机器学习模型的训练和推理阶段保护隐私的解决方案和技术。&lt;/p&gt;
&lt;p&gt;首先，他们讨论了在机器学习模型中应用额外对策以增强隐私的重要性。他们展示了几种可能的未经授权访问或滥用共享数据的场景。根据他们的分类，机器学习过程中可以涉及三方：数据拥有者、处理者以及接收结果的人。当这些参与者属于同一方或同一人时，能够实现最高的安全性，然而并非所有情况都是如此。他们提到了五种隐私泄露的威胁等级，这些威胁包括：私密数据的明文存储、重构攻击、模型反演攻击、成员推断攻击和再识别攻击。&lt;/p&gt;
&lt;p&gt;在私密数据的明文存储中，数据在转移到计算部分时以明文形式存储在存储设备中，这可能会影响隐私，尤其是在数据存储被攻破或遭遇不忠诚员工的内部威胁时。专家建议仅发送必要的特征，而不是发送整个数据行，但这种解决方案会导致第二种威胁——重构攻击。获取提取特征的知识或元数据有助于重建原始数据，从而可能导致隐私泄露。&lt;/p&gt;
&lt;p&gt;在模型反演攻击中，攻击者通过使用机器学习模型的输出，生成与用于构建该模型的特征向量相似的向量。这些攻击利用了作为响应返回的置信度数据。知道某个成员或参与者的数据是否被用于训练阶段，会导致第四种威胁——成员推断攻击。为保护参与者在构建数据集过程中的隐私，提出了一种解决方案，即删除与个人身份信息（PII）相关的任何特征或列。然而，根据第五种威胁——再识别攻击，攻击者可以通过将收集到的数据集与其他数据集结合，来恢复这些被遗漏的数据。&lt;/p&gt;
&lt;p&gt;接着，研究讨论了几种保护机器学习隐私的方法和解决方案，包括联邦学习、同态加密、安全多方计算和差分隐私。每种方法都进行了详细阐述，包括其基本原理、优点以及潜在应用。&lt;/p&gt;
&lt;p&gt;研究还涵盖了PPML中的问题和权衡，以及处理敏感数据时出现的伦理和法律问题。这些问题包括对模型准确性、计算时间和通信开销的影响。&lt;/p&gt;
&lt;p&gt;该论文提供了对PPML领域最新进展的全面且易于理解的总结，并强调了该领域需要进一步研究和发展的必要性，以应对机器学习中隐私保护不断变化的风险和需求。&lt;/p&gt;
&lt;h4 id=&#34;liu-guo-lam--zhao-2022&#34;&gt;🧃Liu, Guo, Lam, &amp;amp; Zhao, 2022
&lt;/h4&gt;&lt;p&gt;本文的作者提出了一种可扩展的隐私保护方案，该方案能够容忍任何参与者在任何时候的掉线。他们在提案中使用了同态加密伪随机生成器（HPRG）和沙密尔秘密共享方案。该提议的解决方案通过避免从客户端构造用于生成掩码的种子，减少了通信开销。此外，在客户端掉线的情况下，也无需像他们在比较中使用的基于SecAgg的方案那样向服务器发送额外的沙密尔共享数据。他们声称，提出的方案在抗掉线恢复能力上比其他解决方案更强。&lt;/p&gt;
$$
SIM^{U,t,k}_C (x_U, U_1, U_2, U_3) ≡ REAL^{U,t,k}_C (x_U, U_1, U_2, U_3)
$$$$
SIM^{U,t,k}_C (x_U, U_1, U_2, U_3) ≡ REAL^{U,t,k}_C (x_U, U_1, U_2, U_3)
$$&lt;p&gt;
另外两个理论讨论了防御活跃恶意客户端的安全性。为提供所需的安全性，提出了一种基于 HPRG 的安全聚合协议。第三个定理关注恶意客户端和诚实服务器。对于所有的 U, t, k，其中 |C| &amp;lt; t, $x_{U/C}$ 和 C，使用算法 MC，其中 C ⊆ U，所提出的协议是一个计算 FHSecAgg 的安全协议。最后一个定理涉及包括服务器在内的恶意客户端的安全性。&lt;/p&gt;
&lt;p&gt;为了理解这些理论，我们需要了解其中术语的含义。$U$ 表示一组具有本地训练模型 $x_U$ 的客户端。$S$ 表示模型的服务器端。这两种理论中的等式表示两边分布上的恒等关系。关于所提出协议的概述，私有输入可以总结为每个客户端的本地训练模型或梯度，它们可以表示为向量、用于初始化秘密密钥的密钥、以及签名的秘密密钥。在服务器端，存储了用于验证秘密通道和签名的密钥。另一方面，协议的公共输入包括客户端的数量、丢失客户端的阈值和签名过程的公钥。本地训练模型的聚合结果可以视为协议的输出。图 2 显示了协议的步骤，并对每个步骤进行了简要描述。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105101503786.png&#34;
	width=&#34;1068&#34;
	height=&#34;666&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105101503786_hu12456287249950315205.png 480w, https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105101503786_hu10669958393845248634.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241105101503786&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;文章中进行了若干测试，证明了他们所提到的假设。&lt;/p&gt;
&lt;h4 id=&#34;gupta--singh-2022&#34;&gt;🧉Gupta &amp;amp; Singh, 2022
&lt;/h4&gt;&lt;p&gt;在本文中，作者提出了一种基于云的PPML模型，结合了差分隐私和机器学习模型。该模型明确了不可信方之间的通信协议。进行的实验包括朴素贝叶斯（NB）分类器，并应用于多个数据集，最终达到了95%的准确率，超过了已有文献中的结果。图3展示了所提模型，该模型被称为基于数据和分类服务的差分隐私保护机器学习模型（DA-PMLM）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102127794.png&#34;
	width=&#34;781&#34;
	height=&#34;713&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102127794_hu3910818257146447941.png 480w, https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102127794_hu6649347863904803145.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241105102127794&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;109&#34;
		data-flex-basis=&#34;262px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;从图3中，可以看到该架构中有四个主要参与者：数据所有者（DOid），即云客户端，是数据的创建者，并愿意将其共享到云端；分类器所有者（CO），是负责向云服务提供商（CSP）提供分类服务的一方，CSP 进一步向DOid提供服务；最后是请求用户（RUid），即从CSP请求所有者数据的实体。数据所有者在共享其私人数据之前，通过差分隐私向数据中注入噪声。&lt;/p&gt;
&lt;p&gt;他们证明了以下理论：“在所提出的模型中，分类模型的隐私保护机制满足ε-差分隐私的并行组合。”&lt;/p&gt;
&lt;p&gt;图4展示了所提模型的分类和数据流。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102353362.png&#34;
	width=&#34;953&#34;
	height=&#34;461&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102353362_hu16699203993426064.png 480w, https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102353362_hu6502330861095835153.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241105102353362&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;206&#34;
		data-flex-basis=&#34;496px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在接收到带有噪声的数据 $D_{N1}^1, D_{N2}^2, \dots, D_{Nn}^n$ 来自相应的数据所有者（DOs）后，云服务提供商（CSP）通过应用标准化函数（使用Z-score标准化公式）对其进行预处理。得到的值可以用符号 $D_{\hat{N_i}}^i$ 表示。与任何机器学习过程类似，给定的数据集被划分为训练部分和测试部分。之后，进行分类模型（CM）的训练和测试，在实验中使用了朴素贝叶斯（NB）分类器，以进行评估测量。&lt;/p&gt;
&lt;h4 id=&#34;lee-et-al-2022&#34;&gt;🧊Lee, et al., 2022
&lt;/h4&gt;&lt;p&gt;本文的作者声称，当前的PPML解决方案仅限于非标准的机器学习模型。这些解决方案在实际数据集上未能证明取得良好的结果。此外，所使用的激活函数从算术角度来看较为简单，并且替换了非算术激活函数。它们还避免使用自助法（bootstrapping）。当前的解决方案中也无法实现大量层数，他们的提议包括使用标准的ResNet-20模型与RNS-CKKS全同态加密（FHE）结合自助法进行实现。为了验证他们的模型，他们将其应用于一个标准化的数据集——CIFAR-10。所得结果与使用原始ResNet-20模型且未加密时的结果非常接近，最终的准确率约为92.43%。&lt;/p&gt;
&lt;h2 id=&#34;ppml中的挑战&#34;&gt;🍻PPML中的挑战
&lt;/h2&gt;&lt;p&gt;PPML面临着若干挑战，这些挑战阻碍了其在实际应用中的广泛采用。主要的挑战之一是隐私与效用之间的权衡。像差分隐私和安全多方计算这样的PPML技术通常会引入噪声或通信开销，这可能显著影响机器学习模型的准确性和效率。另一个挑战是不同PPML技术之间缺乏标准化和互操作性，使得比较和结合不同方法的结果变得困难。此外，PPML需要专门的专业知识和资源，如加密学和安全计算的知识，这些可能并不容易为所有组织所拥有。最后，PPML还涉及法律和伦理问题，如合规性和透明度，这些问题必须仔细处理，以确保敏感数据的负责任和可信赖的使用。&lt;/p&gt;
&lt;h2 id=&#34;结论与未来工作&#34;&gt;🥂结论与未来工作
&lt;/h2&gt;&lt;p&gt;敏感数据的隐私必须得到保护，而隐私保护机器学习（PPML）这一研究领域正迅速发展。随着机器学习在医疗、银行和社交媒体等行业的广泛应用，隐私保护系统的重要性愈加突出。为了确保人们的敏感信息不被泄露，能够在尊重隐私的前提下支持机器学习的隐私保护算法和协议至关重要。该领域的研究进展涉及信息理论、机器学习和密码学等计算机科学多个分支，依赖于跨学科的合作。PPML是一个具有挑战性且复杂的问题，但它是实现机器学习带来益处的前提，而不牺牲个人隐私。在本次综述中，我们概述了PPML的主要难题和未来的研究方向，并回顾了PPML中使用的主要方法和技术。为了克服该领域中的重大问题，我们希望本综述能够成为对PPML感兴趣的研究人员和实践者的重要参考资源。未来的研究工作将包括在真实数据上的PPML研究，并与传统机器学习模型的表现进行比较。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
