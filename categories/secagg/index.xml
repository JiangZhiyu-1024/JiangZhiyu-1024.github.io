<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>SecAgg on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/categories/secagg/</link>
        <description>Recent content in SecAgg on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Mon, 11 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/categories/secagg/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>端到端隐私保护的多机构医学影像深度学习</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E7%AB%AF%E5%88%B0%E7%AB%AF%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E7%9A%84%E5%A4%9A%E6%9C%BA%E6%9E%84%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
        <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E7%AB%AF%E5%88%B0%E7%AB%AF%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E7%9A%84%E5%A4%9A%E6%9C%BA%E6%9E%84%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E7%AB%AF%E5%88%B0%E7%AB%AF%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E7%9A%84%E5%A4%9A%E6%9C%BA%E6%9E%84%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/fog-7482180_1280.jpg" alt="Featured image of post 端到端隐私保护的多机构医学影像深度学习" /&gt;&lt;h1 id=&#34;端到端隐私保护的多机构医学影像深度学习&#34;&gt;端到端隐私保护的多机构医学影像深度学习
&lt;/h1&gt;&lt;p&gt;使用大型、多国数据集来构建高性能医学影像人工智能系统需要隐私保护机器学习的创新，以便模型能够在不需要数据传输的情况下对敏感数据进行训练。我们在此介绍&lt;strong&gt;PriMIA&lt;/strong&gt;（隐私保护的医学影像分析），这是一个免费、开源的软件框架，用于医学影像数据的差分隐私、加密聚合的联邦学习和加密推理。我们在实际案例中测试了PriMIA，通过一个专家级深度卷积神经网络对儿童胸部X光片进行分类，所得到的模型分类性能与在本地非安全环境中训练的模型相当。我们从理论和实证上评估了该框架的性能和隐私保障，并展示了所提供的保护措施如何防止通过基于梯度的模型反演攻击来重构可用数据。最后，我们在端到端的加密远程推理场景中成功应用了训练模型，利用安全多方计算来防止数据和模型的泄露（还好有最后一句，不然差点又不看这个论文了）。&lt;/p&gt;
&lt;p&gt;人工智能（AI）和机器学习（ML）在生物医学数据分析中的快速发展最近带来了令人鼓舞的成果，展示了AI系统能够在多种情境下协助临床医生，例如在医学影像中进行癌症的早期检测。这类系统正逐步超越概念验证阶段，预计将在未来几年实现广泛应用，正如专利申请数量和监管审批不断增加所显示的那样。高性能AI系统的共同特征是需要大量多样的数据集来训练ML模型，这通常通过数据所有者自愿共享数据以及多机构或多国数据集的积累来实现。通常情况下，患者数据会在原始机构进行匿名化或假名化处理，然后传输至分析和模型训练的场所（即集中式数据共享）。然而，匿名化已被证明不足以抵御再识别攻击。因此，大规模的患者数据收集、聚合和传输从法律和伦理角度来说至关重要。此外，控制个人健康数据的存储、传输和使用是患者的一项基本权利。集中式数据共享在实际中几乎剥夺了这种控制权，导致数据主权的丧失。此外，匿名化数据一旦传输，就难以进行追溯性的更正或增强，例如引入后来获得的额外临床信息。&lt;/p&gt;
&lt;p&gt;尽管存在这些顾虑，对数据驱动解决方案的需求不断增加，预计将进一步推动健康相关数据的收集，不仅来自医学影像数据集、临床记录和医院患者数据，还包括例如通过可穿戴健康传感器和移动设备收集的数据。因此，需要创新的解决方案来平衡数据使用与隐私保护。安全且隐私保护的机器学习（PPML）旨在保护数据的安全、隐私和保密性，同时仍然允许从数据中得出有用的结论或将其用于模型开发。实际中，PPML使得即便在本地数据有限的低信任环境中也能进行先进的模型开发。这种环境在医学领域很常见，因为数据所有者无法依赖其他方的隐私和保密合规性。PPML还可以为模型所有者提供保障，确保其模型在使用过程中不会被修改、盗用或滥用，例如通过加密保护。通过缓解资产保护的顾虑，这为可持续的协作模型开发和商业部署奠定了基础。&lt;/p&gt;
&lt;h2 id=&#34;先前研究的证据&#34;&gt;先前研究的证据
&lt;/h2&gt;&lt;p&gt;近期研究显示了PPML在生物医学科学，尤其是医学影像中的实用性。例如，联邦学习（FL）是一种基于将机器学习模型分发给数据所有者（即计算节点）进行分布式训练的去中心化计算技术，而非将数据集集中收集。该方法被提议用于促进跨国协作，同时避免数据传输。在COVID-19疫情背景下，FL被用于保留数据主权并执行数据仓库的本地治理政策。在医学影像领域，最近的研究表明，基于脑肿瘤分割或乳腺密度分类的深度学习模型的联邦训练，其表现与本地训练相当，并且能够更广泛地纳入多样化数据源，提升了模型的泛化能力。&lt;/p&gt;
&lt;p&gt;然而，FL本身并非完全的隐私保护技术。先前研究表明，反演攻击能够通过模型权重或梯度更新重构图像，甚至具有令人惊讶的视觉细节。此外，在“推理即服务”的场景下，模型暴露给不可信的第三方可能导致模型被滥用或直接盗用。因此，FL必须结合其他隐私增强技术才能真正保护隐私。例如，通过安全聚合（SecAgg）模型权重或梯度更新，或差分隐私（DP），可以防止数据集重构攻击，而在模型推理过程中使用安全多方计算（SMPC）协议可以保护正在使用的模型。我们在先前的工作中对这些技术进行了概述。&lt;/p&gt;
&lt;h2 id=&#34;目标与贡献&#34;&gt;目标与贡献
&lt;/h2&gt;&lt;p&gt;PPML在医学影像中的临床应用需要开发安全与隐私框架，并在复杂的临床任务中进行验证。我们在此介绍PriMIA，这是一个免费、开源的框架，用于医学影像的端到端隐私保护去中心化深度学习。我们的框架结合了差分隐私的联邦模型训练、模型更新的加密聚合以及加密的远程推理。我们的贡献包括以下创新：&lt;/p&gt;
&lt;p&gt;我们展示了在公共互联网环境下，通过PriMIA的隐私增强技术辅助联邦学习（FL），对具有临床挑战性的儿童胸部X光片分类任务进行深度卷积神经网络（CNN）训练的过程。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们的框架兼容多种医学影像数据格式，用户配置简单，并在FL训练中引入了功能性改进（加权梯度下降/联邦平均、丰富的数据增强、本地提前停止、联邦范围的超参数优化、DP数据集统计交换），提升了灵活性、可用性、安全性和性能。&lt;/li&gt;
&lt;li&gt;我们比较了采用与不采用隐私增强技术训练的模型、在聚合数据集上集中训练的模型、在数据子集上个性化训练的模型以及在未见的真实数据集上与专家放射科医生的分类表现，评估医学影像研究中的各种典型情境。&lt;/li&gt;
&lt;li&gt;我们评估了框架的理论和实证隐私与安全保障，并提供了针对多种训练场景下模型的先进梯度反演攻击的应用示例。&lt;/li&gt;
&lt;li&gt;最后，我们展示了在安全的“推理即服务”场景中使用训练模型的案例，实现了数据和模型在明文中不被泄露，并展示了我们SMPC协议在推理延迟方面的改进。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;库功能&#34;&gt;库功能
&lt;/h2&gt;&lt;p&gt;PriMIA是作为PySyft/PyGrid开源PPML工具生态系统的扩展开发的。PySyft（https://github.com/OpenMined/PySyft）是一个Python框架，允许远程执行机器学习任务（例如，张量操作），并通过与常见的机器学习框架（如PyTorch）接口进行加密深度学习。PyGrid提供了服务器/客户端功能，用于在服务器和边缘计算设备上部署此类工作流。有关这些框架所提供的通用功能的详细描述，请参见我们之前的工作。PriMIA基于这些功能，针对医学影像特定应用进行了扩展，原生支持医学影像数据格式，如DICOM，并能够在任意模态和维度的医学数据集上进行操作（例如，计算机断层扫描、X光、超声和磁共振成像）。除了上述PPML技术外，它还提供了解决医学影像分析工作流中常见挑战的方案，例如数据集不平衡、先进的图像增强、全联邦超参数调优功能。此外，它还提供了一个可访问的用户界面，支持从用户机器上的本地实验到远程计算节点上的分布式训练的应用，以促进在医学联盟中应用PPML最佳实践。库的源代码、文档和公开可用的数据可以在https://doi.org/10.5281/zenodo.454559918上找到。&lt;/p&gt;
&lt;h2 id=&#34;案例研究系统设计与威胁模型&#34;&gt;案例研究、系统设计与威胁模型
&lt;/h2&gt;&lt;p&gt;我们通过在公共互联网的云计算节点上训练一个包含1110万个参数的ResNet18 CNN模型，展示了PriMIA在临床数据上的应用案例。该模型在由Kermany等人最初提出的儿童肺炎数据集上进行训练，目的是将儿童胸部X光片分类为以下三类之一：正常（无感染迹象）、病毒性肺炎或细菌性肺炎。肺炎是导致儿童死亡的主要原因之一。胸部X光检查常规用于鉴别诊断和治疗选择，但分类儿童胸部X光片具有挑战性。该案例研究的设置基于以下真实场景：&lt;/p&gt;
&lt;h2 id=&#34;fl训练阶段&#34;&gt;FL训练阶段
&lt;/h2&gt;&lt;p&gt;三个医院的联合体希望训练一个深度学习模型，用于胸部X光片分类。由于它们自身既没有足够的数据，也没有足够的专业知识来在这些数据上训练模型，因此它们寻求模型开发者的支持，以在中央服务器上协调训练。在训练阶段，我们将持有患者数据的医院称为数据所有者。在本文中，我们使用“模型”一词来指代深度神经网络的结构和参数。我们假设训练阶段采用先诚实后好奇的威胁模型，如前所述。这里，参与者信任彼此不会主动破坏学习协议，避免有意降低效用，例如主动提供对抗性输入或低质量数据（诚实）。然而，假设个别参与者及其合谋团体会主动尝试从其他参与者的数据中提取私人信息（好奇）。我们的框架的隐私增强技术旨在防止这种行为，我们将在后续部分详细描述。简而言之，差分隐私梯度下降将差分隐私的保证属性扩展到深度神经网络训练中。具体来说，它限制了数据集中单个患者的最坏隐私损失，并提供了针对模型反演/重构攻击的隐私保障，这些攻击可能发生在联邦参与者或在推理时对模型所有者进行攻击。PriMIA为每个FL节点实现了差分隐私（局部差分隐私），以提供患者级的隐私保证。每个节点的隐私预算使用Rényi差分隐私会计师进行。SMPC允许各方在不泄露各自贡献的情况下共同计算函数。在训练过程中，它用于安全地平均网络权重更新（SecAgg）。SecAgg使用基于SPDZ协议的加法秘密共享。训练阶段如图1所示，最终所有参与者都持有完全训练好的最终模型副本。&lt;/p&gt;
&lt;h2 id=&#34;远程推理阶段&#34;&gt;远程推理阶段
&lt;/h2&gt;&lt;p&gt;一旦模型完全训练完成，就可以用于远程推理。在我们的案例研究中，我们假设一个不同的数据所有者——在远程位置的医生，持有一些患者数据，并希望从模型中获得推理结果以协助诊断。推理服务由模型所有者通过互联网提供。数据和模型所有者之间不信任彼此，并希望他们的数据和模型保持私密。PriMIA的SMPC协议保证了推理阶段模型和数据的加密安全性。我们在之前的工作中描述的AriaNN框架被用来适应端到端加密推理。一个常见的SMPC技术是利用提前生成的加密安全随机数（加密原语）（即所谓的离线阶段），以加速某些计算。提供这些原语的可信系统（例如硬件设备）被称为加密提供者，它不参与实际的推理过程（在线阶段），也永远不会接触到任何一方的数据。实际上，加密原语的“储备”可以提前提供给协议参与者，在多个推理过程中使用。加密推理过程总结如图2所示。&lt;/p&gt;
&lt;h3 id=&#34;分类性能&#34;&gt;分类性能：
&lt;/h3&gt;&lt;p&gt;我们训练了不使用SecAgg或DP的FL模型（DP-/SecAgg-），仅使用SecAgg的模型（DP-/SecAgg+）和同时使用这两种技术的模型（DP+/SecAgg+）。此外，我们还在单台机器上对整个数据集进行了训练（集中训练），并对数据所有者的单独数据子集进行了个性化训练。集中训练的模型代表了引言中描述的集中数据共享场景。个性化模型则代表了每个机构仅基于其自身数据进行训练的情况，这是当前医学影像研究工作流程中的典型做法。FL旨在通过使模型训练效果优于个性化训练，并且理想情况下达到与集中训练模型相当的效果。我们在验证集上测试了这些模型的分类性能，并与两名专家放射科医师在测试集1（145张图像）上的分类表现进行了比较，同时还与测试集2（345张图像）上的临床实际数据进行了对比。我们使用了准确度、敏感性/特异性（召回率）、受试者工作特征曲线下的面积（ROC-AUC）和马修斯相关系数（MCC）27作为评估指标。详细信息请参见方法部分。模型和专家在数据集上的分类性能见表1。未使用SecAgg和DP的FL模型表现最好，与集中训练模型之间没有显著的统计差异。加入SecAgg后，模型性能略微下降，但未达到统计显著性。无论是FL模型还是集中训练模型，在性能上均显著优于人工观察者。DP训练过程（ε = 6.0，δ = 1.9 × 10^-4，α值（发散度阶数）为4.4）显著降低了模型性能，但该模型的表现仍与人工观察者相当，并在测试集1和2的样本外数据上保持了稳定的表现。我们注意到，ε值代表训练结束时所消耗的总隐私预算。仅在数据所有者的个别数据子集上训练的个性化模型仅在验证数据上表现相当，但在测试集1和2的样本外数据上表现显著较差，表明其泛化能力较差。这些结果的统计评估以及评分者间/模型间的一致性度量可以在补充材料2部分和补充表格1、2中找到。&lt;/p&gt;
&lt;h3 id=&#34;训练和推理性能基准测试&#34;&gt;训练和推理性能基准测试：
&lt;/h3&gt;&lt;p&gt;为了评估PriMIA隐私增强技术对性能的影响，我们在多种场景中对训练和推理性能进行了基准测试，如图3所示。训练时长是以固定批量大小计算的每批次平均时间，以便将其与数据集大小解耦。与本地训练相比，FL由于网络通信的开销会导致性能下降，加入SecAgg和DP后，性能下降更加明显，当同时使用SecAgg和DP时，训练时间是原来的三倍。大型神经网络架构由于需要更多的网络传输时间，因此需要更长的训练时间，这也为我们研究中使用ResNet18架构而非更大ResNet架构提供了依据。增加更多的工作节点会导致时间线性增加，尤其是在使用SecAgg时，因该协议的通信开销。然而，由于每轮操作的数量较少，该协议对多方的扩展性较好：对扩展性的线性回归分析得出t(w) = 0.57w + 2.61，其中t表示时间（秒），w表示工作节点的数量（R² = 0.98，p &amp;lt; 0.001，N = 100个样本，每个工作节点数量测试）。没有SecAgg时，训练时间几乎是常数。对于较大的数据集大小，批次训练时间保持不变，表明训练时间仅依赖于数据集大小，在其他条件相同的情况下。最后，我们基于功能秘密共享（FSS）协议28对加密推理实现进行了基准测试，该协议在执行比较操作、最大池化和批量归一化层时相较于广泛使用的SecureNN29提供了更高的效率。使用FSS进行加密推理显著减少了推理时间，特别是在高延迟设置中，FSS相比SecureNN提供了更好的性能。实现细节可以在方法部分找到，统计评估可以在补充材料3部分找到。&lt;/p&gt;
&lt;h3 id=&#34;模型反演攻击&#34;&gt;模型反演攻击：
&lt;/h3&gt;&lt;p&gt;先前的研究13,30表明，模型反演攻击能够重建特征或整个数据集记录（在我们的案例中是胸部X光图像），因此在FL设置中构成患者隐私的威胁。为了说明使用和不使用PriMIA提供的隐私增强技术训练的模型的易受攻击性，我们利用了改进的深度梯度泄漏攻击31,32，并进行了如方法部分所述的小修改。我们选择这种方法是因为它是第一个被证明对我们的案例研究中使用的ResNet18架构高度有效的技术。图4展示了胸部X光案例研究的示例结果。&lt;/p&gt;
&lt;p&gt;我们使用逐像素均方误差（MSE）、信噪比（SNR）和Fréchet初始距离（FID）指标来量化攻击成功率。实证评估表明，攻击的成功率高度依赖于梯度更新的L2范数和所使用的批量大小。为了生成最佳情况下的基准攻击，我们在训练开始时，以批量大小为1攻击集中训练模型，此时损失幅度（因此梯度范数）最高。对于我们案例研究中使用SecAgg的FL模型，攻击未成功，可能是因为其有效批量大小为600。与DP的隐私保障一致，当使用DP训练时，攻击未能成功。结果表明，即使模型在本地被攻击，或未使用SecAgg时，DP也能抵消攻击，这些结果可见于补充材料5部分和补充图2。为了进一步强调在医疗影像设置中隐私相关攻击的高风险，以及因此在协作模型训练中隐私增强技术的重要性，我们在公开的MedNIST数据集上进行了额外实验，并能够在未使用DP的情况下恢复揭示敏感患者属性的图像。启用DP时无法恢复任何图像（图5）。关于攻击的更多细节和统计评估可以在方法部分和补充材料4、6部分找到。&lt;/p&gt;
&lt;p&gt;讨论：我们介绍了PriMIA，这是一个用于医疗影像隐私保护FL和加密推理的开源框架。我们展示了在儿科胸部X光图像分类这一具有挑战性的临床任务中，如何进行去中心化的协作训练，并训练出了一个专家级深度卷积神经网络。此外，我们还展示了端到端加密推理，可以用于安全的诊断服务，无需披露机密数据或暴露模型。我们的工作是向医疗影像工作流程中实现下一代隐私保护方法的第一步。它适用于多机构研究和企业模型开发环境，能够保护数据治理和患者健康数据的主权。我们的框架可以用于推理即服务场景，在这些场景中，诊断支持可以远程提供，同时提供隐私、机密性和资产保护的理论和实证保障。PriMIA代表了我们以前工作17的针对性进化，面向医疗保健领域的部署。虽然我们在所展示的案例研究中专注于分类任务，但PriMIA高度可适应多种医疗影像分析工作流程，支持不同的网络架构、数据集等。在补充材料7部分和补充图3中，我们展示了一个额外的案例研究，聚焦于腹部计算机断层扫描的语义分割，以展示这一灵活性。&lt;/p&gt;
&lt;h3 id=&#34;模型分类性能&#34;&gt;模型分类性能：
&lt;/h3&gt;&lt;p&gt;最近的研究评估了数据质量（过于同质/独立同分布的数据与过于异质的数据）和分布式系统拓扑对联邦模型性能的影响，例如模型对样本外数据的泛化能力。在我们的案例研究中，使用FL训练的模型与集中训练模型表现相当，类似于参考文献5，并且优于人类观察者。仅在数据子集上训练的模型（个性化模型）在样本外数据上的表现显著下降。由于个性化模型训练是大多数单中心医疗影像研究中的标准做法，这一发现提醒我们，通过FL包含来自多个来源的大量更具多样性的数据，可以训练出具有更好泛化性能的模型，这也是当前最佳实践所要求的33。DP模型训练能够提供客观的隐私保障，并增强对模型反演攻击的抵抗力30,32。使用DP会降低模型性能，但其表现仍与人类观察者相当。与此同时，所选模型实现的DP保障（ε = 6）仅为中等水平。这一现象（隐私与效用的权衡）是深度学习与DP领域中一个众所周知的观察结果。例如，先前的研究23在CIFAR-10数据集上达到了大约8的ε值，另一项研究报告了ε值在6.9到8.48之间34。这两项研究也报告了最终模型性能的下降。我们认为改进DP模型训练的方法是未来研究的一个有前景的方向。&lt;/p&gt;
&lt;h3 id=&#34;fl功能改进&#34;&gt;FL功能改进：
&lt;/h3&gt;&lt;p&gt;为了提高框架的可用性、灵活性以及FL模型性能，我们的框架包括以下功能改进。（1）除了采用最近显示出改进收敛结果的Adam优化器形式的自适应客户端优化外，我们还包括了一系列先进的图像增强技术，包括MixUp，已被证明具有增强隐私保护的属性36。（2）我们实施了技术来解决节点之间（本地早停）以及数据集类别之间（类别加权梯度下降和联邦平均37）的数据量不平衡问题。（3）我们提供了在整个联盟中进行集中协调超参数优化的功能，使用的是树状Parzen估计器算法38。展示我们使用超参数选择框架搜索最优FL模型的实验数据可以在补充材料1部分和补充图1中找到。上述所有训练优化都在节点本地实施，并且不会对隐私保障产生负面影响。然而，在使用DP时，超参数调优必须考虑，因为它依赖于多次训练重复。&lt;/p&gt;
&lt;h3 id=&#34;关于隐私增强技术的讨论&#34;&gt;关于隐私增强技术的讨论：
&lt;/h3&gt;&lt;p&gt;在FL过程中引入提供可证明隐私和安全保障的方法是向广泛实施隐私保护AI技术迈出的关键一步8。我们在攻击实验中成功重建未保护模型的图像，强调了此类攻击对患者隐私的风险，这一点也在之前的研究中有所讨论6,39。DP训练在遭遇来自联盟成员或推理过程中的攻击时提供了客观的隐私保障，并不限于我们示例中使用的基于梯度的反演攻击。利用SMPC的SecAgg即使在最多n−1个成员串通的情况下，也仅将聚合模型更新披露给各方。这种我们提出的DP安全聚合数据集统计（均值和标准差）的方法可以保护FL参与者免受数据泄露，特别是在模型构建中包含非影像数据时（例如临床记录，其中如年龄等特征的均值代表敏感信息）。最后，加密推理不会向任何一方泄露数据或模型信息。与完全同态加密协议40（依赖于基于密钥的加密技术）相比，后者在神经网络训练和推理中的实现受到加密过程的计算复杂性和由于函数近似（例如激活函数）带来的性能下降的限制，通信开销传统上一直是SMPC的限制因素。在我们最近的工作中，我们引入了AriaNN26，这是一种利用函数秘密共享（FSS）28并基于SPDZ25构建的SMPC协议。它代表了一种替代协议，如SecureNN29或Falcon41，并通过一次通信回合计算私有比较。这使得FSS比其他SMPC协议在通信上更加高效，特别是在各方地理位置远离且通信延迟高时，例如我们研究中展示的在公共网络上执行推理的情况。通过本案例，我们确认了在其他数据集上获得的结果：在高延迟环境下，安全推理从FSS协议中获得的好处成比例更大。因此，我们建议在诚实但好奇的环境下，当希望减少延迟时，使用FSS而非SecureNN。&lt;/p&gt;
&lt;h2 id=&#34;与先前工作的比较&#34;&gt;与先前工作的比较：
&lt;/h2&gt;&lt;p&gt;当前有几项研究旨在将隐私保护机器学习（PPML）技术引入生物医学影像领域：Silva等人42提出了一种面向生物医学的前端FL框架，但未考虑DP、SecAgg或加密推理。Xu及其同事（https://bit.ly/3pl5dD1）提供了一个使用同态加密进行SecAgg的FL框架，但未使用DP或提供加密推理能力。Sheller等人43展示了一个基于分割的FL应用案例，但没有评估DP、SecAgg或加密推理选项。Li等人44也展示了一个FL分割任务。他们的DP实现依赖于一种替代技术（稀疏向量），且框架未提供安全聚合或加密推理功能。Lu及其同事45的研究展示了带有DP的FL，但他们的应用案例集中于病理切片，未使用SecAgg或提供加密推理能力。Li等人46使用了DP，但假设了固定的敏感性并未进行隐私分析，他们的框架不提供SecAgg或加密推理。&lt;/p&gt;
&lt;h2 id=&#34;局限性&#34;&gt;局限性：
&lt;/h2&gt;&lt;p&gt;我们考虑了以下几点局限性。部署我们的系统的计算要求很高，尽管我们提出了协议改进，但加密推理带来的延迟仍然远高于未加密推理。当前的远程执行环境仅提供实验性的图形处理单元（GPU）支持，计划在即将发布的版本中提供完整支持。FL模型的成功在很大程度上依赖于节点上的高质量数据。数据的审计和整理、量化单个数据集对模型的贡献或检测局部过拟合的方法仍在研究中47。我们的库设计用于诚实但好奇的环境，我们认为这代表了医疗联盟中的标准。因此，尽管我们提供了全面的隐私保护措施，但没有针对恶意贡献低质量或对抗性数据到FL过程中的具体反制措施，也未验证/保证数据所有者推理设置中使用的模型是所承诺的模型。此外，我们指出，关于理论威胁模型的讨论是一种抽象层次，无法完全代表现实生活中的复杂情况。例如，威胁建模通常是在代表整个医院的FL参与者层面进行的，但这不能考虑到为这些医院工作的每个个体及其具体动机。类似地，关于FL中参与者报酬或模型所有权的问题超出了我们当前研究的范围。未来的研究需要进一步阐明这些细节。最后，如上所述，使用DP会导致模型隐私和效用之间的直接权衡。未来的工作需要通过改进隐私分析和训练技术来解决这一权衡，因为当前研究的隐私保障（包括我们研究中大约6.0的ε值）尚不够严格，不能被认为是普遍适用的。&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论：
&lt;/h2&gt;&lt;p&gt;我们提出了一个免费的开源软件框架，用于隐私保护的FL和医疗影像数据的端到端加密推理，并在一个具有临床意义的实际案例研究中展示了该框架。进一步的研究和开发将促进我们框架的更大规模部署，验证我们在不同跨机构数据上的发现，并推动PPML技术在医疗保健及其他领域的广泛应用。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
