<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>安全推理 on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/categories/%E5%AE%89%E5%85%A8%E6%8E%A8%E7%90%86/</link>
        <description>Recent content in 安全推理 on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Sun, 08 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/categories/%E5%AE%89%E5%85%A8%E6%8E%A8%E7%90%86/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>CipherGPT：安全的两方 GPT 推理</title>
        <link>https://JiangZhiyu-1024.github.io/p/ciphergpt%E5%AE%89%E5%85%A8%E7%9A%84%E4%B8%A4%E6%96%B9-gpt-%E6%8E%A8%E7%90%86/</link>
        <pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/ciphergpt%E5%AE%89%E5%85%A8%E7%9A%84%E4%B8%A4%E6%96%B9-gpt-%E6%8E%A8%E7%90%86/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/ciphergpt%E5%AE%89%E5%85%A8%E7%9A%84%E4%B8%A4%E6%96%B9-gpt-%E6%8E%A8%E7%90%86/lake-192990_1280.jpg" alt="Featured image of post CipherGPT：安全的两方 GPT 推理" /&gt;&lt;h1 id=&#34;ciphergpt安全的两方-gpt-推理&#34;&gt;CipherGPT：安全的两方 GPT 推理
&lt;/h1&gt;&lt;p&gt;摘要——ChatGPT 被认为是人工智能领域的一次重要革命，但它引发了用户隐私的严重担忧，因为用户提交的数据可能包含敏感信息。现有的安全推理解决方案在支持类似 GPT 的模型时面临显著挑战，这主要由于模型参数数量巨大以及激活函数的复杂性。在本文中，我们开发了 CipherGPT，这是首个针对 GPT 的安全两方推理框架，基于一系列创新协议构建。首先，我们提出了一种专为 GPT 推理定制的安全矩阵乘法协议，其性能相比当前最先进技术（SOTA），在速度上实现了高达 2.5 倍的提升，带宽消耗减少了 11.2 倍。同时，我们提出了一种新颖的 GELU 激活函数安全计算协议，在运行时间、通信开销和精度方面分别超越 SOTA 4.2 倍、3.4 倍和 10.9 倍。此外，我们设计了首个支持 top-k 采样的协议。我们为 CipherGPT 提供了全面的实现和基准测试，特别是对每个操作的运行时间和通信量以及其相应的比例进行了测量。我们相信，这将为该领域的未来研究提供重要参考。&lt;/p&gt;
&lt;h2 id=&#34;引言&#34;&gt;引言
&lt;/h2&gt;&lt;p&gt;ChatGPT 是基于开创性的生成式预训练变换器（GPT）架构 [34] 构建的大型语言模型（LLM），被认为是人工智能领域的一次重大革命。凭借庞大的知识库和卓越的语言能力，ChatGPT 擅长执行各种任务，包括问答、文章润色、建议提供以及会话交流。它还可以作为虚拟助手，高效支持客户服务、信息检索和语言翻译等应用。OpenAI 已将 ChatGPT 开发为在线推理服务，并为开发者提供了远程 API，用户只需提交提示或消息即可方便地享受 GPT 推理服务。&lt;/p&gt;
&lt;p&gt;然而，这种服务模式不可避免地将用户隐私置于风险之中，因为用户提交的数据可能包含敏感信息。这类隐私问题可能会限制 GPT 在数据保密性至关重要的某些场景中的部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;安全推理&lt;/strong&gt;（Secure inference）[20], [30], [28], [31], [37], [36], [27], [25] 是一种两方密码协议，用于在推理阶段保证以下安全性：服务器（S）无法得知客户端（C）的输入内容，而客户端仅能获取推理结果，对模型的其他部分一无所知。大致来说，这一过程通过服务器和客户端利用加密技术（如同态加密和秘密共享）在加密模型和加密输入上进行计算来实现。通常会引入预处理阶段，以准备一些昂贵且与输入无关的计算，从而提高在线阶段的效率。&lt;/p&gt;
&lt;p&gt;不幸的是，现有的安全推理协议在支持 GPT 方面能力有限。例如，Cheetah [27] 专为卷积神经网络（如 ResNet50）设计，而 Iron [25] 则仅支持单个变换器。然而，像 GPT-2 这样的 LLM 包含 12 个变换器，涉及大量高维矩阵乘法和复杂的数学函数（如 GELU）。因此，GPT 的出现确实为安全推理领域带来了新的挑战。&lt;/p&gt;
&lt;h3 id=&#34;我们的贡献&#34;&gt;我们的贡献
&lt;/h3&gt;&lt;p&gt;在本文中，我们提出了 &lt;strong&gt;CipherGPT&lt;/strong&gt;，这是首个用于安全 GPT 推理的框架，基于一系列新颖的协议构建而成。&lt;/p&gt;
&lt;h4 id=&#34;基于-vole-的矩阵乘法&#34;&gt;基于 VOLE 的矩阵乘法
&lt;/h4&gt;&lt;p&gt;GPT 接受较长的句子作为输入，并以自回归的方式生成响应单词。具体来说，在生成一个响应单词后，该单词会被添加到输入句子中，新的句子则作为模型输入用于生成下一个响应单词。每生成一个响应单词都需要进行一次模型推理，而这包含在每一层中执行一次矩阵乘法（简称 MatrixMul）。&lt;/p&gt;
&lt;p&gt;在预处理阶段，对于每一层，我们将针对各个响应单词的 MatrixMul 合并为一个不平衡的 MatrixMul，并通过 sVOLE 进行处理。&lt;/p&gt;
&lt;h4 id=&#34;矢量隐私线性评估-vole&#34;&gt;矢量隐私线性评估 (VOLE)
&lt;/h4&gt;&lt;p&gt;矢量隐私线性评估（VOLE）[8], [10], [48], [9] 用于生成诸如 w = ux + v 这样的关联，其中发送方输入为 x，获得长度为 n 的向量 w，接收方获得长度为 n 的向量 (u, v)。子域 VOLE (sVOLE) [10] 是 VOLE 的一种推广形式；理想情况下，sVOLE 能完成 k 次普通 VOLE 实例的任务，同时成本与运行单次 VOLE 实例相当。当n ≫ k 时，sVOLE 更具成本效益，这使其在计算不平衡的矩阵乘法（MatrixMul）时尤为实用。&lt;/p&gt;
&lt;h4 id=&#34;基于样条的-gelu&#34;&gt;基于样条的 GELU
&lt;/h4&gt;&lt;p&gt;GPT 使用 GELU 作为其激活函数，其表达式为：&lt;/p&gt;
&lt;p&gt;$\text{GELU}(x) = 0.5x\left(1 + \tanh\left(\sqrt{2/\pi}(x + 0.044715x^3)\right)\right)$,&lt;/p&gt;
&lt;p&gt;其中：
$\tanh(x) = 2\text{Sigmoid}(2x) - 1, \quad \text{Sigmoid}(x) = \frac{1}{1+e^{-x}}$.&lt;/p&gt;
&lt;p&gt;为安全计算 GELU，当前最先进的方法（SOTA）[36], [25] 采用查找表（LUT）近似$e^{-x}$，并用另一个查找表近似倒数。这一多步过程还需要在每一步扩展或截断位宽以平衡精度和效率。&lt;/p&gt;
&lt;p&gt;相比之下，我们的目标是在单步中整体计算 GELU。为此，我们将 GELU 分为多个区间，并使用线性函数 y = ax + d 对每个区间内的曲线进行近似。这种基于样条的近似最初由 Liu 等人 [30] 提出，其中使用混淆电路确定 x 所属的区间并计算相应的线性函数。我们通过利用查找表（LUT）确定区间，并以秘密共享的方式计算相应的线性函数，显著提高了其性能。&lt;/p&gt;
&lt;p&gt;与现有的 SOTA GELU 计算方法 [25] 相比，我们节省了一个查找表、两次截断和三次秘密共享乘法操作，同时需要更小的查找表。此外，我们的单步方法精度更高，因为它避免了多步方法中固有的误差累积问题。&lt;/p&gt;
&lt;h3 id=&#34;基于洗牌的-top-k-选择&#34;&gt;基于洗牌的 Top-K 选择
&lt;/h3&gt;&lt;p&gt;从长度为 n 的秘密共享向量中选择 Top-K 元素的一种直接方法是对向量进行安全排序，这通常通过数据无关的排序算法（如 Bitonic 排序网络 [26]）来实现。我们的第一个见解来自 [4]，即通过以下步骤实现安全排序：首先对输入元素进行安全洗牌，然后应用基于比较的排序协议（例如快速排序）将洗牌后的元素排列为有序状态。在排序过程中获得的比较结果不会泄露原始元素的信息，因为这些元素已经被洗牌。&lt;/p&gt;
&lt;p&gt;我们的第二个见解是，若采用快速排序，则无需对整个向量排序。相反，我们可以利用改进版的快速排序算法。通常，快速排序从向量中随机选择一个元素作为基准值（pivot），并将其与其他元素进行比较。根据比较结果，将向量划分为两部分：小于基准值的元素和大于基准值的元素。快速排序随后递归地处理这两个部分。在我们的场景中，只需递归处理包含 Top-K 最大元素的部分，从而将安全比较的数量从 $O(n \log n) $降至 O(n)。&lt;/p&gt;
&lt;h3 id=&#34;安全采样&#34;&gt;安全采样
&lt;/h3&gt;&lt;p&gt;我们还解决了基于秘密共享概率从向量中安全选择一个元素的问题。具体而言，给定一个包含 K 个元素的向量，其中每个元素都与一个秘密共享概率 $p_i$ 相关联，第 j 个元素被选择的概率为 $p_j$。我们的协议仅需 (K - 1) 次安全比较和 K 个多路复用器。据我们所知，这是首次对安全采样的探索。&lt;/p&gt;
&lt;h3 id=&#34;我们的贡献总结&#34;&gt;我们的贡献总结
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定制化的安全矩阵乘法&lt;/strong&gt;：针对 GPT 优化的安全矩阵乘法协议，与现有最优方法相比，运行速度提高了最多 2.5 倍，带宽需求降低了 11.2 倍（详见第 3 节）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;新颖的 GELU 安全计算协议&lt;/strong&gt;：在运行时间、通信开销和精度方面分别比现有方法快 4.2 倍、少 3.4 倍和高 10.9 倍（详见第 4 节）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;创新的 Top-K 采样解决方案&lt;/strong&gt;：实现了 Top-K 概率选择，并基于选定概率进行采样（详见第 5 和第 6 节）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;首个用于安全 GPT 推理的框架&lt;/strong&gt;：构建了全面的基准测试，可为该研究方向的探索提供参考（详见第 7 和第 8 节）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;预备知识&#34;&gt;&lt;strong&gt;预备知识&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;在本节中，我们介绍理解本文所需的基本概念。表1总结了本文中常用的符号。&lt;/p&gt;
&lt;h3 id=&#34;安全推理与威胁模型&#34;&gt;&lt;strong&gt;安全推理与威胁模型&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;安全推理是一种双方加密协议，用于在客户端 C 和服务器 S 之间进行模型推理。该协议确保 C 仅获得关于模型架构和推理结果的知识，而将 S 的模型的其他细节保持隐藏。同样，S 也无法获知 C 的输入以及推理结果。在 GPT 推理的背景下，C 的输入是提示（prompt），而 S 的模型包括多个 Transformer 解码器迭代和一个 vec2word 层。有关 GPT 的更多信息请参见第7节。我们假设 C 或 S 之一可能是半诚实的对手，即遵循协议规范但试图在协议执行过程中尽可能多地收集信息。我们假设对手在计算上是有界的，并使用 λ 表示计算安全参数。&lt;/p&gt;
&lt;h3 id=&#34;加密原语&#34;&gt;&lt;strong&gt;加密原语&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;秘密共享&lt;/strong&gt;
我们使用基于不同2的幂环的2-out-of-2加法秘密共享方案。对于 $x \in \mathbb{Z}_{2^l}$，我们用 $\langle x \rangle_l = (\langle x \rangle_l^S, \langle x \rangle_l^C)$ 来表示它的共享，其中 $x = \langle x \rangle_l^S + \langle x \rangle_l^C \mod 2^l$。为了简化表达，当上下文不影响时，我们省略 $\langle x \rangle_l$ 中的 $l$ 标记。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;非统一比特宽度的乘法&lt;/strong&gt;
理想功能 $F_{\text{Mult}}$ 接受 $\langle x \rangle_g$ 和 $\langle y \rangle_h$ 作为输入，并返回 $\langle z \rangle_l$，其中 $z = x \cdot y$ 且 $l = g + h$。实现该功能的一种简单方法是首先将两个输入扩展为 $l$ 比特，然后使用标准的秘密共享乘法协议，假设比特宽度统一。SIRNN [36] 提供了一种协议，性能比这个简单方案提高了 1.5 倍。该协议的通信复杂度为 $\mu(\lambda + \frac{\mu}{2} + \frac{1}{2}) + mn$，其中 $\mu = \min(m, n)$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;安全比较&lt;/strong&gt;
理想功能 $F_{\text{CMP}}$ 接受 $\langle x \rangle_l$ 和 $\langle y \rangle_l$ 作为输入，并返回 $\langle b \rangle_1$，其中如果 $x \geq y$，则 $b = 1$，否则 $b = 0$。CrypTFlow2 [37] 提供了一种高效的 $F_{\text{CMP}}$ 协议，通信成本小于 $\lambda l + 14l$ 比特，并且具有 $\log l$ 轮次。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;安全多路复用器&lt;/strong&gt;
理想功能 $F_{\text{MUX}}$ 接受 $\langle x \rangle_l$ 和 $\langle b \rangle_1$ 作为输入，并返回 $\langle y \rangle_l$，其中当 $b = 1$ 时 $y = x$，当 $b = 0$ 时 $y = 0$。本文采用的安全多路复用器由 SIRNN [36] 提供，通信需求为 $2(\lambda + l)$ 比特。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;安全截断&lt;/strong&gt;
理想功能 $F_{\text{Trunc}}$ 接受 $\langle x \rangle_l$ 和 $s$ 作为输入，并返回 $\langle y \rangle_l$，其中 $y = x \gg s$。SIRNN [36] 提供了一种安全截断协议，通信成本小于 $\lambda(l + 3) + 15l + s + 20$ 比特，并且具有 $\log l + 3$ 轮次。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;截断后缩减&lt;/strong&gt;
理想功能 $F_{\text{TR}}$ 接受 $\langle x \rangle_l$ 和 $s$ 作为输入，并返回 $\langle y \rangle_{l - s}$，其中 $y = x \gg s$。注意，$F_{\text{TR}}$ 与 $F_{\text{Trunc}}$ 的区别在于，$F_{\text{TR}}$ 将输出通过右移操作缩减到更小的环中，而 $F_{\text{Trunc}}$ 将输出保留在原始环中。SIRNN [36] 提供了一种协议来实现 $F_{\text{TR}}$，其通信成本小于 $\lambda(s + 1) + l + 13s$ 比特。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;查找表&lt;/strong&gt;
理想功能 $F_{\text{LUT}}$ 接受 $\langle i \rangle$ 作为输入，并返回 $\langle T[i] \rangle$，其中 $T$ 是一个具有 $M$ 项的表。该功能可以通过对 $M$-1OT [17] 的一次调用来实现。更高效的解决方案是首先将 LUT 描述转换为布尔表达式，然后使用多输入内积 [13] 评估它。该协议在预处理阶段的通信成本为 $(mt + 4) \cdot (2^M - M - 1)$ 比特（其中 $mt$ 表示生成布尔乘法三元组的成本），在线阶段的通信成本为 $2\sigma$ 比特。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;秘密共享洗牌&lt;/strong&gt;
理想功能 $F_{\text{Shuffle}}$ 接受 $\langle x \rangle$ 和 $\langle \pi \rangle$ 作为输入，并返回 $\langle \pi(x) \rangle$，其中 $\pi$ 是一个置换函数。Chase 等人 [14] 提出了使用轻量级原语（如 OT 和 P RG）构建此功能的高效方案。他们的方法通过使用可打孔的伪随机函数（PRF）来构建一个置换和共享协议，使得两方能够使用由一方选择的置换来置换输入向量。这个置换和共享协议需要运行两次，每一方选择一次置换。为了对 $n$ 个 $l$ 比特元素进行洗牌，该协议的通信成本与 $\lambda n \log n + n l \log n / \log T$ 成正比，计算成本为 $(n T \log n / \log T) \cdot (l / \lambda)$ 次对称密钥操作，其中 $T$ 的值介于 16 和 256 之间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;子域向量无关线性评估&lt;/strong&gt;
VOLE 是一种双方功能，它接受来自发送方的标量 $x \in F_p$ 并生成 VOLE 关联：w = ux + v,
其中接收方学习到 $(u, v) \in_R F_{pn} \times F_{pn}$，发送方学习到 $w \in_R F_{pn}$。VOLE 通常基于带噪声的学习奇偶性（LPN）和可打孔的伪随机函数（PRF）构造，因此其复杂度几乎与 $n$ 无关。子域 VOLE（sVOLE）是 VOLE 的一种推广，其中 $u \in_R F_{pn}$，$x \in F_q$，$w, v \in_R F_{qn}$，且 $q = p^m$。注意，sVOLE 完成的任务与运行 $m$ 个普通 VOLE 实例相同，但成本更低。VOLE 和 sVOLE 最初是设计用于有限域上工作的。Baum 等人 [6] 提出了在有限环（如 $\mathbb{Z}_{2^l}$）上工作的方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;同态加密&lt;/strong&gt;
全同态加密（FHE）是一种加密方案，允许对加密数据执行任意操作 [19]。在实际应用中，它通常以分级方式使用：操作只能执行有限次数，否则密文无法解密。在大多数 FHE 加密系统中 [12], [11], [18], [15]，明文被编码为来自商环 $\mathbb{Z}_p[x]/(x^N + 1)$ 的多项式，其中 $N$ 是 2 的幂，$p$ 是明文模数。明文多项式随后被加密成密文多项式 $\mathbb{Z}_q[x]/(x^N + 1)$，其中 $q$ 是密文模数，它决定了安全级别以及可以执行操作的次数。&lt;/p&gt;
&lt;h2 id=&#34;安全矩阵乘法&#34;&gt;&lt;strong&gt;安全矩阵乘法&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;矩阵乘法操作（MatrixMul）接受来自$C$和$S$的两个输入矩阵$X \in \mathbb{Z}^{n \times m}&lt;em&gt;{2^l}$和$Y \in \mathbb{Z}^{m \times k}&lt;/em&gt;{2^l}$，并输出$\langle Z \rangle$，其中$Z = XY \in \mathbb{Z}^{n \times k}_{2^l}$。现有的大多数解决方案通过同态乘法和加法以隐私保护的方式计算上述公式。通常采用SIMD技术将$N$个元素批处理到一个RLWE密文中以摊销成本，但这需要昂贵的同态旋转来求和【28】。Cheetah【27】用系数打包替代SIMD以消除昂贵的旋转操作。然而，它仍然需要传输$\geq 2n\sqrt{mk} / \sqrt{N}$个RLWE密文，并执行$\geq nmk / N$次密文-明文同态乘法。&lt;/p&gt;
&lt;p&gt;需要注意的是，GPT需要以自回归的方式生成响应词语。因此，单次GPT推理需要对不同的$X$与相同的$Y$运行MatrixMul操作。我们旨在利用GPT的这一特性减少MatrixMul的摊销成本。令$X = [x_1, x_2, \cdots, x_m]$（其中$x_i \in \mathbb{Z}^n_{2^l}$是$X$的每列），$Y^T = [y&amp;rsquo;_1, y&amp;rsquo;_2, \cdots, y&amp;rsquo;&lt;em&gt;m]$（其中$y&amp;rsquo;&lt;em&gt;i \in \mathbb{Z}^k&lt;/em&gt;{2^l}$是$Y$的每行），则$Z = \sum&lt;/em&gt;{i=1}^m (x_i \otimes y&amp;rsquo;_i)$。&lt;/p&gt;
&lt;p&gt;假设$S$和$C$需要生成$t$个响应词语，因此有$t$个输入矩阵：
&lt;/p&gt;
$$
X_1 = [x_{1,1}, x_{1,2}, \cdots, x_{1,m}],\   X_2 = [x_{2,1}, x_{2,2}, \cdots, x_{2,m}],\   \cdots,\   X_t = [x_{t,1}, x_{t,2}, \cdots, x_{t,m}].
$$&lt;p&gt;
令$x&amp;rsquo;&lt;em&gt;i = x&lt;/em&gt;{1,i} || x_{2,i} || \cdots || x_{t,i}, \ \forall i \in [1, m]$，则
&lt;/p&gt;
$$
x&#39;_i \otimes y&#39;_i = (x_{1,i} \otimes y&#39;_i) || (x_{2,i} \otimes y&#39;_i) || \cdots || (x_{t,i} \otimes y&#39;_i).
$$&lt;p&gt;
因此，
&lt;/p&gt;
$$
\sum_{i=1}^m (x&#39;_i \otimes y&#39;_i) = Z_1 || Z_2 || \cdots || Z_t。
$$&lt;p&gt;
因此，我们可以通过$m$次外积计算$t$次MatrixMul操作。鉴于$Y$是预先已知的，我们可以引入一个预处理阶段，让$S$和$C$生成$m$个sVOLE相关：
&lt;/p&gt;
$$
W_i = u_i \otimes y&#39;_i + V_i, \ \forall i \in [1, m],
$$&lt;p&gt;
其中$C$持有$u_i \in \mathbb{Z}^{(t \cdot n)}&lt;em&gt;{2^l}$（这是长度为$t \cdot n$的向量）和$V_i \in \mathbb{Z}^{(t \cdot n) \times k}&lt;/em&gt;{2^l}$，$S$持有$y&amp;rsquo;&lt;em&gt;i \in \mathbb{Z}^k&lt;/em&gt;{2^l}$和$W_i \in \mathbb{Z}^{(t \cdot n) \times k}_{2^l}$。&lt;/p&gt;
&lt;p&gt;在在线阶段，对于输入矩阵$X_j = [x_{j,1}, x_{j,2}, \cdots, x_{j,m}]$，$C$发送
&lt;/p&gt;
$$
\langle x_{j,i} \rangle_S := x_{j,i} - u_i[(j-1)n + 1, \cdots, j \cdot n], \ \forall i \in [1, m]
$$&lt;p&gt;
给$S$，然后$S$计算：
&lt;/p&gt;
$$
\langle x_{j,i} \rangle_S \otimes y&#39;_i = (x_{j,i} - u_i[(j-1)n + 1, \cdots, j \cdot n]) \otimes y&#39;_i   = x_{j,i} \otimes y&#39;_i - u_i[(j-1)n + 1, \cdots, j \cdot n] \otimes y&#39;_i.
$$&lt;p&gt;
然后我们有：
&lt;/p&gt;
$$
x_{j,i} \otimes y&#39;_i = \langle x_{j,i} \rangle_S \otimes y&#39;_i + u_i[(j-1)n + 1, \cdots, j \cdot n] \otimes y&#39;_i   = \langle x_{j,i} \rangle_S \otimes y&#39;_i + W_i[(j-1)kn + 1, \cdots, j \cdot k \cdot n]   - V_i[(j-1)kn + 1, \cdots, j \cdot k \cdot n].
$$&lt;p&gt;
注意，$S$持有：
&lt;/p&gt;
$$
\langle x_{j,i} \rangle_S \otimes y&#39;_i + W_i[(j-1)kn + 1, \cdots, j \cdot k \cdot n],
$$&lt;p&gt;
而$C$持有：
&lt;/p&gt;
$$
V_i[(j-1)kn + 1, \cdots, j \cdot k \cdot n]
$$&lt;p&gt;
这意味着$S$和$C$共同秘密共享了$x_{j,i} \otimes y&amp;rsquo;&lt;em&gt;i$，从而他们可以局部计算出$Z_j = \sum&lt;/em&gt;{i=1}^m (x_{j,i} \otimes y&amp;rsquo;_i)$的秘密共享值。他们可以以这种方式计算出所有的$Z$。&lt;/p&gt;
&lt;p&gt;表 2 比较了 Cheetah [27]、Iron [25] 和 CipherGPT 在 MatrixMul 操作中的开销。回顾一下，sVOLE 的复杂度几乎与 $n$ 无关。因此，我们的 MatrixMul 中的公钥操作数量也与 $n$ 无关。具体来说，我们需要传输 $2 \cdot e \cdot m \cdot k / N$ 个 RLWE 密文，并执行 $e \cdot m \cdot k / N$ 次密文-明文乘法来执行反向 VOLE，这两者都与 $n$ 无关。当我们将大量矩阵合并在一起时，即当 $n$ 很大时，我们的节省变得显著。此外，这些开销需要除以 $t$ 来计算摊销成本。&lt;/p&gt;
&lt;p&gt;在计算方面，我们节省了 $t \cdot n \cdot e$ 次密文-明文乘法。假设 $n = 256, m = 768, k = 64, t = 256$ 且 $e = 144$（这是 GPT-2 的真实参数），我们节省了 3,065 次密文-明文乘法，这大约需要超过 4 秒。尽管我们需要做额外的 $(c \cdot m \cdot n \cdot k)$ 次 AES 操作来扩展种子，但在 AES-NI 的帮助下，这可以在大约 100 毫秒内完成。&lt;/p&gt;
&lt;p&gt;在通信方面，我们至少节省了 94 个 RLWE 密文，相当于约 10MB，而 CipherGPT 中 OTs 和明文带来的通信开销仅约为 1.5MB。&lt;/p&gt;
&lt;h2 id=&#34;secure-gelu&#34;&gt;&lt;strong&gt;Secure GELU&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;在本节中，我们首先提供 GELU 协议的高级概述，然后深入探讨其技术细节。&lt;/p&gt;
&lt;h3 id=&#34;直觉&#34;&gt;&lt;strong&gt;直觉&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;图 1（左侧）展示了 $y = GELU(x)$ 的原始曲线。对于小的 $x$ 值，曲线从零开始，当 $x$ 接近 $-\alpha$ 时开始偏离零。随着 $x$ 进一步增加，$GELU(x)$ 逐渐近似于线性函数 $y = x$。基于这一观察，我们将曲线分为三个大区间：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当 $x &amp;lt; -\alpha$ 时，$y = 0$；&lt;/li&gt;
&lt;li&gt;当 $-\alpha \leq x \leq \alpha$ 时，$y = GELU(x)$；&lt;/li&gt;
&lt;li&gt;当 $x &amp;gt; \alpha$ 时，$y = x$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第一个和第三个区间的计算是直接的。对于第二个区间，我们使用多项式样条来近似曲线。如图 1（中间）所示，我们将第二个区间划分为多个小区间，并使用线性函数（$y = ax + d$）在每个小区间内近似曲线。详细的线性函数求解过程可以参考 [30] 中的第 5.3.2 节。值得注意的是，这种近似不需要对模型的训练阶段做任何修改。&lt;/p&gt;
&lt;p&gt;我们可以使用查找表（LUT）来找到 $x$ 所在的小区间，并以秘密共享的方式计算对应的线性函数。然而，对于区间 $[-\alpha, \alpha]$，我们首先需要确定 $x$ 的符号，然后分别在区间 $[-\alpha, 0]$ 和 $[0, \alpha]$ 中查找。为了避免这种情况，我们将整个曲线右移 $\alpha$，如图 1（右侧）所示，这样第二个区间变为 $[0, 2\alpha]$，使得我们可以执行一次查找。&lt;/p&gt;
&lt;h3 id=&#34;详细说明&#34;&gt;&lt;strong&gt;详细说明&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;算法 1 详细描述了如何安全地计算 $y := GELU(x)$。注意，模型的初始输入已经经过了 $L$ 位的左移，这会影响 $x$ 的值，从而导致 $x$ 本身也发生 $L$ 位的左移。为了保持所需的对齐方式，我们将 $\alpha$ 放大 $2^L$ 倍（第 1 行）。然后，分割值变为 $\alpha&amp;rsquo; := 2^L \alpha$。曲线的右移也需要考虑缩放因子。也就是说，我们不直接将曲线右移 $\alpha$，而是应该将其右移 $\alpha&amp;rsquo;$。类似地，为了确保正确对齐，输入 GELU 的值应该调整为 $x&amp;rsquo; := x + \alpha&amp;rsquo;$，这可以通过将 $\alpha&amp;rsquo;$ 添加到 $x$ 的任何一个分享值来实现（第 2 行）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;处理小区间&lt;/strong&gt;
令 $\beta := 2\alpha&amp;rsquo;$，现在第二个大区间变为 $[0, \beta]$。我们假设 $x&amp;rsquo;$ 落在这个大区间内；稍后我们会处理这一假设不成立的情况。由于 $x&amp;rsquo; \in [0, \beta]$，我们只需要考虑 $x&amp;rsquo;$ 的低 $h := \log \beta$ 位。为此，我们让 $S$ 和 $C$ 提取出 $\langle x&amp;rsquo; \rangle_l$ 的低 $h$ 位，并得到 $\langle x&amp;rsquo; \rangle_h$（第 5 行），这可以在本地完成，不需要通信。假设 $[0, \beta]$ 已经被划分为 $2^s$ 个小区间。那么，我们可以通过检查 $\langle x&amp;rsquo; \rangle_h$ 的高 $s$ 位来找到 $\langle x&amp;rsquo; \rangle_h$ 所在的区间。为此，我们让 $S$ 和 $C$ 在 $\langle x&amp;rsquo; \rangle_h$ 上运行截断再缩减协议（第 6 行），得到 $\langle i \rangle_s$，其中 $i \in Z_{2^s}$ 表示 $x&amp;rsquo;$ 所在的小区间的索引。$S$ 持有一个表 $T$，其中每个条目存储了对应小区间的线性函数系数。在获得 $i \in Z_{2^s}$ 后，$S$ 和 $C$ 执行 LUT 协议，以秘密共享的形式获得 $T$ 的第 $i$ 项（$\langle a_i \rangle_g, \langle d_i \rangle_l$）（第 7 行）。然后，他们在 $\langle a_i \rangle_g$ 和 $\langle x&amp;rsquo; \rangle_h$ 上运行“不同位宽的乘法”（第 8 行），得到 $\langle a x \rangle_l$，其中 $l = g + h$。最后，将 $\langle d_i \rangle_l$ 添加到 $\langle a x \rangle_l$，得到 $\langle z \rangle_l$，这可能就是 $GELU(x)$ 的结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;处理大区间&lt;/strong&gt;
注意，上述处理小区间的过程仅在 $x&amp;rsquo; \in [0, \beta]$ 时有效。实际上，第 5 行的截断操作会导致当 $x&amp;rsquo; \notin [0, \beta]$ 时丢失 $x&amp;rsquo;$ 的信息。为此，我们使用多路复用器确保当 $x&amp;rsquo; \notin [0, \beta]$ 时，$\langle z \rangle_l$ 不会被返回。$S$ 和 $C$ 首先安全地比较 $x&amp;rsquo;$ 与 $\beta$，并得到 $b$（第 10 行），如果 $x&amp;rsquo; \geq \beta$，则 $b = 1$，否则 $b = 0$。然后，他们安全地比较 $x&amp;rsquo;$ 与 0，并得到 $b&amp;rsquo;$（第 11 行），如果 $x&amp;rsquo; \geq 0$，则 $b&amp;rsquo; = 1$，否则 $b&amp;rsquo; = 0$。注意，$b$ 和 $b&amp;rsquo;$ 的组合只有以下三种可能性（而不是四种）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$b = 1$ 且 $b&amp;rsquo; = 1$&lt;/li&gt;
&lt;li&gt;$b = 0$ 且 $b&amp;rsquo; = 1$&lt;/li&gt;
&lt;li&gt;$b = 0$ 且 $b&amp;rsquo; = 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中第二种情况 $b \oplus b&amp;rsquo; = 1$ 表示 $x&amp;rsquo; \in [0, \beta]$，而其他两种情况 $b \oplus b&amp;rsquo; = 0$ 表示 $x&amp;rsquo; \notin [0, \beta]$。因此，我们可以使用 $b \oplus b&amp;rsquo;$ 作为控制信号来实现多路复用器对 $z$ 的处理。具体而言，$S$ 和 $C$ 运行多路复用器，输入为 $\langle z \rangle_l$ 和 $\langle b \rangle_1 \oplus \langle b&amp;rsquo; \rangle_1$，结果为 $\langle u \rangle_l$（第 12 行），其中当 $b \oplus b&amp;rsquo; = 1$ 时，$u = z$，否则 $u = 0$。&lt;/p&gt;
&lt;p&gt;接下来，$S$ 和 $C$ 运行另一个多路复用器，输入为 $\langle x \rangle_l$ 和 $\langle b \rangle_1$，结果为 $\langle v \rangle_l$（第 13 行），其中当 $b = 1$ 时，$v = x$，否则 $v = 0$。该多路复用器决定是否 $x&amp;rsquo; &amp;gt; \beta$；如果是，返回 $v = x&amp;rsquo;$。GELU(x) 的最终结果为 $\langle y \rangle_l := \langle u \rangle_l + \langle v \rangle_l$。注意，无需额外的多路复用器来处理 $x&amp;rsquo; &amp;lt; 0$ 的情况，因为当 $x&amp;rsquo; &amp;lt; 0$ 时，$y = 0$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;表 3&lt;/strong&gt; 比较了 SIRNN [36]、Iron [25] 和我们针对安全 GELU 的解决方案之间的加密操作数量。显然，我们的解决方案更加轻量。此外，我们的解决方案在精度上也更优：SIRNN 和 Iron 中的多步过程涉及分别逼近指数运算和倒数运算，在每一步引入了精度误差；这些误差在整个过程中积累，导致最终的大误差，而我们的方法通过单步处理避免了这个问题。我们的实验结果（参见表 4）验证了这一猜想。&lt;/p&gt;
&lt;h2 id=&#34;安全的-top-k-选择&#34;&gt;安全的 Top-K 选择
&lt;/h2&gt;&lt;p&gt;在 vec2word 层，GPT 模型生成一个包含所有可能单词概率的向量。从这个向量中，需要选择出前 K 个最大的概率，并根据这些选择的概率来采样最终的响应单词。本节将重点讨论如何从长度为 $n$ 的向量中选择前 K 个最大值。在后续部分，我们将讨论如何从这 K 个选定的概率中采样一个值。&lt;strong&gt;算法 2&lt;/strong&gt; 提供了我们 Top-K 协议的详细描述。总体来说，输入元素首先进行安全洗牌（第 1 行）；然后使用基于比较的选择方法，从洗牌后的列表中识别出前 K 个元素（第 2 行）。&lt;strong&gt;算法 2&lt;/strong&gt; 中的选择函数是递归执行的。在每次递归中，向量的最后一个元素作为基准元素进行选择（第 5 行）；然后将向量分为两部分：小于基准元素的部分，记为 SL，大于或等于基准元素的部分，记为 SR（第 6 行到第 15 行）。为了划分向量，所有元素都与基准元素进行比较（第 8 行）。比较结果可以被揭示（第 9 行），而不会泄露原始元素的隐私。这是因为原始元素已经被洗牌，从而比较结果与实际值无关。如果 SR 的大小（记为 $K&amp;rsquo;$）恰好等于 K，意味着 SR 中的所有元素都是我们需要选择的前 K 个最大元素（第 19 行）。如果 $K&amp;rsquo; &amp;gt; K$，则在 SR 上执行下一次递归，进一步缩小选择范围（第 21 行）。另一方面，如果 $K&amp;rsquo; &amp;lt; K$，则在 SL 上执行下一次递归，选择前 $(K - K&amp;rsquo;)$ 个元素，然后将它们与 SR 合并，得到最终的前 K 个元素（第 21 行）。值得注意的是，只有 CMP（第 8 行）需要 $S$ 和 $C$ 之间的交互；算法的其余步骤可以由每一方在本地执行，无需任何交互。选择函数需要 $O(n)$ 次 CMP。&lt;/p&gt;
&lt;h2 id=&#34;安全的采样&#34;&gt;&lt;strong&gt;安全的采样&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;在本节中，我们详细解释了我们的安全抽样协议。该协议以$K$个秘密共享的概率$(p_1, \dots, p_K)$为输入，其中每个概率通过将其乘以$2^L$并去掉小数部分来缩放为整数$x_i$。协议的输出是一个秘密共享的索引$j$，满足：$Pr(j = i) = \frac{x_i}{\sum_{k=1}^K x_k}$。我们将在第7.5节解释如何将该索引映射到响应词。&lt;/p&gt;
&lt;p&gt;算法3提供了安全抽样协议的详细描述。该协议基于以下观察：对于随机$p&amp;rsquo; \in [0, 1]$，选定的索引$j$满足：$\sum_{k=1}^{j-1} p_k \leq p&amp;rsquo; &amp;lt; \sum_{k=1}^{j} p_k$。由于$(p_1, \dots, p_K)$已经被缩放为$2^L$，因此$p&amp;rsquo;$也应该相应地缩放。为此，我们让$C$从$0$到$2^L - 1$中抽样一个整数$v$（第1行）。$S$和$C$安全地比较$v$与每个$\sum_{k=1}^i x_k, \ \forall i \in [1, K]$（第2-6行），结果是一个秘密共享的比特向量$\langle b \rangle$，满足：$b_i = 1, \ \forall 1 \leq i &amp;lt; j \quad \text{且} \quad b_i = 0, \ \forall j \leq i \leq K$。&lt;/p&gt;
&lt;p&gt;我们的下一步是构建另一个秘密共享的比特向量$\langle b&amp;rsquo; \rangle$，满足：$b&amp;rsquo;_i = 0, \ \forall i \neq j \quad \text{且} \quad b&amp;rsquo;&lt;em&gt;j = 1$。这可以通过对$\langle b \rangle$中的每对相邻比特执行异或操作来实现（第7-10行）。然后，所需的索引为：$\langle j \rangle := \sum&lt;/em&gt;{i=1}^K \text{FMUX}(i, \langle b&amp;rsquo;_i \rangle_1)$（第11行）。&lt;/p&gt;
&lt;p&gt;我们需要注意的是，由于最终的输出$j$对$C$是未知的，因此$v$由$C$单独抽样是可以接受的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/ciphergpt%E5%AE%89%E5%85%A8%E7%9A%84%E4%B8%A4%E6%96%B9-gpt-%E6%8E%A8%E7%90%86/image-20241215212331083.png&#34;
	width=&#34;685&#34;
	height=&#34;620&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/ciphergpt%E5%AE%89%E5%85%A8%E7%9A%84%E4%B8%A4%E6%96%B9-gpt-%E6%8E%A8%E7%90%86/image-20241215212331083_hu2762029947057730686.png 480w, https://JiangZhiyu-1024.github.io/p/ciphergpt%E5%AE%89%E5%85%A8%E7%9A%84%E4%B8%A4%E6%96%B9-gpt-%E6%8E%A8%E7%90%86/image-20241215212331083_hu6571275859154383001.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241215212331083&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;110&#34;
		data-flex-basis=&#34;265px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;ciphergpt-框架&#34;&gt;&lt;strong&gt;CipherGPT 框架&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/ciphergpt%E5%AE%89%E5%85%A8%E7%9A%84%E4%B8%A4%E6%96%B9-gpt-%E6%8E%A8%E7%90%86/image-20241209192050004.png&#34;
	width=&#34;539&#34;
	height=&#34;759&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/ciphergpt%E5%AE%89%E5%85%A8%E7%9A%84%E4%B8%A4%E6%96%B9-gpt-%E6%8E%A8%E7%90%86/image-20241209192050004_hu9764489677226742952.png 480w, https://JiangZhiyu-1024.github.io/p/ciphergpt%E5%AE%89%E5%85%A8%E7%9A%84%E4%B8%A4%E6%96%B9-gpt-%E6%8E%A8%E7%90%86/image-20241209192050004_hu1828554940819418041.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241209192050004&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;71&#34;
		data-flex-basis=&#34;170px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;图 2 展示了 GPT 的架构和工作流程。大致来说，它接收一系列单词，将它们编码成词嵌入，并通过多次迭代的 Transformer 解码器进行处理。每次迭代包括一个自注意力层和一个前馈神经网络。Transformer 解码器的输出被传入一个 &lt;code&gt;vec2word&lt;/code&gt; 层，生成预测的响应单词。接下来，我们将详细解释如何安全地计算这个过程。&lt;/p&gt;
&lt;h3 id=&#34;嵌入&#34;&gt;&lt;strong&gt;嵌入&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;首先，它将每个输入单词映射为一个长度为 $m$ 的数值向量，称为词嵌入，这通过在嵌入矩阵中找到对应的行来实现。接下来，每个词嵌入都通过位置嵌入进行增强，位置嵌入由单词在输入序列中的位置决定。位置嵌入是预定义的，并且逐元素地与词嵌入相加。我们使用加法同态加密（AHE）来完成词嵌入和位置嵌入的处理：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;S&lt;/strong&gt; 使用 AHE 对嵌入矩阵的每一行进行加密，并将所有生成的密文传输给 &lt;strong&gt;C&lt;/strong&gt;。在实践中，我们通过将每一行表示为 RLWE 密文的多项式系数来加密整行。注意，词嵌入是浮动的；&lt;strong&gt;S&lt;/strong&gt; 将它们通过左移 $L$ 位并去掉小数部分转换为整数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;C&lt;/strong&gt; 根据输入单词定位相应的密文，并为每个密文添加一个随机数 $r_i$：$E(w_1 + r_1), \dots, E(w_n + r_n)$，然后将其返回给 &lt;strong&gt;S&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;S&lt;/strong&gt; 解密密文，得到 $w_1 + r_1, \dots, w_n + r_n$，然后加上位置嵌入：$w_1 + r_1 + p_1, \dots, w_n + r_n + p_n$。&lt;/li&gt;
&lt;li&gt;现在，每个嵌入都被秘密共享，其中 $\langle x_i \rangle_C = -r_i$ 和 $\langle x_i \rangle_S = w_i + r_i + p_i$。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们指出，步骤 1 只需要执行一次，除非嵌入矩阵发生变化，否则可以无限次使用。&lt;/p&gt;
&lt;h3 id=&#34;层归一化-layer-normalization&#34;&gt;&lt;strong&gt;层归一化 (Layer Normalization)&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;在输入编码后，$n$ 个输入单词变成了一个秘密共享矩阵 $\langle X \rangle$，其中 $X \in \mathbb{Z}^{n \times m}_{2^l}$。接下来，需要对每一行 $x \in \mathbb{Z}_2^l$ 执行层归一化 (LayerNorm)。具体来说，$x$ 中的每个元素 $x_i$ 进行如下归一化处理：&lt;/p&gt;
&lt;p&gt;$x_i := \frac{x_i - E[x]}{\sqrt{Var[x] + \epsilon}} \cdot \gamma + \beta,$&lt;/p&gt;
&lt;p&gt;其中 $E[x] = \frac{1}{n} \sum x_i$，$Var[x] = \frac{1}{n-1} \sum (x_i - E[x])^2$，$\gamma$ 和 $\beta$ 是可学习的参数，$\epsilon$ 是一个小值，用于避免除以零。&lt;/p&gt;
&lt;p&gt;为了安全地计算 LayerNorm，我们让 &lt;strong&gt;S&lt;/strong&gt; 和 &lt;strong&gt;C&lt;/strong&gt; 执行以下操作：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;运行 FMult 计算每个 $vari := (x_i - E[x])^2$；&lt;/li&gt;
&lt;li&gt;运行 FLUT 计算 $\frac{1}{\sqrt{Var[x] + \epsilon}}$；&lt;/li&gt;
&lt;li&gt;运行 FMult 计算 $\frac{x_i - E[x]}{\sqrt{Var[x] + \epsilon}}$；&lt;/li&gt;
&lt;li&gt;运行 FMult 计算 $\frac{x_i - E[x]}{\sqrt{Var[x] + \epsilon}} \cdot \gamma$；&lt;/li&gt;
&lt;li&gt;运行 FTR 将结果缩减为 $L$ 位，并将宽度截断为 $l$ 位。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;原则上，&lt;strong&gt;S&lt;/strong&gt; 和 &lt;strong&gt;C&lt;/strong&gt; 需要使用“统一比特宽度的乘法”来乘以两个秘密共享的值，然后对每次乘法运行安全的截断（保持缩放为 $L$ 位）。然而，为了减少精度损失，我们让 &lt;strong&gt;S&lt;/strong&gt; 和 &lt;strong&gt;C&lt;/strong&gt; 使用“非统一比特宽度的乘法”（FMult）进行乘法计算，并在 LayerNorm 计算完成后，仅执行一次“截断再缩减”（FTR）。&lt;/p&gt;
&lt;h3 id=&#34;掩码自注意力-masked-self-attention&#34;&gt;&lt;strong&gt;掩码自注意力 (Masked Self-Attention)&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;自注意力是一种机制，通过将序列中的不同位置联系起来，能够计算出序列的表示 [43]。计算自注意力的第一步是创建三个矩阵：查询矩阵 $Q$、键矩阵 $K$ 和值矩阵 $V$。这是通过将归一化的嵌入矩阵 $X \in \mathbb{Z}^{n \times m}&lt;em&gt;{2^l}$ 与三个训练过程中获得的矩阵（$W_Q \in \mathbb{Z}^{m \times m}&lt;/em&gt;{2^l}$，$W_K \in \mathbb{Z}^{m \times m}&lt;em&gt;{2^l}$ 和 $W_V \in \mathbb{Z}^{m \times m}&lt;/em&gt;{2^l}$）相乘来实现的：&lt;/p&gt;
&lt;p&gt;$\langle Q \rangle := \langle X \rangle \langle W_Q \rangle; \quad \langle K \rangle := \langle X \rangle \langle W_K \rangle; \quad \langle V \rangle := \langle X \rangle \langle W_V \rangle.$&lt;/p&gt;
&lt;p&gt;由于 $W_Q$、$W_K$ 和 $W_V$ 是预先知道的，这样的矩阵乘法可以通过我们在第 3 节中描述的基于 sVOLE 的方案来计算。矩阵乘法后，&lt;strong&gt;S&lt;/strong&gt; 和 &lt;strong&gt;C&lt;/strong&gt; 需要运行 FTrunc 以确保缩放保持在 $L$ 位。为了简化起见，以下部分我们省略了关于截断的描述。&lt;/p&gt;
&lt;h3 id=&#34;多头注意力-multi-headed-attention&#34;&gt;多头注意力 (Multi-headed Attention)
&lt;/h3&gt;&lt;p&gt;接下来，$\langle Q \rangle$、$\langle K \rangle$ 和 $\langle V \rangle$ 将被划分为 $M$ 个部分，称为多头注意力，其中 $M$ 代表注意力头的数量。令 $m&amp;rsquo; = \frac{m}{M}$，我们有：&lt;/p&gt;
&lt;p&gt;$ \langle q_1 \rangle \parallel \cdots \parallel \langle q_M \rangle = \langle Q \rangle, \quad \text{每个} , q_i \in \mathbb{Z}^{n \times m&amp;rsquo;}_{2^l};$&lt;/p&gt;
&lt;p&gt;$\langle k_1 \rangle \parallel \cdots \parallel \langle k_M \rangle = \langle K \rangle, \quad \text{每个} , k_i \in \mathbb{Z}^{n \times m&amp;rsquo;}_{2^l};$&lt;/p&gt;
&lt;p&gt;$\langle v_1 \rangle \parallel \cdots \parallel \langle v_M \rangle = \langle V \rangle, \quad \text{每个} , v_i \in \mathbb{Z}^{n \times m&amp;rsquo;}_{2^l}.$&lt;/p&gt;
&lt;p&gt;接着，通过查询矩阵和键矩阵的乘积计算得分矩阵：&lt;/p&gt;
&lt;p&gt;$\langle s_i \rangle := \langle q_i \rangle \cdot \langle k_i \rangle^T \quad \forall i \in [M].$&lt;/p&gt;
&lt;p&gt;每个 $s_i \in \mathbb{Z}^{n \times n}_{2^l}$ 中的得分决定了在编码当前单词时，应该关注其他单词的程度。在这种情况下，由于查询矩阵 $q_i$ 和键矩阵 $k_i$ 事先都不可知，我们的基于 sVOLE 的矩阵乘法无法应用。相反，我们采用了 [25] 中提出的基于 AHE 的矩阵乘法。&lt;/p&gt;
&lt;h3 id=&#34;自注意力掩码-self-attention-masking&#34;&gt;自注意力掩码 (Self-attention Masking)
&lt;/h3&gt;&lt;p&gt;在多头注意力之后，应用自注意力掩码将每个 $s_i$ 的上三角部分置为零。这样，左侧的每个单词会比右侧的单词获得更高的注意力分数，因此模型在实际应用中只关注前面的单词。这个步骤可以由 &lt;strong&gt;S&lt;/strong&gt; 和 &lt;strong&gt;C&lt;/strong&gt; 本地执行，无需任何交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Softmax.&lt;/strong&gt;
对每个 $\langle s_i \rangle$ 的每一行应用 softmax 操作，确保该行的得分被归一化，所有值都为正，并且总和为 1。为了安全地计算 softmax，我们在 [25] 中的方法进行了优化，步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;给定一行 $x \in \mathbb{Z}^n_{2^l}$ 作为输入，我们首先对每个 $x_i$ 进行归一化：$x&amp;rsquo;_i := x_i - \max(x)$，并得到一个负值向量。&lt;/li&gt;
&lt;li&gt;我们通过仅考虑区间 $[-16, 0]$ 来优化负值的指数协议。具体而言，我们使用 FCMP 比较 $x&amp;rsquo;_i$ 与 $-16 \times 2^l$，并使用 FMUX 在 $x&amp;rsquo;_i &amp;lt; -16 \times 2^l$ 时将指数结果设为 0。原始的指数协议运行一个具有 $2^l$ 条目的 FLUT，而我们仅需要一个具有 $2^{L+4}$ 条目的 FLUT。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Output.&lt;/strong&gt;
在自注意力的最后一步，softmax 后的得分用于加权值矩阵中的值：&lt;/p&gt;
&lt;p&gt;$\langle z_i \rangle := \langle s_i \rangle \langle v_i \rangle \quad \forall i \in [M],$&lt;/p&gt;
&lt;p&gt;这同样是通过基于 AHE 的矩阵乘法 [25] 实现的。然后，所有的 $\langle z_i \rangle$ 被重新组合在一起：&lt;/p&gt;
&lt;p&gt;$\langle Z \rangle := \langle z_1 \rangle \parallel \cdots \parallel \langle z_n \rangle.$&lt;/p&gt;
&lt;p&gt;自注意力的输出是：&lt;/p&gt;
&lt;p&gt;$\langle X \rangle := \langle X \rangle + \langle Z \rangle.$&lt;/p&gt;
&lt;h3 id=&#34;前馈网络&#34;&gt;&lt;strong&gt;前馈网络&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;自注意力的输出将经过一个 LayerNorm 操作。得到的归一化值随后被送入一个前馈神经网络，该网络由两个全连接（FC）层和一个激活层组成。第一个 FC 层的计算为：&lt;/p&gt;
&lt;p&gt;$\langle X_1 \rangle := \langle X \rangle \langle W_1 \rangle + B_1,$&lt;/p&gt;
&lt;p&gt;其中 $X \in \mathbb{Z}^{n \times m}&lt;em&gt;{2^l}$，$W_1 \in \mathbb{Z}^{m \times k}&lt;/em&gt;{2^l}$，$B_1 \in \mathbb{Z}^{n \times k}&lt;em&gt;{2^l}$，$X_1 \in \mathbb{Z}^{n \times k}&lt;/em&gt;{2^l}$。接着，对 $X_1$ 的每个元素应用 ΠGELU（参见第 4 节），得到 $X&amp;rsquo;_1$。第二个 FC 层的计算为：&lt;/p&gt;
&lt;p&gt;$\langle X_2 \rangle := \langle X&amp;rsquo;_1 \rangle \langle W_2 \rangle + B_2,$&lt;/p&gt;
&lt;p&gt;其中 $X&amp;rsquo;&lt;em&gt;1 \in \mathbb{Z}^{n \times k}&lt;/em&gt;{2^l}$，$W_2 \in \mathbb{Z}^{k \times m}&lt;em&gt;{2^l}$，$B_2 \in \mathbb{Z}^{n \times m}&lt;/em&gt;{2^l}$，$X_2 \in \mathbb{Z}^{n \times m}_{2^l}$。注意，$W_1$ 和 $W_2$ 是预先已知的，因此我们可以应用基于 sVOLE 的矩阵乘法（参见第 3 节）来计算这两个 FC 层的输出。输出将再次经过多个自注意力和前馈的迭代，每次迭代使用不同的权重，同时保持相同的结构。&lt;/p&gt;
&lt;h3 id=&#34;vec2word&#34;&gt;&lt;strong&gt;Vec2word&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;经过多次自注意力和前馈迭代后，最终的输出将通过一个 vec2word 层生成预测的响应词。vec2word 的初始操作涉及一个矩阵乘法（MatrixMul），用于生成所有可能词的 one-hot 编码：&lt;/p&gt;
&lt;p&gt;$\langle y_0 \rangle := \langle x \rangle \langle W \rangle,$&lt;/p&gt;
&lt;p&gt;其中 $W \in \mathbb{Z}^{m \times k}&lt;em&gt;{2^l}$，$y_0 \in \mathbb{Z}^{k}&lt;/em&gt;{2^l}$，$x \in \mathbb{Z}^{m}&lt;em&gt;{2^l}$ 是 $X \in \mathbb{Z}^{n \times m}&lt;/em&gt;{2^l}$ 的最后一行（由于 GPT 在推理时采用的优化）。此时，$k$ 代表所有可能词的数量，这个数量是非常大的。因此，我们的 sVOLE 基于的矩阵乘法不适用于此处，因此我们采用了基于 AHE 的矩阵乘法 [25]。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Top-K&lt;/strong&gt;
为了在多样性和准确性之间保持平衡，从 $y_0$ 中选择最大的 $K$ 个值：&lt;/p&gt;
&lt;p&gt;$\langle y_1 \rangle \leftarrow \Pi_{TopK}(\langle y_0 \rangle), \quad \text{其中} \quad y_1 \in \mathbb{Z}^{K}_{2^l}.$&lt;/p&gt;
&lt;p&gt;这是通过我们在第 5 节中描述的协议实现的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Temperature&lt;/strong&gt;
温度 $T$ 决定了 GPT 生成文本的创造性和多样性：较高的温度（例如，$T = 1.5$）生成更多样化和富有创造力的文本，而较低的温度（例如，$T = 0.5$）则生成更加集中的和确定性的文本。它是由 $S$ 持有的超参数，并与 $y_1$ 中的每个值相乘。这个过程可以通过 AHE 容易实现：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;C 向 S 发送其 AHE 加密的共享值 $E(\langle y_{1,1} \rangle_C), \dots, E(\langle y_{1,K} \rangle_C)$。实际上，我们通过将其表示为 RLWE 密文的多项式系数来一起加密它们。&lt;/li&gt;
&lt;li&gt;S 将其共享值加到密文中：$E(\langle y_{1,1} \rangle_C + \langle y_{1,1} \rangle_S), \dots, E(\langle y_{1,K} \rangle_C + \langle y_{1,K} \rangle_S)$。&lt;/li&gt;
&lt;li&gt;S 将所有密文乘以 $T$：$E(T \cdot y_{1,1}), \dots, E(T \cdot y_{1,K})$。&lt;/li&gt;
&lt;li&gt;S 向每个密文添加一个随机数 $r_i$：$E(T \cdot y_{1,1} + r_1), \dots, E(T \cdot y_{1,K} + r_K)$。&lt;/li&gt;
&lt;li&gt;S 将得到的密文返回给 C。&lt;/li&gt;
&lt;li&gt;S 解密密文，现在温度化后的值（用 $y_2$ 表示）被秘密共享：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$\langle y_{2,i} \rangle_C := T \cdot y_{1,i} + r_i \quad \text{和} \quad \langle y_{2,i} \rangle_S := -r_i, \quad \forall i \in [K].$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Random sampling.&lt;/strong&gt;
对 $y_2$ 进行 softmax 操作，得到一个概率向量 $y_3$，然后基于这个概率向量随机采样响应词。这样的随机采样确保生成的输出既多样化又与上下文相关。我们使用第 6 节中描述的安全采样协议来获取一个索引：&lt;/p&gt;
&lt;p&gt;$\langle j \rangle \leftarrow \Pi_{Sample}(y_3).$&lt;/p&gt;
&lt;p&gt;由于词向量是公开已知的，如果 C 获得了索引，它可以从词向量中检索最终的响应词。然而，直接将 $j$ 公开给 C 存在两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在算法 3（第 1 行）中，值 $v$ 是由 C 采样的。因此，如果 C 获得了 $j$，它可能会获取一些关于输入 $x$ 的信息。&lt;/li&gt;
&lt;li&gt;索引 $j$ 不是正确的索引，因为在算法 2（第 1 行）中，概率向量已经被打乱。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;回想一下，算法 2 中的打乱过程大致如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;C 生成一个随机排列 $\pi_C$；S 和 C 共同应用 $\pi_C$ 到输入向量，得到相应的秘密共享值。&lt;/li&gt;
&lt;li&gt;S 生成一个随机排列 $\pi_S$；S 和 C 共同应用 $\pi_S$ 到 $\pi_C$ 的输出，得到相应的秘密共享值。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一个关键观察是，算法 3 第 11 行中的 “i” 是公开的。为此，我们让 S 计算 $i&amp;rsquo; := \pi^{-1}_S(i)$ 并对 $i&amp;rsquo;$ 进行秘密共享。然后，我们将算法 3 第 11 行替换为：&lt;/p&gt;
&lt;p&gt;$\langle j \rangle := \sum_{i=1}^{K} FMUX(\langle i&amp;rsquo; \rangle, \langle b&amp;rsquo;_i \rangle_1).$&lt;/p&gt;
&lt;p&gt;现在，将 $j$ 公开给 C 不会泄露任何关于输入的信息，因为 $\pi^{-1}_S(i)$ 对 C 是未知的。在获得 $j$ 后，C 计算 $j&amp;rsquo; := \pi^{-1}_C(j)$，这是词向量中的正确索引。&lt;/p&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;在本节中，我们提供了 CipherGPT 的完整实现，并系统地评估其性能。&lt;/p&gt;
&lt;h3 id=&#34;实现&#34;&gt;&lt;strong&gt;实现&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;我们在 C++ 中完全实现了 CipherGPT，并设置了安全参数为 128。我们使用了 Microsoft SEAL 同态加密库（版本 4.0）进行 AHE，并使用 hexl5 加速 AXV-512 指令的 HE 操作。具体而言，我们使用 Brakerski-Fan-Vercauteren (BFV) [11], [18] 方案，设定 N = 4096，并使用 SEAL 中的默认参数以实现 128 位安全性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于安全 GELU，我们通过利用 SIRNN6 中的开源代码实现了 LUT、Mult、Trunc、CMP 和 MUX。&lt;/li&gt;
&lt;li&gt;对于基于 sVOLE 的 MatrixMul，我们实现了反向 VOLE 与 AHE，并将 Halftree [24], [23] 优化应用于 PPRF。我们还结合了 [47], [6] 中的优化，但遵循了 [29] 中的建议以防止已知攻击。&lt;/li&gt;
&lt;li&gt;对于 TopK，由于 [14] 中的秘密共享打乱算法未公开开源，我们自行实现了该算法。&lt;/li&gt;
&lt;li&gt;对于 Softmax 和基于 AHE 的 MatrixMul，由于 Iron [36] 未公开开源，我们分别通过利用 SIRNN 和 Cheetah7 中的开源代码重新实现了这些操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;实验设置&#34;&gt;实验设置
&lt;/h3&gt;&lt;p&gt;按照 SIRNN [36] 和 Iron [25] 的设置，我们使用了 LAN 网络环境，其中带宽为 377 MBps，RTT 为 0.8ms。所有实验均在 AWS c5.9xlarge 实例上执行，使用 3.6GHz 的 Intel Xeon 8000 系列 CPU，并采用单线程运行。所有结果均为 5 次实验的平均值，方差非常小。我们基准测试了 Radford [35] 提出的 GPT-2 模型，该模型包含 1.17 亿个参数，12 个 Transformer 解码器，嵌入大小为 768。按照 Cheetah [27] 和 CrypTFlow2 [37] 的做法，我们将浮点数左移 L = 12 位并丢弃小数部分。在推理过程中，我们使用 FTrunc 确保最大值小于 $2^l - 1$，其中 $l = 37$。&lt;/p&gt;
&lt;h3 id=&#34;评估结果&#34;&gt;&lt;strong&gt;评估结果&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;GELU 评估&lt;/strong&gt;
在评估我们的 GELU 协议（即算法 1）时，我们设置了 α = 4 和 s = 8。考虑到 L = 12，实际需要逼近的区间为 $-4 \times 2^{12}$ 到 $4 \times 2^{12}$。具体来说，我们将区间 $-4 \times 2^{12}$ 到 $4 \times 2^{12}$ 划分为 28 个小区间，并使用 256 节样条（spline）逼近该区间内的曲线。
表 4 显示了 Iron 和我们方案在 GELU 上的对比。为了计算一个 3072 长度向量中每个 37 位元素的 GELU，我们的协议需要 694 毫秒和 13.1MB 的带宽。与 Iron [25] 相比，我们的方案在运行时速度上提高了 4.1 倍，并且通信量减少了 3.3 倍。我们通过测试其 ULP（单位最后一位）误差来评估我们逼近的精度，ULP 误差定义为准确的真实结果 y 与逼近结果 $\tilde{y}$ 之间可表示数字的数量 [21]。由于我们已经将浮点数缩放为整数，因此 ULP 误差正是 $|y - \tilde{y}|$。按照 SIRNN [36] 的方法，我们使用穷举测试来评估 ULP 误差：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在 $-16 \times 2^{12}$ 到 $16 \times 2^{12}$ 范围内对所有可能的整数运行安全 GELU 协议，&lt;/li&gt;
&lt;li&gt;比较每个输出与相应的无限精度真实结果之间的 ULP 误差，&lt;/li&gt;
&lt;li&gt;报告最大 ULP 误差和平均 ULP 误差。
结果（见表 4）表明，我们的方案引入的 ULP 误差比 Iron 小得多。Iron 中的多步骤过程（流式 SIRNN）涉及分别近似指数运算和倒数运算，在每个步骤中引入 ULP 误差。这些误差在过程中积累，导致与我们单步方法相比，整体误差更大。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;MatrixMul 评估&lt;/strong&gt;
回顾一下，我们的基于 sVOLE 的 MatrixMul 适用于两个矩阵尺寸不平衡的情况。因此，我们衡量了执行 $Z_{256 \times 768}^{237} \times Z_{768 \times 768}^{237}$ 的摊销成本，其中 Z 的 $768 \times 768$ 矩阵在所有迭代中保持不变。虽然 t 的大小对其他协议可能没有影响，但对于我们的方法来说，t 的大小很重要，因为我们可以一起预处理所有 t 次迭代。我们承认这种比较可能被认为不公平，但它准确地反映了 GPT 推理的设置。图 3 显示了 Iron 和我们协议的对比（在该图中我们未区分预处理时间和在线时间）。考虑到 ChatGPT 在一次回应中通常生成几百个字，t = 256 是一个合理的迭代次数。当响应单词的数量增加到 1024 时，我们的协议展现出更大的性能优势。具体来说，在运行时，我们的协议比 Iron 快 2.5 倍，在通信量上减少了 11.2 倍。
我们协议的摊销运行时为 1,606 毫秒，比 Iron 提高了 2.1 倍；我们的协议的摊销通信量为 8.2MB，比 Iron 减少了 3.8 倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TopK 评估&lt;/strong&gt;
我们基准测试了我们的 TopK 协议（见算法 2），用于从 $Z_{50257}^{237}$ 向量中选择 100 个元素。该操作需要 3,281 毫秒和 136.1MB 的带宽。与常用的 Bitonic 排序网络 [26] 相比，我们的方案在运行时提高了 8.8 倍，并且通信量减少了 14.8 倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CipherGPT 评估&lt;/strong&gt;
我们运行 CipherGPT 生成一个由 256 个响应单词组成的句子。表 5 列出了每个操作的摊销运行时间和通信量，并给出了它们各自的比例。在计算方面，GELU、LayerNorm、MatrixMul、Softmax 和 Trunc 分别占据了 26.68%、21.81%、19.4%、16.49% 和 15.25% 的运行时间。在通信方面，GELU、LayerNorm、Trunc、Softmax 和 MatrixMul 分别占据了 39.98%、26.54%、15.88%、14.83% 和 2.55% 的带宽。&lt;/p&gt;
&lt;p&gt;我们还测量了 CipherGPT 引入的准确性损失。我们从 WikiText-103 数据集 [2] 中随机选择了 1,000 个句子，并使用表 5 所示的配置运行了 CipherGPT。然后，我们将 CipherGPT 的输出与 GPT-original（即原始 GPT 模型，使用浮点数且没有任何截断或近似）生成的输出进行了比较。为了消除 Top-K 采样的干扰，我们为 CipherGPT 和 GPT-original 都设置了 K = 1。评估结果显示，CipherGPT 生成的 99.0% 输出与 GPT-original 生成的输出完全相同。即使是不同的输出，每个“错误”的输出仍然位于运行 GPT-original 生成的相应句子的前五个输出中。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;&lt;strong&gt;讨论&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;我们的基准测试（表 5）显示，CipherGPT 生成一个 token 需要 24 分钟的延迟和 93GB 的带宽。当前这种成本水平是不可行的，可以预见，利用现有的加密工具实现实际应用将面临挑战。然而，随着计算和网络技术的不断进步，以及新应用场景的出现，未来实现安全 GPT 推理的实际应用仍然充满潜力。我们当前实现中使用单线程，意味着通过利用并行计算技术（如 GPU 或 FPGA 加速）仍有显著的加速空间。我们甚至可以探索新型计算架构，如内存计算 [42] 和存储计算 [41]。&lt;/p&gt;
&lt;p&gt;IEEE 标准下的 100 Gigabit Ethernet [1] 的出现为带宽容量提供了大幅提升。一旦部署，这种更高的带宽将有效解决与安全 GPT 推理相关的带宽问题。&lt;/p&gt;
&lt;p&gt;ChatGPT 被设计为实时生成响应，提供快速和互动的对话体验。然而，安全 GPT 推理显著增加了延迟，这在需要实时响应的场景中部署时会带来挑战。另一方面，仍然存在一些实时响应不那么关键的场景，在这些场景中，安全 GPT 推理可以找到有价值的应用。例如，考虑一个机构拥有一套能够有效评估 LLM（大语言模型）性能的提示词集，而模型拥有者希望评估其模型的能力并从该机构获得评分。在这种情况下，保护机构提示词的机密性至关重要，以防止模型拥有者通过在这些提示词上微调他们的模型而获得不公平的优势。同时，保护模型本身的机密性也是一个关键问题，因为该模型被视为其所有者的宝贵专有资产。为此，我们可以使用安全 GPT 推理来保护模型和提示词的机密性；在这种情况下，其较长的延迟是可以接受的。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;&lt;strong&gt;相关工作&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;安全推理可以通过通用的安全双方计算（2PC）[49]，[22] 或全同态加密（FHE）[19] 实现。然而，这些解决方案通常会表现出较高的通信和计算成本。因此，有必要为安全推理开发定制化协议。该领域的工作可以追溯到2010年代初期 [33]，[7]，[46]，许多早期的研究主要关注于简单的机器学习算法，如支持向量机（SVM）和线性回归。CryptoNets [20] 被认为是安全神经网络推理的初步尝试。它仅依赖于FHE，这限制了其在层数较少的神经网络中的应用。此外，它仅支持线性操作和低阶多项式。MiniONN [30] 是首个为安全神经网络推理定制2PC协议的工作。它提出了基于样条的非线性操作近似，这启发了我们对安全GELU的解决方案。GAZELLE [28] 通过将线性层映射到基于SIMD的矩阵-向量乘法和卷积例程，减少了线性层的成本。Cheetah [27] 用系数打包替代SIMD，从而消除了昂贵的旋转操作。Iron [25] 进一步减少了Cheetah的通信复杂度。在激活函数方面，CrypTFlow2 [37] 提出了高效的安全比较和除法协议。SIRNN [36] 提供了针对数学函数（如指数、sigmoid、tanh 和倒数平方根）的加密友好近似，以及相应的2PC实现。&lt;/p&gt;
&lt;p&gt;提高安全推理性能的另一个研究方向是改变模型结构，采用更符合加密要求的结构。例如，DeepSecure [40]、XONN [38] 和 Quotient [3] 专门为二值化神经网络 [16] 设计。DeepSecure 还通过剪枝减少了激活数量。Delphi [31] 提供了一种规划器，通过神经架构搜索自动生成神经网络架构配置，以在性能与精度之间进行权衡。然而，所有这些解决方案都需要重新训练模型，这对于机器学习实践者来说并不理想。一些解决方案 [32]，[45] 利用GPU并行性加速在线阶段，但它们无法对预处理阶段做出任何改进，因为加密操作主导了这些协议中的预处理阶段。最有效的基于GPU的解决方案，即 GForce [32]，对于VGG-16（在CIFAR-10和CIFAR-100上训练）进行一次推理总共需要 14-15 分钟。&lt;/p&gt;
&lt;p&gt;到目前为止的讨论主要集中在双方协议上，因为我们认为安全推理自然适合这种设置。然而，还有一些其他工作 [39]，[44]，[5] 目标是三方设置，其中模型在两个不合作的服务器之间进行秘密共享，客户端与这些服务器交互以获取预测结果。三方协议通常比双方协议更高效，但非合作服务器的假设在实践中常常被认为不现实。&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;&lt;strong&gt;结论&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;针对ChatGPT引发的隐私问题，我们开发了CipherGPT，这是第一个用于安全GPT推理的框架。它包括一系列创新协议，包括为GPT推理定制的安全矩阵乘法协议、一种用于安全计算GELU的新协议，以及第一个用于Top-K采样的协议。我们为CipherGPT提供了一个全面的基准，这可以作为未来该领域研究的参考。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>BumbleBee 大型变换器模型的安全双方推理框架</title>
        <link>https://JiangZhiyu-1024.github.io/p/bumblebee-%E5%A4%A7%E5%9E%8B%E5%8F%98%E6%8D%A2%E5%99%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%89%E5%85%A8%E5%8F%8C%E6%96%B9%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/</link>
        <pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/bumblebee-%E5%A4%A7%E5%9E%8B%E5%8F%98%E6%8D%A2%E5%99%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%89%E5%85%A8%E5%8F%8C%E6%96%B9%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/bumblebee-%E5%A4%A7%E5%9E%8B%E5%8F%98%E6%8D%A2%E5%99%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%89%E5%85%A8%E5%8F%8C%E6%96%B9%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/cottage-2955582_1280.jpg" alt="Featured image of post BumbleBee 大型变换器模型的安全双方推理框架" /&gt;&lt;h1 id=&#34;bumblebee-大型变换器模型的安全双方推理框架&#34;&gt;&lt;strong&gt;BumbleBee: 大型变换器模型的安全双方推理框架&lt;/strong&gt;
&lt;/h1&gt;&lt;h2 id=&#34;摘要&#34;&gt;&lt;strong&gt;摘要&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;基于大型变换器模型的技术已在自然语言处理、计算机视觉等许多实际任务中实现了最先进的性能。然而，随着它们处理的数据和任务敏感性增加，隐私问题在模型部署过程中成为了一个主要关注点。在这项工作中，我们关注于在两方设置下进行私密推理的场景，其中一方持有私密输入，另一方持有模型。我们介绍了BumbleBee，一个快速且适合通信的双方私密变换器推理系统。我们的贡献主要有三点：首先，我们提出了优化的矩阵乘法协议，相较于以往技术，显著降低了80%-90%的通信成本。其次，我们开发了一种针对变换器模型中非线性激活函数的高效协议构建方法。所提出的激活协议在处理速度上实现了显著提升，同时与之前的两种方法相比，通信成本降低了80%-95%。最后，我们对五个变换器模型进行了广泛的基准测试。BumbleBee通过评估LLaMA-7B模型展示了其能力，使用CPU生成一个token大约需要14分钟。我们的结果进一步表明，BumbleBee在性能上超越了Iron（NeurIPS22）一个数量级，并且比BOLT（Oakland24）快三倍，同时通信成本仅为BOLT的十分之一。&lt;/p&gt;
&lt;h2 id=&#34;引言&#34;&gt;&lt;strong&gt;引言&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;基于大型变换器模型，如BERT [19]、GPT [60] 和 ViT [22]，在许多实际任务中实现了最先进（SOTA）的性能，包括人员重识别 [47]、语音助手 [14] 和代码自动补全 [75]。随着变换器模型处理的数据和任务越来越敏感，隐私问题已成为模型部署中的主要关注点之一。私密推理旨在保护模型权重免受用户的干扰，同时确保服务器无法获取用户私密输入的信息。许多近期的研究引入了基于安全多方计算（MPC）[8]、[27]的加密框架，以实现深度学习模型（如卷积神经网络（CNN）[36]、[2]、[41]、[62]）和变换器模型的私密推理[48]、[33]、[79]、[73]。尽管安全双方计算（2PC）可以高效地在几分钟内推理CNN，但在变换器模型上的私密推理则带来了新的挑战，特别是在通信开销方面。例如，[33]、[30]等研究表明，对12层BERT模型的单次私密推理可能需要高达90 GB的通信。此外，[73，表1]报告称，12层ViT模型的单次私密推理可能需要交换约262 GB的消息。因此，这些通信密集型方法对高带宽的需求不可或缺。我们总结了开发高效、通信友好的2PC框架，用于大规模变换器私密评估的两个主要挑战。&lt;/p&gt;
&lt;h4 id=&#34;大规模矩阵的乘法&#34;&gt;&lt;strong&gt;大规模矩阵的乘法&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;变换器模型的推理可能涉及数百次大矩阵的乘法。例如，自然语言处理（NLP）变换器使用嵌入表将一组单词查询转换为数值表示。嵌入表查找可以定义为矩阵乘法 $EV$，其中 $E$ 的每一行是一个对应于输入查询中每个单词索引的独热向量。换句话说，这种乘法的维度可以解析为：单词数 × 词汇表大小 × 嵌入大小，这比卷积神经网络（CNN）中的矩阵要大得多。由于相对于NLP变换器而言，视觉变换器通常具有更大的嵌入大小，因此也可能涉及较大的矩阵。现有的大多数加密协议用于私密矩阵乘法，都依赖于“不可知转移”（Oblivious Transfer，OT）[18]、[57]、[59] 或同态加密（Homomorphic Encryption，HE）[41]、[12]、[36]、[35]。然而，这两种方法各有其局限性。基于OT的私密矩阵乘法方法计算时间较少，但需要传输大量消息；另一方面，基于HE的方法计算量显著增加，但比OT方法更加适合通信。第一个挑战是开发一种既快速又通信友好的矩阵乘法协议。&lt;/p&gt;
&lt;h4 id=&#34;更复杂的激活函数&#34;&gt;&lt;strong&gt;更复杂的激活函数&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;与CNN中使用的简单ReLU激活函数不同，变换器模型包含复杂的激活函数，如softmax、Gaussian Error Linear Unit（GeLU）和Sigmoid Linear Unit（SiLU）。这些激活函数的计算需要基本的数学函数，如指数运算、除法和双曲函数。尽管研究人员已经为这些基本函数开发了特定的协议[11]、[61]、[62]、[44]，但直接在变换器模型中使用这些协议仍然不切实际。主要原因是变换器模型中的激活数量非常庞大。例如，GPT2模型[15]的单次推理需要计算大约 3.9 × 10⁶ 次点对点的GeLU激活。我们的第二个挑战是为这些复杂的激活函数设计高效的2PC协议。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A. 技术细节&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;高效的线性函数协议&lt;/strong&gt;
我们提出了一种叫做“不可知线性变换”（Oblivious Linear Transformation，OLT）的原语。我们将OLT描述为一个双方协议，其中两方分别提供私密矩阵 $Q \in \mathbb{Z}_2^{* 2l}$ 和 $V \in \mathbb{Z}_2^{* 2l}$，并生成它们之间的共享 $QV$。使用OLT，我们可以在模 $2l$ 的环上实现两个加法共享矩阵的乘法。基于同态加密（HE）的方法[36]、[54]提供了一个很好的起点。这些方法的一个重要局限性是显著的通信开销，这来自于输出密文的“稀疏”格式。更具体地说，[36]、[54]中的每个输出密文加密了一个长向量。然而，乘法结果只需要该向量中的一小部分元素。为了进行解密，仍然需要传输整个加密向量。为了克服这一不足，我们提出了一种压缩过程，通过同态方式将加密向量中不必要的条目归零，从而将多个“稀疏”向量合并为一个“密集”向量。因此，通信开销减少，因为需要发送的密文数量变少。与之前的密文压缩方法[13]相比，我们的压缩过程快了大约50倍。总体而言，我们观察到与[36]、[54]相比，通信成本减少了80%-90%。&lt;/p&gt;
&lt;p&gt;除了矩阵乘法，点对点乘法 $x_0 \cdot y_0, x_1 \cdot y_1, \dots$ 也是变换器推理中的一个重要计算。许多基于同态加密的点对点乘法协议[18]、[63]需要设置一个相对较大的明文模数$t$。例如，[18]、[63]都设置HE参数 $t &amp;gt; 22l+40$ 来加密来自 $\mathbb{Z}_2^l$ 的值。在这项工作中，我们提出了一种模数提升函数，用于统一底层HE的秘密共享模数和明文模数。提升函数使我们能够在HE密文上执行模 $2l$ 的算术运算。这使我们能够选择较小的HE参数，即 $t \approx 22l$。我们的实验结果表明，在[63]中应用提升函数后，性能提高了1.3倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为变换器中的激活函数构建高效准确协议的框架&lt;/strong&gt;
我们首先提出了一个通用框架，用于构建高效且精确的2PC协议，适用于许多变换器模型中使用的激活函数。这些激活函数具有一个共同的特性：在靠近原点的短区间内它们相对平滑，且在两侧几乎是线性的。考虑到这一特性，我们提出使用一个或两个低度多项式来逼近激活函数在短区间内的表现，并在两侧使用恒等函数。例如，我们建议用两个低度的多项式 $P(x)$ 和 $Q(x)$ 来逼近SiLU函数，这两个多项式最小化最小二乘误差，如下所示：
&lt;/p&gt;
$$
\text{SiLU}(x) \approx 
\begin{cases} 
-10^{-5} &amp; \text{if } x &lt; -8 \\
P(x) &amp; \text{if } -8 &lt; x \leq -4 \\
Q(x) &amp; \text{if } -4 &lt; x \leq 4 \\
x - 10^{-5} &amp; \text{if } x &gt; 4 
\end{cases}
$$&lt;p&gt;
接着，我们基于两个关键的见解提出了优化方法。首先，我们利用激活函数的平滑性来提高效率。例如，对于输入 $x = -3.98$，即便使用（错误的）第二段，也能得到类似的结果，即 $P(-3.98) \approx Q(-3.98)$。这种平滑性为我们设计一种高效的评估方法提供了空间，尽管会引入轻微的误差。其次，我们提出了优化措施，以提高在对相同输入点评估多个多项式时的摊销效率。具体来说，我们优化后的激活协议比当前在[61]、[53]中详细描述的数值方法快了9到20倍，通信成本减少了80%到95%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;B. 贡献&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;总结来说，我们做出了三项关键贡献：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们开发了一个基于同态加密（HE）的矩阵乘法协议，特别针对模 $2l$ 运算进行了优化，以提升通信效率。事实上，我们的协议已被证明与现有的基于HE的方法相比，通信成本减少了80%。此外，我们的协议还具有高速度。例如，私密矩阵乘法操作（维度为 $128 \times 768 \times 768$）可以在1秒钟内完成，使用一个中等规格的云实例。&lt;/li&gt;
&lt;li&gt;我们实现了所有提出的协议，并开发了一个新的MPC后端，称为BumbleBee，并将其集成到SPU库[53]中。为了进行比较，我们还基于SPU库实现了一个基准版本，使用了几种最先进的2PC协议。通过简单的对比，图1展示了与基准版本相比，提出的协议所带来的改进。总的来说，我们在SIGMA[30]的基础上减少了84%的通信成本，在BOLT[58]的基础上减少了90%，在Iron[33]的基础上减少了92%。&lt;/li&gt;
&lt;li&gt;BumbleBee使得私密变换器推理变得易于使用。现有的工作[48]、[33]、[58]仅考虑了BERT系列模型。与之对比，我们成功地在5个预训练变换器模型上运行了BumbleBee，利用了HuggingFace网站上提供的模型权重和Python程序，包括BERT-base、BERT-large、GPT2-base、LLaMA-7B和ViT-base。我们还在四个公开数据集上评估了BumbleBee的准确性。所有实验都使用了我们提出的协议，而不是通过模拟进行的。我们提供了可重现的实现，网址为：https://github.com/AntCPLab/OpenBumbleBee。我们的方法为精确且可行的私密变换器推理提供了有力证据，即使在不改变神经网络结构的情况下，也能实现这一目标。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;II. 基础知识&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A. 符号约定&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们用 $x = y$ 表示 $x$ 等于 $y$，用 $x := y$ 表示将 $y$ 的值赋给变量 $x$。对于一个交互协议 $\Pi$，我们用 $JxK \gets \Pi$ 表示协议的执行。我们用 $[n]$ 来表示集合 ${0, \dots, n - 1}$，其中 $n \in \mathbb{N}$。对于一个集合 $D$，$x \in R^D$ 表示 $x$ 是从 $D$ 中均匀随机采样的。我们使用 $\lceil \cdot \rceil$、$\lfloor \cdot \rfloor$ 和 $\lfloor \cdot \rceil$ 来分别表示天花板函数、地板函数和四舍五入函数。逻辑与和异或分别表示为 $\land$ 和 $\oplus$。令 $1{P}$ 表示指示函数，当谓词 $P$ 为真时，$1{P}$ 为 1，$P$ 为假时，$1{P}$ 为 0。我们用带有“帽”符号的小写字母，例如 $\hat{a}$，来表示多项式，用 $\hat{a}[j]$ 表示多项式 $\hat{a}$ 的第 $j$ 个系数。我们用点符号 $\cdot$，例如 $\hat{a} \cdot \hat{b}$，来表示多项式的乘法。我们用 $Z_q = Z \cap [0, q)$ 来表示 $q \geq 2$ 时的整数模 $q$。同余式 $x \equiv y \mod 2^l$ 将简写为 $x \equiv_l y$。对于一个二次幂数 $N$，和 $q &amp;gt; 0$，我们写 $R_q$ 来表示整数多项式环 $R_q = Z_q[X] / (X^N + 1)$。我们用粗体字母如 $\mathbf{a}$、$\mathbf{M}$ 来表示向量和矩阵，用 $a[j]$ 来表示向量 $\mathbf{a}$ 的第 $j$ 个分量，用 $M[j, i]$ 来表示矩阵 $\mathbf{M}$ 的 $(j, i)$ 项。Hadamard 乘积表示为 $\mathbf{a} \odot \mathbf{b}$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;B. 加密原语&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1.&lt;strong&gt;加法秘密共享&lt;/strong&gt;：本文中我们使用 2-out-of-2 加法秘密共享方案，基于环 $Z_{2^l}$。一个 $l$ 位（$l \geq 2$）的值 $x$ 被加法共享为 $JxK_0$ 和 $JxK_1$，其中 $JxK_l$ 是由参与方 $P_l$ 持有的 $x$ 的随机份额。我们可以将两个共享值的乘法写作：
&lt;/p&gt;
$$
JxK⋅JyK≡l(JxK0+JxK1)⋅(JyK0+JyK1)≡lJxK0JyK0+JxK1JyK1+JxK0JyK1+JxK1JyK0JxK \cdot JyK \equiv_l (JxK_0 + JxK_1) \cdot (JyK_0 + JyK_1)\equiv_l JxK_0JyK_0 + JxK_1JyK_1 + JxK_0JyK_1 + JxK_1JyK_0
$$&lt;p&gt;
其中，混合项 $JxK_0 JyK_1$ 和 $JxK_1 JyK_0$ 是通过同态加密计算的。对于一个实值 $x ̃ \in \mathbb{R}$，我们首先将其编码为一个定点值 $x = \lfloor x ̃ 2^f \rfloor \in [-2^{l-1}, 2^{l-1})$，并在指定精度 $f &amp;gt; 0$ 下进行秘密共享。我们用 $J·; f K$ 明确表示 $f$ 位定点精度的共享值。当 $l = 1$ 时，我们使用 $JzK_B$ 表示布尔共享。此外，当所有权与上下文无关时，我们省略下标，仅写 $JxK$ 或 $JzK_B$。&lt;/p&gt;
&lt;p&gt;2.&lt;strong&gt;遗忘传输&lt;/strong&gt;：我们依赖于遗忘传输（OT）来进行非线性计算。在一般的 1-out-of-2 OT 协议（$2^1$-OT）中，发送方输入两个长度为 $l$ 位的消息 $m_0$ 和 $m_1$，接收方输入一个选择位 $c \in {0, 1}$。协议结束时，接收方学习到消息 $m_c$，而发送方则什么也不知道。当发送方的消息相关时，相关 OT（COT）在通信上更为高效 [6]。在我们的加法 COT 中，发送方输入一个函数 $f(x) = x + \Delta$，其中 $\Delta \in Z_{2^l}$，接收方输入选择位 $c$。协议结束时，发送方学到 $x \in Z_{2^l}$，而接收方学到 $x + c \cdot \Delta \in Z_{2^l}$。在本工作中，我们使用 Ferret 协议 [77] 来实现低通信量的 COT。&lt;/p&gt;
&lt;p&gt;3.&lt;strong&gt;基于格的加法同态加密&lt;/strong&gt;：同态加密（HE）允许在不知晓解密密钥的情况下，计算函数 $F(x)$ 的加密结果。在本研究中，我们使用基于环学习与误差（RLWE）的同态加密方案 [52]。RLWE方案由一组公共参数 $HE.pp = {N, q, t}$ 定义。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;KeyGen&lt;/strong&gt;：生成RLWE密钥对 $(sk, pk)$，其中私钥 $sk \in R_q$，公钥 $pk \in R_{q^2}$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加密&lt;/strong&gt;：RLWE密文表示为多项式元组 $(\hat{b}, \hat{a}) \in R_{q^2}$。我们使用 $RLWE_{q,t}  pk(\hat{m})$ 来表示在公钥 $pk$ 下加密的 $m \in R_t$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加法 (⊞)&lt;/strong&gt;：给定两个RLWE密文 $ct_0 = (\hat{b}_0, \hat{a}_0)$ 和 $ct_1 = (\hat{b}_1, \hat{a}_1)$，它们分别加密了 $m_0, m_1 \in R_t$，并使用相同的密钥，则操作 $ct_0 \oplus ct_1$ 计算 RLWE 元组 $(\hat{b}_0 + \hat{b}_1, \hat{a}_0 + \hat{a}_1) \in R_q^2$，并且该元组可以解密为 $\hat{m}_0 + \hat{m}_1 \mod R_t$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;乘法 (⊠)&lt;/strong&gt;：给定一个RLWE密文 $ct = (\hat{b}, \hat{a})$，它加密了 $m \in R_t$，以及一个普通多项式 $\hat{c} \in R_t$，则操作 $ct \otimes \hat{c}$ 计算元组 $(\hat{b} \cdot \hat{c}, \hat{a} \cdot \hat{c}) \in R_{q^2}$，并且该元组可以解密为$ \hat{m} \cdot \hat{c} \mod R_t$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SIMD 编码&lt;/strong&gt;：通过选择一个素数 $t$，使得 $t \equiv 1 \mod 2N$，SIMD 技术 [66] 允许将 $v, u \in Z_t^N$ 的 N 个元素向量转换为多项式 $\hat{v}, \hat{u} \in R_t$。多项式 $\hat{v} \cdot \hat{u}$ 可以解码为按点相乘 $u \odot v \mod t$。在私有评估的上下文中，SIMD 技术可以通过一个因子 $1/N$ 来摊销按点相乘的成本。我们用 $\hat{v} := SIMD(v)$ 来表示SIMD编码，并用 $SIMD^{-1}(\cdot)$ 表示解码函数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自同态变换&lt;/strong&gt;：给定一个RLWE密文 $ct \in R_q^2$，它加密了一个多项式 $\hat{m}(X) \in R_t$，以及一个奇数 $g \in [1, 2N)$，操作 $ct&amp;rsquo; := Auto(ct, g)$ 计算新的密文 $ct&amp;rsquo; \in R_q^2$，解密后得到 $m&amp;rsquo;(X) = \hat{m}(X^g) \in R_t$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;C. &lt;strong&gt;基于Transformer的模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;许多现代语言预处理器，如GPT [15] 和 BERT [19]，由一个输入嵌入层和多个Transformer层组成 [70]。类似地，视觉Transformer（ViTs）[22], [5] 也采用了类似的架构，不过它们没有输入嵌入层。基于Transformer的模型主要有两个计算模块：多头注意力机制和前馈神经网络。此外，层归一化（Layer Normalization）用于两个连续层之间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多头注意力&lt;/strong&gt;：一个注意力机制 $Attention(Q, K, V)$ 计算的是 $softmax(QK^\top + M)V$，它可以被描述为将一个查询 $Q$ 和一组键值对 $(K, V)$ 映射为一个加权和。这里，$Q, K, V$ 是输入矩阵的不同线性变换。多头注意力的变体计算 $H$ 个并行的注意力：$Attention(Q_j, K_j, V_j)$，其中 $j \in [H]$，然后将这些 $H$ 个结果矩阵拼接起来。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;层归一化&lt;/strong&gt;：对于一个向量 $x \in \mathbb{R}^d$，令 $\mu = \frac{1}{d} \sum_{j} x[j] \in \mathbb{R}$ 和 $\sigma = \sum_{j \in [d]} (x[j] - \mu)^2 \in \mathbb{R}$。层归一化表示为：&lt;/p&gt;
&lt;p&gt;$LayerNorm(x) = \gamma \cdot \frac{x - \mu}{\sigma} + \beta$&lt;/p&gt;
&lt;p&gt;其中 $\gamma, \beta \in \mathbb{R}$ 是两个超参数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;前馈神经网络&lt;/strong&gt;：前馈神经网络通常包括两个线性变换，并在它们之间应用激活函数，即：&lt;/p&gt;
&lt;p&gt;$FFN(X) = W_0 \cdot F(W_1 \cdot X)$&lt;/p&gt;
&lt;p&gt;其中，$F: \mathbb{R}^* \to \mathbb{R}^*$ 是一个激活函数，如 GeLU 或 SiLU。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;D. 威胁模型与私密推理&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;类似于之前的工作 [41], [55], [62], [36], [3]，我们针对一个静态且半诚实的概率多项式时间（PPT）对手，遵循理想/现实世界范式 [10]。也就是说，我们考虑一个计算能力有限的对手，在协议执行开始时腐化了其中一个方，并遵循协议规范，但试图学习有关诚实方输入的附加信息。BumbleBee 调用了几个小规模私密计算的子协议，概要见表 I。为了简化协议描述和安全性证明，我们使用混合模型描述 BumbleBee。一个调用功能 $F$ 的协议称为在“$F$-混合模型”中。此外，BumbleBee 中某些函数为了更好的效率进行了近似计算。根据定义 [25]，如果一个协议的近似计算揭示的输入信息不比 $F$ 本身更多，则该协议构成了 $F$ 的私密近似。&lt;/p&gt;
&lt;p&gt;在私密 2PC 推理（图 2）中，服务器 $S$ 持有一个 transformer 模型，而客户端 $C$ 向该模型发送查询，如一段文本。假设 $S$ 和 $C$ 均为半诚实模型，BumbleBee 使得 $C$ 只能得知两项信息：transformer 的架构和推理结果。$S$ 要么可以得知结果，要么什么都不能得知，具体取决于应用场景。关于 $C$ 的私密输入和 $S$ 的模型权重的所有其他信息应保持秘密。威胁模型的正式定义见附录。&lt;/p&gt;
&lt;h2 id=&#34;安全矩阵乘法&#34;&gt;安全矩阵乘法
&lt;/h2&gt;&lt;p&gt;在私有变换器推理中，我们需要两种类型的矩阵乘法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;共享矩阵与明文矩阵的乘法。例如，对于每个Transformer模块（参见图2），我们计算共享输入矩阵（由前一个模块计算得出）与服务器的明文权重矩阵之间的乘法。&lt;/li&gt;
&lt;li&gt;注意力机制内部两个秘密共享矩阵的乘法。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们提出了一种原语，称为&amp;quot;隐式线性变换&amp;quot;（Oblivious Linear Transformation，OLT），用于实现这两种矩阵乘法。我们将OLT描述为一种二方协议，分别从两个方获取两个私有矩阵Q和V，并生成它们之间的共享矩阵JQVK。通过OLT原语，我们可以使用一次OLT执行计算共享矩阵JQK和明文矩阵V之间的乘法。另一方面，对于两个共享矩阵的乘法，我们需要进行两次OLT。也就是说，我们通过两个OLT计算两个混合项JQK1 · JVK0和JQK0 · JVK1。&lt;/p&gt;
&lt;h3 id=&#34;基于同态加密he的olt现有方法&#34;&gt;基于同态加密（HE）的OLT现有方法
&lt;/h3&gt;&lt;p&gt;我们的出发点是许多研究中使用的基于同态加密（HE）的OLT方法，例如[54]、[36]、[33]（以下简称为KRDY风格）。KRDY需要两个函数：
πlhs : Zkw×mw  2l 7→ R2l 和 πrhs : Zmw ×nw  2l 7→ R2l，用于将矩阵编码为可以使用RLWE加密的多项式系数。假设 1 ≤ kw · mw · nw ≤ N，这两个函数定义如下：
qˆ := πlhs(Q) 和 vˆ := πrhs(V)，使得
(1) qˆ[0] = Q[i, j]，当 i = 0 且 j = 0 时，
qˆ[N + i · kw · mw − j] = −Q[i, j]，当 i = 0 且 j ∈ [mw]/{0} 时，
qˆ[i · kw · mw − j] = Q[i, j]，当 i &amp;gt; 0 且 j ∈ [mw] 时，
vˆ[k · mw + j] = V[j, k]，当 j ∈ [mw] 且 k ∈ [nw] 时。
qˆ 和 vˆ 的所有其他系数设置为0。
乘积多项式 qˆ · vˆ 直接给出结果矩阵 QV 的一些系数，如以下命题所示。&lt;/p&gt;
&lt;p&gt;命题 1. [54，改编] 假设 1 ≤ kw · mw · nw ≤ N。给定两个多项式 qˆ = πlhs(Q)，vˆ = πrhs(V) ∈ R2l，乘法 U ≡l Q · V 可以通过在环 R2l 上计算多项式的乘积 uˆ = qˆ · vˆ 来评估。即 U[i, k] = uˆ[i · mw · nw + k · mw]，对于所有 i ∈ [kw]，k ∈ [nw]。&lt;/p&gt;
&lt;p&gt;图3提供了πlhs和πrhs编码方法背后的基本思想的简单示意图。在OLT的背景下，我们让P0使用P0的密钥将同态加密的RLWEpk0(πlhs(Q))发送给P1。然后，P1可以通过单次同态乘法，即RLWEpk0(πlhs(Q))⊠πrhs(V)，同态地评估矩阵乘法 Q · V。为了将加密后的矩阵转换为算术共享，P0和P1接着共同调用FH2A功能（参见表I）。当矩阵形状 k·m·n &amp;gt; N 时，我们可以将其拆分成更小的子矩阵，并将KRDY应用于每对对应的子矩阵。我们将划分窗口表示为(kw, mw, nw)。&lt;/p&gt;
&lt;p&gt;在通信成本方面，KRDY需要交换 O(min(km / kwmw, mn / mwnw) + kn / kwnw) 个密文。与[55]、[62]中使用的其他基于HE的OLT方法相比，KRDY OLT不需要任何旋转（在HE中旋转操作非常昂贵），因此相对高效。然而，KRDY OLT的结果中有许多“无用”的系数。根据命题1，结果多项式uˆ中只有kwnw个系数是“有用的”，而N个系数中KRDY仍然需要传输整个uˆ的RLWE密文进行解密，这可能会浪费通信资源。为了提供一个例子，我们考虑一个在变换器模型中常用的维度集合，如k = 16，m = 768，n = 3072。在这种情况下，KRDY协议可能需要交换约25MB的密文。这可能是一个可观的通信开销，因为变换器模型通常包含数百个这样的超大规模矩阵。接下来的部分将详细讨论减少这种通信负担的方法。&lt;/p&gt;
&lt;h3 id=&#34;通过密文打包的首次尝试&#34;&gt;通过密文打包的首次尝试
&lt;/h3&gt;&lt;p&gt;我们的首次尝试是将PackLWEs [13]过程应用于KRDY，我们将其称为KRDY+。PackLWEs过程允许我们从多个RLWE密文中选择任意系数，并通过执行同态自同构将它们组合成一个单一的RLWE密文。在KRDY+的情况下，我们可以将密文数量从 O(kn/(kwnw)) 减少到 O(kn/N)，代价是 O(kn) 的同态自同构操作。再次考虑矩阵形状 (k, m, n) = (16, 768, 3072)。现在，KRDY+交换大约2.9 MB的密文，相较于KRDY的25 MB，这是一个显著的减少。然而，在HE中，自同构操作非常缓慢，实际上对于具有较大kn的矩阵，KRDY+使用的自同构可能比同态乘法本身更为计算密集。&lt;/p&gt;
&lt;h3 id=&#34;密文交织优化&#34;&gt;密文交织优化
&lt;/h3&gt;&lt;p&gt;我们在矩阵乘法的背景下提出了针对PackLWEs过程[13]的专门优化。其主要思路是，与其按照PackLWEs过程逐个“挑选”有用的系数，我们更倾向于“清理”结果密文中的无用系数，然后将它们合并成一个单一的密文。我们简要描述清理过程。让我们以 N = 8 和 aˆ(X) = ∑ᵢ=0⁷ aiXi 为例。根据定义，自同构Auto(ˆa, 9)生成 ∑ᵢ=0⁷ aiXi·9，这相当于 ∑ᵢ=0³ a2iX2i − ∑ᵢ=0³ a2i+1X2i+1 mod X8 + 1。在这里，奇数位置的系数符号被翻转。因此，评估 ˆa + Auto(ˆa, N + 1) 会消除奇数索引的系数，并将偶数索引的系数加倍。更一般地，令我们计算 ˆa + Auto(ˆa, N/2j + 1) 对于任何 N 的2的幂因子。再次以N = 8为例，我们考虑 2j = 2。Auto(ˆa, 8/2 + 1) 等于 ∑ᵢ=0⁷ aiXi·5 ≡ a0 − a2X2 + a4X4 − a6X6 + ∑ᵢ≠0 mod 2 aiXi·5。简而言之，2j倍数位置的系数符号被翻转。假设对于所有 i ≠ 0 mod 2j 的 ˆa[i] 系数已经为零。那么，ˆa + Auto(ˆa, N/2j + 1) 会消除2j的奇数倍位置的系数，同时加倍偶数倍位置的系数。我们定义一个函数 ZeroGap(ˆa, 2r)，它重复公式 r 次 ˆa := aˆ + Auto(ˆa, N/2j + 1) 对于 j = 0, 1, · · · , r − 1。执行这些操作后，结果是一个多项式，其中只有在2r倍数位置的系数是非零的。然而，这些系数被2r的倍数缩放。为了修正这个缩放，我们首先将多项式乘以逆缩放因子 2−r mod q。这隐式地要求使用奇数模数q，这在使用模数 q ≡ 1 mod 2N 的基于RLWE的HE中通常是满足的。此外，ZeroGap过程要求两个需要的系数之间的间隔是2的幂数。这等同于πlhs和πrhs编码中的分区窗口 mw。请注意，只要满足 1 ≤ kwmwnw ≤ N，可以自由选择这个分区窗口。&lt;/p&gt;
&lt;p&gt;有了ZeroGap过程，我们提出了我们的关键贡献之一，即InterLeave过程。InterLeave过程将2r个多项式交织成一个单一的多项式，输入系数以2r步的跨度排列。我们首先给出InterLeave的简单版本，如图4所示。在这个简单版本中，我们分别对每个输入多项式应用ZeroGap过程（步骤2到步骤5）来清理无关系数。然后，我们通过简单的旋转-求和计算（步骤6到步骤8）得到最终结果。我们想强调的是，密文域中的系数旋转可以比SIMD旋转[32]更高效地实现。事实上，图4计算了 O(r · 2r) 次自同构操作，将 N 个系数合并成一个多项式。图5给出了一个InterLeave（简单版本）的玩具示例。&lt;/p&gt;
&lt;p&gt;最终版本。我们现在展示图6中的优化版本，它将图4中的复杂度从 O(r · 2r) 次自同构操作降低到了 O(2r) 次自同构操作。我们将展示这个优化版本的直觉。直观上，利用自同构函数的双线性性质，将两个具有相同索引的自同构操作合并为一个自同构操作是合理的。即，对于任何有效的索引 g，两个自同构操作的和 Auto(ˆa, g) + Auto(ˆb, g) 等于对多项式和的自同构操作。然而，应用相同自同构索引的多项式在求和步骤（图4的步骤8）之前，需要先进行不同单位的右移（即图4的步骤6）。为了解决这个问题，我们利用以下方程。即，当 ˆb[i] = 0 对于所有 i ≠ 0 mod 2j 时，&lt;/p&gt;
$$
Auto(ˆa,N/2j+1)+Auto(ˆb,N/2j+1)⋅X2j=Auto(ˆa−ˆb⋅X2j,N/2j+1)Auto(ˆa, N/2j + 1) + Auto(ˆb, N/2j + 1) \cdot X^{2j} = Auto(ˆa - ˆb \cdot X^{2j}, N/2j + 1)
$$&lt;p&gt;
回想一下，自同构操作 Auto(·, N/2j + 1) 会翻转位于 2j 的奇数倍位置的系数，而保持偶数倍位置的系数不变。同时，通过乘以 $X^{2j}$ 对 ˆb 进行右移，只是改变了 ˆb 的系数位置的奇偶性——从 2j 的奇数倍位置移到偶数倍位置，反之亦然。因此，自同构-然后-右移的模式
&lt;/p&gt;
$$
Auto(ˆb,N/2j+1)⋅X2jAuto(ˆb, N/2j + 1) \cdot X^{2j}
$$&lt;p&gt;
等价于取反-右移-然后自同构的方式
&lt;/p&gt;
$$
Auto(−ˆb⋅X2j,N/2j+1)Auto(-ˆb \cdot X^{2j}, N/2j + 1)
$$&lt;p&gt;
这样，我们就可以利用双线性特性，将具有相同索引的两个自同构操作合并为一个自同构操作。&lt;/p&gt;
&lt;h3 id=&#34;提出的olt协议&#34;&gt;提出的OLT协议
&lt;/h3&gt;&lt;p&gt;我们现在在算法1中描述我们的OLT协议。前三个步骤与KRDY风格的协议 [54]，[36] 类似。具体来说，我们将一个大矩阵分割成子块，并使用 πlhs 和 πrhs 将每个块编码为多项式。然而，在我们的方案中，我们故意选择了一个2的幂次方的分区窗口 mw。我们协议与他们的主要区别在于，在步骤4中使用了 InterLeave 操作，以减少RLWE密文的数量，从 O(kn/(kwnw)) 降低到 O(kn/N )。解析交织多项式并构造结果矩阵的过程在附录的图8中进行了说明。为了在计算和通信开销之间取得良好的平衡，选择合适的分区窗口 mw 是至关重要的。较小的 mw 值可能会在步骤1中过度增加通信开销，而较大的 mw 值会导致步骤4中更多的同态自同构操作。给定矩阵形状 (k, m, n) 和分区窗口 (kw, mw, nw)，步骤1中发送的密文数量为 n1 := m′·min(k′, n′)。执行 InterLeave 所需的同态自同构操作总数为 n2 := ⌈k′n′/mw⌉mw。交织后，步骤5中发送的密文数量为 n3 := ⌈ k′n′ / mw ⌉。为了选择合适的分区窗口，我们可以通过最小化以下目标函数来优化：&lt;/p&gt;
&lt;p&gt;$\ argmin_{kw, mw, nw} ; PC \cdot n2 + PB \cdot (n1 + n3)$&lt;/p&gt;
&lt;p&gt;其中，mw 必须是2的幂，并且满足 $1 \leq kw \cdot mw \cdot nw \leq N$ 的约束。这里，PC 表示计算一个同态自同构操作的成本，PB 表示发送一个密文的成本。较小的 PC/PB 比率可能表明计算能力强大或带宽受限的情景。在这种情况下，我们倾向于选择较大的 mw。相反，如果带宽充足，可以选择较小的 mw 值，以减轻密文交织所需的时间。&lt;/p&gt;
&lt;p&gt;复杂度。根据我们的经验结果，选择 mw ≈ √N 是一个可行的选择。然后，算法1中的协议需要大约 O(kn/√N) 次同态自同构操作用于密文交织。相比之下，KRDY+ 基准协议需要 O(kn) 次同态自同构操作。举个例子，设 mw = 26 且 N = 213。在这种设置下，我们协议的密文压缩时间大约是 KRDY+ 密文压缩时间的 1/26 ≈ 1.6%。KRDY+ 和我们协议的通信开销都是 O(kn/N) 个密文。请注意，BOLT 的主模OLT可能根据矩阵维度 [58] 需要更少的同态自同构操作 O(pk²m²n/N²)。&lt;/p&gt;
&lt;p&gt;安全性。我们仅增加了一个密文压缩步骤来减少通信开销，我们发送的信息是 KRDY 协议的子集，因此我们的安全级别与其相同。定理1的证明参考了 [54]，[36]。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法 1 提议的不可知线性变换 \( $ \Pi_{OLT} $\)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入：&lt;/strong&gt;&lt;br&gt;
发送方 \( S \)：&lt;br&gt;
&lt;/p&gt;
$$ Q \in \mathbb{Z}^{k \times m} $$$$ 2^l $$&lt;p&gt; 和 \( sk \)。&lt;br&gt;
接收方 \( R \)：&lt;br&gt;
&lt;/p&gt;
$$ V \in \mathbb{Z}^{m \times n} $$$$ 2^l $$&lt;p&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出：&lt;/strong&gt;&lt;br&gt;
&lt;/p&gt;
$$ JUK $$&lt;p&gt;，使得&lt;br&gt;
&lt;/p&gt;
$$ U \equiv_l Q \cdot V $$&lt;p&gt;&lt;strong&gt;公共参数：&lt;/strong&gt;&lt;br&gt;
&lt;/p&gt;
$$ pp = (\text{HE.pp}, pk, (kw, mw, nw)) $$&lt;ul&gt;
&lt;li&gt;大小 \( mw \) 是一个 2 的幂，且&lt;br&gt;
$$ 1 \leq kw \cdot mw \cdot nw \leq N $$&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
$$ k&#39; = \lceil \frac{k}{kw} \rceil $$&lt;p&gt;，&lt;br&gt;
&lt;/p&gt;
$$ m&#39; = \lceil \frac{m}{mw} \rceil $$&lt;p&gt;，&lt;br&gt;
&lt;/p&gt;
$$ n&#39; = \lceil \frac{n}{nw} \rceil $$&lt;p&gt;，和&lt;br&gt;
&lt;/p&gt;
$$ \tilde{m} = \lceil \frac{k&#39; \cdot n&#39;}{mw} \rceil $$&lt;ul&gt;
&lt;li&gt;注意：如果&lt;br&gt;
$$ k&#39; &gt; n&#39; $$，则交换发送方和接收方的角色。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;发送方 \( S \) 首先将矩阵 \( Q \) 划分为块矩阵&lt;br&gt;
&lt;/p&gt;
$$ Q_{\alpha, \beta} \in \mathbb{Z}^{kw \times mw} $$&lt;p&gt; \( 2^l \)。然后，\( S \) 对每个块矩阵编码为多项式&lt;br&gt;
&lt;/p&gt;
$$ \hat{q}_{\alpha, \beta} := \pi_{\text{lhs}}(Q_{\alpha, \beta}) $$&lt;p&gt;&lt;br&gt;
其中 \( \alpha \in [k&#39;] \) 且 \( \beta \in [m&#39;] \)。之后，\( S \) 将每个编码后的密文&lt;br&gt;
&lt;/p&gt;
$$ ct&#39;_{\alpha, \beta} := \text{RLWE}_{q, 2l}^{pk} (\hat{q}_{\alpha, \beta}) $$&lt;p&gt;&lt;br&gt;
发送给 \( R \)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接收方 \( R \) 首先将矩阵 \( V \) 划分为块矩阵&lt;br&gt;
&lt;/p&gt;
$$ V_{\beta, \gamma} \in \mathbb{Z}^{mw \times nw} $$&lt;p&gt; \( 2^l \)。然后，\( R \) 对每个块矩阵编码为多项式&lt;br&gt;
&lt;/p&gt;
$$ \hat{v}_{\beta, \gamma} := \pi_{\text{rhs}}(V_{\beta, \gamma}) $$&lt;p&gt;&lt;br&gt;
其中 \( \beta \in [m&#39;] \) 且 \( \gamma \in [n&#39;] \)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在接收到来自 \( S \) 的&lt;br&gt;
&lt;/p&gt;
$$ \{ct&#39;_{\alpha, \beta}\} $$&lt;p&gt; 后，\( R \) 计算一个 RLWE 密文向量 \( c \)，其中&lt;br&gt;
&lt;/p&gt;
$$ c[\alpha \cdot n&#39; + \gamma] := \bigoplus_{\beta \in [m&#39;]} ct&#39;_{\alpha, \beta} \odot \hat{v}_{\beta, \alpha} $$&lt;p&gt;&lt;br&gt;
对于 \( \alpha \in [k&#39;] \)，\( \gamma \in [n&#39;] \)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为了将 \( k&#39; \cdot n&#39; \) 个密文压缩为 \( \tilde{m} \) 个密文而不触及需要的系数，\( R \) 在密文向量 \( c \) 的子向量上运行 InterLeave 操作。例如&lt;br&gt;
&lt;/p&gt;
$$ \tilde{c}[\theta] := \text{InterLeave}([c[\theta \cdot mw], c[\theta \cdot mw + 1], \cdots \mid \{z\} mw]) $$&lt;p&gt;&lt;br&gt;
对于 \( \theta \in [\tilde{m}] \)。&lt;br&gt;
▷ 当 \( k&#39; \cdot n&#39; \) 无法被 \( mw \) 整除时，填充零。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于每个密文 \( \tilde{c}[i] \) 中的密文，运行&lt;br&gt;
&lt;/p&gt;
$$ \hat{c}_{i, 0}, \hat{c}_{i, 1} \leftarrow \text{FH2A}(\tilde{c}[i]) $$&lt;p&gt;&lt;br&gt;
其中 \( S \) 获得 \( \hat{c}_{i, 0} \in R^{2l} \)，\( R \) 获得 \( \hat{c}_{i, 1} \in R^{2l} \)。之后，\( S \) 和 \( R \) 都可以通过附录中的本地过程&lt;br&gt;
&lt;/p&gt;
$$ JUK_l := \text{ParseMat}(\hat{c}_{0, l}, \hat{c}_{1, l}, \cdots) $$&lt;p&gt;&lt;br&gt;
来获取各自的分享。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;定理 1. 算法 1 中的协议 ΠOLT 在 FH2A 混合模型下，能够在半诚实对手存在的情况下，私密地实现 OLT 功能。&lt;/p&gt;
&lt;p&gt;E. 进一步优化&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;批量矩阵乘法：回顾一下，变换器模型中的每个多头注意力涉及 H &amp;gt; 1 个并行的矩阵乘法，例如，Qj · Kj，其中 j ∈ [H]。当这些结果矩阵较小时，即 |Qj · Kj| ≤ N/2 时，我们倾向于应用 InterLeave 对批量乘法的密文进行压缩。简而言之，批量乘法的 O(H) 个 RLWE 密文将进一步打包成 O((H · kn)/N) 个密文。&lt;/li&gt;
&lt;li&gt;动态压缩策略：我们采用一种动态的密文压缩策略，使我们能够在减少通信成本和增加计算成本之间取得平衡。具体来说，当只有一个 RLWE 密文需要发送时，即 ⌈k/kw⌉ · ⌈n/nw⌉ = 1，我们不使用任何密文压缩。另外，当 kn ≪ N 时，我们使用 PackLWEs 而不是 InterLeave，因为前者在小规模情况下可以运行得更快。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;基于-rlwe-的更快批量-ole&#34;&gt;基于 RLWE 的更快批量 OLE
&lt;/h2&gt;&lt;p&gt;除了矩阵乘法，变换器模型中还需要标量乘法（或按批次处理的 Hadamard 乘积）。我们使用符号 Batch OLE（bOLE）[17] 来描述一个二方计算协议，该协议接收一个发送方 S 的向量 x 和接收方 R 的向量 y，并生成它们 Hadamard 乘积的秘密共享 Jx ⊙ yK。在私有推理的背景下，我们证明了带误差的 bOLE 变体（bOLEe）是足够的。我们可以使用更小的 RLWE 参数构建更高效的 bOLEe 协议。我们的 bOLEe 可能会在最终输出中引入最低有效位（LSB）误差。然而，由于在多方计算（MPC）中使用了定点表示，这些 LSB 误差将通过后续的截断被移除，因此我们的方法不会影响整体准确性。与 [63] 中基于 RLWE 的 bOLE 类似，我们还应用了 SIMD 技术 [66] 来实现 bOLE，但具有更好的摊销效率。在 [63] 中，发送方 S 对其私有输入 x ∈ ZN 2l 进行预处理为 SIMD(x)，并将密文 RLWEq,t S (SIMD(x)) 发送给接收方 R。然后，R 回应密文 RLWEq,t S (SIMD(x)) ⊠ SIMD(y) ⊞ SIMD(r) 给 S，r 是一个掩码向量，r ∈ ZN 。实际上，这里的算术运算 x⊙y+r 是在模 t 下进行的，尽管 x 和 y 中的值来自环 Z2l 。由于模 t 与 2l 不可整除，掩码向量 r 应从更大的环中采样以提供统计安全性。具体来说，[63] 从 ZN 22l+σ 中采样 r，以提供 σ 位的统计安全性。此外，为了防止加法溢出，他们需要设置一个较大的明文模 t &amp;gt; 22l+σ+1。我们展示了如何避免这种额外的 σ 位开销，代价是引入 1 位的 LSB 误差。基本上，我们使用提升函数在质数明文模 t 上引入一个中间层的 2l 模。
Lift(x) : ZN 2l → ZN t 通过 ⌊ t / 2l · x⌉ mod t
Down(y) : ZN t → ZN 2l 通过 ⌊ 2l / t · y⌉ mod 2l&lt;/p&gt;
&lt;p&gt;命题 2. 如果 t &amp;gt; 22l，则对于任意 x, y ∈ Z2l，有 Down(Lift(x)·y mod t) ≡l x·y。也就是说，我们可以在质数模 t 上进行模 2l 运算。&lt;/p&gt;
&lt;p&gt;证明. 只需证明误差项可以被舍入为零。Lift(x) · y mod t 可以写为 t / 2l · ((x · y mod 2l) + re · y，其中 re 是舍入误差，|re| ≤ 1/2。然后，误差项被舍入为 ⌊ 2l / t · (re · y)⌉ = 0，当 t &amp;gt; 22l 且 y &amp;lt; 2l 时。
现在，我们可以使用信息学随机掩码 r ∈ ZtN 代替统计掩码。结果中可能会引入 1 位误差。&lt;/p&gt;
&lt;p&gt;命题 3. 设 u′ := Down(Lift(x) · y − r mod t)，v′ := Down(r)，其中 x, y ∈ Z2l 且 r ∈ Zt。如果 t &amp;gt; 22l 且 r 在 Zt 上均匀分布，则 u′ + v′ ≡l x · y + e，其中 e ∈ {0, ±1}。&lt;/p&gt;
&lt;p&gt;算法 2 带误差的 bOLE 协议 ΠbOLEe&lt;/p&gt;
&lt;p&gt;输入：
发送方 S：x ∈ ZN 2l ，私钥 sk。
接收方 R：y ∈ ZN 2l 。
公共参数 pp = {N, t}，其中 t = 1 mod 2N 为质数，且 t &amp;gt; 22l，以及公共密钥 pk。&lt;/p&gt;
&lt;p&gt;输出：
JzK ∈ ZN 2l，使得 ∥z − x ⊙ y mod 2l∥∞ ≤ 1。&lt;/p&gt;
&lt;p&gt;步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;S 将 RLWEq,t pk (xˆ) 发送给 R，其中 xˆ := SIMD(Lift(x))。&lt;/li&gt;
&lt;li&gt;R 计算 yˆ := SIMD(y)。&lt;/li&gt;
&lt;li&gt;收到密文 RLWEq,t sk (xˆ) 后，R 计算 ct := RLWEq,t pk (xˆ) ⊠ yˆ。&lt;/li&gt;
&lt;li&gt;JuˆK ← FH2A(ct) 将其转换为算术共享。假设 S 的共享为 JuˆK0 ∈ Rt，R 的共享为 JuˆK1 ∈ Rt。请注意，我们让 FH2A 函数来捕捉电路隐私（参见备注 1）。&lt;/li&gt;
&lt;li&gt;S 输出 Down(SIMD−1(JuˆK0))。&lt;/li&gt;
&lt;li&gt;R 输出 Down(SIMD−1(JuˆK1))。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;该证明基本遵循 [36, 完整版本，附录 C] 中的类似论证。&lt;/p&gt;
&lt;p&gt;定理 2. 算法 2 中的协议 ΠbOLEe 在 FH2A 混合模型下，能够在半诚实对手存在的情况下，私密地实现 bOLEe 功能（参见表 I）。
定理 2 的正确性简单地遵循 SIMD 打包和命题 3。对于具体的改进，我们的 bOLEe 协议在 l = 64 时需要 t ≈ 2128，而 [63] 的方法需要更大的 t ≈ 2168。我们的实证结果表明，我们的协议比他们的协议快约 1.3 倍。&lt;/p&gt;
&lt;h2 id=&#34;激活函数协议&#34;&gt;激活函数协议
&lt;/h2&gt;&lt;p&gt;我们首先描述用于 GeLU 函数的协议。本节中描述的方法和优化也可以应用于其他函数，如 SiLU 和 ELU（见附录 D）。&lt;/p&gt;
&lt;p&gt;A. 高斯误差线性单元（GeLU）&lt;/p&gt;
&lt;p&gt;GeLU 函数的常见定义是：&lt;/p&gt;
&lt;p&gt;$GeLU(x)=0.5x(1+tanh⁡(2π(x+0.044715x3))).\text{GeLU}(x) = 0.5x(1 + \tanh\left(\frac{\sqrt{2}}{\pi}(x + 0.044715x^3)\right)).$&lt;/p&gt;
&lt;p&gt;遵循现有的研究工作 [58]、[49]、[24]、[21]，我们使用分段函数来近似 GeLU 函数，如下所示：
&lt;/p&gt;
$$
\text{Seg4GeLU}(x) = 
\begin{cases}
-\epsilon &amp; \text{if } x &lt; -5 \\
P_3(x) &amp; \text{if } -5 \leq x \leq -1.97 \\
P_6(x) &amp; \text{if } -1.97 \leq x \leq 3 \\
x - \epsilon &amp; \text{if } x &gt; 3
\end{cases}
$$&lt;p&gt;
其中 $P_b(x)$ 是近似 GeLU 函数的 b 次多项式，定义在一个较短的区间上。例如，设定 $\epsilon = 10^{-5}$，我们绘制了 Seg4GeLU 如图 7a 所示。从图中可以看到，Seg4GeLU 很好地近似了 GeLU 函数。&lt;/p&gt;
&lt;p&gt;我们在算法 3 中的 GeLU 协议基本上遵循（2），使用 Fless 和 Fmux 进行分支选择。为了进一步提高效率，我们讨论并引入了三个独立的优化，这些优化在之前的研究工作 [58]、[49]、[24]、[21] 中未提及。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;近似分支选择&lt;/strong&gt;：第一个优化利用了激活函数的平滑性。具体来说，我们首先找到近似多项式 $P_3(x)$ 和 $P_6(x)$，使得 $P_3(x) \approx P_6(x)$，特别是在枢轴点附近，例如在我们的案例中，$x = -1.97$。公式（2）中的选择可以通过一个小于协议 $1{x &amp;lt; y}$ 来私密地实现，其中 $x, y \in Z_{2l}$。基于这些观察，我们建议计算小于位时忽略一些低位 f′ 位，以减少通信开销。这是一个常见的优化方法，用于在固定点值的秘密共享上执行比较。然而，确保正确的近似多项式至关重要，因为超出枢轴点的波动可能会破坏近似。经验表明，近似的分段选择有助于将算法 3 中 GeLU 协议的通信开销减少 5%。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;批量（近似）分支选择&lt;/strong&gt;：我们的第二个优化是针对 [46] 和 [36] 中基于 OT 的 MSB 协议。该协议使用公式 $MSB(JxK_0) \oplus MSB(JxK_1) \oplus 1{(JxK_0 \mod 2^{l-1}) + (JxK_1 \mod 2^{l-1}) \geq 2^{l-1}}$ 计算算术共享 $JxK$ 的 MSB，其中最后一位的计算需要一个 $M_1$-OT2。因此，对于公式（2）中的分支选择，我们需要 3 次 $M_1$-OT2 调用。请注意，这些“批量”比较是通过一个秘密共享和多个明文阈值进行的。通过这种安排，我们可以将 3 次 $M_1$-OT2 调用合并为一次 $M_1$-OT6，即在 6 位消息上进行 1-of-M OT。尽管这种 OT 组合可能不会显著减少通信开销，但根据我们的实验，它可以将 GeLU 的计算时间减少 35%。这是因为在使用 Ferret OT 时，调用一次 $M_1$-OT2 和调用一次 $M_1$-OT6 的运行时间相似。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优化多项式评估&lt;/strong&gt;：对于公式（2）中的 $P_3(x)$ 和 $P_6(x)$ 的评估，我们建议使用更快的平方协议 $\Pi_{\text{square}}$ 来计算所有偶次幂项，如 $x^2$、$x^4$ 和 $x^6$。实际上，在 2PC 中，执行平方操作的成本是标准乘法操作的一半。此外，我们还提出通过减少一半的通信成本来优化奇次幂项的计算。举个例子，考虑给定 $JxK$ 和 $Jx^2K$ 时，如何计算 $Jx^3K$。我们可以使用两次 bOLE 调用来计算 $Jx^3K$，即 $FbOLE(JxK_0, Jx^2K_1)$ 和 $FbOLE(Jx^2K_0, JxK_1)$。对于算法 2 中的 bOLE 构造，P0 将 SIMD(Lift(JxK_0)) 和 SIMD(Lift(Jx^2K_0)) 的密文发送给 P1。我们注意到，当计算 $Jx^2K$ 和 $Jx^4K$ 时，这两个密文已经被发送给 P1。因此，为了计算立方项，玩家可以跳过算法 2 中的步骤 1，按照其余步骤相同地执行。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;权衡。在我们的评估中，我们证明了这三种优化可以使时间减少35%，通信减少7%。需要注意的是，这些改进伴随着近似误差的增加。具体来说，我们的GeLU评估的平均ULP误差约为11，而[58]的平均ULP误差为4。此外，我们在四个transformer模型中测试了提出的GeLU协议，结果表明推理精度的降级非常小，不超过1%。我们认为这是精度和效率之间的合理权衡。&lt;/p&gt;
&lt;p&gt;B. &lt;strong&gt;Softmax&lt;/strong&gt;
为了数值稳定性[29，第4章]，softmax函数通常计算为：
&lt;/p&gt;
$$
softmax(x)[i]=exp⁡(x[i]−xˉ)∑jexp⁡(x[j]−xˉ),\text{softmax}(x)[i] = \frac{\exp(x[i] - \bar{x})}{\sum_j \exp(x[j] - \bar{x})},
$$&lt;p&gt;
其中 $\bar{x}$ 是输入向量 $x$ 的最大元素。对于二维矩阵，我们对其每一行向量应用公式(3)。值得注意的是，公式(3)中的所有指数操作的输入都是负数。我们利用负操作数来加速私有softmax。具体来说，我们通过简单的裁剪分支来近似指数操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 $x \in [T_{\text{exp}}, 0]$，我们有 $\exp(x) \approx 1 + \frac{x}{2^n}$；&lt;/li&gt;
&lt;li&gt;对于 $x &amp;lt; T_{\text{exp}}$，则 $\exp(x) \approx 0$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于裁剪范围 $T_{\text{exp}}$，我们简单地设置 $T_{\text{exp}}$ 使得 $\exp(T_{\text{exp}}) \approx 2^{-f}$，其中 $f$ 是定点精度。假设我们设置 $f = 18$，那么我们设置 $T_{\text{exp}} = -13$，因为 $\exp(-13) &amp;lt; 2^{-18}$。当 $T_{\text{exp}}$ 固定后，我们可以经验性地设置泰勒展开的次数 $n$。例如，在我们的实验中，对于 $T_{\text{exp}} = -13$，我们设置 $n = 6$，以使平均误差在 $2^{-10}$ 之内。此外，我们还使用上一节提出的近似“小于”操作进行分支选择，因为负输入的指数操作也是平滑的。通过调用 $F_{\text{tnrunc}}$ 可以实现除以 $2^n$，而 $2^n$ 的幂运算是通过一系列的 $\Pi_{\text{square}}$ 来计算的。&lt;/p&gt;
&lt;p&gt;C. &lt;strong&gt;混合位宽评估&lt;/strong&gt;
在运行我们的激活协议之前，我们可以先将共享值切换到较小的环 $l&amp;rsquo; &amp;lt; l$ 以节省通信开销。大环 $Z_{2^l}$ 到小环 $Z_{2^{l&amp;rsquo;}}$ 的共享转换可以通过局部设置 $JxK_l \mod 2^{l&amp;rsquo;}$ 来完成。反方向的转换，我们结合了[61]中的环扩展协议和[16]中的启发式优化。在我们的实现中，每次从 $Z_{2^{l&amp;rsquo;}}$ 到 $Z_{2^l}$ 的转换将在两轮中交换大约 $O(2^{(l - l&amp;rsquo;)})$ 位。这个混合位宽评估的主要问题是乘法引起的溢出。例如，双精度定点 $2^f$ 可能已经大于 $l&amp;rsquo;$。为了解决这个问题，我们必须做出两个妥协：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用额外一次对 $F_{\text{trunc}}$ 的调用来降低激活函数中的定点精度；&lt;/li&gt;
&lt;li&gt;缩小近似区间或增加分支数，以便可以使用较低次数的多项式。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最近的工作[58]在狭窄区间内使用低次多项式近似GeLU，即
&lt;/p&gt;
$$
GeLU(x)≈P4(x)=0.5x+∑i=04ci∣x∣iforx∈[−2.7,2.7].\text{GeLU}(x) \approx P_4(x) = 0.5x + \sum_{i=0}^{4} c_i |x|^i \quad \text{for} \quad x \in [-2.7, 2.7].
$$&lt;p&gt;
在我们的一些实验中，我们对GeLU应用了这个 $P_4(x)$，但对于SiLU函数，我们仍然需要使用两个多项式。&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;&lt;strong&gt;RELATED WORK&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Low Communication OLT&lt;/strong&gt;
许多利用同态SIMD [66] 技术的工作可以直接用于OLT功能，并具有相对较小的通信开销，例如 [32]、[12]、[39]、[35] 等。SIMD技术要求使用质数明文模数 $t$，而不是秘密共享中使用的 $2^l$ 模数。可以利用中国剩余定理接受来自 $Z_{2^l}$ 的秘密共享，但这会使得同态加密侧的计算和通信开销大大增加。另一种构建低通信OLT的策略是使用向量隐式线性评估（VOLE）[9]、[77]、[7]，正如CipherGPT [34]中提出的。然而，这种基于VOLE的方法要求较大的矩阵维度 $k \approx 10^7$ 才能实现低（摊销）通信，因此只适用于自回归变换器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Non-linear Functions&lt;/strong&gt;
对于softmax函数，[45]、[68] 也使用泰勒级数 $(1 + x/2^n)^{2^n}$ 来近似指数运算，只是他们没有应用范围裁剪。例如，CryptGPU根据对一些数据集的检查经验性地设置了 $n = 9$。SiRNN [61] 使用查找表进行初步猜测，并通过几次牛顿迭代进行修正。这些方法提供了精确的指数运算，但在2PC中计算开销较大。[80] 使用另一种数值方法近似softmax函数。然而，它可能需要超过64轮乘法，这使得它在通信上非常密集。Kelkar等人[42]提出了一种新的2PC指数协议，但由于失败概率，它的局限性在于必须使用较大的环 $l \approx 128$ 或严格约束输入域（例如，$|x| \leq 5$）。[49] 通过加扰电路[78]使用12个一次多项式来近似激活函数。最近的研究方法也[20]、[21]、[24]、[58] 使用多个低次多项式来近似GeLU函数。然而，这些方法都没有考虑GeLU函数的平滑性以及批量比较来减少分支选择的开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Private Transformer Inference (2PC)&lt;/strong&gt;
Iron [33]、BOLT [58] 和 CipherGPT [34] 是为变换器设计的2PC推理框架。这三个框架都大量复用了SiRNN框架 [61]、[23] 中基于OT的协议来评估激活函数。BOLT [58] 被认为是安全的双方变换器推理的最先进框架。BumbleBee与BOLT共享一些设计元素，例如非线性函数的分段近似和使用同态加密（HE）的私有矩阵乘法。然而，BumbleBee在通信效率方面超越了BOLT（见图1）。BumbleBee优于BOLT的主要原因总结如下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;共享类型一致的乘法&lt;/strong&gt;
BOLT的矩阵乘法协议在质数模数$p$上运行，而他们的非线性协议则在环模数$2^l$上运行。在私有推理过程中，它们必须在不同的共享类型之间来回切换。为了集成环到质数的转换（例如，参考[42]），BOLT需要一种特殊的共享乘法，其中输出的位宽可以大于输入的位宽。具体来说，BOLT复用了EzPC框架[23]、[61]中的基于OT的协议来实现这种不均匀的共享乘法。相比之下，在BumbleBee中，矩阵乘法协议和共享乘法协议都在环模数$2^l$上运行。这些共享类型一致的乘法使我们能够基于HE构建更高效的共享乘法协议。现有的工作如[63]已经表明，基于HE的共享乘法协议在通信方面比基于OT的协议高效5到6倍。需要注意的是，我们进一步优化了[63]的通信开销，提升了1.3倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;平方操作的成本是标准乘法的一半&lt;/strong&gt;
为了私密评估分段函数，我们需要对秘密共享输入$JxK$评估低次多项式。BOLT采用霍纳法则来评估多项式。例如，BOLT评估一个四次多项式如下：&lt;/p&gt;
&lt;p&gt;$P4(JxK)=(((((a4⋅JxK+a3)⋅JxK)+a2)⋅JxK)+a1)⋅JxK+a0,P_4(JxK) = (((((a_4 \cdot JxK + a_3) \cdot JxK) + a_2) \cdot JxK) + a_1) \cdot JxK + a_0$,&lt;/p&gt;
&lt;p&gt;这需要3次标准的共享乘法。BOLT通过Motzkin多项式预处理将标准乘法的次数减少到2次。相比之下，在BumbleBee中，我们倾向于利用平方操作，因为执行平方操作的成本是标准乘法操作的一半。例如，我们需要两个平方操作（用于二次项和四次项）以及“一半”的标准乘法（用于三次项）来评估四次多项式，更不用说更高效的共享乘法协议了。我们评估低次多项式的方式也比BOLT使用的霍纳法则更高效，尤其是在私有计算的背景下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lower Communication OT&lt;/strong&gt;
最后一个优势来自于OT协议的具体选择。具体来说，BOLT选择使用IKNP OT [38]作为其基础OT协议，而我们则利用Ferret OT [77]。Ferret OT的通信开销比IKNP OT小，但代价是更多的本地计算。我们在工程上进行了努力，将Ferret OT集成到多核CPU中，以最小化其运行时间，包括跨多个线程的同步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;从上述总结来看&lt;/strong&gt;，我们强调了支持环模数的矩阵乘法协议的重要性。正是我们乘法协议中的共享类型一致性，使得BumbleBee能够显著超越BOLT。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;私有变换器推理 ((2+1)-PC)&lt;/strong&gt;
最近的研究，如[48]、[31]、[67]、[30]，已经探讨了双方（例如S和C）可以访问受信任第三方（TTP）帮助的场景。这些方法通常将私有计算过程分为两个独立的阶段：初步预处理阶段，随后是评估阶段。值得注意的是，在预处理阶段，TTP负责生成并分发双方随后在评估阶段使用的所有相关随机性。参照[67]的命名，我们将这种TTP辅助的计算过程称为(2+1)-PC，以便与我们的2PC设置清晰区分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;私有变换器推理 (3PC)&lt;/strong&gt;
PrivFormer [3] 和 PUMA [21] 是为变换器设计的两种基于三方设置 [4]、[56] 的私有推理框架。具体来说，PrivFormer通过使用MPC友好的替代方案（即ReLU注意力机制）替代了softmax注意力，但这需要对模型进行微调。PUMA与BumbleBee共享一些设计元素，如GeLU的分段近似和指数函数的泰勒近似。简而言之，这些3PC方法不使用OT和HE，但通信开销比我们的方案要大。其他相关工作[79]、[48]、[3]考虑了使用不同近似结构的替代模型，这些模型在MPC中更易计算。然而，已有研究表明粗略的近似可能会显著降低模型的准确性[43]，因此他们的工作需要进行模型微调。BumbleBee的一个优势是它不需要任何模型微调；相反，我们的重点是在给定的预训练变换器模型上进行私有推理。值得注意的是，我们的方法也可以适配到他们的模型中，从而实现更好的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VII. 评估&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型与数据集&lt;/strong&gt;
我们在5个变换器模型上评估BumbleBee，包括四个NLP模型，即BERT-base、BERT-large [19]、GPT2-base [15]、LLaMA-7B [69]，以及一个计算机视觉模型ViT-base [74]、[22]。这些模型通过三个超参数进行参数化：块数B、表示的维度D和头数H。我们直接复用了来自公开源的训练模型。为了展示BumbleBee的有效性，我们在4个数据集上进行了私有推理，包括来自GLUE基准的CoLA、RTE和QNLI（用于NLP任务），以及用于图像分类的ImageNet-1k [64]。具体来说，ImageNet-1k数据集是一个1000个不同类别的分类任务，而来自GLUE基准的三个数据集是二分类任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;评估指标&lt;/strong&gt;
在2PC设置下，我们不区分“离线”和“在线”成本，如一些先前的(2+1)-PC工作所做的那样[55]、[30]。我们报告端到端的运行时间，包括通过网络传输密文的时间。但我们没有包括从硬盘加载模型所花费的时间。我们测量了总通信量，包括双方发送的所有消息。我们使用1GB = 210MB = 230字节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;实验环境&lt;/strong&gt;
本文描述的实验主要在两台阿里巴巴云实例（ecs.g7.16xlarge）上进行，这些实例配备了64个vCPU，主频为2.70GHz，内存为256GB。我们尽可能使用多线程。为了模拟不同的网络条件，我们通过Linux中的流量控制命令操控了云实例之间的带宽。具体而言，我们在两种网络设置下进行了基准测试：局域网（LAN），带宽为1Gbps（单向1Gbps），延迟为0.5ms，以及广域网（WAN），带宽为400Mbps，延迟为4ms。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;具体参数&lt;/strong&gt;
我们将秘密共享的l设置为64，将定点精度f设置为18。在对GeLU和指数运算进行混合环优化时，我们将l′设置为32，f′设置为12。具体来说，我们对BERT-base、GPT2-base和ViT-base模型应用了GeLU和指数运算的混合环优化，而对于其他较大的模型，我们仅对GeLU/SiLU激活进行优化。我们扩展了Yacl库中的Ferret实现[1]，以支持各种应用级的OT类型，例如M 1-OT6。对于RLWE，我们使用SEAL库[65]并借助[37]加速在Intel CPU上的计算。对于近似小于运算，我们在50位输入上评估了MSB协议，以进行(2)中的区段选择。对于近似指数运算，我们将Texp设置为-14，并使用n = 6。有关实现参数的更多细节，请参见附录。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;可用性&lt;/strong&gt;
我们提供了可复现的实现，网址为：https://github.com/AntCPLab/OpenBumbleBee。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>第5周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC5%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</link>
        <pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC5%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E7%AC%AC5%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/ai-generated-7926621_1280.jpg" alt="Featured image of post 第5周工作总结" /&gt;&lt;h1 id=&#34;总结&#34;&gt;🎵总结
&lt;/h1&gt;&lt;p&gt;这一周没干啥正经事，周一周二库库读了两天论文，初步了解了一下安全推理的基础知识（就看了一点点），周三图书馆闭馆，晚上上课，所以周三休息一天，周三下午参加了理想汽车AI算法实习生的面试，想找一个这种实习边干活边学习，可惜这并不是啥也不会就能行的，还得学习，沉淀一下。一开始挺紧张的，在脑海里思考了好多遍我该怎么解释，表现自己，结果面试官让我开始自我介绍，他就一只在笑😁（牙很齐很白），不知道他在笑什么，但是看他笑得那么开心我就不紧张了，问了一下transformer为什么要加入位置编码来表示位置信息，LLM微调除了LoRA还有什么，还了解那些大模型，一个也不会，哈哈哈，真不是啥也不会就可以找工作的，不过此行并非失败，而是挺有收获的，面试官一直说，没关系，不会咱就换一个，下去把这个搞清楚就行了，搞得我有点不好意思，在问问题的过程中，面试官还顺带给我解释解释，增长了一些知识，最后我反问到该怎么系统的学习一下才能找到实习（我真是个鬼才🤣，问面试官这问题）&lt;/p&gt;
&lt;p&gt;面试官说：建议我先不要着急找面试，用几个月学习一下，把基础的机器学习和深度学习过一遍，代码复现都能看懂，还问了我研究生方向是大模型安全，看看这方面的论文，把基础知识掌握扎实，上Kaggle上参加或者复现两个项目放到简历里，会更加出色。&lt;/p&gt;
&lt;p&gt;非常感谢面试官的指导，没有想象中的KPI面试，不会有脸色看，更像是前辈教育后辈如何学习🤩。&lt;/p&gt;
&lt;p&gt;周四上午起了个大早直奔图书馆找同学，同学给占了一个之前我没有坐过的位置，有点风，恰好我没有棉袄，差点冻死，于是下午就回了宿舍，周四周五周六就在宿舍待着，写软件工程的文档，哎呦，忒难写，还得画图，周日棉袄到位，在图书馆待了一天，把所有文档完结，做了一个PPT，顺手的事。&lt;/p&gt;
&lt;p&gt;于是这一周就这么过完了&lt;/p&gt;
&lt;h2 id=&#34;阅读&#34;&gt;🎧阅读
&lt;/h2&gt;&lt;p&gt;Secure and Private Machine Learning: A Survey of  Techniques and Applications&lt;/p&gt;
&lt;p&gt;Differential Privacy: A Survey of Results&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;计划&#34;&gt;🥁计划
&lt;/h2&gt;&lt;p&gt;计划就是完成上周未完成的计划，开干！&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文综述了在机器学习中实现安全和隐私保护的各种方法，包括使用同态加密和安全多方计算的技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Inference with Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Inference of Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Machine Learning: A Practical Approach&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Outsourced Computation in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Differentially Private Inference in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Towards Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Model Inference: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
    </channel>
</rss>
