<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>WeeklyReport on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/categories/weeklyreport/</link>
        <description>Recent content in WeeklyReport on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Sun, 15 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/categories/weeklyreport/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>第8周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC8%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</link>
        <pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC8%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E7%AC%AC8%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/flowers-8046973_1280.jpg" alt="Featured image of post 第8周工作总结" /&gt;&lt;h1 id=&#34;总结&#34;&gt;总结
&lt;/h1&gt;&lt;p&gt;好多论文里的东西都看不懂，要不是缺大模型的知识，要不就是缺密码学的知识，虽然我想象力丰富，但我对于那些没亲手经历过得实验真的有点想象不到，大学没有学ai的弊病在此刻体现的淋漓尽致。需要想想办法了。&lt;/p&gt;
&lt;h2 id=&#34;ciphergpt-secure-two-party-gpt-inference&#34;&gt;&lt;strong&gt;CipherGPT: Secure Two-Party GPT Inference&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;本文提出了&lt;strong&gt;CipherGPT&lt;/strong&gt;，这是首个用于安全两方 GPT 推理的框架。该框架通过创新协议解决了大语言模型推理中隐私泄露的问题，尤其适用于一个持有私密数据、另一个持有大语言模型的场景。核心贡献包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出了专为 GPT 推理定制的安全矩阵乘法，提升了 2.5 倍的速度和 11.2 倍的带宽节省。&lt;/li&gt;
&lt;li&gt;提出了用于安全计算 GELU 的新协议，在运行时、通信和精度上分别提高了 4.2 倍、3.4 倍和 10.9 倍。&lt;/li&gt;
&lt;li&gt;开发了首个用于 top-k 采样的协议，并进行了完整的实现和基准测试，证明了其在隐私保护和性能方面的优势。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个新的GPT模型，里面有的细节知识不懂，VOLE，TOP-K采样需要继续了解学习。&lt;/p&gt;
&lt;h2 id=&#34;dp-forward-fine-tuning-and-inference-on-language-models-with-differential-privacy-in-forward-pass&#34;&gt;&lt;strong&gt;DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;该论文提出了&lt;strong&gt;DP-Forward&lt;/strong&gt;，一种结合差分隐私（DP）技术的语言模型微调与推理方法。通过在前向传播过程中应用差分隐私，DP-Forward 能够有效保护数据隐私，同时保证模型的性能。与传统方法相比，DP-Forward 采用了创新的隐私保护机制，确保了微调和推理过程中用户数据的安全。&lt;/p&gt;
&lt;p&gt;《DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass》提出了一种在大规模预训练语言模型中集成差分隐私（Differential Privacy, DP）的方法，重点在于保护用户隐私的同时，不影响模型的训练和推理效果。该方法通过在模型的前向传播（forward pass）阶段引入差分隐私机制，从而实现隐私保护。&lt;/p&gt;
&lt;h3 id=&#34;背景&#34;&gt;背景
&lt;/h3&gt;&lt;p&gt;差分隐私是一种数学定义，用来衡量在数据分析过程中是否泄露了个体的隐私。它的核心思想是通过在数据中添加噪声，确保即使外部观察者获得了分析结果，也无法推断出任何个体的敏感信息。在深度学习中，差分隐私技术通常通过在梯度更新中加入噪声来防止隐私泄露。&lt;/p&gt;
&lt;p&gt;传统的差分隐私方法通常在训练过程中对梯度或参数进行扰动（例如，添加噪声），这会影响模型的性能。&lt;code&gt;DP-Forward&lt;/code&gt;方法则选择在推理过程中添加隐私保护，从而避免对训练过程造成干扰，并使得隐私保护更加精细和局部化。&lt;/p&gt;
&lt;h3 id=&#34;主要贡献&#34;&gt;主要贡献
&lt;/h3&gt;&lt;p&gt;&lt;code&gt;DP-Forward&lt;/code&gt;提出了一种新的差分隐私框架，它在&lt;strong&gt;前向传播&lt;/strong&gt;阶段引入了差分隐私机制，而不是在传统的反向传播阶段进行噪声注入。这样可以在不影响训练过程的情况下保护推理过程中涉及的用户数据的隐私。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;在前向传播中引入差分隐私&lt;/strong&gt;： 在标准的神经网络中，前向传播是将输入数据传递通过各层网络，最终输出预测结果。在&lt;code&gt;DP-Forward&lt;/code&gt;方法中，差分隐私噪声被添加到前向传播的计算过程中，而不是在反向传播阶段。这意味着模型在进行推理时会生成含有噪声的输出，从而保障输入数据的隐私。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;噪声添加方式&lt;/strong&gt;： 具体来说，在前向传播过程中，模型会对每个输入的隐私数据应用特定的噪声生成机制。噪声的强度会根据差分隐私的定义来调节，通常需要满足特定的隐私保证，如$\epsilon$-差分隐私（epsilon-DP）。这个噪声机制会确保即使外部观察者获取了模型输出，他们也无法推测出关于输入数据的任何敏感信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;噪声设计&lt;/strong&gt;： 添加的噪声设计上需要权衡隐私性和准确性。噪声的方差和尺度需要与隐私预算（privacy budget）$\epsilon$的要求相匹配。较高的隐私预算意味着噪声较小，但隐私保护较弱；较低的隐私预算则增强了隐私保护，但可能导致模型输出的准确性下降。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐私保护与性能的平衡&lt;/strong&gt;： 在设计时，&lt;code&gt;DP-Forward&lt;/code&gt;注重在隐私保护和模型性能之间找到平衡点。通过对前向传播过程的噪声控制，能够在保护隐私的同时，最大限度地保留模型的推理精度。这个平衡通常依赖于多个因素，包括模型架构、噪声级别和隐私预算。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;工作流程&#34;&gt;工作流程
&lt;/h3&gt;&lt;p&gt;&lt;code&gt;DP-Forward&lt;/code&gt;的工作流程可以分为以下几个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;模型预训练&lt;/strong&gt;： 和传统的预训练语言模型一样，首先会对模型进行大规模预训练，通常在没有隐私保护的环境下进行。这一步骤仍然是标准的模型训练过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;推理过程中的隐私保护&lt;/strong&gt;： 在推理阶段，当模型接收到用户输入时，会在前向传播过程中注入差分隐私噪声。这确保了输出不包含关于输入的精确信息，从而保护了输入数据的隐私。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐私预算的分配&lt;/strong&gt;： 需要根据预设的隐私预算（$\epsilon$）来确定噪声的大小。隐私预算决定了模型的隐私保护强度，通常在多个推理请求中共享隐私预算，从而在多个推理过程中分配适当的噪声。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出&lt;/strong&gt;： 模型经过前向传播处理后，产生带有噪声的推理结果，输出给用户。由于噪声的加入，推理结果不会泄露输入数据的敏感信息。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;关键技术和挑战&#34;&gt;关键技术和挑战
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;隐私预算管理&lt;/strong&gt;： 隐私预算$\epsilon$是控制差分隐私强度的关键。需要在多个推理请求中合理管理隐私预算，以确保在保证隐私的同时，模型能够提供有用的推理结果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;噪声添加的影响&lt;/strong&gt;： 在前向传播中加入噪声可能导致推理结果的精度下降。如何设计合适的噪声生成机制，以平衡隐私保护与推理效果之间的矛盾，是实现这一方法的关键挑战之一。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型架构调整&lt;/strong&gt;： 有时需要对模型进行轻微的架构调整，以便更好地与差分隐私机制兼容，尤其是在推理过程中涉及大量用户输入的情况下。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;适应性和灵活性&lt;/strong&gt;： 该方法的优点在于其灵活性，它能够适应多种不同的应用场景，不仅限于文本生成，也可以应用于其他类型的推理任务，如图像识别、语音识别等。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;总结-1&#34;&gt;总结
&lt;/h3&gt;&lt;p&gt;&lt;code&gt;DP-Forward&lt;/code&gt;提供了一种新的差分隐私集成方式，专注于在推理过程中保护用户隐私。这种方法的优势在于，它不需要改变模型的训练过程，而是在推理阶段添加噪声保护隐私。通过合适的噪声机制和隐私预算管理，&lt;code&gt;DP-Forward&lt;/code&gt;能够在保证隐私的同时，最大程度上保持推理精度。这对于保护敏感数据的隐私，同时在实际应用中维持良好的性能，提供了有力的支持。&lt;/p&gt;
&lt;p&gt;对我来说还是没有达到我的目的（不给模型方数据），这个是给了，让模型方保护，导致输出是经过差分隐私的，感觉没啥用。&lt;/p&gt;
&lt;h2 id=&#34;incognitext-privacy-enhancing-conditional-text-anonymization-via-llm-based-private-attribute-randomization&#34;&gt;&lt;strong&gt;IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;IncogniText&lt;/strong&gt; 提出了一种基于 LLM 的文本匿名化方法，旨在通过条件随机化隐私属性来增强文本隐私保护。这种方法通过将敏感属性进行随机化处理，确保用户隐私得到有效保护，同时保持文本的上下文和语义连贯性，适用于需要隐私保护的文本生成和处理任务。&lt;/p&gt;
&lt;p&gt;《IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization》提出了一种创新的方法，旨在通过基于大型语言模型（LLM）的隐私增强条件文本匿名化技术来保护文本中的敏感信息。该方法通过结合条件生成和隐私保护机制，有效地隐藏文本中的私密属性，同时保持文本的可读性和有效性。&lt;/p&gt;
&lt;h3 id=&#34;论文概述&#34;&gt;论文概述
&lt;/h3&gt;&lt;p&gt;在现代自然语言处理（NLP）应用中，文本数据通常包含敏感信息（如个人身份、地理位置、健康状况等），这些信息在处理和共享时需要得到保护。传统的文本匿名化方法通常通过替换、删除或模糊敏感信息来实现隐私保护，但这些方法往往会影响文本的上下文和可用性，甚至可能导致文本内容失真。为了克服这一挑战，&lt;strong&gt;IncogniText&lt;/strong&gt; 提出了基于大型语言模型（LLM）进行的条件文本匿名化，通过“私密属性随机化”（Private Attribute Randomization）技术，在不丧失文本质量和结构的情况下保护隐私。&lt;/p&gt;
&lt;h3 id=&#34;主要贡献-1&#34;&gt;主要贡献
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;隐私增强的文本匿名化&lt;/strong&gt;： 该论文提出了一种新的文本匿名化方法，称为&lt;strong&gt;IncogniText&lt;/strong&gt;，它通过结合&lt;strong&gt;条件生成模型&lt;/strong&gt;（conditional generation model）和&lt;strong&gt;隐私保护&lt;/strong&gt;技术来实现文本中敏感属性的匿名化。与传统的替换方法不同，IncogniText能够根据上下文生成与原始文本相似的、具有隐私保护的替代信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;私密属性随机化（Private Attribute Randomization）&lt;/strong&gt;： 论文的核心创新是私密属性随机化，它通过LLM生成新的替代值来替换文本中的私密属性（如名字、地址、电话号码等），同时确保这些替代值不会泄露任何关于原始信息的敏感数据。这种方法不仅能保护隐私，还能保持文本的流畅性和语义完整性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;条件文本生成&lt;/strong&gt;： IncogniText使用条件生成技术来确保生成的匿名文本符合特定的上下文要求。这意味着替代的隐私信息需要满足与原始文本相似的语法结构、语义逻辑和上下文约束。这一技术的关键在于保持文本的自然性和连贯性，同时去除敏感信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;保护隐私而不失文本有效性&lt;/strong&gt;： 传统的匿名化技术（如直接删除或替换敏感词）可能会导致文本的可读性降低或语义失真。IncogniText通过对敏感信息进行“随机化”而非简单替换，确保了文本在隐私保护的同时，仍然可以保留原始文本的有效性和可读性。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;方法详解&#34;&gt;方法详解
&lt;/h3&gt;&lt;p&gt;IncogniText的技术流程可以分为以下几个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;输入文本分析&lt;/strong&gt;： 首先，IncogniText对输入文本进行分析，识别出其中的私密属性。这些私密属性通常包括个人身份信息、地址、电话号码、电子邮件等。在这一步，系统使用预训练的实体识别模型（NER）或其他信息提取技术来自动识别文本中的敏感信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;条件生成模型&lt;/strong&gt;： 对于每个私密属性，IncogniText使用大型语言模型（LLM）来生成一个新的、上下文相关的替代值。例如，如果文本中出现了某个人名，模型会根据周围的上下文生成一个与原始名字语义相近但不真实的名字。这种替代值不仅能够隐藏敏感信息，还能保持文本的流畅性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;私密属性随机化&lt;/strong&gt;： 在生成替代值的过程中，IncogniText应用私密属性随机化的技术，即通过扰动和随机化原始敏感信息的某些部分，从而生成一个“新的”值，而这个新值在上下文中看起来与原始文本高度相似，但不会泄露原始的敏感信息。例如，在生成替代的地址时，系统可以基于地理信息来产生与原始地址相似但并不指向同一位置的地址。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出生成&lt;/strong&gt;： 最终，IncogniText会将修改后的文本重新组合，并确保替代信息符合语法和上下文的要求。这些生成的文本仍然具有可读性和有效性，并且隐私得到有效保护。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;隐私保护机制&#34;&gt;隐私保护机制
&lt;/h3&gt;&lt;p&gt;IncogniText通过以下几个方面加强隐私保护：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;隐私预算控制&lt;/strong&gt;： 隐私保护的效果依赖于生成过程中的随机化程度。为了确保敏感信息不会被泄露，IncogniText采用了类似于&lt;strong&gt;差分隐私&lt;/strong&gt;的隐私预算机制，通过对生成内容的扰动来控制隐私泄露的风险。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;上下文一致性&lt;/strong&gt;： 生成的替代文本与原始文本在语义和上下文中高度一致，这不仅保证了文本的自然性，也避免了由于生成文本过于生硬或不合适而泄露隐私的风险。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加密和安全性&lt;/strong&gt;： 在敏感文本处理过程中，IncogniText可能会采用加密技术来进一步确保数据在传输和存储过程中的安全性，特别是在面临潜在的恶意攻击时。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;评估与实验&#34;&gt;评估与实验
&lt;/h3&gt;&lt;p&gt;论文通过一系列的实验证明了IncogniText的有效性，评估了其在多个标准文本数据集上的表现，包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;文本可读性和质量&lt;/strong&gt;： 实验表明，IncogniText能够生成高质量的匿名化文本，且文本的流畅性、语法正确性与原始文本几乎没有差异。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐私保护效果&lt;/strong&gt;： 通过与传统的匿名化技术（如替换法）进行对比，IncogniText显著提高了隐私保护效果，减少了敏感信息泄露的风险。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型性能&lt;/strong&gt;： 在性能评估中，IncogniText在多个大型语言模型上进行测试，表现出较好的鲁棒性和较低的计算成本。即便是在处理较大规模的文本时，模型依然能够有效地保护隐私。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;结论&#34;&gt;结论
&lt;/h3&gt;&lt;p&gt;《IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization》提出了一种创新的隐私保护方法，通过基于大型语言模型的条件生成和私密属性随机化技术，实现了在不损害文本质量和语义的前提下，有效地保护文本中的敏感信息。该方法为文本匿名化技术提供了一种新的思路，尤其适用于需要保留上下文信息和语义流畅性的隐私保护场景。&lt;/p&gt;
&lt;p&gt;这个也不是我想要的，这就像是在做文本清洗，去除敏感信息。&lt;/p&gt;
&lt;h2 id=&#34;permllm-private-inference-of-large-language-models-within-3-seconds-under-wan&#34;&gt;&lt;strong&gt;PermLLM: Private Inference of Large Language Models within 3 Seconds under WAN&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;PermLLM&lt;/strong&gt; 提出了一个针对大语言模型的隐私保护推理框架，能够在广域网（WAN）环境下实现 3 秒内的快速推理。该框架通过高效的加密技术和优化的通信协议，确保在保护用户数据隐私的前提下，模型推理能够快速完成，具有较好的可扩展性和实时性。&lt;/p&gt;
&lt;p&gt;《PermLLM: Private Inference of Large Language Models within 3 Seconds under WAN》提出了一种新颖的方案——&lt;strong&gt;PermLLM&lt;/strong&gt;，旨在解决在广域网（WAN）环境下进行大语言模型（LLM）推理时的隐私保护问题。论文的重点是如何在保护隐私的同时，提高大语言模型推理的效率，特别是在带宽较低的环境（如WAN）下，确保推理结果能够在3秒内返回，同时有效保证用户数据的隐私。&lt;/p&gt;
&lt;h3 id=&#34;论文概述-1&#34;&gt;论文概述
&lt;/h3&gt;&lt;p&gt;随着大语言模型（LLM）的广泛应用，推理（Inference）过程中的隐私保护变得尤为重要。在许多应用场景中，尤其是在涉及用户敏感数据时，如何确保输入的隐私不被泄露是一个关键问题。与此同时，随着云计算服务的普及，越来越多的推理请求都通过网络传输，而网络带宽（尤其是在广域网WAN环境中）较低时，如何保证推理速度和隐私保护之间的平衡，成为了一个亟待解决的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PermLLM&lt;/strong&gt;通过创新的技术解决了这两个挑战，提出了一个高效且隐私保护的推理框架。其目标是在保证隐私的同时，使得大语言模型能够在网络延迟较高的环境下仍然实现快速推理。&lt;/p&gt;
&lt;h3 id=&#34;主要贡献-2&#34;&gt;主要贡献
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;隐私保护的推理框架&lt;/strong&gt;： 论文提出的&lt;strong&gt;PermLLM&lt;/strong&gt;框架致力于在保护用户隐私的前提下，通过使用加密和分布式计算技术，提供快速且安全的推理结果。与传统的隐私保护方法（如同态加密和多方计算）相比，PermLLM在推理效率上具有明显的优势，尤其是在高延迟的WAN环境中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;高效的推理时间&lt;/strong&gt;： PermLLM的核心创新之一是其在&lt;strong&gt;WAN环境下&lt;/strong&gt;提供&lt;strong&gt;3秒内推理&lt;/strong&gt;的能力。这一性能的突破主要得益于该框架的高效网络协议和加密技术，使得推理请求能够在低带宽和高延迟的环境下迅速完成。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于Permute机制的私密计算&lt;/strong&gt;： 该框架采用了一种称为&lt;strong&gt;Permute（Perm）*&lt;em&gt;的机制，它结合了特定的*&lt;em&gt;隐私保护算法&lt;/em&gt;&lt;/em&gt;（如加密、随机化）和&lt;/strong&gt;推理优化策略**，使得在数据传输过程中不会暴露敏感信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WAN环境中的优化&lt;/strong&gt;： WAN环境中由于网络延迟和带宽的限制，传统的推理方法很难满足快速响应的需求。PermLLM通过对数据流和计算流程的高度优化，显著降低了因网络延迟导致的时间浪费，保证了用户能够在较短时间内获得推理结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;方法详解-1&#34;&gt;方法详解
&lt;/h3&gt;&lt;p&gt;PermLLM框架的工作流程包括以下几个关键步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;输入数据的加密处理&lt;/strong&gt;： 在PermLLM中，用户的数据（如文本、查询等）会在传输前进行加密处理。加密算法可以防止用户输入在传输过程中泄露给服务提供者或中间人。通常使用对称加密或非对称加密技术来确保输入数据的机密性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据传输与隐私保护&lt;/strong&gt;： 在PermLLM框架中，输入数据通过加密的通道传输到远端服务器。在传输过程中，数据经过特定的隐私保护机制（如数据混淆或扰动）处理，以确保即使网络传输过程被监听或攻击，敏感信息依然不会泄露。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Permute机制的应用&lt;/strong&gt;： &lt;strong&gt;Permute&lt;/strong&gt;是PermLLM框架中的一个关键隐私保护机制。它通过对输入数据进行“随机化”或“重排列”，生成多个可能的版本，而每个版本的输出结果仍然能够与原始数据保持一致，但由于其加密和随机化处理，第三方无法直接推测出原始数据的内容。通过这种方式，PermLLM有效保证了用户数据在整个推理过程中的隐私。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;推理过程的优化&lt;/strong&gt;： PermLLM使用了一种高效的推理优化算法，能够在短时间内完成对输入数据的推理。该算法通过优化模型的计算过程（如减少计算节点的数量或对模型的参数进行压缩）来加快推理速度。此外，该框架还通过网络协议的优化，减少了数据传输的延迟。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;快速反馈与结果解密&lt;/strong&gt;： 在模型推理完成后，结果会通过加密传输回用户端，用户端再进行解密操作。解密后的推理结果被返回给用户，整个过程的响应时间被优化至&lt;strong&gt;3秒内&lt;/strong&gt;，即使是在带宽有限、网络延迟较高的WAN环境中。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;隐私保护机制-1&#34;&gt;隐私保护机制
&lt;/h3&gt;&lt;p&gt;PermLLM采用了多种技术来确保隐私保护：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;同态加密（Homomorphic Encryption）&lt;/strong&gt;： 同态加密是一种允许在加密数据上进行计算的技术。通过在加密数据上直接执行推理计算，PermLLM能够确保数据在整个推理过程中不会被解密或暴露，只有最终的推理结果会被解密并返回给用户。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;安全多方计算（Secure Multi-Party Computation, SMPC）&lt;/strong&gt;： 在某些情况下，PermLLM还可能使用SMPC技术，特别是在需要多个计算节点共同处理推理任务时。SMPC允许多个参与方协同计算，但每个参与方的输入数据始终保持私密，且无法泄露其他参与方的敏感信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐私预算控制与分配&lt;/strong&gt;： 为了确保隐私保护不会过度影响推理效率，PermLLM框架中还包含了隐私预算控制机制，允许用户根据需要灵活调整隐私保护级别和计算效率之间的平衡。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;私密数据传输&lt;/strong&gt;： 在数据传输过程中，PermLLM采用了强加密技术，如TLS协议和端到端加密，确保数据在传输过程中不会被窃听或篡改。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;实验与评估&#34;&gt;实验与评估
&lt;/h3&gt;&lt;p&gt;论文中展示了PermLLM在多个实际场景中的表现，包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;性能评估&lt;/strong&gt;： 在WAN环境下，PermLLM展示了其在3秒内完成推理的能力。实验结果表明，即使在带宽有限的条件下，PermLLM也能够保持低延迟和高吞吐量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐私保护效果&lt;/strong&gt;： 通过与传统的隐私保护方法（如普通加密）进行对比，PermLLM显著提高了隐私保护效果，同时保持了推理效率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;网络条件适应性&lt;/strong&gt;： 在不同的网络条件（如高延迟和低带宽）下，PermLLM展现了出色的适应能力，能够调整其网络协议和计算资源，确保推理时间保持在3秒内。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;总结-2&#34;&gt;总结
&lt;/h3&gt;&lt;p&gt;《PermLLM: Private Inference of Large Language Models within 3 Seconds under WAN》提出了一个创新的隐私保护推理框架PermLLM，它通过采用加密技术、优化推理过程和引入&lt;strong&gt;Permute&lt;/strong&gt;隐私机制，成功地实现了在WAN环境下保护用户隐私的同时，提供高效的推理服务。该方法的突破性贡献在于，它不仅能够保证隐私保护的强度，还能够确保推理效率满足快速响应的需求（3秒内）。PermLLM在大语言模型的应用场景中具有广泛的潜力，特别是在需要保护用户隐私的环境中。&lt;/p&gt;
&lt;p&gt;写的太高深了，丁点看不懂，算了下一个&lt;/p&gt;
&lt;h2 id=&#34;pfid-privacy-first-inference-delegation-framework-for-llms&#34;&gt;&lt;strong&gt;PFID: Privacy First Inference Delegation Framework for LLMs&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;论文提出了&lt;strong&gt;PFID&lt;/strong&gt;，一个针对大语言模型的隐私优先推理委托框架。该框架通过将推理过程分委托给第三方服务器来实现隐私保护，同时避免暴露用户数据。PFID 结合了隐私保护与推理效率，能够在确保隐私的同时提供高效的推理服务，适用于跨域的 LLM 应用场景。&lt;/p&gt;
&lt;p&gt;《PFID: Privacy First Inference Delegation Framework for LLMs》这篇论文提出了一种新的框架——&lt;strong&gt;PFID&lt;/strong&gt;（&lt;strong&gt;Privacy First Inference Delegation&lt;/strong&gt;），旨在解决在使用大语言模型（LLM）时的隐私保护问题，尤其是在需要将推理任务委托给外部计算资源（如云端服务）时，确保用户隐私不被泄露。&lt;/p&gt;
&lt;p&gt;PFID框架的核心理念是将推理任务委托给外部计算资源的同时，最大限度地保护用户的私人数据，避免数据泄露和滥用。为了实现这一目标，PFID设计了一整套机制，结合了数据保护技术、隐私保障协议以及高效的推理方法。&lt;/p&gt;
&lt;h3 id=&#34;论文概述-2&#34;&gt;论文概述
&lt;/h3&gt;&lt;p&gt;随着大语言模型（LLM）在各种应用中（如智能客服、内容生成等）取得突破性进展，越来越多的用户希望利用外部服务提供商的计算资源来进行推理。然而，使用外部计算资源时，如何保护用户数据的隐私成为了一个重要问题，特别是在云计算环境中，用户的输入数据和推理过程可能会暴露给服务提供商，造成隐私泄漏的风险。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PFID&lt;/strong&gt;框架通过将推理任务委托给外部计算资源时采用隐私保护技术，确保用户数据在整个推理过程中始终处于受保护的状态，防止敏感信息泄露。&lt;/p&gt;
&lt;h3 id=&#34;主要贡献-3&#34;&gt;主要贡献
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;隐私优先的推理任务委托&lt;/strong&gt;： PFID的核心思想是确保推理过程中的隐私保护，设计了一种&lt;strong&gt;隐私优先&lt;/strong&gt;的推理任务委托机制。用户可以将LLM的推理任务委托给外部计算资源，但通过加密、隔离等手段，确保输入数据不被泄露给服务提供商。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;推理过程中的隐私保护&lt;/strong&gt;： 论文提出了多种隐私保护技术来保证推理过程中敏感信息不会泄露。这些技术包括但不限于加密技术、同态加密（HE）、安全多方计算（SMPC）等，可以防止外部计算节点访问用户的原始数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;高效的隐私保护协议&lt;/strong&gt;： PFID不仅关注隐私保护的强度，还特别关注推理的高效性。它设计了一种高效的隐私保护协议，能够在确保隐私的前提下，最大程度地减少加密和计算过程中的延迟，保证推理任务的快速响应。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多方合作的隐私保护&lt;/strong&gt;： PFID框架考虑到多方协作的情况，尤其是在分布式计算环境中，多个计算节点可能共同完成推理任务。在这种情况下，PFID提供了隐私保护协议，确保每个节点都无法访问完整的数据，仅能访问必要的计算部分，从而有效降低泄露风险。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;方法详解-2&#34;&gt;方法详解
&lt;/h3&gt;&lt;p&gt;PFID框架的设计包括以下几个关键技术组件和流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;任务分配与委托&lt;/strong&gt;： 在PFID框架中，用户首先将推理任务分配给外部计算资源（如云计算服务）。这一步骤需要确保在任务委托过程中，用户数据保持加密状态，任何未经授权的计算节点都不能访问原始数据。为此，PFID框架采用了&lt;strong&gt;加密协议&lt;/strong&gt;，即使在云端计算过程中，用户数据也始终保持加密状态。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;隐私保护协议&lt;/strong&gt;： PFID提出了一种&lt;strong&gt;多层加密隐私保护协议&lt;/strong&gt;，该协议包括以下几个主要组成部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入数据加密&lt;/strong&gt;：用户的输入数据（如文本数据）在本地设备上加密后传输到云端进行推理。加密技术可以采用对称加密、非对称加密或同态加密等技术，确保数据在整个推理过程中的安全。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;同态加密（Homomorphic Encryption, HE）&lt;/strong&gt;：同态加密是支持在加密数据上直接计算的加密方式。PFID使用同态加密来确保即使云计算节点无法查看用户数据，也能对加密数据进行推理操作，从而避免数据泄露。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;安全多方计算（Secure Multi-party Computation, SMPC）&lt;/strong&gt;：当多个计算节点共同完成推理任务时，PFID采用SMPC协议来确保每个节点只能获取部分计算结果，不能知道完整的输入数据或中间计算结果，从而有效地防止隐私泄露。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;快速推理优化&lt;/strong&gt;： 为了避免隐私保护过程中产生过多的计算开销，PFID框架设计了一种高效的推理优化机制。该机制通过优化加密计算的过程，减少加密和解密的开销，使得即使在隐私保护的前提下，推理速度也能保持较快。&lt;/p&gt;
&lt;p&gt;具体而言，PFID优化了以下方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;加密数据处理优化&lt;/strong&gt;：通过合理安排加密计算过程中的任务分配，减少了计算资源的浪费。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;带宽优化&lt;/strong&gt;：在数据传输过程中，PFID采用了高效的压缩和加密策略，减少了数据传输的时间和带宽消耗。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算节点协同&lt;/strong&gt;：在多方计算的环境下，PFID通过高效的任务分配和同步机制，确保各计算节点能够协同工作，减少等待时间，提高整体推理效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;隐私监控与反馈机制&lt;/strong&gt;： PFID框架提供了一个隐私监控和反馈机制，用户可以监控其数据在推理过程中的隐私保护状态。该机制通过日志和报告功能，让用户能够随时了解数据在推理过程中的安全性，确保其隐私得到充分保护。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;隐私保护技术&#34;&gt;隐私保护技术
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;加密技术&lt;/strong&gt;： PFID框架使用多种加密技术来保护用户数据：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;对称加密&lt;/strong&gt;：在客户端和服务器之间使用对称加密进行数据传输。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;非对称加密&lt;/strong&gt;：在云计算服务中使用非对称加密来确保数据的安全传输。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;同态加密&lt;/strong&gt;：通过同态加密，保证数据在加密状态下仍然能够被处理而不暴露原始数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多方计算（SMPC）&lt;/strong&gt;： 当推理任务需要多个计算节点协作时，PFID使用&lt;strong&gt;多方计算&lt;/strong&gt;协议，确保每个节点仅能访问一部分计算数据，不会泄露任何敏感信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐私预算&lt;/strong&gt;： 为了平衡隐私保护和计算效率，PFID框架引入了&lt;strong&gt;隐私预算&lt;/strong&gt;的概念。通过合理设置隐私预算，用户可以调整隐私保护的强度，以适应不同的应用场景和性能要求。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;实验与评估-1&#34;&gt;实验与评估
&lt;/h3&gt;&lt;p&gt;论文通过一系列实验，验证了PFID框架在隐私保护和推理效率方面的表现。实验结果表明：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;隐私保护效果&lt;/strong&gt;： PFID能够有效保护用户数据的隐私，尤其是在多方计算和加密操作中，避免了数据泄露的风险。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;推理效率&lt;/strong&gt;： 通过优化加密计算和网络传输，PFID在保证隐私的前提下，能够提供快速的推理结果，适用于需要实时响应的应用场景。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可扩展性&lt;/strong&gt;： PFID框架具有良好的可扩展性，可以在多个云计算服务之间进行部署，并在大规模并发请求的情况下保持高效性能。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;总结-3&#34;&gt;总结
&lt;/h3&gt;&lt;p&gt;《PFID: Privacy First Inference Delegation Framework for LLMs》提出的PFID框架是一个创新的隐私保护推理解决方案，旨在解决在委托外部计算资源进行大语言模型推理时的隐私保护问题。通过结合加密技术、同态加密、SMPC和隐私预算控制，PFID确保了用户数据的隐私不被泄露，同时提供了高效的推理服务。该框架在保护隐私的同时，能够满足实时响应需求，具有广泛的应用潜力，特别是在需要保护用户数据的敏感领域。&lt;/p&gt;
&lt;p&gt;这篇不错，专业对口，之后拿出来细品！&lt;/p&gt;
&lt;h2 id=&#34;privacyasst-safeguarding-user-privacy-in-tool-using-large-language-model-agents&#34;&gt;&lt;strong&gt;PrivacyAsst: Safeguarding User Privacy in Tool-Using Large Language Model Agents&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;PrivacyAsst&lt;/strong&gt; 旨在为使用工具的大语言模型代理提供隐私保护。该系统通过在模型执行过程中加入隐私保护机制，确保用户数据在与外部工具交互时得到有效保护。该方法通过多层次的隐私防护，避免用户数据泄露，特别适用于需要访问外部工具的 LLM 代理应用。&lt;/p&gt;
&lt;p&gt;这篇论文《PrivacyAsst: Safeguarding User Privacy in Tool-Using Large Language Model Agents》主要探讨了在大语言模型（LLM）代理系统中保护用户隐私的技术和方法。LLM代理系统是指能够通过语言模型（如GPT）与用户进行交互并调用外部工具（如搜索引擎、数据库等）来增强其能力的系统。由于这些系统涉及到与外部资源的交互，如何保护用户隐私变得尤为重要。&lt;/p&gt;
&lt;h3 id=&#34;论文的主要贡献与结构&#34;&gt;论文的主要贡献与结构
&lt;/h3&gt;&lt;h4 id=&#34;背景和动机&#34;&gt;&lt;strong&gt;背景和动机&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;现代大语言模型（如GPT等）在任务执行中常常依赖外部工具，例如搜索引擎、数据库访问、API调用等。这种功能使得模型能够扩展其能力，但同时也增加了隐私泄露的风险。用户的数据可能在调用这些工具时泄露，特别是在缺乏适当隐私保护的情况下。论文的动机就是提出一种方法，能够在LLM代理使用工具的过程中保护用户的隐私。&lt;/p&gt;
&lt;h4 id=&#34;privacyasst隐私保护助手&#34;&gt;&lt;strong&gt;PrivacyAsst：隐私保护助手&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;论文提出了一种名为&lt;strong&gt;PrivacyAsst&lt;/strong&gt;的隐私保护框架，它能够在LLM代理系统中保障用户隐私。具体来说，PrivacyAsst通过对模型与工具交互的监控和控制，确保用户的敏感数据不被泄露或不当使用。&lt;/p&gt;
&lt;h4 id=&#34;隐私泄露的风险&#34;&gt;&lt;strong&gt;隐私泄露的风险&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;论文分析了不同类型的隐私泄露风险，尤其是在LLM代理使用外部工具的过程中。例如，用户可能会在与代理的对话中无意中透露敏感信息，而这些信息会通过调用外部工具（如API或数据库）暴露给第三方。隐私泄露的场景包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用户输入的敏感信息被误用。&lt;/li&gt;
&lt;li&gt;外部工具无法有效保护数据，导致数据暴露。&lt;/li&gt;
&lt;li&gt;大语言模型本身在生成回答时可能泄露用户的私人信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;privacyasst的工作原理&#34;&gt;&lt;strong&gt;PrivacyAsst的工作原理&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;PrivacyAsst框架的核心机制包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;隐私敏感性检测&lt;/strong&gt;：框架能够自动识别用户输入中可能涉及隐私泄露的敏感信息。通过自然语言处理（NLP）技术，PrivacyAsst能检测出个人身份信息（PII）、财务信息、健康信息等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;信息屏蔽与替换&lt;/strong&gt;：在检测到敏感信息时，PrivacyAsst会对这些信息进行屏蔽或替换。例如，将用户输入中的具体地址或联系方式用占位符代替，避免在与外部工具的交互中泄露。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工具调用监控&lt;/strong&gt;：框架还可以监控LLM代理与外部工具的交互，确保敏感信息不被传递到这些工具中。如果需要调用工具但存在隐私泄露风险，PrivacyAsst会阻止该调用或采取适当的保护措施。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;用户控制与透明度&lt;/strong&gt;：用户可以设定隐私保护的策略，决定哪些信息可以被使用，哪些信息需要严格保护。此外，系统也会向用户提供透明的隐私保护措施，使用户能够理解和掌控自己的数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;实验与评估-2&#34;&gt;&lt;strong&gt;实验与评估&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;论文还进行了实验，评估PrivacyAsst框架的有效性。通过与传统的隐私保护方法进行对比，证明了PrivacyAsst能够有效地减少用户隐私泄露的风险，同时对LLM代理的功能影响最小。&lt;/p&gt;
&lt;p&gt;实验结果表明，PrivacyAsst能够显著降低敏感信息泄露的概率，且能够在不显著降低系统性能的情况下保证隐私保护。&lt;/p&gt;
&lt;h4 id=&#34;挑战与未来方向&#34;&gt;&lt;strong&gt;挑战与未来方向&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;论文最后讨论了隐私保护技术在工具使用大语言模型中的挑战和未来研究方向。这些挑战包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何应对更加复杂的隐私攻击，特别是在多方交互的情况下。&lt;/li&gt;
&lt;li&gt;如何平衡隐私保护与系统的功能性，避免过度限制工具的使用。&lt;/li&gt;
&lt;li&gt;如何设计更加智能和自适应的隐私保护系统，以应对不断变化的威胁模型和用户需求。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;总结-4&#34;&gt;总结
&lt;/h3&gt;&lt;p&gt;《PrivacyAsst: Safeguarding User Privacy in Tool-Using Large Language Model Agents》提出了一个针对大语言模型代理使用外部工具过程中的隐私保护框架。通过智能检测、信息屏蔽与替换、工具调用监控以及用户控制，PrivacyAsst能够有效防止用户敏感信息泄露。论文的贡献在于提供了一种实用的隐私保护方法，能够在增强LLM功能的同时，确保用户数据的安全与隐私。&lt;/p&gt;
&lt;p&gt;这个挺有意思，防止LLM在用工具的时候给自己隐私泄露了，但就感觉怪怪的&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;隐私敏感性检测&lt;/strong&gt;和&lt;strong&gt;信息屏蔽&lt;/strong&gt;方法已经在隐私保护领域使用多年，类似的技术早已被应用于各种隐私保护场景中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;工具调用监控&lt;/strong&gt;部分也是一种常见的技术，通常通过API网关或中间件进行操作，来确保数据不被误用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;实际应用的局限性&lt;/strong&gt;：外部工具的监控和数据屏蔽是有限的，很多隐私泄露的风险实际上可能来自于模型内部的知识泄漏，而不仅仅是外部工具的调用。隐私Asst没有讨论如何解决这些问题。&lt;/p&gt;
&lt;h2 id=&#34;privacyrestore-privacy-preserving-inference-in-large-language-models-via-privacy-removal-and-restoration&#34;&gt;&lt;strong&gt;PrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;本文提出了&lt;strong&gt;PrivacyRestore&lt;/strong&gt;，一种在推理过程中进行隐私移除和恢复的隐私保护方法。通过在推理的各个阶段移除敏感数据，并在推理完成后恢复数据的完整性，PrivacyRestore 能够有效保护用户隐私，同时保证推理结果的准确性和一致性。&lt;/p&gt;
&lt;p&gt;这篇论文《PrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration》提出了一种名为&lt;strong&gt;PrivacyRestore&lt;/strong&gt;的方法，用于在大语言模型（LLM）的推理过程中保护用户隐私。论文的目标是解决当前在大语言模型应用中，尤其是用户与模型交互时，隐私泄露的潜在风险。作者提出的方法通过&lt;strong&gt;隐私去除&lt;/strong&gt;和&lt;strong&gt;隐私恢复&lt;/strong&gt;技术，确保用户的敏感信息在推理过程中不被泄露，同时又不影响模型的预测性能。&lt;/p&gt;
&lt;h3 id=&#34;论文的主要贡献与结构-1&#34;&gt;论文的主要贡献与结构
&lt;/h3&gt;&lt;h4 id=&#34;背景和动机-1&#34;&gt;&lt;strong&gt;背景和动机&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;随着大语言模型的广泛应用，如何保护用户隐私在实际应用中变得至关重要。大语言模型的强大能力依赖于它们处理和生成大量信息，这些信息可能包括用户的个人数据。如果模型在推理时泄露了用户的私人信息，可能导致严重的隐私泄露问题。现有的隐私保护方法，如差分隐私，通常无法完全满足大语言模型推理过程中的隐私需求，尤其是当模型需要高质量的推理结果时。&lt;/p&gt;
&lt;p&gt;论文的动机是提出一个新的隐私保护框架，通过去除敏感信息并在推理后恢复隐私，解决现有隐私保护方法在大语言模型中的局限性。&lt;/p&gt;
&lt;h4 id=&#34;privacyrestore隐私去除和恢复&#34;&gt;&lt;strong&gt;PrivacyRestore：隐私去除和恢复&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;PrivacyRestore框架的核心思想是，在大语言模型的推理过程中，将用户的敏感信息从输入中去除，以防止在推理过程中泄露隐私。推理完成后，再将这些去除的隐私信息恢复回来，使得模型的输出既可以确保隐私保护，又能保持推理的准确性和相关性。&lt;/p&gt;
&lt;p&gt;PrivacyRestore框架可以分为两个主要部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;隐私去除&lt;/strong&gt;：在模型进行推理之前，对用户输入进行处理，去除其中的敏感信息。敏感信息可能包括个人身份信息（PII）、地址、电话、银行账户等。去除这些敏感信息的目的是避免模型在推理过程中接触到这些数据，从而降低泄露的风险。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐私恢复&lt;/strong&gt;：在推理完成后，恢复去除的隐私信息，以确保模型的输出仍然与原始输入保持一致，而不影响推理的质量。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;隐私去除与恢复的技术细节&#34;&gt;&lt;strong&gt;隐私去除与恢复的技术细节&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;隐私去除&lt;/strong&gt;：隐私去除依赖于预处理技术，使用自然语言处理（NLP）方法来识别和去除输入文本中的敏感信息。具体来说，模型会通过实体识别（Named Entity Recognition, NER）技术或其他上下文分析方法识别出敏感内容，并将其替换为占位符或完全去除。例如，用户输入中提到的地址或电话号码会被替换成“[ADDRESS]”或“[PHONE]”等占位符。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐私恢复&lt;/strong&gt;：隐私恢复的核心是在推理过程完成后，基于模型输出的信息，恢复去除的敏感信息。这一过程会根据模型的推理结果以及输入中去除的敏感信息位置进行调整。恢复方法需要确保敏感信息的插入不会改变模型的推理结果，同时保持隐私信息的完整性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;隐私保护与推理质量的平衡&#34;&gt;&lt;strong&gt;隐私保护与推理质量的平衡&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;论文还探讨了隐私保护与推理质量之间的平衡问题。隐私去除和恢复可能对模型的性能产生影响，特别是当敏感信息对于模型的理解至关重要时。作者提出了一种优化策略，通过对去除的信息进行分析，确保隐私保护不会过度损害推理结果的准确性。实验表明，通过精细设计的隐私去除和恢复机制，PrivacyRestore能够在保护隐私的同时，最大限度地保留模型的推理能力。&lt;/p&gt;
&lt;h4 id=&#34;实验与评估-3&#34;&gt;&lt;strong&gt;实验与评估&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;论文进行了广泛的实验评估，验证了PrivacyRestore方法的有效性。实验结果表明，在采用PrivacyRestore框架后，模型在推理过程中能够显著降低隐私泄露的风险。与传统的隐私保护方法（如直接删除敏感信息或使用差分隐私）相比，PrivacyRestore在确保隐私安全的同时，能够保持较高的推理质量。&lt;/p&gt;
&lt;p&gt;具体的评估方法包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;隐私泄露评估&lt;/strong&gt;：评估隐私泄露的概率，特别是在模型推理时是否会泄露去除的敏感信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;推理性能评估&lt;/strong&gt;：通过比较模型在隐私保护前后的推理精度、召回率等指标，评估隐私保护措施对模型性能的影响。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;用户体验评估&lt;/strong&gt;：通过用户调查等方式，评估隐私保护措施对用户体验的影响，确保模型在隐私保护的同时仍能提供用户期望的服务。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;挑战与未来方向-1&#34;&gt;&lt;strong&gt;挑战与未来方向&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;论文最后讨论了PrivacyRestore框架的挑战和未来研究方向：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;去除与恢复的精度问题&lt;/strong&gt;：如何更加精确地去除和恢复敏感信息，尤其是在面对复杂的自然语言输入时。当前的技术可能会对一些细节进行过度简化，从而影响恢复的准确性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多模态隐私保护&lt;/strong&gt;：随着多模态模型（包括文本、图片、视频等）逐渐普及，隐私保护的挑战将更加复杂，如何在多模态推理中实现隐私保护是一个重要的研究方向。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;个性化隐私保护策略&lt;/strong&gt;：不同用户的隐私需求不同，如何根据用户的隐私偏好设计个性化的隐私保护策略，将是未来研究的一个重要方向。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;总结-5&#34;&gt;总结
&lt;/h3&gt;&lt;p&gt;《PrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration》提出了一个创新的隐私保护框架，通过在推理过程中去除敏感信息并在结果输出后恢复隐私，解决了大语言模型在实际应用中可能存在的隐私泄露问题。该框架不仅保护了用户隐私，还尽可能地保留了模型的推理能力，实验结果证明其有效性和实用性。论文为隐私保护技术在大语言模型中的应用提供了新的思路，同时也指出了未来的研究方向。&lt;/p&gt;
&lt;p&gt;这个我也不敢妄加评论，但就是疑惑，谁没事干把自己的隐私信息放进去推理，自己的个人信息放进去就能推理的更准确吗？到头来你把隐私信息找出来用占位符替换，最后再给人换回去，有啥用啊？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;隐私去除&lt;/strong&gt;已经是隐私保护领域的常见做法，很多NLP系统都通过识别和去除敏感信息来保护用户隐私。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;隐私恢复&lt;/strong&gt;的想法也并不是非常创新。恢复去除的敏感信息通常是通过占位符来实现，虽然恢复的准确性可能是一个挑战，但方法并没有提供特别具有突破性的技术细节。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;计划&#34;&gt;计划
&lt;/h2&gt;&lt;p&gt;论文看得多以后会有一点点感悟，有用的论文看不懂，没用的看了想笑，确实没用，估计我以后也会加入吧，再接再厉吧！&lt;/p&gt;
</description>
        </item>
        <item>
        <title>第7周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC7%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</link>
        <pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC7%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E7%AC%AC7%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/leaves-5598709_1280.png" alt="Featured image of post 第7周工作总结" /&gt;&lt;h1 id=&#34;总结&#34;&gt;总结
&lt;/h1&gt;&lt;p&gt;最近运气不太好啊，生病连着生病，两周感冒刚好，准备重新出发了，又不知道为何整出来荨麻疹了，一晚一晚的睡不着觉，很是头大啊。&lt;/p&gt;
&lt;p&gt;上次和学长讨论完，要找一个细致的方向，但最近看了看论文，发现越来越难了，很多细节都不懂，比如密码学的一些东西，感觉需要恶补很多东西啊，不然基础的不懂，他们整体构筑起来的论文我就有点看不懂。&lt;/p&gt;
&lt;p&gt;这周看了一个BumbleBee: 大型变换器模型的安全双方推理框架，感觉有一点点感悟，但不多，由于生病折磨，这周就没怎么看，感觉缺的背景知识有点多，需要拿出时间来补一补。&lt;/p&gt;
&lt;p&gt;还看了一个论文：InferDPT：面向黑盒大语言模型的隐私保护推理，这个比较好理解，但我却想不到，想来也是因为缺少背景知识的缘故，他把要发送的文档进行差分隐私发过去，把gpt的结果拿过来给自己的模型，再生成一次结果，这样就保证了大模型一方拿不到完整数据，而且有不错的效率保证。&lt;/p&gt;
&lt;p&gt;CipherGPT：安全的两方 GPT 推理还没看完&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;计划&#34;&gt;计划
&lt;/h2&gt;&lt;p&gt;继续看看论文，补一补背景知识。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>第6周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC6%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</link>
        <pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC6%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E7%AC%AC6%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/ai-generated-8787240_1280.jpg" alt="Featured image of post 第6周工作总结" /&gt;&lt;h1 id=&#34;总结&#34;&gt;总结
&lt;/h1&gt;&lt;p&gt;这一周又没干啥正经事，学科前沿讲座结课了，整了一下结课论文，把软件工程实训汇报完了，有惊无险，但却有点轻微感冒+上火+口腔溃疡，痛不欲生，虽已习惯了高三时期留下的旧疾，但得了口腔溃疡还是痛。&lt;/p&gt;
&lt;p&gt;这周又读了几篇论文，还是没有找到与我要研究东西特别相近的论文，都是关于深度学习的，没有大模型这方面的，这周继续探索一下。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;计划&#34;&gt;计划
&lt;/h2&gt;&lt;p&gt;计划就是完成上周以及上上周未完成的计划，继续干&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Inference with Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Inference of Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Machine Learning: A Practical Approach&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Outsourced Computation in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Differentially Private Inference in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Towards Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Model Inference: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>第5周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC5%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</link>
        <pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC5%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E7%AC%AC5%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/ai-generated-7926621_1280.jpg" alt="Featured image of post 第5周工作总结" /&gt;&lt;h1 id=&#34;总结&#34;&gt;🎵总结
&lt;/h1&gt;&lt;p&gt;这一周没干啥正经事，周一周二库库读了两天论文，初步了解了一下安全推理的基础知识（就看了一点点），周三图书馆闭馆，晚上上课，所以周三休息一天，周三下午参加了理想汽车AI算法实习生的面试，想找一个这种实习边干活边学习，可惜这并不是啥也不会就能行的，还得学习，沉淀一下。一开始挺紧张的，在脑海里思考了好多遍我该怎么解释，表现自己，结果面试官让我开始自我介绍，他就一只在笑😁（牙很齐很白），不知道他在笑什么，但是看他笑得那么开心我就不紧张了，问了一下transformer为什么要加入位置编码来表示位置信息，LLM微调除了LoRA还有什么，还了解那些大模型，一个也不会，哈哈哈，真不是啥也不会就可以找工作的，不过此行并非失败，而是挺有收获的，面试官一直说，没关系，不会咱就换一个，下去把这个搞清楚就行了，搞得我有点不好意思，在问问题的过程中，面试官还顺带给我解释解释，增长了一些知识，最后我反问到该怎么系统的学习一下才能找到实习（我真是个鬼才🤣，问面试官这问题）&lt;/p&gt;
&lt;p&gt;面试官说：建议我先不要着急找面试，用几个月学习一下，把基础的机器学习和深度学习过一遍，代码复现都能看懂，还问了我研究生方向是大模型安全，看看这方面的论文，把基础知识掌握扎实，上Kaggle上参加或者复现两个项目放到简历里，会更加出色。&lt;/p&gt;
&lt;p&gt;非常感谢面试官的指导，没有想象中的KPI面试，不会有脸色看，更像是前辈教育后辈如何学习🤩。&lt;/p&gt;
&lt;p&gt;周四上午起了个大早直奔图书馆找同学，同学给占了一个之前我没有坐过的位置，有点风，恰好我没有棉袄，差点冻死，于是下午就回了宿舍，周四周五周六就在宿舍待着，写软件工程的文档，哎呦，忒难写，还得画图，周日棉袄到位，在图书馆待了一天，把所有文档完结，做了一个PPT，顺手的事。&lt;/p&gt;
&lt;p&gt;于是这一周就这么过完了&lt;/p&gt;
&lt;h2 id=&#34;阅读&#34;&gt;🎧阅读
&lt;/h2&gt;&lt;p&gt;Secure and Private Machine Learning: A Survey of  Techniques and Applications&lt;/p&gt;
&lt;p&gt;Differential Privacy: A Survey of Results&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;计划&#34;&gt;🥁计划
&lt;/h2&gt;&lt;p&gt;计划就是完成上周未完成的计划，开干！&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文综述了在机器学习中实现安全和隐私保护的各种方法，包括使用同态加密和安全多方计算的技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Inference with Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Inference of Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Machine Learning: A Practical Approach&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Outsourced Computation in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Differentially Private Inference in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Towards Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Model Inference: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>第4周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</link>
        <pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/colorful-2609978_1280.jpg" alt="Featured image of post 第4周工作总结" /&gt;&lt;h1 id=&#34;总结&#34;&gt;总结
&lt;/h1&gt;&lt;p&gt;这一周高强度阅读把书都读完了，但是没看代码，后果是有点记不住，等有需要时再去看吧。&lt;/p&gt;
&lt;p&gt;这周看了大模型内容比较多，主要是为了快点看完相关内容找ZD哥确定毕设方向，早点开始，做的好一点。&lt;/p&gt;
&lt;p&gt;周五下午找了ZD哥确定方向，看完方向人有点蒙，可能是大模型看得多，安全那方面看得少，导致我不知道这个方向在干什么，于是开始沉思，思考了半天感觉确实不会，找gpt老师询问，没有得到结果，故有些惆怅，周日又坐在实验室上了一天的软件工程实训，还好马上就结束了，太烦了。今天是周一，先把上周周报做了&lt;/p&gt;
&lt;p&gt;我又重新换了问法，问了gpt老师，这次得到了满意的结果，找到了几篇论文，这周就读这几篇。&lt;/p&gt;
&lt;p&gt;感觉这才是周报，之前的都是书的摘抄。&lt;/p&gt;
&lt;h2 id=&#34;阅读&#34;&gt;阅读
&lt;/h2&gt;&lt;p&gt;LLM预训练数据准备&lt;/p&gt;
&lt;p&gt;transformer模型架构（不知道是不是我本，这个我从不同的地方看了听了好多遍，现在才刚有点感觉）&lt;/p&gt;
&lt;p&gt;Instruction Tuning&lt;/p&gt;
&lt;p&gt;提示学习（实在不行，没有研究天赋就去干点轻松的(bushi)）&lt;/p&gt;
&lt;p&gt;解码与部署&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;后面就是补了两篇差分隐私的文献，读完感觉对要研究的方向还是不清楚，故又找了几篇：&lt;/p&gt;
&lt;h2 id=&#34;计划&#34;&gt;计划
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文综述了在机器学习中实现安全和隐私保护的各种方法，包括使用同态加密和安全多方计算的技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Inference with Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Inference of Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Machine Learning: A Practical Approach&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Outsourced Computation in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Differentially Private Inference in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Towards Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Model Inference: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>第3周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/third-week/</link>
        <pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/third-week/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/third-week/cards-8594729_640.jpg" alt="Featured image of post 第3周工作总结" /&gt;&lt;h1 id=&#34;文献阅读&#34;&gt;文献阅读
&lt;/h1&gt;&lt;h5 id=&#34;每周都完不成上次的任务杂事太多了尽快完成&#34;&gt;每周都完不成上次的任务，杂事太多了，尽快完成：
&lt;/h5&gt;&lt;h2 id=&#34;detecting-formal-thought-disorder-by-deep-contextualized-word-representations&#34;&gt;Detecting formal thought disorder by deep contextualized word representations
&lt;/h2&gt;&lt;p&gt;竟然要收费，不看了，让gpt给讲一下&lt;/p&gt;
&lt;p&gt;这篇论文《通过深度上下文化词表示检测形式思维障碍》研究了如何使用深度学习中的上下文词嵌入模型（如ELMo）来检测精神疾病患者的形式思维障碍（Formal Thought Disorder, FTD）。FTD是一种影响语言表达和思维逻辑的精神疾病症状，常见于精神分裂症患者。&lt;/p&gt;
&lt;p&gt;论文主要利用深度上下文化词嵌入模型，如ELMo，通过生成词在句子中的嵌入表示，捕捉语境中词的多义性和丰富的语义信息。研究团队通过分析患者的语言数据，构建模型来区分正常语言与存在思维障碍的语言表达，从而实现自动化的FTD检测。&lt;/p&gt;
&lt;p&gt;研究的核心贡献是使用更复杂的语义表示（如上下文化词嵌入）来解决传统方法无法有效处理的词义歧义问题。&lt;/p&gt;
&lt;h2 id=&#34;distributed-representations-of-words-and-phrases-and-their-compositionality&#34;&gt;Distributed Representations of Words and Phrases and their Compositionality
&lt;/h2&gt;&lt;p&gt;《Distributed Representations of Words and Phrases and their Compositionality》是由Mikolov等人在2013年提出的一篇重要论文，介绍了&lt;strong&gt;Word2Vec&lt;/strong&gt;模型的改进和创新。这篇文章主要讨论了如何通过分布式词表示（distributed word representations）更好地捕捉单词和短语的语义信息，并且提出了两种核心模型：&lt;strong&gt;CBOW（Continuous Bag of Words）&lt;/strong&gt; 和  &lt;strong&gt;Skip-gram&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这篇论文的主要贡献包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Skip-gram和CBOW模型&lt;/strong&gt;：论文提出了两种用于生成词向量的架构。Skip-gram模型的目标是根据一个词预测其上下文，而CBOW模型则根据上下文词来预测目标词。这两种模型能够有效地捕捉单词之间的语义关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;层次Softmax和负采样&lt;/strong&gt;：为了提高训练大规模语料库的效率，作者引入了&lt;strong&gt;层次Softmax&lt;/strong&gt;和**负采样（negative sampling）**技术，这大幅降低了模型的计算复杂度，使得大规模训练变得可行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;词和短语的组合性&lt;/strong&gt;：论文不仅处理了单词的表示，还处理了短语的表示，通过数据驱动的方式将多词短语映射为词向量，捕捉更复杂的语义结构。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;向量运算反映语义关系&lt;/strong&gt;：Word2Vec生成的词向量能够通过简单的向量运算表达词语之间的关系。例如，向量运算 &lt;code&gt;vec(&amp;quot;King&amp;quot;) - vec(&amp;quot;Man&amp;quot;) + vec(&amp;quot;Woman&amp;quot;) ≈ vec(&amp;quot;Queen&amp;quot;)&lt;/code&gt; 说明了这种分布式表示在捕捉语义关系上的强大能力。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;llm-book&#34;&gt;LLM BOOK
&lt;/h2&gt;&lt;p&gt;找到一本书，准确的来说是一个github仓库，组织者把近些年的LLM相关的东西都整进去了，好全，写的真好，库库看吧那就&lt;/p&gt;
&lt;p&gt;先把资源放在这里：&lt;a class=&#34;link&#34; href=&#34;https://github.com/RUCAIBox/LLMSurvey/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RUCAIBox/LLMSurvey: The official GitHub page for the survey paper &amp;ldquo;A Survey of Large Language Models&amp;rdquo;.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;第一部分-背景与基础知识&#34;&gt;第一部分 背景与基础知识
&lt;/h3&gt;&lt;h4 id=&#34;语言模型的发展历程p16&#34;&gt;语言模型的发展历程（P16）
&lt;/h4&gt;&lt;p&gt;LLM大语言模型那肯定是从LM语言模型来的，语言模型的发展又经历了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;统计语言模型（StatisticalLanguageModel, SLM）&lt;/li&gt;
&lt;li&gt;神经语言模型（NeuralLanguageModel,NLM）&lt;/li&gt;
&lt;li&gt;预训练语言模型（Pre-trainedLanguageModel,PLM）&lt;/li&gt;
&lt;li&gt;大语言模型（LargeLanguageModel, LLM）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;llm的特点p19&#34;&gt;LLM的特点（P19）
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;具有较为丰富的世界知识&lt;/li&gt;
&lt;li&gt;具有较强的通用任务解决能力&lt;/li&gt;
&lt;li&gt;具有较好的复杂任务推理能力&lt;/li&gt;
&lt;li&gt;具有较强的人类指令遵循能力&lt;/li&gt;
&lt;li&gt;具有较好的人类对齐能力&lt;/li&gt;
&lt;li&gt;具有可拓展的工具使用能力&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;大语言模型对于产业应用带来了变革性的技术影响，将会催生一个基于大语言模型的应用生态系统。&lt;/p&gt;
&lt;h3 id=&#34;第二部分-预训练&#34;&gt;第二部分 预训练
&lt;/h3&gt;&lt;p&gt;大语言模型的第一个训练阶段是预训练，预训练语料的规模和质量对于提升大语言模型的能力至关重要。&lt;/p&gt;
&lt;h4 id=&#34;数据来源&#34;&gt;数据来源
&lt;/h4&gt;&lt;p&gt;通用文本数据和专用文本数据。通用文本数据涵盖了网页、书籍和对话文本等。由于通用文本数据规模较大、多样性强且易于获取，大多数大语言模型都会收集大量的通用文本数据，以增强其语言建模能力。此外，为了进一步提升大语言模型在特定专业任务上的表现，人们还将预训练语料的范围扩展至更专业的数据集，如多语数据、科学数据和代码数据等。&lt;/p&gt;
&lt;h5 id=&#34;通用文本数据&#34;&gt;通用文本数据
&lt;/h5&gt;&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026154722061.png&#34;
	width=&#34;792&#34;
	height=&#34;657&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026154722061_hu1725408825362406164.png 480w, https://JiangZhiyu-1024.github.io/p/third-week/image-20241026154722061_hu12406635621069597218.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241026154722061&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;120&#34;
		data-flex-basis=&#34;289px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;现有大语言模型预训练数据中各种数据来源的比例分布图&lt;/center&gt;&gt;
&lt;h5 id=&#34;专用文本数据&#34;&gt;专用文本数据
&lt;/h5&gt;&lt;p&gt;多语文本、 科学文本、代码。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026155259857.png&#34;
	width=&#34;831&#34;
	height=&#34;189&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026155259857_hu14579868356241669728.png 480w, https://JiangZhiyu-1024.github.io/p/third-week/image-20241026155259857_hu13508329583982414925.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241026155259857&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;439&#34;
		data-flex-basis=&#34;1055px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;典型的预训练数据预处理流程图&lt;/center&gt;&gt;
&lt;h4 id=&#34;数据预处理&#34;&gt;数据预处理
&lt;/h4&gt;&lt;p&gt;构建并使用系统化的数据处理框架（如开源库Data-Juicer）&lt;/p&gt;
&lt;p&gt;具体操作（P74）&lt;/p&gt;
&lt;h1 id=&#34;附录&#34;&gt;附录
&lt;/h1&gt;&lt;h2 id=&#34;softmax是啥&#34;&gt;softmax是啥？
&lt;/h2&gt;&lt;h4 id=&#34;softmax是一种常用的函数特别是在多分类任务中用来将一个向量中的元素转换为0到1之间的概率分布其核心作用是&#34;&gt;Softmax是一种常用的函数，特别是在&lt;strong&gt;多分类任务&lt;/strong&gt;中，用来将一个向量中的元素转换为0到1之间的概率分布。其核心作用是：
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;将实数映射为概率分布&lt;/strong&gt;：Softmax 接受一个实数向量，输出一个与输入维度相同的向量，其中每个元素的值表示该元素作为某个类别的概率，且这些概率的和为1。这样就可以根据这些概率来进行分类预测。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;公式&lt;/strong&gt;：Softmax 函数的定义如下：
&lt;/p&gt;
$$
   \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
   $$&lt;p&gt;
其中，\( z_i \) 是输入向量中的某个元素，\( e^{z_i} \) 是该元素的指数值，分母是所有输入元素的指数值的和。这个函数将每个 \( z_i \) 映射为一个在 \( (0,1) \) 之间的概率，并且所有输出的概率之和为1。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;作用机制&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;指数放大效果&lt;/strong&gt;：Softmax 函数通过对输入值取指数（\( e^x \)），会将较大的输入值放大更多，而较小的值则被进一步压缩，突出更大的概率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;归一化&lt;/strong&gt;：Softmax 保证所有输出的概率之和为1，符合概率的定义。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;应用场景&lt;/strong&gt;：在&lt;strong&gt;神经网络的最后一层&lt;/strong&gt;，特别是分类任务中，Softmax 常用于多类分类问题，比如在自然语言处理（NLP）中的词分类、图像识别中的对象分类等。它将网络的输出转化为易于理解的概率值，帮助模型决定输入属于哪个类别。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;举例&#34;&gt;举例
&lt;/h4&gt;&lt;p&gt;假设我们有一个模型输出三个值 \( [2.0, 1.0, 0.1] \)，通过 Softmax 转换后得到的结果是大约 \( [0.71, 0.26, 0.03] \)，其中最大的值0.71表示模型认为输入属于第一个类别的概率最大。&lt;/p&gt;
&lt;p&gt;简言之，Softmax 是一种将输出转换为概率分布的工具，特别适合多分类问题的场景。&lt;/p&gt;
&lt;h2 id=&#34;看了半天论文感觉没有一个时间线很难受不清楚那些论文之间的关系问问gpt&#34;&gt;看了半天论文，感觉没有一个时间线很难受，不清楚那些论文之间的关系，问问gpt：
&lt;/h2&gt;&lt;h3 id=&#34;基础概念与理论&#34;&gt;&lt;strong&gt;基础概念与理论&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;先掌握大模型背后的基本理论，这些知识是理解后续论文的基础：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语言模型的基础：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;经典论文：《Attention Is All You Need》（Transformer）&lt;/li&gt;
&lt;li&gt;相关概念：自注意力机制、序列到序列模型、词嵌入（如Word2Vec、GloVe）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自回归模型与自编码器：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;GPT 系列（GPT, GPT-2, GPT-3）的原理和应用&lt;/li&gt;
&lt;li&gt;BERT 及其衍生模型的预训练与微调方式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;扩展学习与生成式任务：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》&lt;/li&gt;
&lt;li&gt;生成式预训练模型的设计与任务应用（如文本生成、机器翻译）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;模型的训练与优化&#34;&gt;&lt;strong&gt;模型的训练与优化&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;学习如何高效地训练大模型，并且了解模型的优化技术：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;微调与参数高效训练：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Efficient Fine-Tuning Techniques for Large Models: Adapter, LoRA 等&lt;/li&gt;
&lt;li&gt;探索模型压缩技术（知识蒸馏、量化、剪枝等）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对比学习与自监督学习：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;《SimCLR》、《MoCo》 等代表性论文，理解对比学习在大模型中的应用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;模型对齐与安全&#34;&gt;&lt;strong&gt;模型对齐与安全&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;在大模型发展过程中，模型对齐（alignment）和伦理问题变得越来越重要：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型对齐与价值观嵌入：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;《InstructGPT: Training language models to follow instructions with human feedback》&lt;/li&gt;
&lt;li&gt;RLHF（强化学习通过人类反馈）等对齐策略&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型安全与公平性：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;探讨模型偏见问题以及在实际应用中的安全隐患&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;视觉-语言模型与多模态模型&#34;&gt;&lt;strong&gt;视觉-语言模型与多模态模型&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;学习大模型在跨模态任务中的应用，比如视觉-语言任务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CLIP（Contrastive Language-Image Pretraining）&lt;/strong&gt;：理解图像和文本共同表征的训练方法&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DALL-E、Stable Diffusion&lt;/strong&gt;：生成式视觉任务的模型设计与应用&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;视觉语言对话系统&lt;/strong&gt;：如《Visual ChatGPT》&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;前沿研究与新趋势&#34;&gt;&lt;strong&gt;前沿研究与新趋势&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;在深入理解基本模型后，可以关注大模型领域的前沿研究方向：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;大模型与知识图谱的结合&lt;/strong&gt;：如何将知识图谱融入语言模型&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;领域自适应模型&lt;/strong&gt;：如何通过特定领域数据进行模型的进一步微调&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;检索增强生成（RAG）&lt;/strong&gt;：如《RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;实战应用与开源框架&#34;&gt;&lt;strong&gt;实战应用与开源框架&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;阅读开源项目中的技术文档，参与开源项目（如Hugging Face的模型库）&lt;/li&gt;
&lt;li&gt;学习如何应用大模型解决实际问题，并尝试训练和微调自己的模型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样，你可以从基础概念、模型训练到前沿研究和实战应用，逐步深入，既能掌握理论又能结合实践。这个过程可以帮助你筛选出重要的论文和研究方向。&lt;/p&gt;
&lt;h2 id=&#34;对齐是什么意思&#34;&gt;对齐是什么意思？
&lt;/h2&gt;&lt;p&gt;在大型语言模型（LLM）中，“对齐”通常指的是模型的输出与人类意图或价值观的匹配程度。具体来说，它涉及确保模型在生成文本时能够产生符合人类期望的、有益的和安全的内容。对齐的目标是减少模型生成有害、误导或不准确的信息。&lt;/p&gt;
&lt;p&gt;对齐可以通过多种方法实现，包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练数据的选择和清洗&lt;/strong&gt;：使用高质量的数据集进行训练，确保模型接触到的内容符合人类的价值观和期望。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;强化学习&lt;/strong&gt;：利用人类反馈来微调模型，使其在生成内容时更符合人类的意图。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;安全性和伦理性评估&lt;/strong&gt;：在模型发布之前，对其输出进行评估，以识别和修正潜在的偏见或有害内容。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过这些方法，可以提高模型在特定任务或应用场景中的可靠性和安全性。&lt;/p&gt;
&lt;h2 id=&#34;arxiv-论文是什么&#34;&gt;arXiv 论文是什么？
&lt;/h2&gt;&lt;p&gt;arXiv 是一个开放获取的电子预印本存档，主要用于科学研究领域，特别是在物理学、数学、计算机科学、统计学、金融学、生物学和其他相关学科。研究者可以在 arXiv 上免费上传和分享他们的研究论文，通常是在正式出版之前的版本。&lt;/p&gt;
&lt;p&gt;arXiv 的特点包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;快速发布&lt;/strong&gt;：研究者可以迅速分享他们的研究成果，而不必等待同行评审和正式出版的过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;开放获取&lt;/strong&gt;：任何人都可以访问和下载论文，无需订阅或支付费用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;广泛的学科覆盖&lt;/strong&gt;：涵盖多个领域，促进跨学科的交流和合作。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;社区反馈&lt;/strong&gt;：研究者可以在正式发表之前收到来自社区的反馈，帮助他们改进论文。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;arXiv 的论文通常以“预印本”的形式存在，因此它们可能尚未经过同行评审。这意味着在引用这些论文时，研究者需要谨慎，了解其可能的局限性。&lt;/p&gt;
&lt;h2 id=&#34;在arxiv上发还未出版的论文不怕被别人剽窃成果吗&#34;&gt;在arXiv上发还未出版的论文不怕被别人剽窃成果吗？
&lt;/h2&gt;&lt;p&gt;在 arXiv 上发布未出版的论文确实存在被剽窃的风险，但研究者通常认为在 arXiv 上共享研究成果是有益的，原因如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;时间戳和公开记录&lt;/strong&gt;：一旦论文被上传到 arXiv，上传的时间戳可以作为知识产权的证明。这在某种程度上可以保护研究者的原创性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;学术社区的支持&lt;/strong&gt;：arXiv 是一个活跃的学术社区，研究者通常会关注相关领域的论文，并且对剽窃行为持有负面态度。学术界普遍重视诚信，剽窃通常会受到严厉的谴责。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;早期反馈&lt;/strong&gt;：通过发布预印本，研究者可以获得来自同行的反馈，从而改进自己的工作。这也有助于在正式出版之前建立声誉。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;增加曝光率&lt;/strong&gt;：在 arXiv 上发布可以提高研究的可见性，吸引潜在合作者或资助者。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;通用人工智能系统是什么&#34;&gt;通用人工智能系统是什么？
&lt;/h2&gt;&lt;p&gt;通用人工智能系统（AGI, Artificial General Intelligence）指的是一种能够理解、学习和应用知识，完成任何人类所能完成的智力任务的人工智能系统。与当前的狭义人工智能（Narrow AI）相比，后者在特定任务或领域表现出色（如图像识别、自然语言处理等），AGI 的目标是实现一种更广泛的智能，具备跨领域的学习和适应能力。&lt;/p&gt;
&lt;p&gt;AGI 的特点包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;自主学习&lt;/strong&gt;：能够从有限的数据中学习并推理，具备自我改进的能力。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多任务处理&lt;/strong&gt;：能够在多个不同的领域和任务中表现出色，而不局限于特定的应用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;常识推理&lt;/strong&gt;：具备理解和应用常识的能力，能够处理复杂的、不确定的环境。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;灵活性和适应性&lt;/strong&gt;：能够适应新情况和挑战，灵活地应对不同的环境和需求。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;AGI 的发展仍然是一个前沿研究领域，目前尚未实现。许多研究者在探索如何构建这样的系统，同时也讨论其潜在的伦理和社会影响，例如对就业、隐私和安全的影响。&lt;/p&gt;
&lt;h1 id=&#34;思考&#34;&gt;思考
&lt;/h1&gt;&lt;p&gt;随着看的东西多了，和使用chatgpt，我隐隐感觉到，由于chatgpt的语言模型主要是通过预测下一个最可能的词来生成文本，这一过程称为自回归生成（autoregressive generation）。导致gpt经常会顺着你的话去说，即有更小的概论去反对你的话，尽量让你说的话看起来正确，当然，如果你说的是：“太阳从西边升起”，那么他会义无反顾的反驳你。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>第2周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/second-week/</link>
        <pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/second-week/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/second-week/leaves-6729056_640.jpg" alt="Featured image of post 第2周工作总结" /&gt;&lt;h2 id=&#34;文献阅读&#34;&gt;文献阅读
&lt;/h2&gt;&lt;p&gt;看了知乎的科普文章，先从经典的论文开始看：&lt;/p&gt;
&lt;h3 id=&#34;attention-is-all-you-need&#34;&gt;Attention Is All You Need
&lt;/h3&gt;&lt;p&gt;《Attention is All You Need》是由Vaswani等人在2017年提出的一篇重要论文，首次引入了Transformer模型。以下是其主要内容的总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;：传统的序列到序列模型（如RNN和LSTM）在处理长序列时存在瓶颈。Transformer模型通过自注意力机制克服了这些限制，使得模型可以并行处理数据，提升了训练效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型架构&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder-Decoder结构&lt;/strong&gt;：Transformer由两个主要部分组成：编码器和解码器。编码器将输入序列转化为上下文向量，解码器则根据上下文生成输出序列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自注意力机制&lt;/strong&gt;：每个输入的表示都通过自注意力机制与其他输入进行交互，从而捕捉序列中的依赖关系。每个位置的表示通过加权求和得到，权重由输入的相似度计算得到。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多头注意力&lt;/strong&gt;：模型使用多个注意力头并行计算不同的表示，能够更全面地捕捉信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;位置编码&lt;/strong&gt;：由于Transformer没有循环结构，论文引入了位置编码，提供序列中词语的位置信息，以保留词序信息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练与优化&lt;/strong&gt;：Transformer使用了残差连接和层归一化，以提高训练的稳定性和收敛速度。论文中还介绍了基于Adam优化器的训练过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;：Transformer在机器翻译任务上取得了显著的性能提升，并且在其他NLP任务中也展示了强大的通用性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;影响&lt;/strong&gt;：该论文奠定了现代NLP研究的基础，推动了基于Transformer的预训练语言模型（如BERT、GPT等）的发展。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;差分隐私深度学习deep-learning-with-differential-privacy&#34;&gt;差分隐私深度学习(Deep Learning with Differential Privacy)
&lt;/h3&gt;&lt;p&gt;《Deep Learning with Differential Privacy》是由Martin Abadi等人在2016年提出的一篇重要论文，探讨了在深度学习中实现差分隐私的策略。以下是其主要内容的总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;差分隐私&lt;/strong&gt;：一种保护用户数据隐私的技术，旨在确保模型在训练时无法推断出任何单个用户的敏感信息。即使攻击者知道模型和数据集的其余部分，也无法有效地获取关于某个特定用户的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度学习与隐私问题&lt;/strong&gt;：深度学习模型通常依赖大量数据，这使得保护用户隐私变得尤为重要。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型与训练&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;噪声注入&lt;/strong&gt;：通过在模型的梯度更新中添加噪声，可以达到差分隐私的效果。噪声的大小与隐私预算（ε）相关，预算越小，隐私保护越强，但模型的性能可能会受到影响。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Accounting&lt;/strong&gt;：论文提出了一种计算隐私损失的框架，确保在每次训练步骤中监控和控制隐私损失。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算法设计&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DP-SGD（Differentially Private Stochastic Gradient Descent）&lt;/strong&gt;：论文提出了一种改进的随机梯度下降算法，结合了梯度裁剪和噪声注入，以实现差分隐私。这种方法确保在训练过程中，个别数据对模型的影响是被限制的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文通过实验展示了DP-SGD在图像分类任务上的有效性，同时比较了不同噪声级别对模型性能的影响，证明了在保持合理准确性的同时实现差分隐私是可行的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;应用与展望&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文强调了在各类机器学习应用中（如医疗、金融等）保护隐私的重要性，提出将差分隐私技术与深度学习结合是一个有效的解决方案，并鼓励未来的研究在这一领域的进一步探索。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;感觉偏向数学一点，全是公式和证明，好难看懂。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;差分隐私顾名思义就是用来防范差分攻击的
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;加入噪声，改变原来的概率分布
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;hr&gt;
&lt;h3 id=&#34;deep-reinforcement-learning-from-human-preferences&#34;&gt;Deep Reinforcement Learning from Human Preferences
&lt;/h3&gt;&lt;p&gt;《Deep Reinforcement Learning from Human Preferences》是由Christiano等人在2017年提出的一篇重要论文，探讨了如何利用人类偏好来训练强化学习模型。以下是其主要内容的总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;传统的强化学习方法通常依赖于奖励信号来指导学习，但设计有效的奖励函数在复杂任务中往往困难且耗时。&lt;/li&gt;
&lt;li&gt;人类可以提供更直观的偏好信息，这为训练强化学习代理提供了一种新途径。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;方法概述&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;人类偏好收集&lt;/strong&gt;：论文提出了一种方法，通过收集人类对不同行为的偏好来生成奖励信号。人类评估多个策略的表现，从中选择更好的策略，以此来建立奖励模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励模型&lt;/strong&gt;：利用收集到的人类偏好数据，训练一个深度神经网络来预测代理在给定状态下的奖励。这种奖励模型可以用来指导强化学习算法。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练过程&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;深度强化学习&lt;/strong&gt;：在获得奖励模型后，使用深度强化学习算法（如Proximal Policy Optimization，PPO）来训练代理。代理通过与环境交互，不断优化其策略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;迭代学习&lt;/strong&gt;：通过不断收集新的偏好数据并更新奖励模型，实现迭代学习，使得代理能够逐步提升性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文在多个复杂任务（如Atari游戏和模拟环境）中展示了该方法的有效性，代理能够通过学习人类的偏好，超越基于手动设计的奖励函数的表现。&lt;/li&gt;
&lt;li&gt;结果表明，利用人类偏好进行训练能够加速学习过程，并提升代理的最终性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;意义与展望&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该方法展示了人类偏好在强化学习中的潜力，提出了一种新的思路来解决奖励设计问题。&lt;/li&gt;
&lt;li&gt;论文强调了未来研究中结合人类反馈和偏好的重要性，特别是在涉及复杂决策和安全性问题的领域。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;附录&#34;&gt;附录
&lt;/h2&gt;&lt;h3 id=&#34;残差连接residual-connection和层归一化layer-normalization是什么&#34;&gt;残差连接（residual connection）和层归一化（layer normalization）是什么？
&lt;/h3&gt;&lt;p&gt;残差连接（Residual Connection）和 层归一化（Layer Normalization）是深度学习中常用的两种技术，主要用于提高神经网络的训练效率和性能。下面是它们的定义和作用：&lt;/p&gt;
&lt;h4 id=&#34;残差连接residual-connection&#34;&gt;残差连接（Residual Connection）
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：残差连接是一种将输入直接与输出相加的结构，通常在深度神经网络中使用。公式表示为：
&lt;/p&gt;
$$
\text{Output} = \text{Layer}(x) + x
$$&lt;p&gt;
其中 \(x\) 是输入，\(\text{Layer}(x)\) 是通过某个层（如卷积层或全连接层）处理后的输出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;缓解梯度消失问题&lt;/strong&gt;：在非常深的网络中，梯度可能会随着层数增加而消失，导致网络难以训练。残差连接提供了一条捷径，使得梯度可以更容易地通过网络传播。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加速收敛&lt;/strong&gt;：通过引入直接路径，残差连接有助于提高网络的收敛速度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提高模型性能&lt;/strong&gt;：它允许模型学习恒等映射，这使得训练更深的网络成为可能，提升了模型的表现。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;层归一化layer-normalization&#34;&gt;层归一化（Layer Normalization）
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：层归一化是一种对网络中每一层的激活进行归一化的技术，它通过计算每个样本的均值和标准差来调整激活值。层归一化的公式为：
&lt;/p&gt;
$$
\text{LayerNorm}(x) = \frac{x - \mu}{\sigma + \epsilon} \cdot \gamma + \beta
$$&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(x\) 是输入。&lt;/li&gt;
&lt;li&gt;\(\mu\) 是输入的均值。&lt;/li&gt;
&lt;li&gt;\(\sigma\) 是输入的标准差。&lt;/li&gt;
&lt;li&gt;\(\epsilon\) 是一个小常数，避免除以零。&lt;/li&gt;
&lt;li&gt;\(\gamma\) 和 \(\beta\) 是可学习的参数，用于缩放和平移归一化的结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;减少内部协变量偏移&lt;/strong&gt;：层归一化帮助减少模型训练过程中每层输入的变化，使得网络在训练时更加稳定。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提高训练速度&lt;/strong&gt;：通过标准化输入，层归一化可以使得训练过程更加平滑，从而加快收敛速度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;适用于变长序列&lt;/strong&gt;：与批量归一化（Batch Normalization）不同，层归一化可以直接应用于变长的输入序列，适合于RNN和Transformer等模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;残差连接&lt;/strong&gt;主要通过提供捷径来缓解深层网络中的梯度消失问题，并加速训练过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;层归一化&lt;/strong&gt;则通过标准化激活值来提高训练的稳定性和速度。这两者结合使用，能够显著提升深度学习模型的表现。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>第1周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/first-week/</link>
        <pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/first-week/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/first-week/banlan.jpg" alt="Featured image of post 第1周工作总结" /&gt;&lt;h1 id=&#34;工作总结&#34;&gt;&lt;strong&gt;工作总结&lt;/strong&gt;：
&lt;/h1&gt;&lt;h2 id=&#34;文献阅读&#34;&gt;文献阅读
&lt;/h2&gt;&lt;h3 id=&#34;综述类&#34;&gt;综述类：
&lt;/h3&gt;&lt;p&gt;2023年国内大模型发展综述与趋势研判_赵子忠&lt;/p&gt;
&lt;p&gt;AIGC大模型测评综述：使能技术、安全隐患和应对_许志伟&lt;/p&gt;
&lt;p&gt;AI大模型发展综述_张乾君&lt;/p&gt;
&lt;h3 id=&#34;深入&#34;&gt;深入：
&lt;/h3&gt;&lt;hr&gt;
&lt;p&gt;《A Neural Probabilistic Language Model》是Yoshua Bengio等人在2003年发表的一篇重要论文，提出了一种基于神经网络的语言模型方法。这篇论文的主要目标是通过神经网络来改进传统的语言模型。&lt;/p&gt;
&lt;p&gt;传统的语言模型的任务是估计一个词序列的概率：
P(w1,w2,…,wT)
在这种情况下，模型需要根据上下文词来预测当前词的概率。传统的做法一般是通过统计方法（如n元语法模型）来估计这些概率，但这类方法有一个很大的问题：它们无法处理高维数据，尤其是在词汇量非常大的情况下，计算复杂度极高，且泛化能力有限。&lt;/p&gt;
&lt;p&gt;Bengio等人提出了一种新的神经网络语言模型，目的是克服这个问题。这个模型的核心思想是使用一个多层感知器来学习词的分布式表示（即词向量），并基于这些词向量来估计概率。具体过程大致如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;词嵌入&lt;/strong&gt;：每个词通过一个嵌入矩阵被映射为一个连续向量，这种向量的维度比词汇表要小得多。这一步的目的是将离散的词映射到一个连续的低维空间，减少计算复杂度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;神经网络建模&lt;/strong&gt;：利用一个前馈神经网络（通常是多层感知器），根据前面的若干个词的词向量，来预测下一个词的条件概率。通过这个网络，模型可以学习到词与词之间的相似性和上下文关系。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练&lt;/strong&gt;：模型通过最大化训练数据的似然估计来进行训练。具体来说，模型会调整嵌入矩阵和网络的参数，使得预测的概率尽可能接近真实的词序列概率。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这篇论文的创新之处在于提出了“词向量”的概念，并展示了使用神经网络来处理语言建模问题的潜力。这为后来的深度学习在自然语言处理（NLP）中的应用奠定了基础。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;附录&#34;&gt;附录
&lt;/h1&gt;&lt;h2 id=&#34;什么是语言模型&#34;&gt;什么是语言模型
&lt;/h2&gt;&lt;p&gt;语言模型（Language Model, LM）是自然语言处理中的一个核心概念，其主要任务是对一个词序列的概率分布进行建模。简单来说，语言模型的目标是估计一段文本或句子出现的概率，或者根据前面的词来预测下一个词。&lt;/p&gt;
&lt;h3 id=&#34;语言模型的定义&#34;&gt;语言模型的定义
&lt;/h3&gt;&lt;p&gt;给定一个词序列
&lt;/p&gt;
$$
P( w_1, w_2, \dots, w_T )
$$&lt;p&gt;
，语言模型的目标是计算这个序列的联合概率：
&lt;/p&gt;
$$
P(w_1, w_2, \dots, w_T)
$$&lt;p&gt;
这通常可以分解为条件概率的乘积：
&lt;/p&gt;
$$
P(w_1, w_2, \dots, w_T) = P(w_1) \cdot P(w_2 | w_1) \cdot P(w_3 | w_1, w_2) \cdot \dots \cdot P(w_T | w_1, w_2, \dots, w_{T-1})
$$&lt;p&gt;
这意味着每个词的概率取决于它前面的词。一个好的语言模型能够通过学习语料库中的模式，准确地捕捉这些条件概率。&lt;/p&gt;
&lt;h3 id=&#34;语言模型的用途&#34;&gt;语言模型的用途
&lt;/h3&gt;&lt;p&gt;语言模型有广泛的应用场景，包括但不限于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;文本生成&lt;/strong&gt;：语言模型可以用来自动生成自然语言文本。例如，给定一部分句子，模型可以预测并生成合理的下一个词，从而逐步生成完整的句子或段落。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;机器翻译&lt;/strong&gt;：在机器翻译中，语言模型帮助翻译系统生成符合目标语言语法的翻译结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;语音识别&lt;/strong&gt;：语言模型能够帮助语音识别系统，在可能的识别结果中选择最合适的词序列。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;拼写纠正&lt;/strong&gt;：在文本输入或搜索引擎中，语言模型可以用于纠正拼写错误，提供更符合语言习惯的结果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;传统语言模型&#34;&gt;传统语言模型
&lt;/h3&gt;&lt;p&gt;在神经网络语言模型之前，最常用的语言模型是&lt;strong&gt;n元语法模型（n-gram Model）&lt;/strong&gt;。n元语法模型通过假设一个词的概率只与它前面的n-1个词有关来简化问题。举个例子，三元语法模型（trigram model）认为：
&lt;/p&gt;
$$
P(w_T | w_1, w_2, \dots, w_{T-1}) \approx P(w_T | w_{T-2}, w_{T-1})
$$&lt;p&gt;这种简化降低了计算复杂度，但也限制了模型的表达能力，因为它只能捕捉到短距离的上下文信息，无法充分考虑长距离的依赖关系。&lt;/p&gt;
&lt;h3 id=&#34;神经网络语言模型&#34;&gt;神经网络语言模型
&lt;/h3&gt;&lt;p&gt;传统语言模型的一个主要问题是词汇量增大时，模型的计算复杂度随之增加。为了解决这个问题，Yoshua Bengio等人在《A Neural Probabilistic Language Model》论文中提出了使用神经网络进行语言建模的想法。这种神经网络模型通过将词表示为&lt;strong&gt;词向量（word embeddings）&lt;/strong&gt;，并通过深度学习模型来捕捉更复杂的语境和词间关系，从而解决了传统语言模型的许多不足之处。&lt;/p&gt;
&lt;p&gt;总之，语言模型的核心任务就是对自然语言的概率进行建模，并通过学习语言中的统计模式来预测文本。&lt;/p&gt;
&lt;h2 id=&#34;什么是纬度灾难&#34;&gt;什么是纬度灾难？
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;维度灾难&lt;/strong&gt;（Curse of Dimensionality）是指在处理高维数据时，随着数据维度增加，问题的复杂性呈指数级增长，导致很多传统方法在高维空间中变得低效或不可行的现象。&lt;/p&gt;
&lt;p&gt;其影响包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;数据稀疏性&lt;/strong&gt;：在高维空间中，数据点变得非常稀疏，因为数据的总量相对于空间的体积变得极其小，导致模型难以从有限的数据中捕捉到模式和关系。简单来说，随着维度的增加，需要的样本量也随之爆炸性增长，否则模型可能会过拟合或无法泛化。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;计算复杂度增加&lt;/strong&gt;：在高维空间中，计算距离、相似性等操作的复杂度大大增加，使得很多算法在高维情况下效率低下。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型的泛化能力下降&lt;/strong&gt;：由于数据点过于稀疏，模型很难学习到有用的特征，从而可能导致泛化能力下降。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;具体来说，维度灾难对机器学习、统计学、最优化等领域带来了极大的挑战，尤其是当模型需要处理很多特征或变量时。为应对这个问题，研究人员提出了许多方法，如&lt;strong&gt;降维技术&lt;/strong&gt;（如主成分分析PCA）和&lt;strong&gt;分布式表示&lt;/strong&gt;（如词嵌入），以减少有效维度或通过分布式方法来表示复杂的数据结构。&lt;/p&gt;
&lt;h2 id=&#34;有概率为什么要引入似然的概念他们有什么不同&#34;&gt;有概率为什么要引入似然的概念，他们有什么不同？
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;概率&lt;/strong&gt;和&lt;strong&gt;似然&lt;/strong&gt;虽然看起来相似，但在统计学中它们有不同的定义和用途：&lt;/p&gt;
&lt;h3 id=&#34;概率probability&#34;&gt;&lt;strong&gt;概率（Probability）&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;概率是用来描述随机事件发生的可能性。在给定模型参数的情况下，概率告诉我们某个事件或观测结果发生的机会有多大。&lt;/p&gt;
&lt;p&gt;假设我们有一个随机变量 \( X \)，它取某个值 \( x \) 的概率可以表示为 \( P(X = x|\theta) \)，其中 \( \theta \) 是模型的参数。例如，如果我们有一个硬币掷出的模型，参数 \( \theta \) 可以是硬币正面朝上的概率（比如 \( \theta = 0.5 \)）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：给定模型的参数 \( \theta \)，事件 \( X = x \) 发生的概率是多少？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;似然likelihood&#34;&gt;&lt;strong&gt;似然（Likelihood）&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;似然是从&lt;strong&gt;反方向&lt;/strong&gt;来考虑的。它描述的是&lt;strong&gt;在已知观测数据的前提下，模型参数的可能性&lt;/strong&gt;。具体来说，似然是给定观测数据后，模型参数如何使观测数据变得最可能的一个度量。&lt;/p&gt;
&lt;p&gt;假设我们已经观察到数据 \( X = x \)，现在我们想知道，在不同的模型参数 \( \theta \) 下，这个数据出现的可能性有多大。似然可以表示为 \( L(\theta|X = x) \)，或者更直观地写作 \( P(X = x|\theta) \)，虽然数学表达式相同，但意义不同——在这里我们关注的是参数 \( \theta \) 的值使观测到的数据最可能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：给定观测到的数据 \( X = x \)，模型参数 \( \theta \) 有多大可能是正确的？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;概率和似然的区别&#34;&gt;概率和似然的区别
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;概率&lt;/strong&gt;：我们知道参数 \( \theta \)，希望知道某个事件 \( X \) 发生的概率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题：&lt;strong&gt;给定参数，事件的概率是多少？&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;似然&lt;/strong&gt;：我们知道事件（观测数据），希望推断出最有可能的参数 \( \theta \)。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题：&lt;strong&gt;给定观测数据，哪个参数最可能是正确的？&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;例子&#34;&gt;例子
&lt;/h3&gt;&lt;h4 id=&#34;抛硬币的例子&#34;&gt;抛硬币的例子
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;概率&lt;/strong&gt;：假设你有一枚硬币，已知它是公平的，即 \( \theta = 0.5 \)，那么掷硬币得到正面的概率是 \( P(\text{正面}) = 0.5 \)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;似然&lt;/strong&gt;：现在，你掷硬币10次，得到8次正面和2次反面。你不知道硬币的公平性（即不知道 \( \theta \) 的值）。你想根据这次实验结果估计硬币正面朝上的概率 \( \theta \)。此时，你需要用&lt;strong&gt;似然&lt;/strong&gt;来衡量在不同 \( \theta \) 值下，观测结果（8次正面，2次反面）出现的可能性有多大，从而找到最有可能的 \( \theta \)。&lt;/p&gt;
&lt;p&gt;似然函数 \( L(\theta|X) \) 可能在某个 \( \theta \) 值处达到最大值，这个 \( \theta \) 就是最能解释观测数据的参数值。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;概率&lt;/strong&gt;用于给定模型参数时预测事件的发生可能性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;似然&lt;/strong&gt;用于在已知观测数据时，推断哪个参数最能解释这些数据。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
