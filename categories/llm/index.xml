<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>LLM on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/categories/llm/</link>
        <description>Recent content in LLM on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Fri, 10 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/categories/llm/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>大模型基础知识补习&amp;GPT2代码分析</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E8%A1%A5%E4%B9%A0gpt2%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</link>
        <pubDate>Fri, 10 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E8%A1%A5%E4%B9%A0gpt2%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E8%A1%A5%E4%B9%A0gpt2%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/tree-838666_1280.jpg" alt="Featured image of post 大模型基础知识补习&amp;GPT2代码分析" /&gt;&lt;h1 id=&#34;大模型基础知识&#34;&gt;大模型基础知识
&lt;/h1&gt;&lt;p&gt;代码来源：https://github.com/openai/gpt-2&lt;/p&gt;
&lt;p&gt;GPT2源代码提供了以下几个文件：&lt;/p&gt;
&lt;p&gt;encoder.py&lt;/p&gt;
&lt;p&gt;generate_unconditional_samples.py&lt;/p&gt;
&lt;p&gt;interactive_conditional_samples.py&lt;/p&gt;
&lt;p&gt;model.py&lt;/p&gt;
&lt;p&gt;sample.py&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;model
&lt;/h2&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;def default_hparams():
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    return HParams(
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        n_vocab=0,        # 词汇表大小（通常是词汇表中包含的词语或符号数量）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        n_ctx=1024,       # 输入序列的最大长度（例如，一个输入文本序列的最大 token 数量）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        n_embd=768,       # 嵌入层的维度大小（每个词或符号在嵌入空间中的表示长度）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        n_head=12,        # 多头自注意力机制中的头数（即注意力头的数量）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        n_layer=12,       # Transformer 模型中的层数（即堆叠的 transformer 层数）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    )
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;词汇表&#34;&gt;词汇表
&lt;/h3&gt;&lt;p&gt;词汇表（Vocabulary）是自然语言处理（NLP）模型中用于将语言转换为数字表示的一组词或符号的集合。在深度学习模型中，词汇表用于将文本数据中的每个单词或符号映射为一个唯一的数字（通常是一个整数），以便计算机能够理解和处理。&lt;/p&gt;
&lt;h4 id=&#34;词汇表的作用&#34;&gt;词汇表的作用：
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;映射文本到数字&lt;/strong&gt;：由于机器学习模型无法直接处理文本数据，必须将文本中的每个词或符号转换为数字。词汇表就是这个映射的核心工具。例如，对于句子“我爱自然语言处理”，我们需要将“我”、“爱”、“自然”、“语言”、“处理”这些词转化为模型可以理解的数字。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;限制模型输入的规模&lt;/strong&gt;：在实际操作中，词汇表的大小有限。通常，我们不会为每个可能的字词创建一个索引，而是根据训练数据中出现的词汇频率来限制词汇表的大小。这样，词汇表的规模可以控制模型的输入维度，避免过大的计算和内存开销。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;处理未知词汇&lt;/strong&gt;：词汇表通常会为所有未出现在训练数据中的词汇或符号保留一个特殊的“未知词”标记（如 &lt;code&gt;[UNK]&lt;/code&gt;）。当模型遇到这些未见过的词时，会用这个特殊标记来代替。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;词汇表的组成&#34;&gt;词汇表的组成：
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;词&lt;/strong&gt;：如果你使用的是基于单词的词汇表，那么词汇表中的元素就是单词。例如，假设你有一个很小的词汇表：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;css
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;复制代码
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[&amp;#39;我&amp;#39;, &amp;#39;爱&amp;#39;, &amp;#39;自然&amp;#39;, &amp;#39;语言&amp;#39;, &amp;#39;处理&amp;#39;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;子词&lt;/strong&gt;（Subword）：对于一些较为复杂的语言模型，尤其是像 GPT-2 和 BERT 这样的模型，它们通常使用 &lt;strong&gt;子词&lt;/strong&gt; 作为词汇表的基本单元。子词是一个比单词更小的语言单位，它可以是词的一部分或者是一个常见的词根、前缀、后缀等。例如，词汇表中可能包含 &lt;code&gt;##ing&lt;/code&gt;、&lt;code&gt;un&lt;/code&gt; 等子词单元，而不是整个单词 &lt;code&gt;running&lt;/code&gt; 或 &lt;code&gt;undo&lt;/code&gt;。这有助于模型更好地处理未见过的词汇，并减少词汇表的大小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;特殊标记&lt;/strong&gt;：除了实际的词或子词，词汇表通常还会包含一些特殊标记：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;[PAD]&lt;/code&gt;：填充标记，用于将输入文本填充到相同的长度。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[UNK]&lt;/code&gt;：未知标记，用于处理那些在训练集中没有出现过的词。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[CLS]&lt;/code&gt;：分类标记，通常用于标记输入的开始（特别是在 BERT 等模型中）。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[SEP]&lt;/code&gt;：分隔标记，通常用于分隔不同句子或文本段落（在 BERT 等模型中也有使用）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;向量空间vector-space&#34;&gt;向量空间（Vector Space）
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;向量空间&lt;/strong&gt;是一个数学概念，指的是所有向量的集合，可以在其中进行加法和标量乘法等操作。在机器学习和自然语言处理中，我们通常将每个词、子词或符号表示为一个高维空间中的向量。例如，一个简单的二维向量空间可能包含了这样的一组向量：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;(1, 0)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(0, 1)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(1, 1)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 NLP 中，这些向量并没有具体的几何含义，而是用来表示不同的语义特征。每个向量的元素（即各维度的值）代表了某种抽象的语义属性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;词向量空间&lt;/strong&gt;意味着每个词都被映射为一个向量，且这些向量之间有一定的关系。例如，&amp;ldquo;king&amp;rdquo; 和 &amp;ldquo;queen&amp;rdquo; 可能在向量空间中非常接近，因为它们的语义相似，尽管它们的拼写不同。嵌入的目标就是将语言中的词语、句子或其他语言单位映射到这样一个向量空间中。&lt;/p&gt;
&lt;h3 id=&#34;嵌入embedding&#34;&gt;嵌入（Embedding）
&lt;/h3&gt;&lt;p&gt;**嵌入（Embedding）**是将一个离散的词汇项（如单词、子词）转换为连续的向量表示的过程。这些向量通常具有固定的维度（例如 300 维、768 维等），并且能够捕捉该词的语义信息。&lt;/p&gt;
&lt;p&gt;换句话说，嵌入是对词汇的数字表示，它通过将每个词或符号映射到一个向量空间中，使得模型能够理解和处理语言中的复杂结构。&lt;/p&gt;
&lt;p&gt;例如，假设你有以下三个词汇：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;猫&amp;rdquo;（cat）&lt;/li&gt;
&lt;li&gt;&amp;ldquo;狗&amp;rdquo;（dog）&lt;/li&gt;
&lt;li&gt;&amp;ldquo;汽车&amp;rdquo;（car）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过嵌入，这些词语会被映射为固定长度的向量，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;猫&amp;rdquo; → &lt;code&gt;[0.45, 0.87, 0.34, ...]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;狗&amp;rdquo; → &lt;code&gt;[0.42, 0.89, 0.31, ...]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;汽车&amp;rdquo; → &lt;code&gt;[0.21, 0.08, 0.76, ...]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些向量的维度（例如 300 维或 768 维）反映了嵌入空间的复杂度。模型通过训练来优化这些向量，使得语义相似的词在嵌入空间中距离更近，语义不同的词距离较远。&lt;/p&gt;
&lt;h3 id=&#34;向量vector&#34;&gt;向量（Vector）
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;向量&lt;/strong&gt;是在数学和计算机科学中用来表示方向和大小的对象。每个词或符号被嵌入为一个&lt;strong&gt;向量&lt;/strong&gt;，这个向量通常是一个浮动的数字数组。向量的每个元素都表示一个特定的特征维度。&lt;/p&gt;
&lt;p&gt;在 NLP 中，词的向量通常有以下几个特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;稠密表示&lt;/strong&gt;：每个词的向量是一个低维的稠密向量，不像传统的“one-hot”编码（仅使用0和1来表示词汇）。稠密向量包含了丰富的语义信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;语义关系&lt;/strong&gt;：通过训练，词向量可以捕捉到词汇之间的语义关系。例如，“king” 和 “queen” 可能有相似的向量，因为它们共享许多语义特征。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如，假设我们训练了一个模型，得到以下的词向量：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;苹果&amp;rdquo; → &lt;code&gt;[0.5, -0.2, 0.1, 0.7]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;香蕉&amp;rdquo; → &lt;code&gt;[0.4, -0.1, 0.2, 0.6]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些向量的数值表示了每个词的特征和语义信息。在训练过程中，模型会通过调整这些向量，使得语义相似的词的向量在空间中彼此接近。&lt;/p&gt;
&lt;h3 id=&#34;为什么要使用嵌入&#34;&gt;为什么要使用嵌入？
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;捕捉语义相似性&lt;/strong&gt;：通过嵌入，模型可以理解哪些词是相似的，哪些是不同的。比如“猫”和“狗”虽然拼写不同，但它们都是“动物”，因此它们的向量会很接近。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;降维&lt;/strong&gt;：将词语从“one-hot”编码的高维空间（例如，假设有 10,000 个不同的词，one-hot 向量就是 10,000 维的）映射到一个低维空间（如 300 维）。这样可以减少计算的复杂度和内存消耗。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;增强模型理解&lt;/strong&gt;：通过将词转换为向量，模型可以更加有效地理解文本中的语义、句法关系和上下文信息。这对于复杂的任务（如文本分类、情感分析、翻译等）非常重要。&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;shape_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Deal with dynamic shape in tensorflow cleanly.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;static&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;as_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 获取静态形状&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;dynamic&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 获取动态形状&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dynamic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;None&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;enumerate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;static&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 返回合并后的形状&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;static = x.shape.as_list()&lt;/code&gt;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x.shape&lt;/code&gt; 是 TensorFlow 张量 &lt;code&gt;x&lt;/code&gt; 的静态形状。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;as_list()&lt;/code&gt; 将这个形状从 &lt;code&gt;TensorShape&lt;/code&gt; 对象转换为 Python 列表。静态形状的值如果是已知的，会直接作为数值存在。如果某个维度的值是 &lt;code&gt;None&lt;/code&gt;（表示该维度在静态形状中不确定），那么会是 &lt;code&gt;None&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;例如，如果 &lt;code&gt;x&lt;/code&gt; 的形状是 &lt;code&gt;[None, 32, 32, 3]&lt;/code&gt;，则 &lt;code&gt;static&lt;/code&gt; 会是 &lt;code&gt;[None, 32, 32, 3]&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;dynamic = tf.shape(x)&lt;/code&gt;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.shape(x)&lt;/code&gt; 返回张量 &lt;code&gt;x&lt;/code&gt; 的动态形状，它是一个 TensorFlow 张量，表示运行时该张量的形状。与静态形状不同，动态形状会根据实际运行时的输入数据进行计算。&lt;/li&gt;
&lt;li&gt;返回值是一个形状为 &lt;code&gt;[d0, d1, ..., dn]&lt;/code&gt; 的张量（&lt;code&gt;d0&lt;/code&gt;，&lt;code&gt;d1&lt;/code&gt; 等是张量的各个维度的动态大小）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;return [dynamic[i] if s is None else s for i, s in enumerate(static)]&lt;/code&gt;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这个列表推导式的目的是将静态形状和动态形状合并成一个形状列表。如果某个静态形状的维度是 &lt;code&gt;None&lt;/code&gt;（即在静态形状中未指定），那么就使用对应的动态形状。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;enumerate(static)&lt;/code&gt; 会遍历静态形状中的每个维度，如果该维度是 &lt;code&gt;None&lt;/code&gt;，就从动态形状中取对应的维度值；如果该维度不是 &lt;code&gt;None&lt;/code&gt;，则保留静态形状中的值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假设 &lt;code&gt;static = [None, 32, 32, 3]&lt;/code&gt;（第一维度 &lt;code&gt;None&lt;/code&gt;，表示它是动态的）和 &lt;code&gt;dynamic = [64, 32, 32, 3]&lt;/code&gt;（运行时该张量的实际形状是 &lt;code&gt;[64, 32, 32, 3]&lt;/code&gt;）。&lt;/li&gt;
&lt;li&gt;那么，函数的返回值将是 &lt;code&gt;[64, 32, 32, 3]&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为什么需要这个函数？&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在 TensorFlow 中，许多操作要求处理的张量形状是已知的（静态形状），但有些情况下形状在执行时才会动态确定，特别是在处理输入数据的批量大小、图像尺寸等变量时。&lt;code&gt;shape_list&lt;/code&gt; 函数就是用来统一处理这两种情况的，使得无论是在静态还是动态形状下，代码都能够正常运行而不会出错。&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;举个例子：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;假设我们有一个张量 &lt;code&gt;x&lt;/code&gt;，它的形状是 &lt;code&gt;[None, 64, 64, 3]&lt;/code&gt;（批量大小未知）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;import tensorflow as tf
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;x = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])  # 动态批量大小
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;shape = shape_list(x)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;print(shape)  # 输出形状
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;在这个例子中，如果你传递一个大小为 &lt;code&gt;[32, 64, 64, 3]&lt;/code&gt; 的输入，&lt;code&gt;shape_list&lt;/code&gt; 函数会返回 &lt;code&gt;[32, 64, 64, 3]&lt;/code&gt;，即将 &lt;code&gt;None&lt;/code&gt; 替换为实际的批量大小 &lt;code&gt;32&lt;/code&gt;。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;def softmax(x, axis=-1): 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    x = x - tf.reduce_max(x, axis=axis, keepdims=True)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    ex = tf.exp(x)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;这个函数实现了 Softmax 操作，将输入 &lt;code&gt;x&lt;/code&gt; 转换为概率分布，常用于多分类模型的输出层。通过减去最大值避免指数计算中的数值溢出，确保计算稳定性。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;def gelu(x):
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;GELU&lt;/strong&gt; 是一种平滑的、概率性的激活函数，能够更自然地通过高斯分布近似非线性激活。相比于 ReLU，GELU 在训练时通常能更好地保留梯度，并且能够减少死神经元的问题。&lt;/p&gt;
&lt;p&gt;代码中使用了 &lt;code&gt;tf.tanh&lt;/code&gt; 和 &lt;code&gt;tf.pow&lt;/code&gt;（TensorFlow 的函数）与 NumPy 的常量进行计算，确保了在 TensorFlow 中的高效计算。&lt;/p&gt;
&lt;p&gt;Layer Normalization 是一种在深度学习中常用的技术，旨在对每个样本的特征进行归一化，使得特征的均值为0，方差为1，从而加速训练并提高模型的性能。&lt;/p&gt;
&lt;p&gt;代码解释：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;epsilon&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1e-5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Normalize to mean = 0, std = 1, then do a diagonal affine transform.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;variable_scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;g&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initializer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_initializer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initializer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_initializer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;x&lt;/code&gt;&lt;/strong&gt;: 输入张量，通常是经过激活函数后的某一层的输出。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;scope&lt;/code&gt;&lt;/strong&gt;: 变量作用域，用于创建和管理变量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;axis=-1&lt;/code&gt;&lt;/strong&gt;: 归一化时的维度，通常在 Layer Normalization 中是最后一维（即特征维度）。&lt;code&gt;axis=-1&lt;/code&gt; 表示归一化最后一维，通常是一个样本的所有特征。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;epsilon=1e-5&lt;/code&gt;&lt;/strong&gt;: 为了避免在除法中出现除零错误，&lt;code&gt;epsilon&lt;/code&gt; 是一个很小的常数（默认值为 1e−51e-51e−5）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;n_state = x.shape[-1].value&lt;/code&gt;&lt;/strong&gt;: 获取输入张量 &lt;code&gt;x&lt;/code&gt; 在最后一维的大小，代表特征数目。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;接下来，定义了两个变量 &lt;code&gt;g&lt;/code&gt; 和 &lt;code&gt;b&lt;/code&gt;，分别用于对归一化后的结果进行线性变换：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;g&lt;/code&gt;&lt;/strong&gt; 是缩放因子（通常称为 Gamma）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;b&lt;/code&gt;&lt;/strong&gt; 是平移因子（通常称为 Beta）。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;n&#34;&gt;g&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initializer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_initializer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initializer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_initializer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;g&lt;/code&gt; 和 &lt;code&gt;b&lt;/code&gt; 都是与输入 &lt;code&gt;x&lt;/code&gt; 的最后一维特征数目相同的向量。&lt;code&gt;g&lt;/code&gt; 初始化为 1，&lt;code&gt;b&lt;/code&gt; 初始化为 0。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;归一化步骤&#34;&gt;归一化步骤：
&lt;/h3&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        u = tf.reduce_mean(x, axis=axis, keepdims=True)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        s = tf.reduce_mean(tf.square(x - u), axis=axis, keepdims=True)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;u = tf.reduce_mean(x, axis=axis, keepdims=True)&lt;/code&gt;&lt;/strong&gt;: 计算输入 &lt;code&gt;x&lt;/code&gt; 在指定 &lt;code&gt;axis&lt;/code&gt; 维度上的均值（平均值）。这里，&lt;code&gt;axis&lt;/code&gt; 默认为 -1，即沿着最后一维计算均值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;s = tf.reduce_mean(tf.square(x - u), axis=axis, keepdims=True)&lt;/code&gt;&lt;/strong&gt;: 计算输入 &lt;code&gt;x&lt;/code&gt; 与均值 &lt;code&gt;u&lt;/code&gt; 的差的平方的平均值，即方差。&lt;code&gt;keepdims=True&lt;/code&gt; 保证输出维度与输入相同，便于后续操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        x = (x - u) * tf.rsqrt(s + epsilon)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;code&gt;x = (x - u) * tf.rsqrt(s + epsilon)&lt;/code&gt;&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;对输入&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;x
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;进行标准化（即归一化处理），使得均值为 0，方差为 1。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x - u&lt;/code&gt;：减去均值，使得每个元素的均值为 0。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.rsqrt(s + epsilon)&lt;/code&gt;：计算标准差的倒数并进行缩放，&lt;code&gt;rsqrt&lt;/code&gt; 是 &lt;code&gt;1 / sqrt(x)&lt;/code&gt;。&lt;code&gt;epsilon&lt;/code&gt; 用于防止除 0 错误。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        x = x * g + b
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        return x
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;x = x \* g + b&lt;/code&gt;&lt;/strong&gt;: 最后，对标准化后的 &lt;code&gt;x&lt;/code&gt; 进行线性变换，使用参数 &lt;code&gt;g&lt;/code&gt;（缩放因子）和 &lt;code&gt;b&lt;/code&gt;（平移因子）。这种变换使得归一化的结果可以恢复一定的表达能力，类似于 Batch Normalization 中的 affine 变换。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;return x&lt;/code&gt;&lt;/strong&gt;: 返回归一化并经过变换的结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;def split_states(x, n):
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;Reshape the last dimension of x into [n, x.shape[-1]/n].&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    *start, m = shape_list(x)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    return tf.reshape(x, start + [n, m//n])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;这个函数 &lt;code&gt;split_states&lt;/code&gt; 的作用是将输入张量 &lt;code&gt;x&lt;/code&gt; 的最后一维拆分成两个部分，其中一个部分的大小是 &lt;code&gt;n&lt;/code&gt;，另一个部分的大小是 &lt;code&gt;m // n&lt;/code&gt;（&lt;code&gt;m&lt;/code&gt; 是输入张量的最后一维大小）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;\*start, m = shape_list(x)&lt;/code&gt;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;shape_list(x)&lt;/code&gt; 会返回张量 &lt;code&gt;x&lt;/code&gt; 的形状（作为一个列表）。&lt;code&gt;*start&lt;/code&gt; 会将除了最后一维之外的所有维度（即张量的前几维）存入 &lt;code&gt;start&lt;/code&gt; 变量中，&lt;code&gt;m&lt;/code&gt; 则保存张量最后一维的大小。&lt;/li&gt;
&lt;li&gt;例如，如果 &lt;code&gt;x&lt;/code&gt; 的形状是 &lt;code&gt;[batch_size, time_steps, features]&lt;/code&gt;，那么 &lt;code&gt;start&lt;/code&gt; 将保存 &lt;code&gt;[batch_size, time_steps]&lt;/code&gt;，&lt;code&gt;m&lt;/code&gt; 将保存 &lt;code&gt;features&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;tf.reshape(x, start + [n, m // n])&lt;/code&gt;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;start&lt;/code&gt; 是 &lt;code&gt;x&lt;/code&gt; 的前几个维度的列表，&lt;code&gt;n&lt;/code&gt; 是传入的参数，表示你想将最后一维拆分成 &lt;code&gt;n&lt;/code&gt; 部分，&lt;code&gt;m // n&lt;/code&gt; 是每一部分的大小。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.reshape&lt;/code&gt; 函数将输入张量 &lt;code&gt;x&lt;/code&gt; 重新调整形状，形状变为 &lt;code&gt;[start, n, m // n]&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;例如，如果 &lt;code&gt;x&lt;/code&gt; 的形状是 &lt;code&gt;[batch_size, time_steps, features]&lt;/code&gt;，并且传入 &lt;code&gt;n = 2&lt;/code&gt;，那么新的形状将是 &lt;code&gt;[batch_size, time_steps, 2, features // 2]&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;def merge_states(x):
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;Smash the last two dimensions of x into a single dimension.&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    *start, a, b = shape_list(x)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    return tf.reshape(x, start + [a*b])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;这个函数 &lt;code&gt;merge_states&lt;/code&gt; 的作用是将输入张量 &lt;code&gt;x&lt;/code&gt; 的最后两个维度合并成一个维度。也就是说，它通过将最后两个维度相乘，将它们“压扁”为一个新的维度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;\*start, a, b = shape_list(x)&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;shape_list(x)&lt;/code&gt; 返回张量 &lt;code&gt;x&lt;/code&gt; 的形状，作为一个列表。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;*start&lt;/code&gt; 会将 &lt;code&gt;x&lt;/code&gt; 的所有维度（除了最后两个维度）存入 &lt;code&gt;start&lt;/code&gt;，&lt;code&gt;a&lt;/code&gt; 和 &lt;code&gt;b&lt;/code&gt; 分别保存 &lt;code&gt;x&lt;/code&gt; 的倒数第二维和最后一维的大小。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;举个例子：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假设 &lt;code&gt;x&lt;/code&gt; 的形状是 &lt;code&gt;[batch_size, time_steps, features, channels]&lt;/code&gt;，那么 &lt;code&gt;start = [batch_size, time_steps, features]&lt;/code&gt;，&lt;code&gt;a = features&lt;/code&gt;，&lt;code&gt;b = channels&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;tf.reshape(x, start + [a \* b])&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.reshape(x, start + [a * b])&lt;/code&gt; 会将 &lt;code&gt;x&lt;/code&gt; 重新调整形状，新的形状由 &lt;code&gt;start&lt;/code&gt;（原先的所有维度，除了最后两个）和 &lt;code&gt;a * b&lt;/code&gt;（将最后两个维度相乘的结果）组成。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;a * b&lt;/code&gt; 是将原本的最后两个维度合并成一个新的维度。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个 &lt;code&gt;conv1d&lt;/code&gt; 函数实现了一个 &lt;strong&gt;1D 卷积&lt;/strong&gt; 操作。虽然它的名字和传统的 1D 卷积类似，但它实际上通过矩阵乘法和适当的形状变换来模拟 1D 卷积的操作。以下是代码逐行解释：&lt;/p&gt;
&lt;h3 id=&#34;代码解释&#34;&gt;代码解释：
&lt;/h3&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;conv1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w_init_stdev&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.02&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;variable_scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shape_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initializer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random_normal_initializer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stddev&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w_init_stdev&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initializer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_initializer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]))&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;\*start, nx = shape_list(x)&lt;/code&gt;&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;shape_list(x)&lt;/code&gt; 返回张量 &lt;code&gt;x&lt;/code&gt; 的形状，作为一个列表。&lt;code&gt;*start&lt;/code&gt; 会将除了最后一维之外的所有维度（即张量的前几维）存入 &lt;code&gt;start&lt;/code&gt;，&lt;code&gt;nx&lt;/code&gt; 保存 &lt;code&gt;x&lt;/code&gt; 的最后一维大小。&lt;/li&gt;
&lt;li&gt;比如，如果 &lt;code&gt;x&lt;/code&gt; 的形状是 &lt;code&gt;[batch_size, length, in_channels]&lt;/code&gt;，那么 &lt;code&gt;start&lt;/code&gt; 将是 &lt;code&gt;[batch_size, length]&lt;/code&gt;，&lt;code&gt;nx&lt;/code&gt; 就是 &lt;code&gt;in_channels&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;w = tf.get_variable(&#39;w&#39;, [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))&lt;/code&gt;&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;创建一个卷积核（权重） &lt;code&gt;w&lt;/code&gt;，其形状为 &lt;code&gt;[1, nx, nf]&lt;/code&gt;。这里的 &lt;code&gt;1&lt;/code&gt; 是卷积核的大小（即卷积操作是 1D 的），&lt;code&gt;nx&lt;/code&gt; 是输入的最后一维大小（通常是输入通道数），&lt;code&gt;nf&lt;/code&gt; 是卷积操作后输出通道的数量。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;initializer=tf.random_normal_initializer(stddev=w_init_stdev)&lt;/code&gt; 初始化权重值为从正态分布中采样，标准差为 &lt;code&gt;w_init_stdev&lt;/code&gt;，默认为 &lt;code&gt;0.02&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;b = tf.get_variable(&#39;b&#39;, [nf], initializer=tf.constant_initializer(0))&lt;/code&gt;&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;创建偏置 &lt;code&gt;b&lt;/code&gt;，其形状为 &lt;code&gt;[nf]&lt;/code&gt;，即与输出通道数相同，初始化为零。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf])) + b, start + [nf])&lt;/code&gt;&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;tf.reshape(x, [-1, nx])&lt;/code&gt;&lt;/strong&gt;: 将输入张量 &lt;code&gt;x&lt;/code&gt; 重塑为形状 &lt;code&gt;[-1, nx]&lt;/code&gt;，即将所有前面的维度展平，保留最后一维 &lt;code&gt;nx&lt;/code&gt;。这一步相当于将输入的多维张量展平成二维矩阵，每一行代表一个数据样本的输入通道。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;tf.reshape(w, [-1, nf])&lt;/code&gt;&lt;/strong&gt;: 将卷积核 &lt;code&gt;w&lt;/code&gt; 重塑为形状 &lt;code&gt;[-1, nf]&lt;/code&gt;，这意味着卷积核的权重被展平为一个矩阵，其中每个输入通道的权重被分配到输出通道。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;tf.matmul(...)&lt;/code&gt;&lt;/strong&gt;: 执行矩阵乘法。此处是将展平后的输入张量 &lt;code&gt;x&lt;/code&gt; 与展平后的卷积核 &lt;code&gt;w&lt;/code&gt; 相乘，类似于卷积操作中的加权求和过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;+ b&lt;/code&gt;&lt;/strong&gt;: 将偏置加到结果中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;tf.reshape(..., start + [nf])&lt;/code&gt;&lt;/strong&gt;: 将矩阵乘法后的结果再重塑回原来的形状，保留除了最后一维以外的所有维度，最后一维变为 &lt;code&gt;nf&lt;/code&gt;，即输出通道数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;return c&lt;/code&gt;&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;返回卷积操作后的结果 &lt;code&gt;c&lt;/code&gt;，它的形状是 &lt;code&gt;[batch_size, length, nf]&lt;/code&gt;，即输出的批次大小、长度和输出通道数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结：
&lt;/h3&gt;&lt;p&gt;这个 &lt;code&gt;conv1d&lt;/code&gt; 函数实现的是一种通过矩阵乘法模拟 1D 卷积操作的方式。传统的 1D 卷积通过滑动窗口对输入进行局部加权求和，而这个实现通过将输入展平成矩阵后与卷积核进行矩阵乘法，从而实现类似的效果。&lt;/p&gt;
&lt;h3 id=&#34;主要特点&#34;&gt;主要特点：
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;权重形状&lt;/strong&gt;：&lt;code&gt;[1, nx, nf]&lt;/code&gt;，表示卷积核的大小为 &lt;code&gt;1xnx&lt;/code&gt;，即卷积操作是对输入的每个位置进行加权求和，输出通道数为 &lt;code&gt;nf&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;矩阵乘法实现卷积&lt;/strong&gt;：通过 &lt;code&gt;tf.matmul&lt;/code&gt; 将输入张量和卷积核做矩阵乘法，而不是传统的滑动窗口卷积。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;展平操作&lt;/strong&gt;：输入张量和卷积核都通过 &lt;code&gt;tf.reshape&lt;/code&gt; 进行了展平处理，以便进行矩阵乘法。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这种方法虽然在实现上与传统的卷积不同，但效果是相同的，并且可以通过矩阵乘法的方式提高计算效率或适应某些特定的任务需求。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>第4周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</link>
        <pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/colorful-2609978_1280.jpg" alt="Featured image of post 第4周工作总结" /&gt;&lt;h1 id=&#34;总结&#34;&gt;总结
&lt;/h1&gt;&lt;p&gt;这一周高强度阅读把书都读完了，但是没看代码，后果是有点记不住，等有需要时再去看吧。&lt;/p&gt;
&lt;p&gt;这周看了大模型内容比较多，主要是为了快点看完相关内容找ZD哥确定毕设方向，早点开始，做的好一点。&lt;/p&gt;
&lt;p&gt;周五下午找了ZD哥确定方向，看完方向人有点蒙，可能是大模型看得多，安全那方面看得少，导致我不知道这个方向在干什么，于是开始沉思，思考了半天感觉确实不会，找gpt老师询问，没有得到结果，故有些惆怅，周日又坐在实验室上了一天的软件工程实训，还好马上就结束了，太烦了。今天是周一，先把上周周报做了&lt;/p&gt;
&lt;p&gt;我又重新换了问法，问了gpt老师，这次得到了满意的结果，找到了几篇论文，这周就读这几篇。&lt;/p&gt;
&lt;p&gt;感觉这才是周报，之前的都是书的摘抄。&lt;/p&gt;
&lt;h2 id=&#34;阅读&#34;&gt;阅读
&lt;/h2&gt;&lt;p&gt;LLM预训练数据准备&lt;/p&gt;
&lt;p&gt;transformer模型架构（不知道是不是我本，这个我从不同的地方看了听了好多遍，现在才刚有点感觉）&lt;/p&gt;
&lt;p&gt;Instruction Tuning&lt;/p&gt;
&lt;p&gt;提示学习（实在不行，没有研究天赋就去干点轻松的(bushi)）&lt;/p&gt;
&lt;p&gt;解码与部署&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;后面就是补了两篇差分隐私的文献，读完感觉对要研究的方向还是不清楚，故又找了几篇：&lt;/p&gt;
&lt;h2 id=&#34;计划&#34;&gt;计划
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文综述了在机器学习中实现安全和隐私保护的各种方法，包括使用同态加密和安全多方计算的技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Inference with Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Inference of Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Machine Learning: A Practical Approach&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Outsourced Computation in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Differentially Private Inference in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Towards Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Model Inference: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>transformer模型架构</title>
        <link>https://JiangZhiyu-1024.github.io/p/transformermodelbase/</link>
        <pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/transformermodelbase/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/transformermodelbase/house-7497002_1280.png" alt="Featured image of post transformer模型架构" /&gt;&lt;h1 id=&#34;模型架构&#34;&gt;模型架构
&lt;/h1&gt;&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391.png&#34;
	width=&#34;807&#34;
	height=&#34;689&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391_hu13532172091181392611.png 480w, https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391_hu12135867853004229579.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241029095402391&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;281px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;大语言模型架构配置表&lt;/center&gt;&gt;
&lt;h2 id=&#34;transformer-模型&#34;&gt;🍈Transformer 模型
&lt;/h2&gt;&lt;p&gt;当前主流的大语言模型都是基于Transformer模型进行设计的。Transformer是由多层的多头自注意力（Multi-headSelf-attention）模块堆叠而成的神经网络模型。原始的Transformer 模型由编码器和解码器两个部分构成，而这两个部分实际上可以独立使用，例如基于编码器架构的BERT模型[1]和解码器架构的GPT模型[2]。与BERT等早期的预训练语言模型相比，大语言模型的特点是使用了更长的向量维度、更深的层数，进而包含了更大规模的模型参数，并主要使用解码器架构，对于Transformer 本身的结构与配置改变并不大。&lt;/p&gt;
&lt;h3 id=&#34;输入编码&#34;&gt;🍉输入编码
&lt;/h3&gt;&lt;p&gt;在Transformer 模型中，输入的词元序列(𝒖 = [𝑢1,𝑢2,&amp;hellip;,𝑢𝑇]) 首先经过一个输入嵌入模块（InputEmbeddingModule）转化成词向量序列。具体来说，为了捕获词汇本身的语义信息，每个词元在输入嵌入模块中被映射成为一个可学习的、具有固定维度的词向量$v_t$ ∈$R^H$。由于Transformer的编码器结构本身无法识别序列中元素的顺序，位置编码（PositionEmbedding,PE）被引入来表示序列中的位置信息。给定一个词元$u_t$，位置编码根据其在输入中的绝对位置分配一个固定长度的嵌入向量$p_t$ ∈$R^H$。然后，每个词元对应的词向量和位置向量将直接相加，生成了最终的输入嵌入序列𝑿=[𝒙1,&amp;hellip;,𝒙𝑇]，并且被传入到后续层中：$x_t=v_t+p_t$.&lt;/p&gt;
&lt;p&gt;通过这种建模方法的表示，Transformer 模型可以利用位置编码 𝒑𝑡 建模不同词元的位置信息。由于不同词元的位置编码仅由其位置唯一决定，因此这种位置建模方式被称为绝对位置编码。尽管绝对位置编码能够一定程度上建模位置信息，然而它只能局限于建模训练样本中出现的位置，无法建模训练数据中未出现过的位置，因此极大地限制了它们处理长文本的能力。&lt;/p&gt;
&lt;h3 id=&#34;多头自注意力机制&#34;&gt;🍋多头自注意力机制
&lt;/h3&gt;&lt;p&gt;多头自注意力是Transformer模型的核心创新技术。相比于循环神经网络（Recurrent Neural Network, RNN）和卷积神经网络（ConvolutionalNeuralNetwork,CNN）等传统神经网络，多头自注意力机制能够直接建模任意距离的词元之间的交互关系。作为对比，循环神经网络迭代地利用前一个时刻的状态更新当前时刻的状态，因此在处理较长序列的时候，常常会出现梯度爆炸或者梯度消失的问题。而在卷积神经网络中，只有位于同一个卷积核的窗口中的词元可以直接进行交互，通过堆叠层数来实现远距离词元间信息的交换。&lt;/p&gt;
&lt;p&gt;多头自注意力机制通常由多个自注意力模块组成。在每个自注意力模块中，对于输入的词元序列，将其映射为相应的查询（Query,𝑸）、键（Key,𝑲）和值（Value,𝑽）三个矩阵。然后，对于每个查询，将和所有没有被掩盖的键之间计算点积。这些点积值进一步除以$\sqrt{D}$进行缩放（𝐷是键对应的向量维度），被传入到softmax函数中用于权重的计算。进一步，这些权重将作用于与键相关联的值，通过加权和的形式计算得到最终的输出。在数学上，上述过程可以表示为：
&lt;/p&gt;
$$
Q = XW^Q,
$$$$
K = XW^K,
$$$$
V = XW^V,
$$$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{D}})V.
$$&lt;p&gt;与单头注意力相比，多头注意力机制的主要区别在于它使用了𝐻组结构相同，但映射参数不同的自注意力模块。输入序列首先通过不同的权重矩阵被映射为一组查询、键和值。每组查询、键和值的映射构成一个“头”，并独立地计算自注意力的输出。最后，不同头的输出被拼接在一起，并通过一个权重矩阵$W^O$∈$R^{H \times H}$ 进行映射，产生最终的输出。如下面的公式所示：
&lt;/p&gt;
$$
head_n = Attention(XW^Q_n,XW^K_n,XW^V_n)
$$$$
MHA = Concat(head_1,...,head_N)W^O
$$&lt;p&gt;由上述内容可见，自注意力机制能够直接建模序列中任意两个位置之间的关系，进而有效捕获长程依赖关系，具有更强的序列建模能力。另一个主要的优势是，自注意力的计算过程对于基于硬件的并行优化（如GPU、TPU等）非常友好，因此能够支持大规模参数的高效优化。&lt;/p&gt;
&lt;h3 id=&#34;前馈网络层&#34;&gt;🍏前馈网络层
&lt;/h3&gt;&lt;p&gt;为了学习复杂的函数关系和特征，Transformer 模型引入了一个前馈网络层（Feed Forward Netwok, FFN），对于每个位置的隐藏状态进行非线性变换和特征提取。具体来说，给定输入𝒙，Transformer中的前馈神经网络由两个线性变换和一个非线性激活函数组成：
&lt;/p&gt;
$$
FFN(X) = σ(XW^U + b_1)W^D + b_2
$$&lt;p&gt;
其中$W^U$ ∈ $R^{H \times H}$ 和$W^D$ ∈ $R^{H \times H}$  分别是第一层和第二层的线性变换权重矩阵，$b_1$ ∈ $R^{𝐻^′}$ 和 $b_2$ ∈ $R^H$ 是偏置项，𝜎是激活函数（在原始的Transformer中，采用 ReLU 作为激活函数）。前馈网络层通过激活函数引入了非线性映射变换，提升了 模型的表达能力，从而更好地捕获复杂的交互关系。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440.png&#34;
	width=&#34;650&#34;
	height=&#34;691&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440_hu8196060707081265206.png 480w, https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440_hu16997779109624366145.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241029105634440&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;94&#34;
		data-flex-basis=&#34;225px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;Transformer 架构图&lt;/center&gt;
&lt;h3 id=&#34;编码器&#34;&gt;🍐编码器
&lt;/h3&gt;&lt;p&gt;在Transformer 模型中，编码器（Encoder）的作用是将每个输入词元都编码成一个上下文语义相关的表示向量。编码器结构由多个相同的层堆叠而成，其中每一层都包含多头自注意力模块和前馈网络模块。在注意力和前馈网络后，模型使用层归一化和残差连接来加强模型的训练稳定度。其中，残差连接（Residual Connection）将输入与该层的输出相加，实现了信息在不同层的跳跃传递，从而缓解梯度爆炸和消失的问题。而LayerNorm则对数据进行重新放缩，提升模型的训练稳定性。编码器接受经过位置编码层的词嵌入序列𝑿作为输入，通过多个堆叠的编码器层来建模上下文信息，进而对于整个输入序列进行编码表示。由于输入数据是完全可见的，编码器中的自注意力模块通常采用双向注意力，每个位置的词元表示能够有效融合上下文的语义关系。在编码器-解码器架构中，编码器的输出将作为解码器（Decoder）的输入，进行后续计算。形式化来说，第𝑙层（𝑙∈{1,&amp;hellip;,𝐿}）的编码器的数据处理过程如下所示：
&lt;/p&gt;
$$
X^′_l = LayerNorm(MHA(X_{l-1})+X_{l-1})
$$$$
X_l = LayerNorm(FFN(X^′_l)+X^′_l)
$$&lt;p&gt;其中，$X^′_l$ 和 $X_l$ 分别是该Transformer层的输入和输出，$X^′_l$是该层中输入经过多头注意力模块后的中间表示，LayerNorm表示层归一化。&lt;/p&gt;
&lt;h3 id=&#34;解码器&#34;&gt;🥥解码器
&lt;/h3&gt;&lt;p&gt;Transformer 架构中的解码器基于来自编码器编码后的最后一层的输出表示以及已经由模型生成的词元序列，执行后续的序列生成任务。与编码器不同，解码器需要引入掩码自注意力（MaskedSelf-attention）模块，用来在计算注意力分数的时候掩盖当前位置之后的词，以保证生成目标序列时不依赖于未来的信息。除了建模目标序列的内部关系，解码器还引入了与编码器相关联的多头注意力层，从而关注编码器输出的上下文信息$X_L$。同编码器类似，在每个模块之后，Transformer 解码器也采用了层归一化和残差连接。在经过解码器之后，模型会通过一个全连接层将输出映射到大小为𝑉的目标词汇表的概率分布，并基于某种解码策略生成对应的词元。在训练过程中，解码器可以通过一次前向传播，让每个词元的输出用于预测下一个词元。而在解码过程，解码器需要经过一个逐步的生成过程，将自回归地生成完整的目标序列。解码器的数据流程如下所示：
&lt;/p&gt;
$$
Y^′_l = LayerNorm(MaskedMHA(Y_{l-1})+Y_{l-1})
$$$$
Y^&#34;_l = LayerNorm(CrossMHA(Y^′_l,X_L)+Y^′_l)
$$$$
Y_l = LayerNorm(FFN(Y^&#34;_l)+Y^&#34;_l)
$$&lt;p&gt;其中，$Y_{l-1}$ 和 $Y_l$ 分别是该Transformer 层的输入和输出，$Y^′_l$ 和 $Y^&amp;quot;_l$ 是该层中输入经过掩码多头注意力MaskedMHA和交叉多头注意力CrossMHA模块后的中间表示，LayerNorm 表示层归一化。然后将最后一层的输入𝒀𝐿映射到词表的维度上：
&lt;/p&gt;
$$
O = softmax(W^LY_L)
$$&lt;p&gt;
其中，𝑶 ∈$R^{H \times V}$ 是模型最终的输出，代表下一个词在词表上的概率分布；$W^L$ ∈ $R^{H \times V}$ 是将输入表示映射到词汇表维度的参数矩阵，而$W^LY_L$是概率化前的中间值，通常被称为logits。&lt;/p&gt;
&lt;p&gt;[1]. JacobDevlin et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. In: NAACL-HLT. 2019.&lt;/p&gt;
&lt;p&gt;[2]. Alec Radford et al. “Improving language understanding by generative pre-training”. In: OpenAI Blog (2018).&lt;/p&gt;
</description>
        </item>
        <item>
        <title>第3周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/third-week/</link>
        <pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/third-week/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/third-week/cards-8594729_640.jpg" alt="Featured image of post 第3周工作总结" /&gt;&lt;h1 id=&#34;文献阅读&#34;&gt;文献阅读
&lt;/h1&gt;&lt;h5 id=&#34;每周都完不成上次的任务杂事太多了尽快完成&#34;&gt;每周都完不成上次的任务，杂事太多了，尽快完成：
&lt;/h5&gt;&lt;h2 id=&#34;detecting-formal-thought-disorder-by-deep-contextualized-word-representations&#34;&gt;Detecting formal thought disorder by deep contextualized word representations
&lt;/h2&gt;&lt;p&gt;竟然要收费，不看了，让gpt给讲一下&lt;/p&gt;
&lt;p&gt;这篇论文《通过深度上下文化词表示检测形式思维障碍》研究了如何使用深度学习中的上下文词嵌入模型（如ELMo）来检测精神疾病患者的形式思维障碍（Formal Thought Disorder, FTD）。FTD是一种影响语言表达和思维逻辑的精神疾病症状，常见于精神分裂症患者。&lt;/p&gt;
&lt;p&gt;论文主要利用深度上下文化词嵌入模型，如ELMo，通过生成词在句子中的嵌入表示，捕捉语境中词的多义性和丰富的语义信息。研究团队通过分析患者的语言数据，构建模型来区分正常语言与存在思维障碍的语言表达，从而实现自动化的FTD检测。&lt;/p&gt;
&lt;p&gt;研究的核心贡献是使用更复杂的语义表示（如上下文化词嵌入）来解决传统方法无法有效处理的词义歧义问题。&lt;/p&gt;
&lt;h2 id=&#34;distributed-representations-of-words-and-phrases-and-their-compositionality&#34;&gt;Distributed Representations of Words and Phrases and their Compositionality
&lt;/h2&gt;&lt;p&gt;《Distributed Representations of Words and Phrases and their Compositionality》是由Mikolov等人在2013年提出的一篇重要论文，介绍了&lt;strong&gt;Word2Vec&lt;/strong&gt;模型的改进和创新。这篇文章主要讨论了如何通过分布式词表示（distributed word representations）更好地捕捉单词和短语的语义信息，并且提出了两种核心模型：&lt;strong&gt;CBOW（Continuous Bag of Words）&lt;/strong&gt; 和  &lt;strong&gt;Skip-gram&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这篇论文的主要贡献包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Skip-gram和CBOW模型&lt;/strong&gt;：论文提出了两种用于生成词向量的架构。Skip-gram模型的目标是根据一个词预测其上下文，而CBOW模型则根据上下文词来预测目标词。这两种模型能够有效地捕捉单词之间的语义关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;层次Softmax和负采样&lt;/strong&gt;：为了提高训练大规模语料库的效率，作者引入了&lt;strong&gt;层次Softmax&lt;/strong&gt;和**负采样（negative sampling）**技术，这大幅降低了模型的计算复杂度，使得大规模训练变得可行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;词和短语的组合性&lt;/strong&gt;：论文不仅处理了单词的表示，还处理了短语的表示，通过数据驱动的方式将多词短语映射为词向量，捕捉更复杂的语义结构。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;向量运算反映语义关系&lt;/strong&gt;：Word2Vec生成的词向量能够通过简单的向量运算表达词语之间的关系。例如，向量运算 &lt;code&gt;vec(&amp;quot;King&amp;quot;) - vec(&amp;quot;Man&amp;quot;) + vec(&amp;quot;Woman&amp;quot;) ≈ vec(&amp;quot;Queen&amp;quot;)&lt;/code&gt; 说明了这种分布式表示在捕捉语义关系上的强大能力。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;llm-book&#34;&gt;LLM BOOK
&lt;/h2&gt;&lt;p&gt;找到一本书，准确的来说是一个github仓库，组织者把近些年的LLM相关的东西都整进去了，好全，写的真好，库库看吧那就&lt;/p&gt;
&lt;p&gt;先把资源放在这里：&lt;a class=&#34;link&#34; href=&#34;https://github.com/RUCAIBox/LLMSurvey/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RUCAIBox/LLMSurvey: The official GitHub page for the survey paper &amp;ldquo;A Survey of Large Language Models&amp;rdquo;.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;第一部分-背景与基础知识&#34;&gt;第一部分 背景与基础知识
&lt;/h3&gt;&lt;h4 id=&#34;语言模型的发展历程p16&#34;&gt;语言模型的发展历程（P16）
&lt;/h4&gt;&lt;p&gt;LLM大语言模型那肯定是从LM语言模型来的，语言模型的发展又经历了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;统计语言模型（StatisticalLanguageModel, SLM）&lt;/li&gt;
&lt;li&gt;神经语言模型（NeuralLanguageModel,NLM）&lt;/li&gt;
&lt;li&gt;预训练语言模型（Pre-trainedLanguageModel,PLM）&lt;/li&gt;
&lt;li&gt;大语言模型（LargeLanguageModel, LLM）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;llm的特点p19&#34;&gt;LLM的特点（P19）
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;具有较为丰富的世界知识&lt;/li&gt;
&lt;li&gt;具有较强的通用任务解决能力&lt;/li&gt;
&lt;li&gt;具有较好的复杂任务推理能力&lt;/li&gt;
&lt;li&gt;具有较强的人类指令遵循能力&lt;/li&gt;
&lt;li&gt;具有较好的人类对齐能力&lt;/li&gt;
&lt;li&gt;具有可拓展的工具使用能力&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;大语言模型对于产业应用带来了变革性的技术影响，将会催生一个基于大语言模型的应用生态系统。&lt;/p&gt;
&lt;h3 id=&#34;第二部分-预训练&#34;&gt;第二部分 预训练
&lt;/h3&gt;&lt;p&gt;大语言模型的第一个训练阶段是预训练，预训练语料的规模和质量对于提升大语言模型的能力至关重要。&lt;/p&gt;
&lt;h4 id=&#34;数据来源&#34;&gt;数据来源
&lt;/h4&gt;&lt;p&gt;通用文本数据和专用文本数据。通用文本数据涵盖了网页、书籍和对话文本等。由于通用文本数据规模较大、多样性强且易于获取，大多数大语言模型都会收集大量的通用文本数据，以增强其语言建模能力。此外，为了进一步提升大语言模型在特定专业任务上的表现，人们还将预训练语料的范围扩展至更专业的数据集，如多语数据、科学数据和代码数据等。&lt;/p&gt;
&lt;h5 id=&#34;通用文本数据&#34;&gt;通用文本数据
&lt;/h5&gt;&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026154722061.png&#34;
	width=&#34;792&#34;
	height=&#34;657&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026154722061_hu1725408825362406164.png 480w, https://JiangZhiyu-1024.github.io/p/third-week/image-20241026154722061_hu12406635621069597218.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241026154722061&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;120&#34;
		data-flex-basis=&#34;289px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;现有大语言模型预训练数据中各种数据来源的比例分布图&lt;/center&gt;&gt;
&lt;h5 id=&#34;专用文本数据&#34;&gt;专用文本数据
&lt;/h5&gt;&lt;p&gt;多语文本、 科学文本、代码。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026155259857.png&#34;
	width=&#34;831&#34;
	height=&#34;189&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026155259857_hu14579868356241669728.png 480w, https://JiangZhiyu-1024.github.io/p/third-week/image-20241026155259857_hu13508329583982414925.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241026155259857&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;439&#34;
		data-flex-basis=&#34;1055px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;典型的预训练数据预处理流程图&lt;/center&gt;&gt;
&lt;h4 id=&#34;数据预处理&#34;&gt;数据预处理
&lt;/h4&gt;&lt;p&gt;构建并使用系统化的数据处理框架（如开源库Data-Juicer）&lt;/p&gt;
&lt;p&gt;具体操作（P74）&lt;/p&gt;
&lt;h1 id=&#34;附录&#34;&gt;附录
&lt;/h1&gt;&lt;h2 id=&#34;softmax是啥&#34;&gt;softmax是啥？
&lt;/h2&gt;&lt;h4 id=&#34;softmax是一种常用的函数特别是在多分类任务中用来将一个向量中的元素转换为0到1之间的概率分布其核心作用是&#34;&gt;Softmax是一种常用的函数，特别是在&lt;strong&gt;多分类任务&lt;/strong&gt;中，用来将一个向量中的元素转换为0到1之间的概率分布。其核心作用是：
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;将实数映射为概率分布&lt;/strong&gt;：Softmax 接受一个实数向量，输出一个与输入维度相同的向量，其中每个元素的值表示该元素作为某个类别的概率，且这些概率的和为1。这样就可以根据这些概率来进行分类预测。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;公式&lt;/strong&gt;：Softmax 函数的定义如下：
&lt;/p&gt;
$$
   \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
   $$&lt;p&gt;
其中，\( z_i \) 是输入向量中的某个元素，\( e^{z_i} \) 是该元素的指数值，分母是所有输入元素的指数值的和。这个函数将每个 \( z_i \) 映射为一个在 \( (0,1) \) 之间的概率，并且所有输出的概率之和为1。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;作用机制&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;指数放大效果&lt;/strong&gt;：Softmax 函数通过对输入值取指数（\( e^x \)），会将较大的输入值放大更多，而较小的值则被进一步压缩，突出更大的概率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;归一化&lt;/strong&gt;：Softmax 保证所有输出的概率之和为1，符合概率的定义。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;应用场景&lt;/strong&gt;：在&lt;strong&gt;神经网络的最后一层&lt;/strong&gt;，特别是分类任务中，Softmax 常用于多类分类问题，比如在自然语言处理（NLP）中的词分类、图像识别中的对象分类等。它将网络的输出转化为易于理解的概率值，帮助模型决定输入属于哪个类别。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;举例&#34;&gt;举例
&lt;/h4&gt;&lt;p&gt;假设我们有一个模型输出三个值 \( [2.0, 1.0, 0.1] \)，通过 Softmax 转换后得到的结果是大约 \( [0.71, 0.26, 0.03] \)，其中最大的值0.71表示模型认为输入属于第一个类别的概率最大。&lt;/p&gt;
&lt;p&gt;简言之，Softmax 是一种将输出转换为概率分布的工具，特别适合多分类问题的场景。&lt;/p&gt;
&lt;h2 id=&#34;看了半天论文感觉没有一个时间线很难受不清楚那些论文之间的关系问问gpt&#34;&gt;看了半天论文，感觉没有一个时间线很难受，不清楚那些论文之间的关系，问问gpt：
&lt;/h2&gt;&lt;h3 id=&#34;基础概念与理论&#34;&gt;&lt;strong&gt;基础概念与理论&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;先掌握大模型背后的基本理论，这些知识是理解后续论文的基础：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语言模型的基础：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;经典论文：《Attention Is All You Need》（Transformer）&lt;/li&gt;
&lt;li&gt;相关概念：自注意力机制、序列到序列模型、词嵌入（如Word2Vec、GloVe）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自回归模型与自编码器：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;GPT 系列（GPT, GPT-2, GPT-3）的原理和应用&lt;/li&gt;
&lt;li&gt;BERT 及其衍生模型的预训练与微调方式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;扩展学习与生成式任务：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》&lt;/li&gt;
&lt;li&gt;生成式预训练模型的设计与任务应用（如文本生成、机器翻译）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;模型的训练与优化&#34;&gt;&lt;strong&gt;模型的训练与优化&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;学习如何高效地训练大模型，并且了解模型的优化技术：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;微调与参数高效训练：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Efficient Fine-Tuning Techniques for Large Models: Adapter, LoRA 等&lt;/li&gt;
&lt;li&gt;探索模型压缩技术（知识蒸馏、量化、剪枝等）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对比学习与自监督学习：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;《SimCLR》、《MoCo》 等代表性论文，理解对比学习在大模型中的应用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;模型对齐与安全&#34;&gt;&lt;strong&gt;模型对齐与安全&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;在大模型发展过程中，模型对齐（alignment）和伦理问题变得越来越重要：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型对齐与价值观嵌入：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;《InstructGPT: Training language models to follow instructions with human feedback》&lt;/li&gt;
&lt;li&gt;RLHF（强化学习通过人类反馈）等对齐策略&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型安全与公平性：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;探讨模型偏见问题以及在实际应用中的安全隐患&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;视觉-语言模型与多模态模型&#34;&gt;&lt;strong&gt;视觉-语言模型与多模态模型&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;学习大模型在跨模态任务中的应用，比如视觉-语言任务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CLIP（Contrastive Language-Image Pretraining）&lt;/strong&gt;：理解图像和文本共同表征的训练方法&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DALL-E、Stable Diffusion&lt;/strong&gt;：生成式视觉任务的模型设计与应用&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;视觉语言对话系统&lt;/strong&gt;：如《Visual ChatGPT》&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;前沿研究与新趋势&#34;&gt;&lt;strong&gt;前沿研究与新趋势&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;在深入理解基本模型后，可以关注大模型领域的前沿研究方向：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;大模型与知识图谱的结合&lt;/strong&gt;：如何将知识图谱融入语言模型&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;领域自适应模型&lt;/strong&gt;：如何通过特定领域数据进行模型的进一步微调&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;检索增强生成（RAG）&lt;/strong&gt;：如《RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;实战应用与开源框架&#34;&gt;&lt;strong&gt;实战应用与开源框架&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;阅读开源项目中的技术文档，参与开源项目（如Hugging Face的模型库）&lt;/li&gt;
&lt;li&gt;学习如何应用大模型解决实际问题，并尝试训练和微调自己的模型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样，你可以从基础概念、模型训练到前沿研究和实战应用，逐步深入，既能掌握理论又能结合实践。这个过程可以帮助你筛选出重要的论文和研究方向。&lt;/p&gt;
&lt;h2 id=&#34;对齐是什么意思&#34;&gt;对齐是什么意思？
&lt;/h2&gt;&lt;p&gt;在大型语言模型（LLM）中，“对齐”通常指的是模型的输出与人类意图或价值观的匹配程度。具体来说，它涉及确保模型在生成文本时能够产生符合人类期望的、有益的和安全的内容。对齐的目标是减少模型生成有害、误导或不准确的信息。&lt;/p&gt;
&lt;p&gt;对齐可以通过多种方法实现，包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练数据的选择和清洗&lt;/strong&gt;：使用高质量的数据集进行训练，确保模型接触到的内容符合人类的价值观和期望。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;强化学习&lt;/strong&gt;：利用人类反馈来微调模型，使其在生成内容时更符合人类的意图。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;安全性和伦理性评估&lt;/strong&gt;：在模型发布之前，对其输出进行评估，以识别和修正潜在的偏见或有害内容。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过这些方法，可以提高模型在特定任务或应用场景中的可靠性和安全性。&lt;/p&gt;
&lt;h2 id=&#34;arxiv-论文是什么&#34;&gt;arXiv 论文是什么？
&lt;/h2&gt;&lt;p&gt;arXiv 是一个开放获取的电子预印本存档，主要用于科学研究领域，特别是在物理学、数学、计算机科学、统计学、金融学、生物学和其他相关学科。研究者可以在 arXiv 上免费上传和分享他们的研究论文，通常是在正式出版之前的版本。&lt;/p&gt;
&lt;p&gt;arXiv 的特点包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;快速发布&lt;/strong&gt;：研究者可以迅速分享他们的研究成果，而不必等待同行评审和正式出版的过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;开放获取&lt;/strong&gt;：任何人都可以访问和下载论文，无需订阅或支付费用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;广泛的学科覆盖&lt;/strong&gt;：涵盖多个领域，促进跨学科的交流和合作。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;社区反馈&lt;/strong&gt;：研究者可以在正式发表之前收到来自社区的反馈，帮助他们改进论文。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;arXiv 的论文通常以“预印本”的形式存在，因此它们可能尚未经过同行评审。这意味着在引用这些论文时，研究者需要谨慎，了解其可能的局限性。&lt;/p&gt;
&lt;h2 id=&#34;在arxiv上发还未出版的论文不怕被别人剽窃成果吗&#34;&gt;在arXiv上发还未出版的论文不怕被别人剽窃成果吗？
&lt;/h2&gt;&lt;p&gt;在 arXiv 上发布未出版的论文确实存在被剽窃的风险，但研究者通常认为在 arXiv 上共享研究成果是有益的，原因如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;时间戳和公开记录&lt;/strong&gt;：一旦论文被上传到 arXiv，上传的时间戳可以作为知识产权的证明。这在某种程度上可以保护研究者的原创性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;学术社区的支持&lt;/strong&gt;：arXiv 是一个活跃的学术社区，研究者通常会关注相关领域的论文，并且对剽窃行为持有负面态度。学术界普遍重视诚信，剽窃通常会受到严厉的谴责。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;早期反馈&lt;/strong&gt;：通过发布预印本，研究者可以获得来自同行的反馈，从而改进自己的工作。这也有助于在正式出版之前建立声誉。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;增加曝光率&lt;/strong&gt;：在 arXiv 上发布可以提高研究的可见性，吸引潜在合作者或资助者。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;通用人工智能系统是什么&#34;&gt;通用人工智能系统是什么？
&lt;/h2&gt;&lt;p&gt;通用人工智能系统（AGI, Artificial General Intelligence）指的是一种能够理解、学习和应用知识，完成任何人类所能完成的智力任务的人工智能系统。与当前的狭义人工智能（Narrow AI）相比，后者在特定任务或领域表现出色（如图像识别、自然语言处理等），AGI 的目标是实现一种更广泛的智能，具备跨领域的学习和适应能力。&lt;/p&gt;
&lt;p&gt;AGI 的特点包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;自主学习&lt;/strong&gt;：能够从有限的数据中学习并推理，具备自我改进的能力。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多任务处理&lt;/strong&gt;：能够在多个不同的领域和任务中表现出色，而不局限于特定的应用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;常识推理&lt;/strong&gt;：具备理解和应用常识的能力，能够处理复杂的、不确定的环境。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;灵活性和适应性&lt;/strong&gt;：能够适应新情况和挑战，灵活地应对不同的环境和需求。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;AGI 的发展仍然是一个前沿研究领域，目前尚未实现。许多研究者在探索如何构建这样的系统，同时也讨论其潜在的伦理和社会影响，例如对就业、隐私和安全的影响。&lt;/p&gt;
&lt;h1 id=&#34;思考&#34;&gt;思考
&lt;/h1&gt;&lt;p&gt;随着看的东西多了，和使用chatgpt，我隐隐感觉到，由于chatgpt的语言模型主要是通过预测下一个最可能的词来生成文本，这一过程称为自回归生成（autoregressive generation）。导致gpt经常会顺着你的话去说，即有更小的概论去反对你的话，尽量让你说的话看起来正确，当然，如果你说的是：“太阳从西边升起”，那么他会义无反顾的反驳你。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>第2周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/second-week/</link>
        <pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/second-week/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/second-week/leaves-6729056_640.jpg" alt="Featured image of post 第2周工作总结" /&gt;&lt;h2 id=&#34;文献阅读&#34;&gt;文献阅读
&lt;/h2&gt;&lt;p&gt;看了知乎的科普文章，先从经典的论文开始看：&lt;/p&gt;
&lt;h3 id=&#34;attention-is-all-you-need&#34;&gt;Attention Is All You Need
&lt;/h3&gt;&lt;p&gt;《Attention is All You Need》是由Vaswani等人在2017年提出的一篇重要论文，首次引入了Transformer模型。以下是其主要内容的总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;：传统的序列到序列模型（如RNN和LSTM）在处理长序列时存在瓶颈。Transformer模型通过自注意力机制克服了这些限制，使得模型可以并行处理数据，提升了训练效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型架构&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder-Decoder结构&lt;/strong&gt;：Transformer由两个主要部分组成：编码器和解码器。编码器将输入序列转化为上下文向量，解码器则根据上下文生成输出序列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自注意力机制&lt;/strong&gt;：每个输入的表示都通过自注意力机制与其他输入进行交互，从而捕捉序列中的依赖关系。每个位置的表示通过加权求和得到，权重由输入的相似度计算得到。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多头注意力&lt;/strong&gt;：模型使用多个注意力头并行计算不同的表示，能够更全面地捕捉信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;位置编码&lt;/strong&gt;：由于Transformer没有循环结构，论文引入了位置编码，提供序列中词语的位置信息，以保留词序信息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练与优化&lt;/strong&gt;：Transformer使用了残差连接和层归一化，以提高训练的稳定性和收敛速度。论文中还介绍了基于Adam优化器的训练过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;：Transformer在机器翻译任务上取得了显著的性能提升，并且在其他NLP任务中也展示了强大的通用性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;影响&lt;/strong&gt;：该论文奠定了现代NLP研究的基础，推动了基于Transformer的预训练语言模型（如BERT、GPT等）的发展。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;差分隐私深度学习deep-learning-with-differential-privacy&#34;&gt;差分隐私深度学习(Deep Learning with Differential Privacy)
&lt;/h3&gt;&lt;p&gt;《Deep Learning with Differential Privacy》是由Martin Abadi等人在2016年提出的一篇重要论文，探讨了在深度学习中实现差分隐私的策略。以下是其主要内容的总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;差分隐私&lt;/strong&gt;：一种保护用户数据隐私的技术，旨在确保模型在训练时无法推断出任何单个用户的敏感信息。即使攻击者知道模型和数据集的其余部分，也无法有效地获取关于某个特定用户的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度学习与隐私问题&lt;/strong&gt;：深度学习模型通常依赖大量数据，这使得保护用户隐私变得尤为重要。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型与训练&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;噪声注入&lt;/strong&gt;：通过在模型的梯度更新中添加噪声，可以达到差分隐私的效果。噪声的大小与隐私预算（ε）相关，预算越小，隐私保护越强，但模型的性能可能会受到影响。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Accounting&lt;/strong&gt;：论文提出了一种计算隐私损失的框架，确保在每次训练步骤中监控和控制隐私损失。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算法设计&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DP-SGD（Differentially Private Stochastic Gradient Descent）&lt;/strong&gt;：论文提出了一种改进的随机梯度下降算法，结合了梯度裁剪和噪声注入，以实现差分隐私。这种方法确保在训练过程中，个别数据对模型的影响是被限制的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文通过实验展示了DP-SGD在图像分类任务上的有效性，同时比较了不同噪声级别对模型性能的影响，证明了在保持合理准确性的同时实现差分隐私是可行的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;应用与展望&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文强调了在各类机器学习应用中（如医疗、金融等）保护隐私的重要性，提出将差分隐私技术与深度学习结合是一个有效的解决方案，并鼓励未来的研究在这一领域的进一步探索。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;感觉偏向数学一点，全是公式和证明，好难看懂。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;差分隐私顾名思义就是用来防范差分攻击的
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;加入噪声，改变原来的概率分布
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;hr&gt;
&lt;h3 id=&#34;deep-reinforcement-learning-from-human-preferences&#34;&gt;Deep Reinforcement Learning from Human Preferences
&lt;/h3&gt;&lt;p&gt;《Deep Reinforcement Learning from Human Preferences》是由Christiano等人在2017年提出的一篇重要论文，探讨了如何利用人类偏好来训练强化学习模型。以下是其主要内容的总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;传统的强化学习方法通常依赖于奖励信号来指导学习，但设计有效的奖励函数在复杂任务中往往困难且耗时。&lt;/li&gt;
&lt;li&gt;人类可以提供更直观的偏好信息，这为训练强化学习代理提供了一种新途径。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;方法概述&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;人类偏好收集&lt;/strong&gt;：论文提出了一种方法，通过收集人类对不同行为的偏好来生成奖励信号。人类评估多个策略的表现，从中选择更好的策略，以此来建立奖励模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励模型&lt;/strong&gt;：利用收集到的人类偏好数据，训练一个深度神经网络来预测代理在给定状态下的奖励。这种奖励模型可以用来指导强化学习算法。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练过程&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;深度强化学习&lt;/strong&gt;：在获得奖励模型后，使用深度强化学习算法（如Proximal Policy Optimization，PPO）来训练代理。代理通过与环境交互，不断优化其策略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;迭代学习&lt;/strong&gt;：通过不断收集新的偏好数据并更新奖励模型，实现迭代学习，使得代理能够逐步提升性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文在多个复杂任务（如Atari游戏和模拟环境）中展示了该方法的有效性，代理能够通过学习人类的偏好，超越基于手动设计的奖励函数的表现。&lt;/li&gt;
&lt;li&gt;结果表明，利用人类偏好进行训练能够加速学习过程，并提升代理的最终性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;意义与展望&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该方法展示了人类偏好在强化学习中的潜力，提出了一种新的思路来解决奖励设计问题。&lt;/li&gt;
&lt;li&gt;论文强调了未来研究中结合人类反馈和偏好的重要性，特别是在涉及复杂决策和安全性问题的领域。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;附录&#34;&gt;附录
&lt;/h2&gt;&lt;h3 id=&#34;残差连接residual-connection和层归一化layer-normalization是什么&#34;&gt;残差连接（residual connection）和层归一化（layer normalization）是什么？
&lt;/h3&gt;&lt;p&gt;残差连接（Residual Connection）和 层归一化（Layer Normalization）是深度学习中常用的两种技术，主要用于提高神经网络的训练效率和性能。下面是它们的定义和作用：&lt;/p&gt;
&lt;h4 id=&#34;残差连接residual-connection&#34;&gt;残差连接（Residual Connection）
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：残差连接是一种将输入直接与输出相加的结构，通常在深度神经网络中使用。公式表示为：
&lt;/p&gt;
$$
\text{Output} = \text{Layer}(x) + x
$$&lt;p&gt;
其中 \(x\) 是输入，\(\text{Layer}(x)\) 是通过某个层（如卷积层或全连接层）处理后的输出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;缓解梯度消失问题&lt;/strong&gt;：在非常深的网络中，梯度可能会随着层数增加而消失，导致网络难以训练。残差连接提供了一条捷径，使得梯度可以更容易地通过网络传播。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加速收敛&lt;/strong&gt;：通过引入直接路径，残差连接有助于提高网络的收敛速度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提高模型性能&lt;/strong&gt;：它允许模型学习恒等映射，这使得训练更深的网络成为可能，提升了模型的表现。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;层归一化layer-normalization&#34;&gt;层归一化（Layer Normalization）
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：层归一化是一种对网络中每一层的激活进行归一化的技术，它通过计算每个样本的均值和标准差来调整激活值。层归一化的公式为：
&lt;/p&gt;
$$
\text{LayerNorm}(x) = \frac{x - \mu}{\sigma + \epsilon} \cdot \gamma + \beta
$$&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(x\) 是输入。&lt;/li&gt;
&lt;li&gt;\(\mu\) 是输入的均值。&lt;/li&gt;
&lt;li&gt;\(\sigma\) 是输入的标准差。&lt;/li&gt;
&lt;li&gt;\(\epsilon\) 是一个小常数，避免除以零。&lt;/li&gt;
&lt;li&gt;\(\gamma\) 和 \(\beta\) 是可学习的参数，用于缩放和平移归一化的结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;减少内部协变量偏移&lt;/strong&gt;：层归一化帮助减少模型训练过程中每层输入的变化，使得网络在训练时更加稳定。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提高训练速度&lt;/strong&gt;：通过标准化输入，层归一化可以使得训练过程更加平滑，从而加快收敛速度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;适用于变长序列&lt;/strong&gt;：与批量归一化（Batch Normalization）不同，层归一化可以直接应用于变长的输入序列，适合于RNN和Transformer等模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;残差连接&lt;/strong&gt;主要通过提供捷径来缓解深层网络中的梯度消失问题，并加速训练过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;层归一化&lt;/strong&gt;则通过标准化激活值来提高训练的稳定性和速度。这两者结合使用，能够显著提升深度学习模型的表现。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>第1周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/first-week/</link>
        <pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/first-week/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/first-week/banlan.jpg" alt="Featured image of post 第1周工作总结" /&gt;&lt;h1 id=&#34;工作总结&#34;&gt;&lt;strong&gt;工作总结&lt;/strong&gt;：
&lt;/h1&gt;&lt;h2 id=&#34;文献阅读&#34;&gt;文献阅读
&lt;/h2&gt;&lt;h3 id=&#34;综述类&#34;&gt;综述类：
&lt;/h3&gt;&lt;p&gt;2023年国内大模型发展综述与趋势研判_赵子忠&lt;/p&gt;
&lt;p&gt;AIGC大模型测评综述：使能技术、安全隐患和应对_许志伟&lt;/p&gt;
&lt;p&gt;AI大模型发展综述_张乾君&lt;/p&gt;
&lt;h3 id=&#34;深入&#34;&gt;深入：
&lt;/h3&gt;&lt;hr&gt;
&lt;p&gt;《A Neural Probabilistic Language Model》是Yoshua Bengio等人在2003年发表的一篇重要论文，提出了一种基于神经网络的语言模型方法。这篇论文的主要目标是通过神经网络来改进传统的语言模型。&lt;/p&gt;
&lt;p&gt;传统的语言模型的任务是估计一个词序列的概率：
P(w1,w2,…,wT)
在这种情况下，模型需要根据上下文词来预测当前词的概率。传统的做法一般是通过统计方法（如n元语法模型）来估计这些概率，但这类方法有一个很大的问题：它们无法处理高维数据，尤其是在词汇量非常大的情况下，计算复杂度极高，且泛化能力有限。&lt;/p&gt;
&lt;p&gt;Bengio等人提出了一种新的神经网络语言模型，目的是克服这个问题。这个模型的核心思想是使用一个多层感知器来学习词的分布式表示（即词向量），并基于这些词向量来估计概率。具体过程大致如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;词嵌入&lt;/strong&gt;：每个词通过一个嵌入矩阵被映射为一个连续向量，这种向量的维度比词汇表要小得多。这一步的目的是将离散的词映射到一个连续的低维空间，减少计算复杂度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;神经网络建模&lt;/strong&gt;：利用一个前馈神经网络（通常是多层感知器），根据前面的若干个词的词向量，来预测下一个词的条件概率。通过这个网络，模型可以学习到词与词之间的相似性和上下文关系。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练&lt;/strong&gt;：模型通过最大化训练数据的似然估计来进行训练。具体来说，模型会调整嵌入矩阵和网络的参数，使得预测的概率尽可能接近真实的词序列概率。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这篇论文的创新之处在于提出了“词向量”的概念，并展示了使用神经网络来处理语言建模问题的潜力。这为后来的深度学习在自然语言处理（NLP）中的应用奠定了基础。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;附录&#34;&gt;附录
&lt;/h1&gt;&lt;h2 id=&#34;什么是语言模型&#34;&gt;什么是语言模型
&lt;/h2&gt;&lt;p&gt;语言模型（Language Model, LM）是自然语言处理中的一个核心概念，其主要任务是对一个词序列的概率分布进行建模。简单来说，语言模型的目标是估计一段文本或句子出现的概率，或者根据前面的词来预测下一个词。&lt;/p&gt;
&lt;h3 id=&#34;语言模型的定义&#34;&gt;语言模型的定义
&lt;/h3&gt;&lt;p&gt;给定一个词序列
&lt;/p&gt;
$$
P( w_1, w_2, \dots, w_T )
$$&lt;p&gt;
，语言模型的目标是计算这个序列的联合概率：
&lt;/p&gt;
$$
P(w_1, w_2, \dots, w_T)
$$&lt;p&gt;
这通常可以分解为条件概率的乘积：
&lt;/p&gt;
$$
P(w_1, w_2, \dots, w_T) = P(w_1) \cdot P(w_2 | w_1) \cdot P(w_3 | w_1, w_2) \cdot \dots \cdot P(w_T | w_1, w_2, \dots, w_{T-1})
$$&lt;p&gt;
这意味着每个词的概率取决于它前面的词。一个好的语言模型能够通过学习语料库中的模式，准确地捕捉这些条件概率。&lt;/p&gt;
&lt;h3 id=&#34;语言模型的用途&#34;&gt;语言模型的用途
&lt;/h3&gt;&lt;p&gt;语言模型有广泛的应用场景，包括但不限于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;文本生成&lt;/strong&gt;：语言模型可以用来自动生成自然语言文本。例如，给定一部分句子，模型可以预测并生成合理的下一个词，从而逐步生成完整的句子或段落。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;机器翻译&lt;/strong&gt;：在机器翻译中，语言模型帮助翻译系统生成符合目标语言语法的翻译结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;语音识别&lt;/strong&gt;：语言模型能够帮助语音识别系统，在可能的识别结果中选择最合适的词序列。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;拼写纠正&lt;/strong&gt;：在文本输入或搜索引擎中，语言模型可以用于纠正拼写错误，提供更符合语言习惯的结果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;传统语言模型&#34;&gt;传统语言模型
&lt;/h3&gt;&lt;p&gt;在神经网络语言模型之前，最常用的语言模型是&lt;strong&gt;n元语法模型（n-gram Model）&lt;/strong&gt;。n元语法模型通过假设一个词的概率只与它前面的n-1个词有关来简化问题。举个例子，三元语法模型（trigram model）认为：
&lt;/p&gt;
$$
P(w_T | w_1, w_2, \dots, w_{T-1}) \approx P(w_T | w_{T-2}, w_{T-1})
$$&lt;p&gt;这种简化降低了计算复杂度，但也限制了模型的表达能力，因为它只能捕捉到短距离的上下文信息，无法充分考虑长距离的依赖关系。&lt;/p&gt;
&lt;h3 id=&#34;神经网络语言模型&#34;&gt;神经网络语言模型
&lt;/h3&gt;&lt;p&gt;传统语言模型的一个主要问题是词汇量增大时，模型的计算复杂度随之增加。为了解决这个问题，Yoshua Bengio等人在《A Neural Probabilistic Language Model》论文中提出了使用神经网络进行语言建模的想法。这种神经网络模型通过将词表示为&lt;strong&gt;词向量（word embeddings）&lt;/strong&gt;，并通过深度学习模型来捕捉更复杂的语境和词间关系，从而解决了传统语言模型的许多不足之处。&lt;/p&gt;
&lt;p&gt;总之，语言模型的核心任务就是对自然语言的概率进行建模，并通过学习语言中的统计模式来预测文本。&lt;/p&gt;
&lt;h2 id=&#34;什么是纬度灾难&#34;&gt;什么是纬度灾难？
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;维度灾难&lt;/strong&gt;（Curse of Dimensionality）是指在处理高维数据时，随着数据维度增加，问题的复杂性呈指数级增长，导致很多传统方法在高维空间中变得低效或不可行的现象。&lt;/p&gt;
&lt;p&gt;其影响包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;数据稀疏性&lt;/strong&gt;：在高维空间中，数据点变得非常稀疏，因为数据的总量相对于空间的体积变得极其小，导致模型难以从有限的数据中捕捉到模式和关系。简单来说，随着维度的增加，需要的样本量也随之爆炸性增长，否则模型可能会过拟合或无法泛化。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;计算复杂度增加&lt;/strong&gt;：在高维空间中，计算距离、相似性等操作的复杂度大大增加，使得很多算法在高维情况下效率低下。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型的泛化能力下降&lt;/strong&gt;：由于数据点过于稀疏，模型很难学习到有用的特征，从而可能导致泛化能力下降。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;具体来说，维度灾难对机器学习、统计学、最优化等领域带来了极大的挑战，尤其是当模型需要处理很多特征或变量时。为应对这个问题，研究人员提出了许多方法，如&lt;strong&gt;降维技术&lt;/strong&gt;（如主成分分析PCA）和&lt;strong&gt;分布式表示&lt;/strong&gt;（如词嵌入），以减少有效维度或通过分布式方法来表示复杂的数据结构。&lt;/p&gt;
&lt;h2 id=&#34;有概率为什么要引入似然的概念他们有什么不同&#34;&gt;有概率为什么要引入似然的概念，他们有什么不同？
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;概率&lt;/strong&gt;和&lt;strong&gt;似然&lt;/strong&gt;虽然看起来相似，但在统计学中它们有不同的定义和用途：&lt;/p&gt;
&lt;h3 id=&#34;概率probability&#34;&gt;&lt;strong&gt;概率（Probability）&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;概率是用来描述随机事件发生的可能性。在给定模型参数的情况下，概率告诉我们某个事件或观测结果发生的机会有多大。&lt;/p&gt;
&lt;p&gt;假设我们有一个随机变量 \( X \)，它取某个值 \( x \) 的概率可以表示为 \( P(X = x|\theta) \)，其中 \( \theta \) 是模型的参数。例如，如果我们有一个硬币掷出的模型，参数 \( \theta \) 可以是硬币正面朝上的概率（比如 \( \theta = 0.5 \)）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：给定模型的参数 \( \theta \)，事件 \( X = x \) 发生的概率是多少？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;似然likelihood&#34;&gt;&lt;strong&gt;似然（Likelihood）&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;似然是从&lt;strong&gt;反方向&lt;/strong&gt;来考虑的。它描述的是&lt;strong&gt;在已知观测数据的前提下，模型参数的可能性&lt;/strong&gt;。具体来说，似然是给定观测数据后，模型参数如何使观测数据变得最可能的一个度量。&lt;/p&gt;
&lt;p&gt;假设我们已经观察到数据 \( X = x \)，现在我们想知道，在不同的模型参数 \( \theta \) 下，这个数据出现的可能性有多大。似然可以表示为 \( L(\theta|X = x) \)，或者更直观地写作 \( P(X = x|\theta) \)，虽然数学表达式相同，但意义不同——在这里我们关注的是参数 \( \theta \) 的值使观测到的数据最可能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：给定观测到的数据 \( X = x \)，模型参数 \( \theta \) 有多大可能是正确的？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;概率和似然的区别&#34;&gt;概率和似然的区别
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;概率&lt;/strong&gt;：我们知道参数 \( \theta \)，希望知道某个事件 \( X \) 发生的概率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题：&lt;strong&gt;给定参数，事件的概率是多少？&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;似然&lt;/strong&gt;：我们知道事件（观测数据），希望推断出最有可能的参数 \( \theta \)。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题：&lt;strong&gt;给定观测数据，哪个参数最可能是正确的？&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;例子&#34;&gt;例子
&lt;/h3&gt;&lt;h4 id=&#34;抛硬币的例子&#34;&gt;抛硬币的例子
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;概率&lt;/strong&gt;：假设你有一枚硬币，已知它是公平的，即 \( \theta = 0.5 \)，那么掷硬币得到正面的概率是 \( P(\text{正面}) = 0.5 \)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;似然&lt;/strong&gt;：现在，你掷硬币10次，得到8次正面和2次反面。你不知道硬币的公平性（即不知道 \( \theta \) 的值）。你想根据这次实验结果估计硬币正面朝上的概率 \( \theta \)。此时，你需要用&lt;strong&gt;似然&lt;/strong&gt;来衡量在不同 \( \theta \) 值下，观测结果（8次正面，2次反面）出现的可能性有多大，从而找到最有可能的 \( \theta \)。&lt;/p&gt;
&lt;p&gt;似然函数 \( L(\theta|X) \) 可能在某个 \( \theta \) 值处达到最大值，这个 \( \theta \) 就是最能解释观测数据的参数值。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;概率&lt;/strong&gt;用于给定模型参数时预测事件的发生可能性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;似然&lt;/strong&gt;用于在已知观测数据时，推断哪个参数最能解释这些数据。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
