<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>LLM on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/categories/llm/</link>
        <description>Recent content in LLM on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Fri, 10 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/categories/llm/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>大模型基础知识补习&amp;GPT2代码分析</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E8%A1%A5%E4%B9%A0gpt2%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</link>
        <pubDate>Fri, 10 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E8%A1%A5%E4%B9%A0gpt2%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E8%A1%A5%E4%B9%A0gpt2%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/tree-838666_1280.jpg" alt="Featured image of post 大模型基础知识补习&amp;GPT2代码分析" /&gt;&lt;h1 id=&#34;大模型基础知识&#34;&gt;大模型基础知识
&lt;/h1&gt;&lt;p&gt;代码来源：https://github.com/openai/gpt-2&lt;/p&gt;
&lt;p&gt;GPT2源代码提供了以下几个文件：&lt;/p&gt;
&lt;p&gt;encoder.py&lt;/p&gt;
&lt;p&gt;generate_unconditional_samples.py&lt;/p&gt;
&lt;p&gt;interactive_conditional_samples.py&lt;/p&gt;
&lt;p&gt;model.py&lt;/p&gt;
&lt;p&gt;sample.py&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;model
&lt;/h2&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;def default_hparams():
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    return HParams(
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        n_vocab=0,        # 词汇表大小（通常是词汇表中包含的词语或符号数量）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        n_ctx=1024,       # 输入序列的最大长度（例如，一个输入文本序列的最大 token 数量）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        n_embd=768,       # 嵌入层的维度大小（每个词或符号在嵌入空间中的表示长度）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        n_head=12,        # 多头自注意力机制中的头数（即注意力头的数量）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        n_layer=12,       # Transformer 模型中的层数（即堆叠的 transformer 层数）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    )
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;词汇表&#34;&gt;词汇表
&lt;/h3&gt;&lt;p&gt;词汇表（Vocabulary）是自然语言处理（NLP）模型中用于将语言转换为数字表示的一组词或符号的集合。在深度学习模型中，词汇表用于将文本数据中的每个单词或符号映射为一个唯一的数字（通常是一个整数），以便计算机能够理解和处理。&lt;/p&gt;
&lt;h4 id=&#34;词汇表的作用&#34;&gt;词汇表的作用：
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;映射文本到数字&lt;/strong&gt;：由于机器学习模型无法直接处理文本数据，必须将文本中的每个词或符号转换为数字。词汇表就是这个映射的核心工具。例如，对于句子“我爱自然语言处理”，我们需要将“我”、“爱”、“自然”、“语言”、“处理”这些词转化为模型可以理解的数字。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;限制模型输入的规模&lt;/strong&gt;：在实际操作中，词汇表的大小有限。通常，我们不会为每个可能的字词创建一个索引，而是根据训练数据中出现的词汇频率来限制词汇表的大小。这样，词汇表的规模可以控制模型的输入维度，避免过大的计算和内存开销。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;处理未知词汇&lt;/strong&gt;：词汇表通常会为所有未出现在训练数据中的词汇或符号保留一个特殊的“未知词”标记（如 &lt;code&gt;[UNK]&lt;/code&gt;）。当模型遇到这些未见过的词时，会用这个特殊标记来代替。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;词汇表的组成&#34;&gt;词汇表的组成：
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;词&lt;/strong&gt;：如果你使用的是基于单词的词汇表，那么词汇表中的元素就是单词。例如，假设你有一个很小的词汇表：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;css
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;复制代码
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[&amp;#39;我&amp;#39;, &amp;#39;爱&amp;#39;, &amp;#39;自然&amp;#39;, &amp;#39;语言&amp;#39;, &amp;#39;处理&amp;#39;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;子词&lt;/strong&gt;（Subword）：对于一些较为复杂的语言模型，尤其是像 GPT-2 和 BERT 这样的模型，它们通常使用 &lt;strong&gt;子词&lt;/strong&gt; 作为词汇表的基本单元。子词是一个比单词更小的语言单位，它可以是词的一部分或者是一个常见的词根、前缀、后缀等。例如，词汇表中可能包含 &lt;code&gt;##ing&lt;/code&gt;、&lt;code&gt;un&lt;/code&gt; 等子词单元，而不是整个单词 &lt;code&gt;running&lt;/code&gt; 或 &lt;code&gt;undo&lt;/code&gt;。这有助于模型更好地处理未见过的词汇，并减少词汇表的大小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;特殊标记&lt;/strong&gt;：除了实际的词或子词，词汇表通常还会包含一些特殊标记：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;[PAD]&lt;/code&gt;：填充标记，用于将输入文本填充到相同的长度。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[UNK]&lt;/code&gt;：未知标记，用于处理那些在训练集中没有出现过的词。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[CLS]&lt;/code&gt;：分类标记，通常用于标记输入的开始（特别是在 BERT 等模型中）。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[SEP]&lt;/code&gt;：分隔标记，通常用于分隔不同句子或文本段落（在 BERT 等模型中也有使用）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;向量空间vector-space&#34;&gt;向量空间（Vector Space）
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;向量空间&lt;/strong&gt;是一个数学概念，指的是所有向量的集合，可以在其中进行加法和标量乘法等操作。在机器学习和自然语言处理中，我们通常将每个词、子词或符号表示为一个高维空间中的向量。例如，一个简单的二维向量空间可能包含了这样的一组向量：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;(1, 0)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(0, 1)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(1, 1)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 NLP 中，这些向量并没有具体的几何含义，而是用来表示不同的语义特征。每个向量的元素（即各维度的值）代表了某种抽象的语义属性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;词向量空间&lt;/strong&gt;意味着每个词都被映射为一个向量，且这些向量之间有一定的关系。例如，&amp;ldquo;king&amp;rdquo; 和 &amp;ldquo;queen&amp;rdquo; 可能在向量空间中非常接近，因为它们的语义相似，尽管它们的拼写不同。嵌入的目标就是将语言中的词语、句子或其他语言单位映射到这样一个向量空间中。&lt;/p&gt;
&lt;h3 id=&#34;嵌入embedding&#34;&gt;嵌入（Embedding）
&lt;/h3&gt;&lt;p&gt;**嵌入（Embedding）**是将一个离散的词汇项（如单词、子词）转换为连续的向量表示的过程。这些向量通常具有固定的维度（例如 300 维、768 维等），并且能够捕捉该词的语义信息。&lt;/p&gt;
&lt;p&gt;换句话说，嵌入是对词汇的数字表示，它通过将每个词或符号映射到一个向量空间中，使得模型能够理解和处理语言中的复杂结构。&lt;/p&gt;
&lt;p&gt;例如，假设你有以下三个词汇：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;猫&amp;rdquo;（cat）&lt;/li&gt;
&lt;li&gt;&amp;ldquo;狗&amp;rdquo;（dog）&lt;/li&gt;
&lt;li&gt;&amp;ldquo;汽车&amp;rdquo;（car）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过嵌入，这些词语会被映射为固定长度的向量，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;猫&amp;rdquo; → &lt;code&gt;[0.45, 0.87, 0.34, ...]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;狗&amp;rdquo; → &lt;code&gt;[0.42, 0.89, 0.31, ...]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;汽车&amp;rdquo; → &lt;code&gt;[0.21, 0.08, 0.76, ...]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些向量的维度（例如 300 维或 768 维）反映了嵌入空间的复杂度。模型通过训练来优化这些向量，使得语义相似的词在嵌入空间中距离更近，语义不同的词距离较远。&lt;/p&gt;
&lt;h3 id=&#34;向量vector&#34;&gt;向量（Vector）
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;向量&lt;/strong&gt;是在数学和计算机科学中用来表示方向和大小的对象。每个词或符号被嵌入为一个&lt;strong&gt;向量&lt;/strong&gt;，这个向量通常是一个浮动的数字数组。向量的每个元素都表示一个特定的特征维度。&lt;/p&gt;
&lt;p&gt;在 NLP 中，词的向量通常有以下几个特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;稠密表示&lt;/strong&gt;：每个词的向量是一个低维的稠密向量，不像传统的“one-hot”编码（仅使用0和1来表示词汇）。稠密向量包含了丰富的语义信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;语义关系&lt;/strong&gt;：通过训练，词向量可以捕捉到词汇之间的语义关系。例如，“king” 和 “queen” 可能有相似的向量，因为它们共享许多语义特征。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如，假设我们训练了一个模型，得到以下的词向量：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;苹果&amp;rdquo; → &lt;code&gt;[0.5, -0.2, 0.1, 0.7]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;香蕉&amp;rdquo; → &lt;code&gt;[0.4, -0.1, 0.2, 0.6]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些向量的数值表示了每个词的特征和语义信息。在训练过程中，模型会通过调整这些向量，使得语义相似的词的向量在空间中彼此接近。&lt;/p&gt;
&lt;h3 id=&#34;为什么要使用嵌入&#34;&gt;为什么要使用嵌入？
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;捕捉语义相似性&lt;/strong&gt;：通过嵌入，模型可以理解哪些词是相似的，哪些是不同的。比如“猫”和“狗”虽然拼写不同，但它们都是“动物”，因此它们的向量会很接近。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;降维&lt;/strong&gt;：将词语从“one-hot”编码的高维空间（例如，假设有 10,000 个不同的词，one-hot 向量就是 10,000 维的）映射到一个低维空间（如 300 维）。这样可以减少计算的复杂度和内存消耗。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;增强模型理解&lt;/strong&gt;：通过将词转换为向量，模型可以更加有效地理解文本中的语义、句法关系和上下文信息。这对于复杂的任务（如文本分类、情感分析、翻译等）非常重要。&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;shape_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Deal with dynamic shape in tensorflow cleanly.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;static&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;as_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 获取静态形状&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;dynamic&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 获取动态形状&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dynamic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;None&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;enumerate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;static&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 返回合并后的形状&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;static = x.shape.as_list()&lt;/code&gt;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x.shape&lt;/code&gt; 是 TensorFlow 张量 &lt;code&gt;x&lt;/code&gt; 的静态形状。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;as_list()&lt;/code&gt; 将这个形状从 &lt;code&gt;TensorShape&lt;/code&gt; 对象转换为 Python 列表。静态形状的值如果是已知的，会直接作为数值存在。如果某个维度的值是 &lt;code&gt;None&lt;/code&gt;（表示该维度在静态形状中不确定），那么会是 &lt;code&gt;None&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;例如，如果 &lt;code&gt;x&lt;/code&gt; 的形状是 &lt;code&gt;[None, 32, 32, 3]&lt;/code&gt;，则 &lt;code&gt;static&lt;/code&gt; 会是 &lt;code&gt;[None, 32, 32, 3]&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;dynamic = tf.shape(x)&lt;/code&gt;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.shape(x)&lt;/code&gt; 返回张量 &lt;code&gt;x&lt;/code&gt; 的动态形状，它是一个 TensorFlow 张量，表示运行时该张量的形状。与静态形状不同，动态形状会根据实际运行时的输入数据进行计算。&lt;/li&gt;
&lt;li&gt;返回值是一个形状为 &lt;code&gt;[d0, d1, ..., dn]&lt;/code&gt; 的张量（&lt;code&gt;d0&lt;/code&gt;，&lt;code&gt;d1&lt;/code&gt; 等是张量的各个维度的动态大小）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;return [dynamic[i] if s is None else s for i, s in enumerate(static)]&lt;/code&gt;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这个列表推导式的目的是将静态形状和动态形状合并成一个形状列表。如果某个静态形状的维度是 &lt;code&gt;None&lt;/code&gt;（即在静态形状中未指定），那么就使用对应的动态形状。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;enumerate(static)&lt;/code&gt; 会遍历静态形状中的每个维度，如果该维度是 &lt;code&gt;None&lt;/code&gt;，就从动态形状中取对应的维度值；如果该维度不是 &lt;code&gt;None&lt;/code&gt;，则保留静态形状中的值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假设 &lt;code&gt;static = [None, 32, 32, 3]&lt;/code&gt;（第一维度 &lt;code&gt;None&lt;/code&gt;，表示它是动态的）和 &lt;code&gt;dynamic = [64, 32, 32, 3]&lt;/code&gt;（运行时该张量的实际形状是 &lt;code&gt;[64, 32, 32, 3]&lt;/code&gt;）。&lt;/li&gt;
&lt;li&gt;那么，函数的返回值将是 &lt;code&gt;[64, 32, 32, 3]&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为什么需要这个函数？&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在 TensorFlow 中，许多操作要求处理的张量形状是已知的（静态形状），但有些情况下形状在执行时才会动态确定，特别是在处理输入数据的批量大小、图像尺寸等变量时。&lt;code&gt;shape_list&lt;/code&gt; 函数就是用来统一处理这两种情况的，使得无论是在静态还是动态形状下，代码都能够正常运行而不会出错。&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;举个例子：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;假设我们有一个张量 &lt;code&gt;x&lt;/code&gt;，它的形状是 &lt;code&gt;[None, 64, 64, 3]&lt;/code&gt;（批量大小未知）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;import tensorflow as tf
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;x = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])  # 动态批量大小
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;shape = shape_list(x)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;print(shape)  # 输出形状
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;在这个例子中，如果你传递一个大小为 &lt;code&gt;[32, 64, 64, 3]&lt;/code&gt; 的输入，&lt;code&gt;shape_list&lt;/code&gt; 函数会返回 &lt;code&gt;[32, 64, 64, 3]&lt;/code&gt;，即将 &lt;code&gt;None&lt;/code&gt; 替换为实际的批量大小 &lt;code&gt;32&lt;/code&gt;。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;def softmax(x, axis=-1): 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    x = x - tf.reduce_max(x, axis=axis, keepdims=True)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    ex = tf.exp(x)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;这个函数实现了 Softmax 操作，将输入 &lt;code&gt;x&lt;/code&gt; 转换为概率分布，常用于多分类模型的输出层。通过减去最大值避免指数计算中的数值溢出，确保计算稳定性。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;def gelu(x):
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;GELU&lt;/strong&gt; 是一种平滑的、概率性的激活函数，能够更自然地通过高斯分布近似非线性激活。相比于 ReLU，GELU 在训练时通常能更好地保留梯度，并且能够减少死神经元的问题。&lt;/p&gt;
&lt;p&gt;代码中使用了 &lt;code&gt;tf.tanh&lt;/code&gt; 和 &lt;code&gt;tf.pow&lt;/code&gt;（TensorFlow 的函数）与 NumPy 的常量进行计算，确保了在 TensorFlow 中的高效计算。&lt;/p&gt;
&lt;p&gt;Layer Normalization 是一种在深度学习中常用的技术，旨在对每个样本的特征进行归一化，使得特征的均值为0，方差为1，从而加速训练并提高模型的性能。&lt;/p&gt;
&lt;p&gt;代码解释：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;epsilon&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1e-5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Normalize to mean = 0, std = 1, then do a diagonal affine transform.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;variable_scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;g&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initializer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_initializer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initializer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_initializer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;x&lt;/code&gt;&lt;/strong&gt;: 输入张量，通常是经过激活函数后的某一层的输出。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;scope&lt;/code&gt;&lt;/strong&gt;: 变量作用域，用于创建和管理变量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;axis=-1&lt;/code&gt;&lt;/strong&gt;: 归一化时的维度，通常在 Layer Normalization 中是最后一维（即特征维度）。&lt;code&gt;axis=-1&lt;/code&gt; 表示归一化最后一维，通常是一个样本的所有特征。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;epsilon=1e-5&lt;/code&gt;&lt;/strong&gt;: 为了避免在除法中出现除零错误，&lt;code&gt;epsilon&lt;/code&gt; 是一个很小的常数（默认值为 1e−51e-51e−5）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;n_state = x.shape[-1].value&lt;/code&gt;&lt;/strong&gt;: 获取输入张量 &lt;code&gt;x&lt;/code&gt; 在最后一维的大小，代表特征数目。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;接下来，定义了两个变量 &lt;code&gt;g&lt;/code&gt; 和 &lt;code&gt;b&lt;/code&gt;，分别用于对归一化后的结果进行线性变换：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;g&lt;/code&gt;&lt;/strong&gt; 是缩放因子（通常称为 Gamma）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;b&lt;/code&gt;&lt;/strong&gt; 是平移因子（通常称为 Beta）。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;n&#34;&gt;g&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initializer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_initializer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initializer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_initializer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;g&lt;/code&gt; 和 &lt;code&gt;b&lt;/code&gt; 都是与输入 &lt;code&gt;x&lt;/code&gt; 的最后一维特征数目相同的向量。&lt;code&gt;g&lt;/code&gt; 初始化为 1，&lt;code&gt;b&lt;/code&gt; 初始化为 0。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;归一化步骤&#34;&gt;归一化步骤：
&lt;/h3&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        u = tf.reduce_mean(x, axis=axis, keepdims=True)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        s = tf.reduce_mean(tf.square(x - u), axis=axis, keepdims=True)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;u = tf.reduce_mean(x, axis=axis, keepdims=True)&lt;/code&gt;&lt;/strong&gt;: 计算输入 &lt;code&gt;x&lt;/code&gt; 在指定 &lt;code&gt;axis&lt;/code&gt; 维度上的均值（平均值）。这里，&lt;code&gt;axis&lt;/code&gt; 默认为 -1，即沿着最后一维计算均值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;s = tf.reduce_mean(tf.square(x - u), axis=axis, keepdims=True)&lt;/code&gt;&lt;/strong&gt;: 计算输入 &lt;code&gt;x&lt;/code&gt; 与均值 &lt;code&gt;u&lt;/code&gt; 的差的平方的平均值，即方差。&lt;code&gt;keepdims=True&lt;/code&gt; 保证输出维度与输入相同，便于后续操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        x = (x - u) * tf.rsqrt(s + epsilon)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;code&gt;x = (x - u) * tf.rsqrt(s + epsilon)&lt;/code&gt;&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;对输入&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;x
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;进行标准化（即归一化处理），使得均值为 0，方差为 1。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x - u&lt;/code&gt;：减去均值，使得每个元素的均值为 0。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.rsqrt(s + epsilon)&lt;/code&gt;：计算标准差的倒数并进行缩放，&lt;code&gt;rsqrt&lt;/code&gt; 是 &lt;code&gt;1 / sqrt(x)&lt;/code&gt;。&lt;code&gt;epsilon&lt;/code&gt; 用于防止除 0 错误。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        x = x * g + b
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        return x
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;x = x \* g + b&lt;/code&gt;&lt;/strong&gt;: 最后，对标准化后的 &lt;code&gt;x&lt;/code&gt; 进行线性变换，使用参数 &lt;code&gt;g&lt;/code&gt;（缩放因子）和 &lt;code&gt;b&lt;/code&gt;（平移因子）。这种变换使得归一化的结果可以恢复一定的表达能力，类似于 Batch Normalization 中的 affine 变换。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;return x&lt;/code&gt;&lt;/strong&gt;: 返回归一化并经过变换的结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;def split_states(x, n):
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;Reshape the last dimension of x into [n, x.shape[-1]/n].&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    *start, m = shape_list(x)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    return tf.reshape(x, start + [n, m//n])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;这个函数 &lt;code&gt;split_states&lt;/code&gt; 的作用是将输入张量 &lt;code&gt;x&lt;/code&gt; 的最后一维拆分成两个部分，其中一个部分的大小是 &lt;code&gt;n&lt;/code&gt;，另一个部分的大小是 &lt;code&gt;m // n&lt;/code&gt;（&lt;code&gt;m&lt;/code&gt; 是输入张量的最后一维大小）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;\*start, m = shape_list(x)&lt;/code&gt;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;shape_list(x)&lt;/code&gt; 会返回张量 &lt;code&gt;x&lt;/code&gt; 的形状（作为一个列表）。&lt;code&gt;*start&lt;/code&gt; 会将除了最后一维之外的所有维度（即张量的前几维）存入 &lt;code&gt;start&lt;/code&gt; 变量中，&lt;code&gt;m&lt;/code&gt; 则保存张量最后一维的大小。&lt;/li&gt;
&lt;li&gt;例如，如果 &lt;code&gt;x&lt;/code&gt; 的形状是 &lt;code&gt;[batch_size, time_steps, features]&lt;/code&gt;，那么 &lt;code&gt;start&lt;/code&gt; 将保存 &lt;code&gt;[batch_size, time_steps]&lt;/code&gt;，&lt;code&gt;m&lt;/code&gt; 将保存 &lt;code&gt;features&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;tf.reshape(x, start + [n, m // n])&lt;/code&gt;&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;start&lt;/code&gt; 是 &lt;code&gt;x&lt;/code&gt; 的前几个维度的列表，&lt;code&gt;n&lt;/code&gt; 是传入的参数，表示你想将最后一维拆分成 &lt;code&gt;n&lt;/code&gt; 部分，&lt;code&gt;m // n&lt;/code&gt; 是每一部分的大小。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.reshape&lt;/code&gt; 函数将输入张量 &lt;code&gt;x&lt;/code&gt; 重新调整形状，形状变为 &lt;code&gt;[start, n, m // n]&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;例如，如果 &lt;code&gt;x&lt;/code&gt; 的形状是 &lt;code&gt;[batch_size, time_steps, features]&lt;/code&gt;，并且传入 &lt;code&gt;n = 2&lt;/code&gt;，那么新的形状将是 &lt;code&gt;[batch_size, time_steps, 2, features // 2]&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;def merge_states(x):
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;Smash the last two dimensions of x into a single dimension.&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    *start, a, b = shape_list(x)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    return tf.reshape(x, start + [a*b])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;这个函数 &lt;code&gt;merge_states&lt;/code&gt; 的作用是将输入张量 &lt;code&gt;x&lt;/code&gt; 的最后两个维度合并成一个维度。也就是说，它通过将最后两个维度相乘，将它们“压扁”为一个新的维度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;\*start, a, b = shape_list(x)&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;shape_list(x)&lt;/code&gt; 返回张量 &lt;code&gt;x&lt;/code&gt; 的形状，作为一个列表。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;*start&lt;/code&gt; 会将 &lt;code&gt;x&lt;/code&gt; 的所有维度（除了最后两个维度）存入 &lt;code&gt;start&lt;/code&gt;，&lt;code&gt;a&lt;/code&gt; 和 &lt;code&gt;b&lt;/code&gt; 分别保存 &lt;code&gt;x&lt;/code&gt; 的倒数第二维和最后一维的大小。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;举个例子：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假设 &lt;code&gt;x&lt;/code&gt; 的形状是 &lt;code&gt;[batch_size, time_steps, features, channels]&lt;/code&gt;，那么 &lt;code&gt;start = [batch_size, time_steps, features]&lt;/code&gt;，&lt;code&gt;a = features&lt;/code&gt;，&lt;code&gt;b = channels&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;tf.reshape(x, start + [a \* b])&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.reshape(x, start + [a * b])&lt;/code&gt; 会将 &lt;code&gt;x&lt;/code&gt; 重新调整形状，新的形状由 &lt;code&gt;start&lt;/code&gt;（原先的所有维度，除了最后两个）和 &lt;code&gt;a * b&lt;/code&gt;（将最后两个维度相乘的结果）组成。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;a * b&lt;/code&gt; 是将原本的最后两个维度合并成一个新的维度。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个 &lt;code&gt;conv1d&lt;/code&gt; 函数实现了一个 &lt;strong&gt;1D 卷积&lt;/strong&gt; 操作。虽然它的名字和传统的 1D 卷积类似，但它实际上通过矩阵乘法和适当的形状变换来模拟 1D 卷积的操作。以下是代码逐行解释：&lt;/p&gt;
&lt;h3 id=&#34;代码解释&#34;&gt;代码解释：
&lt;/h3&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;conv1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w_init_stdev&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.02&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;variable_scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shape_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initializer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random_normal_initializer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stddev&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w_init_stdev&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initializer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_initializer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]))&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;\*start, nx = shape_list(x)&lt;/code&gt;&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;shape_list(x)&lt;/code&gt; 返回张量 &lt;code&gt;x&lt;/code&gt; 的形状，作为一个列表。&lt;code&gt;*start&lt;/code&gt; 会将除了最后一维之外的所有维度（即张量的前几维）存入 &lt;code&gt;start&lt;/code&gt;，&lt;code&gt;nx&lt;/code&gt; 保存 &lt;code&gt;x&lt;/code&gt; 的最后一维大小。&lt;/li&gt;
&lt;li&gt;比如，如果 &lt;code&gt;x&lt;/code&gt; 的形状是 &lt;code&gt;[batch_size, length, in_channels]&lt;/code&gt;，那么 &lt;code&gt;start&lt;/code&gt; 将是 &lt;code&gt;[batch_size, length]&lt;/code&gt;，&lt;code&gt;nx&lt;/code&gt; 就是 &lt;code&gt;in_channels&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;w = tf.get_variable(&#39;w&#39;, [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))&lt;/code&gt;&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;创建一个卷积核（权重） &lt;code&gt;w&lt;/code&gt;，其形状为 &lt;code&gt;[1, nx, nf]&lt;/code&gt;。这里的 &lt;code&gt;1&lt;/code&gt; 是卷积核的大小（即卷积操作是 1D 的），&lt;code&gt;nx&lt;/code&gt; 是输入的最后一维大小（通常是输入通道数），&lt;code&gt;nf&lt;/code&gt; 是卷积操作后输出通道的数量。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;initializer=tf.random_normal_initializer(stddev=w_init_stdev)&lt;/code&gt; 初始化权重值为从正态分布中采样，标准差为 &lt;code&gt;w_init_stdev&lt;/code&gt;，默认为 &lt;code&gt;0.02&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;b = tf.get_variable(&#39;b&#39;, [nf], initializer=tf.constant_initializer(0))&lt;/code&gt;&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;创建偏置 &lt;code&gt;b&lt;/code&gt;，其形状为 &lt;code&gt;[nf]&lt;/code&gt;，即与输出通道数相同，初始化为零。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf])) + b, start + [nf])&lt;/code&gt;&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;tf.reshape(x, [-1, nx])&lt;/code&gt;&lt;/strong&gt;: 将输入张量 &lt;code&gt;x&lt;/code&gt; 重塑为形状 &lt;code&gt;[-1, nx]&lt;/code&gt;，即将所有前面的维度展平，保留最后一维 &lt;code&gt;nx&lt;/code&gt;。这一步相当于将输入的多维张量展平成二维矩阵，每一行代表一个数据样本的输入通道。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;tf.reshape(w, [-1, nf])&lt;/code&gt;&lt;/strong&gt;: 将卷积核 &lt;code&gt;w&lt;/code&gt; 重塑为形状 &lt;code&gt;[-1, nf]&lt;/code&gt;，这意味着卷积核的权重被展平为一个矩阵，其中每个输入通道的权重被分配到输出通道。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;tf.matmul(...)&lt;/code&gt;&lt;/strong&gt;: 执行矩阵乘法。此处是将展平后的输入张量 &lt;code&gt;x&lt;/code&gt; 与展平后的卷积核 &lt;code&gt;w&lt;/code&gt; 相乘，类似于卷积操作中的加权求和过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;+ b&lt;/code&gt;&lt;/strong&gt;: 将偏置加到结果中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;tf.reshape(..., start + [nf])&lt;/code&gt;&lt;/strong&gt;: 将矩阵乘法后的结果再重塑回原来的形状，保留除了最后一维以外的所有维度，最后一维变为 &lt;code&gt;nf&lt;/code&gt;，即输出通道数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;return c&lt;/code&gt;&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;返回卷积操作后的结果 &lt;code&gt;c&lt;/code&gt;，它的形状是 &lt;code&gt;[batch_size, length, nf]&lt;/code&gt;，即输出的批次大小、长度和输出通道数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结：
&lt;/h3&gt;&lt;p&gt;这个 &lt;code&gt;conv1d&lt;/code&gt; 函数实现的是一种通过矩阵乘法模拟 1D 卷积操作的方式。传统的 1D 卷积通过滑动窗口对输入进行局部加权求和，而这个实现通过将输入展平成矩阵后与卷积核进行矩阵乘法，从而实现类似的效果。&lt;/p&gt;
&lt;h3 id=&#34;主要特点&#34;&gt;主要特点：
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;权重形状&lt;/strong&gt;：&lt;code&gt;[1, nx, nf]&lt;/code&gt;，表示卷积核的大小为 &lt;code&gt;1xnx&lt;/code&gt;，即卷积操作是对输入的每个位置进行加权求和，输出通道数为 &lt;code&gt;nf&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;矩阵乘法实现卷积&lt;/strong&gt;：通过 &lt;code&gt;tf.matmul&lt;/code&gt; 将输入张量和卷积核做矩阵乘法，而不是传统的滑动窗口卷积。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;展平操作&lt;/strong&gt;：输入张量和卷积核都通过 &lt;code&gt;tf.reshape&lt;/code&gt; 进行了展平处理，以便进行矩阵乘法。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这种方法虽然在实现上与传统的卷积不同，但效果是相同的，并且可以通过矩阵乘法的方式提高计算效率或适应某些特定的任务需求。&lt;/p&gt;
&lt;p&gt;这段代码定义了一个函数 &lt;code&gt;attention_mask&lt;/code&gt;，用于生成注意力掩码（attention mask）&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;attention_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;1&amp;#39;s in the lower triangle, counting from the lower right corner.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn&amp;#39;t produce garbage on TPUs.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[:,&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;m&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ns&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cast&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;40
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;41
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;42
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;43
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;44
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;attn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;past&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hparams&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;assert&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ndims&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# Should be [batch, sequence, features]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;assert&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hparams&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_head&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;past&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;assert&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;past&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ndims&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# Should be [batch, 2, heads, sequence, features], where 2 is [k, v]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;split_heads&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# From [batch, sequence, features] to [batch, heads, sequence, features]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split_states&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hparams&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;merge_heads&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# Reverse of split_heads&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;merge_states&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transpose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;mask_attn_weights&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ns&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shape_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attention_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cast&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1e10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;multihead_attn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;q&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# q, k, v have shape [batch, heads, sequence, features]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;q&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;transpose_b&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rsqrt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cast&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask_attn_weights&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;softmax&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;variable_scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conv1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;c_attn&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;q&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split_heads&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;present&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stack&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;past&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;pk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unstack&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;past&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;concat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;concat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;multihead_attn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;q&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;merge_heads&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conv1d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;c_proj&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;present&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
        </item>
        <item>
        <title>调研论文速览2</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E8%B0%83%E7%A0%94%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%882/</link>
        <pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E8%B0%83%E7%A0%94%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%882/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E8%B0%83%E7%A0%94%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%882/tree-3358468_1280.jpg" alt="Featured image of post 调研论文速览2" /&gt;&lt;h1 id=&#34;调研论文速览2&#34;&gt;调研论文速览2
&lt;/h1&gt;&lt;h2 id=&#34;a-practical-and-privacy-preserving-framework-for--real-world-large-language-model-services&#34;&gt;A Practical and Privacy-Preserving Framework for  Real-World Large Language Model Services
&lt;/h2&gt;&lt;p&gt;《一种实际且隐私保护的框架用于现实世界的大语言模型服务》&lt;/p&gt;
&lt;p&gt;大语言模型（LLMs）在文本理解和生成方面展现了卓越的能力，越来越多地被应用于各个领域以提升生产力。然而，由于训练和维护这些模型的高成本，再加上一些LLM是专有的，个人往往依赖于LLM公司提供的在线人工智能即服务（AIaaS）。这种商业模式带来了显著的隐私风险，因为服务提供商可能会利用用户的痕迹模式和行为数据。本文提出了一种实际且隐私保护的框架，通过防止服务提供商将请求与提交者个人关联，确保用户的匿名性。我们的框架基于部分盲签名，保证用户请求的不可链接性。此外，我们还提出了两种针对基于订阅和基于API的服务模型的策略，确保既保护用户的隐私，又维护服务提供商的利益。该框架设计与现有LLM系统无缝集成，因为它不需要修改底层架构。实验结果表明，我们的框架几乎不增加计算和通信开销，使其成为现实应用中可行的解决方案。&lt;/p&gt;
&lt;h3 id=&#34;引言&#34;&gt;引言
&lt;/h3&gt;&lt;p&gt;训练架构和数据的快速进展使得大语言模型（LLMs）在内容分析和生成方面表现出色。鉴于这些能力，LLM已广泛应用于各个领域，以提高生产力和效率，包括医学[1]、机器人技术[2, 3]和教育[4]。然而，训练和维护这些模型的高成本（例如，LLaMa-3是在15万亿多语言标记上进行预训练的[5]），以及一些LLM无法公开访问的事实，为个人和小企业开发和维护自己的LLM带来了重大挑战。因此，许多专注于LLM的公司开始提供在线LLM服务，这通常被称为人工智能即服务（AIaaS）（例如，OpenAI的ChatGPT、Google的Gemini）。尽管AIaaS带来了好处，但依赖这些服务引发了严重的隐私问题，特别是用户信息可能被曝光。服务提供商可能会记录与用户请求相关的信息，如请求者的身份、请求时间和响应。这些信息可能被用来追溯查询到个人用户，从而损害用户的隐私。因此，LLM服务中迫切需要保证匿名性，这与其他在线服务中的隐私保护需求类似。&lt;/p&gt;
&lt;p&gt;在LLM服务中的隐私保护方法中，一种常见的做法是加密用户请求数据，使得服务器无法解释请求的内容。常用的技术包括同态加密（HE）[6, 7, 8]、多方计算（MPC）[9, 10]和秘密共享（SS）[11]。然而，这些方法通常需要大量资源，带来显著的计算需求和通信开销，同时还需要修改现有架构以支持加密。此外，虽然这些技术可以防止服务提供商了解查询的内容，但请求和响应的时间等侧信道信息仍然可能无意中泄露用户的活动模式。&lt;/p&gt;
&lt;p&gt;为了解决这些问题，一个实用的安全框架应满足几个关键属性，以保证LLM服务中的用户匿名性。首先，该框架应防止服务提供商将请求与单个用户关联，从而保护匿名性。其次，它应具有足够的通用性，能够与现有LLM架构兼容，确保实用性并与其他系统模块无缝集成。本文提出了一个满足这些要求的框架。我们的框架将现有的LLM服务视为黑盒，并作为一个附加层操作，对底层系统完全透明，即无需对现有基础设施进行更改。此外，该框架采用盲签名来确保请求的完全匿名性，防止服务提供商追溯请求的来源。具体而言，系统采用部分盲签名和定制策略来解决LLM服务中两种常见的商业模式。对于订阅模式，基于部分盲签名的方案通过限制订阅期，允许用户在指定时间内进行无限次请求。相比之下，基于API的模式采用另一种基于部分盲签名的方案，结合请求级别的计费策略，限制用户的请求次数或资源（例如，预定义的请求中发送的令牌数量）。&lt;/p&gt;
&lt;p&gt;本框架的贡献如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该框架提供了一种实用的解决方案，在保持LLM服务中用户匿名性的同时，适应现有的商业模式。&lt;/li&gt;
&lt;li&gt;提议的系统可以无缝集成到当前系统中，几乎不增加计算和通信开销。&lt;/li&gt;
&lt;li&gt;框架展示了部分盲签名与定制策略在实际场景中的应用，展示了它在其他在线服务中的扩展潜力。&lt;/li&gt;
&lt;li&gt;提供了框架的实验结果，衡量了系统中部分盲签名的性能。这些结果为部分盲签名相关的计算和通信成本提供了有价值的见解。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;相关工作&#34;&gt;相关工作
&lt;/h3&gt;&lt;h4 id=&#34;a-语言模型&#34;&gt;A. 语言模型
&lt;/h4&gt;&lt;p&gt;语言模型（LMs）是预测给定上下文中后续或缺失标记概率的统计模型。大语言模型（LLMs）是预训练语言模型（PLMs）的一类，利用前所未有的规模进行机器学习[12]。LLM与其他PLM的主要区别在于其庞大的训练数据和模型规模，这使得它们具备了突现能力——这些能力是通过扩展模型规模而产生的行为[13]。这些模型在内容分析和生成方面展现了卓越的性能，通常能够在各种任务中实现类人表现[14]。LLM已广泛应用于多个领域，包括教育、金融和医疗保健[15]。此外，一种名为“提示工程”的技术应运而生，用于优化LLM在特定任务中的输出[16, 17]。&lt;/p&gt;
&lt;p&gt;LLM的开发涉及三个主要阶段：预训练、微调和推理。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;预训练&lt;/strong&gt;：在预训练阶段，LLM在大规模数据集上进行训练，以获取一般知识。这些数据集通常包含来自不同来源的内容，如博客、文章和书籍。例如，2024年8月的CommonCrawl数据集包含了23亿网页。GPT-3是在包括过滤后的CommonCrawl数据和维基百科在内的数据集上进行预训练的，数据量约为3000亿个标记[18]。类似地，LLaMA-3的预训练数据集包含了15万亿个多语言标记[5]。预训练过程计算密集且耗时。例如，训练LLaMA-3.1 70B模型需要估计700万小时的H100-80GB硬件计算时间[4]。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;微调&lt;/strong&gt;：预训练完成后，LLM会针对特定任务进行微调。用于微调的数据集通常较小，并包含领域特定的知识。在这个阶段，通常会采用人类反馈的强化学习（RLHF）来优化模型的性能，正如GPT-4的训练过程中所采用的那样[19]。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;推理&lt;/strong&gt;：一旦完成预训练和微调，LLM就可以用于推理。然而，部署LLM面临着显著的存储和计算挑战，因为它们的架构复杂，模型规模庞大，通常包含数十亿个参数。因此，对于个人用户来说，在本地运行LLM通常是不切实际的。为了解决这些挑战，许多公司提供了AI即服务（AIaaS）的LLM服务，例如OpenAI的ChatGPT和Google的Gemini。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;b-llm推理阶段中的隐私保护&#34;&gt;B. LLM推理阶段中的隐私保护
&lt;/h4&gt;&lt;p&gt;在当前的商业模式中，LLM公司通常部署训练好的模型，并通过在线服务将推理阶段暴露给用户。这些服务允许用户提交输入数据，系统基于这些数据返回LLM生成的推理结果。然而，这种方法可能会暴露用户潜在的隐私风险。为了利用LLM进行文本分析或生成，用户通常需要在请求中提供信息，其中可能包括敏感数据。&lt;/p&gt;
&lt;p&gt;为了解决这些问题，许多研究工作专注于增强LLM推理过程中的用户隐私保护。**同态加密（HE）**是其中一种方法，它允许对加密数据进行计算。通过将HE集成到LLM中，用户可以提交加密的输入数据进行推理，只有最终结果需要解密。像[7]和[20]这样的研究提出了基于HE的协议，以高效地执行矩阵乘法——这一在变换器模型中常见的操作，从而实现对加密数据的计算。另一种方法是[6]提出的加密友好的近似技术，用于优化变换器中的组件，以减少处理加密数据时的计算开销。&lt;/p&gt;
&lt;p&gt;另一种密码学技术是&lt;strong&gt;多方计算（MPC）&lt;/strong&gt;，在这种方法中，多方共同计算部分结果，且没有任何一方可以访问完整的输入或结果。最终的输出通过结合每一方的部分结果来重建。[21]提出了一种协议，确保在包括模型所有者、MPC服务器和用户的三方系统中，任何单一的MPC服务器都无法独立恢复用户的输入数据。&lt;/p&gt;
&lt;p&gt;**差分隐私（DP）**是另一种保护用户隐私的著名技术，确保无法从数据中推断出单个用户的信息。[22]将DP应用于扰动LLM输出，而[23]提出了DP-forward技术，在前向传播过程中扰动嵌入矩阵，以确保训练和推理阶段的安全性。尽管这些方法提供了不同程度的安全性，但它们通常会引入大量的计算和通信开销，导致在实际场景中的推理变得不切实际。例如，在一个安全的三方协议下，生成一个64个单词的句子可能需要长达20分钟的时间[21]。此外，某些侧信道漏洞，如用户请求的时机和数据的大小，仍然可能被服务提供商利用。&lt;/p&gt;
&lt;h4 id=&#34;盲签名&#34;&gt;盲签名
&lt;/h4&gt;&lt;p&gt;盲签名方案最早由Chaum [24]提出，是一种交互式密码协议，允许签名者在不查看消息内容的情况下对其进行认证。这保证了请求者的匿名性，同时保持签名消息的完整性。该方案的一个变体，称为&lt;strong&gt;部分盲签名&lt;/strong&gt;，随后被提出，允许请求者和签名者共同预先商定部分消息内容。这种变体在实际场景中尤为有用，例如在需要共享信息（如过期日期或发行时间戳）时 [25]。盲签名已广泛应用于确保用户匿名的系统中。Chaum的开创性工作——不可追踪的数字现金——使用盲签名来追踪双重支付，同时确保交易无法追溯到花费者的身份 [26]。此外，盲签名还被集成到许多电子投票系统中，用以保护选民的身份并维护投票过程的机密性 [27]。类似地，[28]提议将盲签名集成到比特币协议中，以使交易接收者与其公钥解关联，从而增强隐私性。&lt;/p&gt;
&lt;h2 id=&#34;bolt-privacy-preserving-accurate-and-efficient-inference-for-transformers&#34;&gt;BOLT: Privacy-Preserving, Accurate and Efficient Inference for Transformers
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;——变压器（transformers）的出现带来了传统机器学习任务的重大进展。然而，它们的广泛应用引发了对推理过程中敏感信息泄露的潜在担忧。现有的基于安全多方计算（MPC）的方法在应用于变压器模型时面临局限性，主要由于模型规模庞大以及资源密集型的矩阵乘法运算。本文提出了BOLT，一个用于变压器模型的隐私保护推理框架，支持高效的矩阵乘法和非线性计算。结合我们创新的机器学习优化方法，BOLT将通信成本降低了10.91倍。我们在不同数据集上的评估表明，BOLT在保持与浮点模型相当的准确性的同时，在各种网络环境下实现了比现有最先进系统快4.8到9.5倍的推理速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;引言&lt;/strong&gt;
变压器模型（Transformer models）近年来已成为一项颠覆性技术。ChatGPT [1] 使得基于变压器的语言模型的强大功能对所有人都可获取。与传统的监督学习和任务特定学习相比，大型变压器模型在大量未标注的文本数据上进行训练，并且直接适用于多种应用，如翻译、内容生成或问答。例如，它们可以在医疗领域用于通过分析电子健康记录、医学文献和临床笔记来识别模式和风险因素，从而显著推动诊断、治疗和药物发现 [66]，[5]，[62]。变压器是一种通用的神经网络架构，其特点是使用注意力机制 [71]。注意力机制能够有效地捕捉标记之间的关系，建模上下文信息并捕获输入标记序列中的长程依赖性。然而，尽管这些强大的模型具有优异的性能，但也伴随着隐私等方面的风险 [46]，[69]。ChatGPT 是机器学习即服务（MLaaS）的一个例子，其中服务器（OpenAI）托管一个专有模型，用户将数据输入模型并返回预测结果。然而，MLaaS 设置会引发隐私问题：用户要么必须将私密数据上传到公司的服务器，要么服务器需要将专有模型存储在用户的边缘设备上。ChatGPT 操作的就是前者设置。因此，用户数据隐私的严重问题被提出，甚至导致意大利暂时禁用 ChatGPT [47]，[55]，[45]。虽然隐私权与有效的数据分析似乎存在冲突，但我们可以采用一种称为安全多方计算（MPC）的密码学方法，在不影响功能的情况下保障数据和模型的隐私。从高层次看，MPC 允许 n 个参与方 p1, …, pn，具有相应的输入 x1, …, xn，通过计算公共函数 f(x1, …, xn) 的输出，而无需向其他方泄露各自的 xi。每个方的数据在整个计算过程中都有效“加密”。在计算结束时，每个方只会知道最终结果，而无法获得任何额外的信息。虽然 MPC 可以用于通用计算，但由于加密原语的开销，它通常会带来大量的计算和通信成本。近期的研究集中在开发针对特定工作负载的优化解决方案，以提高卷积神经网络（CNN）的私密评估和训练效率 [49]，[50]，[25]，[41]，[60]，[4]。然而，保护变压器模型的隐私尤其具有挑战性，原因有以下几点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;变压器的规模远大于卷积神经网络，包含数亿到数十亿个参数 [17]，[52]，[56]，[57]。&lt;/li&gt;
&lt;li&gt;先前关于私密神经网络的研究提出了优化的私密矩阵-向量乘法协议，但变压器需要进行大规模的矩阵-矩阵乘法。&lt;/li&gt;
&lt;li&gt;安全地评估变压器中的成千上万个输入以进行复杂的非线性计算是非常昂贵的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;因此，需要开发新的协议来保护变压器推理的隐私。根据我们所知，Iron [28] 是目前最先进的系统，研究了如何在标准变压器推理过程中完全保护数据隐私。然而，由于其显著的性能开销，它在提供实际可行的解决方案方面存在不足。例如，我们对 Iron 的实现要求 280.99 GB 的通信，用时 216 分钟完成 BERT-base 模型（110M 参数 [17]）在 WAN 设置（100 Mbps, 80 ms）下的端到端推理。随着系统和硬件的持续发展 [63]，[64]，[36]，[37]，[68]，我们认为，减少网络通信至关重要，因为计算可以加速和并行化。BOLT 中的线性和非线性协议以及机器学习优化方法显著减少了通信开销，同时保持了计算效率，标志着隐私保护变压器推理的实用性取得了重要进展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我们的贡献&lt;/strong&gt;
我们提出了 BOLT，一个新颖的隐私保护推理系统，用于变压器模型，它通过减少通信开销来解决上述挑战，从而提高了端到端的运行时性能。BOLT 保证了客户端输入数据的机密性，同时保护了服务提供商的知识产权——其模型。BOLT 集成了加密改进、非线性函数的准确高效近似以及从机器学习角度出发的算法增强。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;通信优化和计算高效的线性操作&lt;/strong&gt;
在 BOLT 中，我们面临的第一个挑战是变压器模型中的线性操作要复杂得多，且具有较高的乘法深度。许多过去的安全推理协议仅将同态加密（HE）用于明文-密文的矩阵-向量乘法，这些操作通常出现在卷积神经网络中。Iron 通过使用 HE 和 MPC 的混合来解决这个问题，但这种方法导致了较高的通信成本。相反，BOLT 的加密改进仅使用 HE 来有效节省通信成本。我们在 HE 中开发了计算高效、低深度的算法。我们的第一个思路是对密文-明文矩阵-矩阵乘法进行替代性解释，这样可以实现优化的密文打包。先前的工作 [35]，[28] 浪费了一部分通信，因为它们让一些密文槽“空着”。然而，单纯改变打包方式并不够，因为这仍然需要大量计算开销较大的旋转操作来执行变压器模型中的矩阵乘法。我们提出了第二个优化方案来解决这个问题：我们将婴儿步-巨人步策略 [27]，[7] 应用于我们的矩阵乘法算法，以减少输入密文旋转的次数，而是将旋转操作应用于部分求和的中间结果。这使得 BERT-base 模型的密文旋转次数减少了 2.33 倍到 9.33 倍。最后，我们设计了高效且低乘法深度的密文-密文矩阵乘法算法，以提高 HE 中的计算效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;准确且高效的非线性操作&lt;/strong&gt;
对于非线性操作，准确和高效的协议也是至关重要的。Iron 表明，超过 75% 的总运行时间都花费在非线性层上，这是因为将明文非线性公式直接转换为 MPC 协议时，涉及的复杂计算非常昂贵。这些计算包括三角函数和指数函数，这些在 MPC 中本身就已经非常昂贵 [59]。为了解决这个问题，我们为 GELU 和 Tanh 引入了两种高精度的多项式近似，分别是 4 阶和 5 阶多项式，并且优化了 Softmax 程序。考虑到即使是四次或五次乘法，在输入维度较大的情况下（例如，BERT-base 中 12 层的每个 GELU 函数的维度为 128 × 3072），这些操作也可能非常昂贵，我们基于 [51] 提出了一个额外的优化，这个优化对于安全计算领域具有独立意义：一种多项式预处理技术，允许在已知多项式系数的情况下，将评估阶数为 n 的多项式（霍纳法则）的乘法次数减少到大约 ⌈ n / 2 ⌉ 次。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;机器学习优化&lt;/strong&gt;
我们还提出了机器学习优化，以提高效率和准确性。具体而言，我们开发了基于注意力得分的“无关词消除”。这些得分衡量了输入标记之间的相关性，作为标记重要性的度量。然后，我们应用无关的比托尼克排序（Oblivious Bitonic Sort）来丢弃贡献较小的标记，从而显著减少输入大小。此外，我们还利用安全计算感知的微调来弥合安全计算和浮动点计算之间的差距，进一步提高准确性。&lt;/p&gt;
&lt;p&gt;结合我们所有的优化，BOLT 在不同的网络设置下比 Iron 减少了 10.91 倍的通信，并且总运行时间提高了 4.8 到 9.5 倍。我们认为这是向实用的隐私保护变压器推理迈出的重要一步。我们的代码已开源，您可以在 &lt;a class=&#34;link&#34; href=&#34;https://github.com/Clive2312/BOLT&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/Clive2312/BOLT&lt;/a&gt; 找到。&lt;/p&gt;
&lt;h2 id=&#34;curl-private-llms-through-wavelet-encoded-look-up-tables&#34;&gt;Curl: Private LLMs through Wavelet-Encoded Look-Up Tables
&lt;/h2&gt;&lt;p&gt;Curl: 通过小波编码查找表实现私有化大语言模型（LLMs）&lt;/p&gt;
&lt;p&gt;最近，变压器模型的进展已经彻底改变了机器学习，成为大规模语言模型（LLMs）的核心。然而，将这些系统集成到日常应用中时，暴露客户查询给模型所有者引发了隐私问题。安全多方计算（MPC）允许各方在保持敏感用户输入和专有模型私密的情况下评估机器学习应用程序。由于MPC本身的成本，最近的研究提出了针对特定模型的优化，但这些优化妨碍了机器学习研究人员的广泛采用。CrypTen（NeurIPS’21）旨在通过常见的机器学习抽象（如张量和模块化神经网络）暴露MPC原语来解决这个问题。不幸的是，CrypTen和许多其他MPC框架依赖于非线性函数的多项式近似，导致较高的误差和通信复杂性。&lt;/p&gt;
&lt;p&gt;本文介绍了Curl，这是一个易于使用的MPC框架，通过查找表来评估非线性函数，从而获得更好的近似，并显著减少了回合和通信开销。Curl暴露了与CrypTen类似的编程模型，并且通过张量实现高度并行化。Curl的核心依赖于离散小波变换，以减少查找表的大小而不牺牲准确性，从而在非线性函数（如对数和倒数）的评估中，相较于CrypTen实现了最多19倍的回合和通信减少。我们在包括BERT、GPT-2和GPT Neo在内的多种LLM上评估了Curl，并与一些最先进的相关工作进行了比较，如Iron（NeurIPS’22）和Bolt（S&amp;amp;P’24），在通信和延迟方面至少减少了1.9倍。&lt;/p&gt;
&lt;p&gt;最后，我们通过在独立模型中证明广泛使用的概率截断协议的安全性，解决了关于其安全性的长期争议。此问题具有独立意义，因为许多相关工作依赖于这种截断样式。&lt;/p&gt;
&lt;h3 id=&#34;引言-1&#34;&gt;引言
&lt;/h3&gt;&lt;p&gt;像GPT-2、GPT-4 [1]、BERT [19] 和 LLaMA [66] 这样的“大型语言模型”（LLMs）已经成为展示人工智能能力的典范。LLM帮助个人和企业完成日常任务，从机器翻译 [5]、文本生成 [31] 到问答系统 [56] 等等。为了生成类人响应，LLM已在大量数据上进行训练，并通过与用户的互动不断学习。然而，随着LLM日益融入人们的生活，隐私问题成为了一个关键关注点，因为个人经常分享敏感信息，包括姓名、地址、信用卡号码，甚至是财务和医疗信息 [60]。更进一步，个性化AI的发展也在推动这种趋势，OpenAI为ChatGPT启用了记忆功能 [28]。为了使一个基于LLM的助手能够理解个人偏好、习惯和工作流程，并提供量身定制的帮助，它需要访问大量个人数据。随着这些个性化数据的保留并用于持续改进LLM，数据泄露或未经授权访问的可能性对个人隐私构成了巨大的风险。&lt;/p&gt;
&lt;p&gt;隐私增强技术（PETs），如多方计算（MPC） [27,75]，为隐私保护机器学习（PPML）用例提供了大量可能性。在最常见的设置中，服务器拥有一个专有模型，并旨在将其作为服务提供给客户，以便客户可以在不暴露任何数据给模型的情况下使用该模型 [13,45,46,37,32]。目标是使客户仅获得推理结果，而不会获取有关模型的任何信息，同时模型提供者不会了解客户的输入。另一个常见的PPML用例涉及多个不信任方共同安全地训练一个模型，使用他们的敏感数据，但彼此之间不暴露任何数据 [52,65,44,73,39]。一些MPC系统利用线性秘密共享方案，因为它们允许以最小的通信开销执行线性操作 [17,24]。然而，非线性操作（如平方根、对数）通常会带来更大的挑战，并需要专门的技术，如多项式近似 [45]、查找表评估 [69] 或其他特定协议的解决方案 [8,16]。此外，结合领域知识可以显著提高这些协议的效率和效果，通过调整参数更好地适应特定的使用场景 [35,47,57]。&lt;/p&gt;
&lt;h3 id=&#34;相关工作-1&#34;&gt;相关工作
&lt;/h3&gt;&lt;h4 id=&#34;聚焦于函数秘密共享&#34;&gt;聚焦于函数秘密共享
&lt;/h4&gt;&lt;p&gt;最近，几项安全计算研究通过利用函数秘密共享（FSS） [3,69,64,55,54] 取得了进展。Pika [69] 扩展了 [3] 的前期工作，展示了一种通过FSS安全评估查找表（LUTs）的新方法。尽管它在像MNIST和CIFAR-10这样的流行数据集上展现了有效性，但其扩展性仍然面临挑战。正如Grotto [64] 所指出的，对于大型LUT，其计算成本可能由于需要大量的分布式点函数（DPF）评估而使得协议不可行。与之相比，Curl通过集成离散小波变换（DWT）技术，避免了对DPF评估的需求，从而解决了这一挑战。此外，我们的技术还能通过减少LUT的大小，降低计算和通信的复杂性，从而使基于FSS的框架受益。&lt;/p&gt;
&lt;p&gt;另一方面，Grotto [64] 提出了利用自定义样条和DPF来高效处理部分函数的创新协议。然而，与Sigma [32]等替代方案相比，它在计算和通信开销方面仍面临挑战。Orca [39] 展示了GPU加速在FSS协议中的潜力，特别是为卷积神经网络（CNNs）量身定制的协议。然而，由于其依赖于复杂的非线性操作，Orca在面向其他架构（如变压器模型）时的适用性受到质疑 [39]。Sigma [32] 在Orca的基础上进行构建，并通过适用于小范围的协议，依赖最小大小的LUT来区别于其他方案。尽管其通过自定义协议和小尺寸的LUT展现了效率，但它仍然需要大量的计算和通信资源（例如1TB的RAM和大约9Gbps的通信链路）。此外，它使用确定性截断，尽管声称提高了安全性，但与概率截断方法相比，其速度仍显不足。不幸的是，所有基于FSS的工作都集中在两方设置中，因为当参与方超过两方时，FSS变得不切实际。&lt;/p&gt;
&lt;h3 id=&#34;聚焦于预处理&#34;&gt;聚焦于预处理
&lt;/h3&gt;&lt;p&gt;由Pika [69] 发起的这一研究方向，集中在带有额外经销商方的两方设置。在这种情况下，预处理阶段需要经销商准备并分发一个大小为O(n)的元素。在假设经销商不会与任何一方串通的前提下，减少对这种假设的依赖是理想的。这促使了无需经销商的协议的发展。值得注意的是，一些先前的工作已经试图在两方设置中实现这一点，使得双方能够独立生成所需的预处理材料 [38,18,6]。&lt;/p&gt;
&lt;p&gt;一次性真值表（OTTT）协议 [38] 使用布尔电路表示每个可能输入的表，导致相对于比特大小的指数级复杂性。OP-LUT协议 [18] 尝试增强OTTT预处理阶段，但仅在小比特大小下取得改进，通信成本依然是指数级的。SP-LUT协议 [18] 显著增强了预处理阶段，但修改了在线阶段，导致在线阶段在比特大小上的通信成本呈指数级增长。FLUTE [6] 相比于之前的工作有所进步，但在设置阶段仍然需要O(2n)的通信。因此，寻找一个具有亚指数级通信复杂度的预处理阶段，同时保持在线阶段的高效性，仍然是一个未解决的挑战。&lt;/p&gt;
&lt;h3 id=&#34;聚焦于全同态加密&#34;&gt;聚焦于全同态加密
&lt;/h3&gt;&lt;p&gt;全同态加密（FHE）方案通过使用查找表（LUT）显著提高了性能并开启了新的应用场景。这一概念最初由Ducas和Micciancio [22] 提出，他们设计了一种方法，使用LUT在FHE中评估任意二进制门。基于这一基础，Chillotti等人 [10] 通过一组分层多路复用器实现了任意函数的评估，从而发展出了Torus FHE（TFHE）方案。尽管他们开发了一种快速的自引导机制，能够在大约10毫秒内在CPU上执行，但其采用受限。这主要是因为多路复用器的控制输入需要新的密文，这意味着无法对其进行先前计算——而且这种方法要求将程序表示为确定性自动机。&lt;/p&gt;
&lt;p&gt;TFHE的进一步改进出现在[11]中，提出了可编程自引导（PBS）技术，允许高效和通用的LUT评估。为了增强适应性，HELM [29] 在PBS技术的基础上进行了扩展，并引入了一个框架，能够自动将Verilog硬件描述语言（HDL）转换为加密电路。HELM有三种操作模式。第一种模式专门处理二进制门。第二种模式使用安全的LUT评估来处理整数。第三种混合模式与二进制电路一起工作，并在返回二进制领域之前，使用LUT评估进行整数的安全处理。然而，HELM的限制在于它只能处理非常低精度的LUT。这一限制的原因在于，将整数转换为多个比特需要多个n对1的LUT。&lt;/p&gt;
&lt;p&gt;最近，Ripple [30] 提出了基于离散小波变换（DWT）理论的压缩技术，用于减小LUT的大小，从而加速PBS技术在平滑函数上的应用。&lt;/p&gt;
&lt;h3 id=&#34;聚焦于安全llm推理&#34;&gt;聚焦于安全LLM推理
&lt;/h3&gt;&lt;p&gt;由于变压器相比传统神经网络具有更高的复杂性，安全LLM推理框架最近才开始受到关注。确保安全推理的技术包括一些著名的实现，例如THE-X [9] 和Iron [35]，它们是最早使用同态加密进行矩阵乘法和非线性函数计算的框架。随后，几项研究探讨了多方计算（MPC）技术在私有变压器推理系统中的应用 [47,21]。MPCFormer [47] 利用CrypTen [45] MPC引擎，而Puma [21] 则使用SecretFlow-SPU [51] MPC引擎来评估MPC在LLM推理中的适用性。MPCFormer引入了一种蒸馏过程，强模型训练弱模型以提高准确性，从而弥补使用小（2阶）非线性函数近似的局限性。然而，MPCFormer面临着近似精度损失的问题，需要对模型进行微调。另一方面，Puma使用GeLU激活函数的分段近似，这需要进行多达6阶的比较和多项式计算。&lt;/p&gt;
&lt;p&gt;最近的研究越来越多地关注混合解决方案，将特定操作的最佳技术结合起来 [57,39]。Bolt [57] 开发了一种整合全同态加密和MPC的解决方案。与Puma [21] 类似，Bolt使用分段近似，需要进行比较，这增加了轮次复杂度和通信开销。为了提高效率，Bolt采用了单词消除技术，同时保持准确性。此外，Bolt通过增加多项式的阶数来增强MPCFormer的多项式近似，并通过Motzkin多项式预处理程序减轻相应的效率损失。与此不同，Curl通过使用优化的查找表协议避免了比较和多项式近似，从而减少了通常由多项式近似带来的轮次和通信复杂度，同时保持了准确性。&lt;/p&gt;
&lt;h3 id=&#34;专注于安全截断&#34;&gt;&lt;strong&gt;专注于安全截断&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;机器学习任务使用浮点数来构建精确的模型[34]。然而，广泛认为，使用浮点数会导致多方计算（MPC）在效率方面变得不切实际。为此，大多数关于机器学习在MPC中的研究采用了定点表示，而不是浮点表示，这需要一个截断协议来保持恒定的精度。在现有的截断方法中——确定性、最接近整数和概率性——确定性和概率性方法（见第3节）在安全计算环境中应用最为广泛[8,48]。此外，许多研究倾向于使用概率性截断，因为它具有较低的通信成本，从而提高了性能，例如[53,52,59,13,14,46,44,63]。尽管如此，一些最新的研究表达了对概率性截断的担忧，并选择了更加资源密集的协议来实现确定性截断[48,33,32]。关于概率性截断的担忧包括安全性和四舍五入误差。&lt;/p&gt;
&lt;h3 id=&#34;正确性误差&#34;&gt;&lt;strong&gt;正确性误差&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;在讨论上述两个问题之前，我们回顾一下，由Catrina和Hoogh提出的广泛使用且廉价的概率性截断协议在域上具有正确性误差，误差概率为 $2^{-(n - |x|)}$，其中 $|x|$ 是截断输入 $x$ 的最大位数。这就要求使用显著更大的环，并设置 $n \geq |x| + \kappa$，其中 $\kappa$ 是统计安全参数。Damgård 等人[15]提出了一个概率性协议，正确性误差为零，但它需要一个非常规的位分解协议。随后的研究[13,25]优化了[8,15]，并展示了一个常规回合的概率性截断协议，其通信和计算成本与Catrina和Hoogh[8]相匹配，但没有正确性误差，并且允许 $n \geq |x| + 1$。CrypTen[45] 提出了一个受[70]启发的协议，用于公有值除法，可以简化为一个带有非零正确性误差的概率性截断。&lt;/p&gt;
&lt;h3 id=&#34;安全性&#34;&gt;&lt;strong&gt;安全性&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;最近，Li等人[48]发现了[8]中概率性截断安全性证明的一个安全漏洞，揭示了该协议泄露的信息与理想的概率性截断功能之间的差异。值得注意的是，同样的问题也出现在[25]中的 $n$-最优概率性截断中。这导致了最近一些论文，如[32,32]，因对安全性问题的担忧而放弃了廉价的概率性截断协议。在本文中，我们展示了[25]中的 $n$-最优概率性截断协议（以及[8]中的原始概率性截断）安全地实现了我们定义的自然理想概率性截断功能。这使我们能够在不妥协安全性的情况下，利用概率性截断的效率优势（见第3节）。&lt;/p&gt;
&lt;h3 id=&#34;四舍五入误差&#34;&gt;&lt;strong&gt;四舍五入误差&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;LLAMA论文[33]最近建议使用确定性截断，声称它能提高推理精度。然而，这一主张并没有得到实验证据的支持，而早期的研究表明，概率性截断可能会导致精度下降的结果，也未通过实验证明[63,13]。有趣的是，Gupta等人[34]证明，使用16位定点数并具有12位精度和概率性截断的计算，能够实现与32位浮点数几乎等效的精度。相反，在这样的条件下，截断到最接近整数无法有效地训练。此外，尽管这不是工作的主要焦点，Keller和Sun[43]提供了实验证据，表明在神经网络中，概率性截断与截断到最接近整数之间的差异很小。实际上，在28个不同的实验中，13个显示概率性截断具有更好的准确性，13个显示截断到最接近整数更优，2个实验的准确性相同。&lt;/p&gt;
&lt;h2 id=&#34;我们的贡献&#34;&gt;&lt;strong&gt;我们的贡献&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;在本文中，我们介绍了Curl，一个用户友好的多方计算（MPC）框架，旨在通过查找表（LUT）高效地评估非线性函数。Curl解决了安全协议中三个至关重要的方面：效率、准确性和安全性。Curl采用了一种新颖的LUT压缩技术，利用离散小波变换（DWT）实现高效的近似，并将误差降到最小。这有效地减小了原始LUT的大小，同时保持了非线性函数的准确性，从而超越了传统的多项式分段近似方法。通过最小化通信成本，Curl显著提高了端到端的运行时性能，并支持包括GeLU、SiLU、高斯误差函数、Sigmoid和双曲正切在内的广泛激活函数。此外，Curl将[57,32]中的GeLU协议中使用的分段近似方法推广到任何有界的奇偶函数，适用于任何收敛到分段多项式的函数。Curl的核心技术扩展到LLM（大语言模型）使用的复杂非线性函数，确保了客户输入数据的隐私，同时保护了服务提供商的知识产权，例如其模型。&lt;/p&gt;
&lt;p&gt;我们在多个LLM模型上评估了Curl，包括BERT（tiny、base、large）、GPT-2和GPT-Neo，涵盖了CPU和GPU后端。我们的评估显示，Curl在技术上超越了现有的最先进框架，达到了至少减少6.5倍的回合数和1.9倍的通信开销。最后，Curl通过引入自然理想功能、相应协议并提供仿真安全性证明，解决了关于高效概率性截断的安全性问题。许多依赖这种截断方式的工作已经被[48]证明存在安全漏洞，因为它们无法在真实-理想范式下进行仿真。我们的结果具有独立意义，因为我们的证明解决了这一争议，并恢复了人们对安全概率性截断协议的信心。&lt;/p&gt;
&lt;p&gt;我们总结了以下贡献：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于DWT压缩的创新框架，在低回合数和通信复杂度的前提下实现高精度。将其应用于LLM推理时，与现有方法相比，Curl实现了至少6.5倍的回合复杂度降低和1.9倍的通信降低。&lt;/li&gt;
&lt;li&gt;Curl基于用户友好的CrypTen[45]框架，提供了高度的灵活性和较低的采用门槛，便于开发者和研究人员使用，从而促进了安全计算技术的普及。作为概念验证，我们实现了多种LLM模型和多个非线性函数，均可在CPU和GPU上运行。&lt;/li&gt;
&lt;li&gt;我们引入了一种新颖的自然理想功能，用于高效的概率性截断，并证明了Escudero等人[25]的安全性。该结果具有独立意义。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;east-efficient-and-accurate-secure-transformer-framework-for-inference&#34;&gt;East: Efficient and Accurate Secure Transformer Framework for Inference
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;East: 高效且准确的安全Transformer推理框架&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;
Transformer因其强大的优势，已成功应用于实际场景，如ChatGPT。然而，在服务过程中，用户的输入会泄露给模型提供者。随着人们对隐私的关注，隐私保护的Transformer推理服务需求日益增加。非线性函数的安全协议在隐私保护的Transformer推理中至关重要，但这一领域的研究仍不充分。因此，设计实用的安全协议以处理非线性函数，尽管困难，却对模型性能具有重要意义。本文提出了一个名为East的框架，旨在实现高效且准确的安全Transformer推理。首先，我们提出了一种新的不可知分段多项式评估算法，并将其应用于激活函数，相较于现有方法，显著降低了GELU的运行时和通信开销，分别提升了1.5倍和2.5倍。其次，我们精心设计了softmax和层归一化的安全协议，确保准确地保持所需功能。第三，我们详细进行了若干优化，以提高整体效率。我们将East应用于BERT，并且结果表明，推理精度与明文推理一致，且无需微调。与Iron相比，我们在通信成本上降低了约1.8倍，运行时减少了约1.2倍。&lt;/p&gt;
&lt;p&gt;THE-X[11] 是首个探索使用加密技术进行隐私保护Transformer推理（PPTi）的工作。在THE-X[11]中，GELU被替换为ReLU，且不支持tanh。THE-X[11]使用近似方法处理softmax和层归一化（LN），将Transformer转换为支持完全同态加密（HE）操作的函数。THE-X[11]存在几个问题：（1）THE-X[11]缺乏对服务器隐私的保护，因为在计算ReLU时，THE-X[11]将中间结果泄露给客户端，这可能导致服务器隐私泄露。（2）与明文推理相比，THE-X[11]的准确性平均下降超过1%。（3）在使用THE-X[11]时，原始模型需要转换为近似形式，这使得原始模型参数无法使用，并需要额外的模型重新训练。一些后续工作（[12][13]，[14]，[15]）通过采用更有效的近似方法并使用MPC确保隐私，解决了前两个问题。这些工作集中在Transformer的MPC友好近似上，而忽略了第三个问题。事实上，在明文训练模型时，MPC友好的Transformer可能会限制其灵活性。类似的通过改变模型架构加速计算的工作也在[16]中使用，该工作在文本生成任务中进行隐私保护。最近的工作Iron[17]提供了解决所有三个问题的方案。&lt;/p&gt;
&lt;p&gt;Iron[17]保留了原始模型架构，并基于同态加密（HE）和秘密共享（SS）构建了多个隐私保护的Transformer协议。在Iron[17]中，矩阵乘法通过优化自Cheetah[9]的HE实现来构建，而非线性函数的安全协议则通过基于查找表（LUT）和SS的原语从SIRNN[18]中设计。最近，另一项工作[19]直接使用加密电路（GC）来处理非线性函数，这与Iron[17]不同。另一项工作[20]为高开销函数设计了高质量的近似，并获得了比[12]更好的结果，但[20]使用了安全三方计算（3PC）技术，这与2PC适用的场景不同。在上述工作中，据我们所知，Iron[17]是目前最先进的2PC PPTi工作。Iron[17]的障碍在于基于LUT的非线性函数带来了相当大的开销。在Iron[17]中，非线性函数的运行时和通信分别占总开销的约80%和87%。因此，设计既高效又安全的非线性函数协议，同时保持模型架构，是一个挑战。&lt;/p&gt;
&lt;p&gt;我们的贡献&lt;/p&gt;
&lt;p&gt;在本文中，我们设计了East，一个高效且准确的安全Transformer推理框架。本文的贡献如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出了用于激活函数（如GELU和tanh）的通信高效协议，采用我们新的隐式近似函数评估方法。与NFGen[21]相比，我们的协议在运行时和通信方面分别提高了约1.5倍和2.6倍。&lt;/li&gt;
&lt;li&gt;通过提出的转换方法和误差限制确定方法，仔细设计了softmax和LN的协议。与Iron[17]相比，我们的协议在softmax的通信开销上减少了约1.3倍，在LN的运行时上减少了约1.2倍。&lt;/li&gt;
&lt;li&gt;进行了多项优化以提升整体性能，包括线性层合并、截断优化、打包通信处理以及填充和掩蔽处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们使用East进行了BERT[22]推理，结果表明推理准确性与明文推理一致。与Iron[17]相比，我们在非线性函数的运行时上减少了约1.2倍，通信开销减少了约1.8倍。&lt;/p&gt;
&lt;h2 id=&#34;efficient-privacy-preserving-kan-inference-using-homomorphic-encryption&#34;&gt;Efficient Privacy-Preserving KAN Inference Using Homomorphic Encryption
&lt;/h2&gt;&lt;p&gt;使用同态加密进行高效的隐私保护KAN推理&lt;/p&gt;
&lt;p&gt;最近提出的Kolmogorov-Arnold网络（KANs）提供了更强的可解释性和更高的模型表现力。然而，KANs在推理过程中也面临隐私泄露的挑战。同态加密（HE）促进了深度学习模型的隐私保护推理，使资源有限的用户能够在确保数据安全的情况下受益于深度学习服务。然而，KANs的复杂结构，尤其是包含像SiLU激活函数和B样条函数等非线性元素，使现有的隐私保护推理技术无法满足需求。为了解决这一问题，我们提出了一种专为KANs量身定制的高效且准确的隐私保护推理方案。我们的方法引入了一种任务特定的多项式近似方法，用于SiLU激活函数，动态调整近似范围以确保在实际数据集上获得高精度。此外，我们还开发了一种高效的方法，用于在HE域内计算B样条函数，利用重复打包、懒惰组合和比较函数等技术。我们在符号公式评估和图像分类任务中评估了我们的隐私保护KAN推理方案的有效性。实验结果表明，我们的模型在多个数据集上实现了与明文KANs相当的准确性，并且优于明文MLPs。此外，在CIFAR-10数据集上，我们的推理延迟比原始方法快了超过7倍。&lt;/p&gt;
&lt;h2 id=&#34;encryption-friendly-llm-architecture&#34;&gt;ENCRYPTION-FRIENDLY LLM ARCHITECTURE
&lt;/h2&gt;&lt;p&gt;适合加密的LLM架构&lt;/p&gt;
&lt;p&gt;大型语言模型（LLMs）根据用户互动提供个性化回应，但这一应用场景引发了严重的隐私问题。同态加密（HE）是一种支持加密状态下算术运算的加密协议，并为隐私保护机器学习（PPML）提供了潜在的解决方案。然而，变压器模型的计算强度给将HE应用于LLMs带来了挑战。在这项工作中，我们提出了一种修改后的HE友好型变压器架构，重点关注个性化（私密）微调后的推理。通过利用LoRA微调和高斯核，我们在保持与明文模型相当的性能的同时，实现了显著的计算加速——微调加速为6.94倍，推理加速为2.3倍。我们的研究结果为在数据保护至关重要的领域提供隐私保护LLM服务提供了可行的概念验证。&lt;/p&gt;
&lt;p&gt;引言&lt;/p&gt;
&lt;p&gt;大型语言模型（LLMs）的出现，如BERT系列（Devlin等，2019；Liu等，2019；Sanh等，2019；Lan等，2020；Clark等，2020；He等，2021）、GPT系列（Radford，2018；Radford等，2019；Tom B. Brown等，2020；OpenAI，2023）和ChatGPT（OpenAI，2024），开启了自然语言处理（NLP）和人工智能（AI）的新时代。LLMs的众多能力中，最受关注之一是它们能够根据用户交互提供个性化响应，特别是在微调的应用下。然而，这种应用场景引发了关于用户隐私的严重担忧。对此，GDPR（欧洲联盟，2016）和CCPA（加利福尼亚州，2018）等法规已进行修订。在意大利，ChatGPT甚至一度被暂时禁用（McCallum，2023），包括苹果和三星在内的多家大型企业也限制了其在公司内部的使用（Mok，2023）。&lt;/p&gt;
&lt;p&gt;隐私保护机器学习（PPML）指的是在保护数据隐私的同时使用机器学习的方法。PPML的技术包括安全多方计算（MPC）（Yao，1982）、差分隐私（Dwork，2006）和同态加密（HE）（Rivest等，1978；Gentry，2009）。其中，只有MPC和HE基于加密假设提供了可证明的安全性。MPC利用方之间的通信，但这些通信使得加速和并行化神经网络的重计算变得具有挑战性。相比之下，HE支持在加密状态下进行算术计算，无需通信。自Gentry（2009）开创性工作以来，已开发出多个HE方案（Brakerski，2012；Brakerski等，2014；Ducas和Micciancio，2015；Chillotti等，2016；Cheon等，2017）。特别是，CKKS（Cheon等，2017）方案对于并行评估大规模实数（而非整数）数据特别高效，且在PPML文献中得到广泛应用（Han等，2019；Lee等，2022a；b；c；2023b）。&lt;/p&gt;
&lt;p&gt;理论上，同态加密（HE）为解决LLMs相关的隐私问题提供了优雅的解决方案。然而，尽管在HE操作的理论和实现上取得了显著的近期进展，保护LLMs的HE仍然面临挑战，主要是由于它们的计算规模。变压器模型（Vaswani等，2017）以大量矩阵乘法和各种非多项式操作为特征，直接将这些操作适应到HE中会导致显著的计算时间增加和精度损失。&lt;/p&gt;
&lt;p&gt;在这项工作中，我们提出了一种修改后的HE友好型变压器架构，重点关注个性化（私密）微调后的推理。我们指出，之前关于同态加密变压器的研究通常忽视了微调，因为它的复杂性。我们的方法有两个主要的算法组件：LoRA（Hu等，2022）微调和使用高斯核（GK）替代注意力机制中的softmax。我们展示了LoRA和GK能够在HE下显著加速加密变压器计算，同时保持与明文模型相当的性能水平。使用CKKS方案对加密数据进行的修改后BERT模型的实验结果证明了其安全处理自然语言数据的能力。我们的研究结果为在数据保护至关重要的领域提供隐私保护LLM服务提供了希望。&lt;/p&gt;
&lt;p&gt;先前的研究&lt;/p&gt;
&lt;p&gt;基于变压器的语言模型和LoRA。自从注意力机制的出现（Vaswani等，2017）以来，变压器已成为语言模型的标准。基于变压器的语言模型有三种类型：仅编码器模型，包括BERT系列（Devlin等，2019；Liu等，2019；Sanh等，2019；Lan等，2020；Clark等，2020；He等，2021），输出可用于下游任务的输入嵌入；编码器-解码器模型，使用原始变压器架构，如MarianMT（Junczys-Dowmunt等，2019）、T5（Raffel等，2020）、BART（Lewis等，2020）、mBART（Liu等，2020）和mT5（Xue等，2021），用于翻译、摘要等任务；仅解码器模型，包括GPT系列（Radford，2018；Radford等，2019；Tom B. Brown等，2020；OpenAI，2023）和Llama系列（Touvron等，2023a；b；Dubey等，2024），生成对用户查询的回答。这些大型语言模型（LLMs）遵循扩展法则（Kaplan等，2020），因此LLMs的规模趋向于不断增长。因此，这些模型需要大量的内存容量进行推理和微调。为了解决这个问题，LoRA（Hu等，2022）主要用于微调预训练的LLM。通过冻结所有其他权重，LoRA适配器被添加到模型的重要层，如微调期间的注意力层。使用LoRA，可以微调LLMs，仅更新不到1%的所有参数。&lt;/p&gt;
&lt;p&gt;使用HE的隐私保护变压器。许多研究者探索了利用HE为变压器模型提供隐私保护的算法。主要有两种场景：交互式，结合了安全多方计算（MPC）与HE，和非交互式，仅依赖HE。在交互式场景中，可以通过方之间的通信来减少加密计算时间。THE-X（Chen等，2022）提出了第一个BERT-tiny推理协议，引入了HE友好的工作流程和通过通信进行的非多项式评估。随后的研究（Hao等，2022；Li等，2023；Akimoto等，2023；Dong等，2023；Pang等，2024）提高了计算时间并降低了通信成本。然而，随着模型规模的增加，交互式方法可能在大规模通信中遇到困难，并且它们还要求数据所有者在计算过程中保持在线。为了解决这些问题，非交互式研究正在进行。Zimerman等（2024）提出了第一个HE友好的变压器模型，替换了原始变压器结构，最小化了逼近域，并获得了经过更改结构的预训练权重。使用这些权重，他们进行了BERT的非交互式推理。NEXUS（Zhang等，2024a）进一步提出了一种非交互式BERT推理方法，在不重新训练的情况下，使用多项式逼近处理非多项式操作。最近，Park等（2024）提出了Powerformer，像我们的方法一样，提出将注意力中的softmax替换为他们的BRP-max函数，以实现同态推理。&lt;/p&gt;
&lt;p&gt;微调在提升变压器模型和提供更个性化的响应中起着至关重要的作用。然而，先前的工作主要集中在安全推理上，因其涉及的显著计算复杂性，尤其是在非交互式设置中，微调这一挑战大多被忽视。只有少数尝试，如Lee等（2022b）和HETAL（Lee等，2023b）的研究，探索了微调，专注于分类头，而忽略了其他关键组件，如注意力层和前馈网络。&lt;/p&gt;
&lt;p&gt;贡献&lt;/p&gt;
&lt;p&gt;我们提出了一种同态加密（HE）友好的变压器架构，重点关注个性化（私密）微调后的推理。我们解决了HE变压器模型的两个计算瓶颈：（i）通过使用LoRA，我们避免了大规模的密文-密文矩阵乘法（CCMMs），（ii）我们使用更简单的高斯核（GK）替代了softmax层，而softmax层在HE下的计算通常具有挑战性。在对HE加密的BERT风格变压器进行实验时，我们展示了微调加速了6.94倍，推理加速了2.3倍。&lt;/p&gt;
&lt;h2 id=&#34;faster-cryptonets-leveraging-sparsity-for--real-world-encrypted-inference&#34;&gt;Faster CryptoNets: Leveraging Sparsity for  Real-World Encrypted Inference
&lt;/h2&gt;&lt;p&gt;更快的 CryptoNets：利用稀疏性实现现实世界中的加密推理&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;—同态加密使得可以在数据保持加密状态的情况下进行任意计算。这一隐私保护特性对机器学习具有吸引力，但由于加密方案的巨大开销，计算时间通常较长。我们提出了Faster CryptoNets，这是一种利用神经网络进行高效加密推理的方法。我们开发了一种剪枝和量化方法，利用底层加密系统中的稀疏表示来加速推理。我们推导出了流行激活函数的最佳近似方法，能够实现最大稀疏编码并最小化近似误差。我们还展示了如何利用隐私安全的训练技术，通过迁移学习和差分隐私来减少加密推理在现实数据集上的开销。实验结果表明，我们的方法在保持竞争性准确度的同时，较以前的方法实现了显著的加速。该研究提高了使用同态加密保护用户隐私的深度学习系统的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;引言&lt;/strong&gt;
随着基于云的机器学习服务的普及，确保敏感医疗记录、财务数据以及进入第三方管道的其他信息的机密性变得尤为重要。传统的机器学习算法需要访问原始数据，这可能带来潜在的安全和隐私风险。在某些领域，如医疗行业，法规可能会禁止使用外部预测服务，特别是在技术无法提供必要的隐私保障时。本文针对加密推理任务，以实现安全的机器学习服务。我们假设第三方服务提供商已经拥有一个训练好的模型，这在“机器学习即服务”范式中是常见的做法。利用加密技术，像研究医院或欺诈检测公司这样的组织可以为用户提供预测服务，同时确保所有相关方的安全保障。我们遵循前人的方法[29]，[68]，采用同态加密（HE）将训练好的机器学习模型转换为支持HE的模型。同态加密[56]允许机器学习模型在加密数据上执行计算。根据设计，输出的预测结果也是加密的，这防止了输入或输出信息泄露给模型的宿主。如图1所示，模型既不解密数据，也不需要私钥[12]。然而，有几个挑战阻碍了加密机器学习的广泛应用。一个主要瓶颈是计算复杂性。对于普通网络，推理通常在毫秒级别完成，而加密网络则需要每个样本数分钟或数小时[29]，[38]。此外，同态加密的算术运算集较小，无法使用现代激活函数[15]，因此需要使用较为简单且性能较低的激活函数。&lt;/p&gt;
&lt;h2 id=&#34;hetal-efficient-privacy-preserving-transfer-learning-with-homomorphic--encryption&#34;&gt;HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic  Encryption
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;
迁移学习是一种公认的标准方法，用于通过向在大规模数据集上预训练的模型添加和微调新的分类层，从而高效地训练解决数据稀缺问题的机器学习模型。尽管许多之前的研究提出了使用同态加密解决机器学习即服务环境中迁移学习的数据隐私问题，但它们大多仅专注于加密推理。本文提出了HETAL，一种基于同态加密的迁移学习算法，旨在通过使用CKKS同态加密方案加密客户数据来保护客户在训练任务中的隐私。HETAL是首个严格提供加密训练的实际方案，采用基于验证的早期停止，并实现了与非加密训练相同的准确度。我们提出了一种高效的加密矩阵乘法算法，其速度比先前的方法快1.8到323倍，并且还提出了一种精度更高、覆盖面更广的softmax近似算法。对五个知名基准数据集的实验结果表明，总训练时间为567至3442秒，均低于一个小时。&lt;/p&gt;
&lt;h2 id=&#34;iron-private-inference-on-transformers&#34;&gt;Iron: Private Inference on Transformers
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Iron: Transformer模型上的私密推理&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们开启了在客户端-服务器环境中对基于Transformer的模型进行私密推理的研究，其中客户端拥有私密输入，服务器持有专有模型。我们的主要贡献是提供了若干种新的安全协议，用于矩阵乘法和复杂的非线性函数（如Softmax、GELU激活函数和LayerNorm），这些都是Transformer模型中的关键组成部分。具体来说，我们首先提出了一种基于同态加密的定制协议，用于矩阵乘法，关键依赖于一种新颖的紧凑打包技术。该设计在最有效的工作下，实现了√m×的通信量减少（m是输出矩阵的行数）。其次，我们通过整合先进的底层协议和专门的优化，设计了三个非线性函数的高效协议。与现有的最先进协议相比，我们的方法减少了大约一半的通信和计算开销。此外，所有协议都具有数值精度，能够保持明文模型的准确性。这些技术共同使我们能够实现Iron，一个高效的基于Transformer的私密推理框架。对多个现实数据集和模型进行的实验表明，Iron在通信和运行时上比现有方法分别减少了3∼14倍和3∼11倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;引言&lt;/strong&gt;
基于Transformer的模型[1–5]由于其强大的表示能力，在自然语言处理（NLP）和计算机视觉（CV）领域取得了巨大的成功。作为一种新型的神经网络架构，Transformer[1]主要利用自注意力机制来计算表示，而不依赖于序列对齐的递归或卷积。基于此的工作，许多Transformer变体，如NLP中的BERT[2]和GPT[3]，以及CV中的ViT[4]和Swin Transformer[5]，在许多现实任务中达到了最先进的性能。Transformer及其他大模型的成功推动了新兴的推理服务和应用[6, 7]。特别是，服务提供商基于Transformer训练复杂的模型，并将其部署为付费推理服务，例如机器翻译和问答服务。客户端提交输入样本并获得所需的响应。然而，当前的推理系统面临严重的隐私问题[8]。一方面，客户端需要将机密输入发送给服务提供商，如果提供商不可信，可能会泄露客户端的数据隐私；另一方面，提供商不愿将专有的Transformer模型分发给客户端，因为模型的构建需要大量数据和计算资源[9]。因此，尽管Transformer在性能上取得了前所未有的突破，但在隐私约束下仍存在一定的差距，这也激励了我们对私密Transformer推理的研究。&lt;/p&gt;
&lt;p&gt;私密推理旨在保护服务器的模型权重不被客户端泄露，同时保证服务器无法获取客户端私密输入的任何信息。近年来，使用安全的2方计算（2PC）技术[10–14]，已在传统神经网络（如卷积神经网络）上实现私密推理。然而，由于结构上的根本差异，私密Transformer推理带来了若干新的挑战。首先，基于Transformer的模型使用大量的高维矩阵乘法，而不是以往研究中广泛使用的矩阵-向量乘法。尽管我们可以直接将先前的矩阵-向量乘法协议扩展到我们的环境，但不幸的是，即使是最有效的设计[14]，由于大量密文的交互，通信开销也非常大。其次，Transformer模型在每个模块中使用复杂的数学函数，如Softmax、GELU激活函数[15]和LayerNorm，而不是像ReLU和Maxpool这样的加密友好型非线性函数。现有方法要么使用精度受损的高阶多项式近似[16, 11]，要么仅支持特定场景下的有限数学函数[17]。更糟糕的是，这些方法计算量大，且通常需要大量的通信（更多相关工作请参见附录A.5）。为了在隐私敏感场景中促进基于Transformer的推理服务的广泛应用，设计高效的协议来处理上述复杂操作至关重要。&lt;/p&gt;
&lt;p&gt;本文设计了Iron，一个高效的混合加密框架，用于私密Transformer推理，确保不会泄露服务器模型权重或客户端输入的任何敏感信息。Iron为Transformer中的复杂操作提供了若干新的专门化协议，以缓解性能开销。具体而言，我们首先提出了一种定制的基于同态加密的矩阵乘法协议。我们的创新点在于，通过设计一种紧凑的打包方法，将更多的明文输入打包到单个密文中，同时保持矩阵乘法的功能。与在Cheetah中实现的最有效的矩阵-向量乘法解决方案[14]相比，我们在通信开销上实现了√m×的改进（m是输出矩阵的行数），这对于各种Transformer模型而言，约减少了8倍。其次，我们精心设计了针对Softmax、GELU和LayerNorm的高效协议。这些协议基于SIRNN[17]，这是用于递归神经网络的私密推理的最先进加密框架，并进行了若干定制优化，如减少Softmax中的指数运算开销，并简化GELU和LayerNorm的计算。这些优化在三个非线性函数上实现了1.3∼1.8倍的运行时减少和1.4∼1.8倍的通信减少。此外，这些协议在数值上具有精确性，能够保持明文模型的准确性。我们还为所设计的协议提供了正式的安全性证明，以展示其安全保障。&lt;/p&gt;
&lt;p&gt;基于上述高效组件，我们实现了私密Transformer推理框架Iron，并在GLUE基准测试[18]上对多种BERT架构[2]（包括BERT-Tiny、BERT-Medium、BERT-Base和BERT-Large）进行了端到端实验。值得注意的是，由于这些模型共享非常相似的架构和相同的操作，Iron可以轻松扩展到其他基于Transformer的模型（如ViT）。实验结果表明，Iron在四个BERT模型上相较于SIRNN实现了3∼14倍的通信减少和3∼11倍的运行时减少。此外，与通用的最先进框架MP-SPDZ[16]相比，Iron在通信和计算效率上分别提升了两个数量级。&lt;/p&gt;
&lt;p&gt;与此同时，另有一项并行工作[19]提出了一个名为THE-X的基于同态加密的隐私保护Transformer推理方法。下面，我们将说明在协议设计和安全性方面的几个重要区别。
（1）协议设计。我们的工作旨在为Transformer模型的复杂操作设计新的高效协议，而THE-X则通过替换为加密友好的操作来实现这一目标。例如，THE-X将GELU替换为更简单的ReLU操作，将Softmax替换为ReLU与多项式的组合。
（2）安全性。我们的工作比THE-X提供了更严格的隐私保护。具体而言，我们的工作使用同态加密和秘密共享技术，隐藏所有层的私密信息（包括中间结果）。这种严格的隐私保证符合最近的最先进私密推理工作[14, 17]。然而，在THE-X中，每个非线性层的输入会泄露给客户端，这在实际应用中可能导致严重的隐私泄露[13]。因此，我们的工作可以用来增强THE-X的安全性。&lt;/p&gt;
&lt;h2 id=&#34;power-softmax-towards-secure-llm-inference-over--encrypted-data&#34;&gt;POWER-SOFTMAX: TOWARDS SECURE LLM INFERENCE OVER  ENCRYPTED DATA
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;POWER-SOFTMAX：面向加密数据的安全LLM推理&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;
现代加密方法，如同态加密（HE），用于实现隐私保护的大型语言模型（LLMs）要求LLMs具有多项式形式。然而，由于Transformer模型包含非多项式组件，如Softmax和层归一化，形成这种表示形式具有挑战性。以往的方法要么直接用高阶多项式来近似预训练模型，这在同态加密下效率较低，要么在训练前将非多项式组件替换为更容易近似的原语，例如将Softmax替换为逐点注意力。后者方法可能会带来可扩展性问题。我们提出了一种新的同态加密友好的自注意力变体，它提供了一种稳定的形式用于训练，并且易于用多项式进行近似，从而实现安全推理。我们的工作引入了第一个具有32层和超过十亿参数的多项式LLM，超越了以往模型的规模十倍以上。所得到的模型展示了与相同大小的标准Transformer相当的推理能力和上下文学习（ICL）能力，标志着该领域的一项突破。最后，我们提供了加密数据上每个计算的详细延迟分解，为进一步优化铺平了道路，并探讨了依赖于我们HE友好变体的Transformer与标准Transformer之间的归纳偏差差异。我们的代码附在补充材料中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;引言&lt;/strong&gt;
隐私保护机器学习（PPML）解决方案，特别是隐私保护的大型语言模型（LLMs）Yan et al.（2024）；Yao et al.（2024），旨在为用户数据、模型拥有者或两者提供机密性保证。实现这一目标的一个显著加密原语是同态加密（HE），它允许在加密数据上执行计算，而不会向（可能不可信的）计算环境泄露任何信息。此外，它还支持非交互式计算，从而提高了这些解决方案的可用性。&lt;/p&gt;
&lt;p&gt;然而，现代的同态加密方案，如CKKS Cheon et al.（2017），面临着仅支持对加密数据执行多项式计算的重大挑战。这一限制使得在HE环境中部署深度学习模型变得复杂，特别是对于依赖非多项式函数（如自注意力中的Softmax）的LLM。为了克服这一问题，现有方法已经将这些非多项式操作通过诸如唯一多项式近似Lee et al.（2021）或微调程序Baruch et al.（2022）等技术转化为多项式形式。虽然这些方法已经使得前馈网络（FFNs）、卷积神经网络（CNNs）Baruch et al.（2023）；Lee et al.（2022），以及小型Transformer Zimerman et al.（2024）；Zhang et al.（2024b）能够在HE上执行，但它们通常会面临稳定性和敏感性问题Zhou et al.（2019）；Goyal et al.（2020），从而阻碍了有效的扩展。&lt;/p&gt;
&lt;p&gt;我们采取了不同的方法。我们并不是修改现有的Transformer模型以适应HE的约束，而是通过CKKS约束的视角重新审视Transformer架构的核心设计原则Vaswani et al.（2017）。具体来说，我们提出：是否存在适用于HE的操作符，可以复制自注意力的关键设计原则？我们找到了肯定的答案，通过引入基于幂运算的自注意力变体，使其更容易用多项式表示。具有这一变体的模型在多个基准测试中保持了与基于Softmax的Transformer相当的性能，并保持了自注意力的核心设计特性。我们还提出了包括长度无关的近似或改进数值稳定性的变体。整体机制提供了一种比以往方法更适合HE且更高效的Transformer解决方案，使我们的方法能够高效扩展到具有32层和14亿参数的LLM。&lt;/p&gt;
&lt;p&gt;我们的主要贡献：（i）我们提出了一种专门针对HE环境的HE友好的自注意力变体。该变体最大限度地减少了非多项式操作的使用，同时保持了注意力机制的核心原则。此外，我们通过引入数值稳定的训练方法和用于推理的长度无关的计算策略，扩展了这一方法。因此，我们的模型能够实现大规模的安全推理，并且比现有方法更高效。（ii）我们利用这一技术开发了RoBERTa的多项式变体，以及首个多项式LLM，具有推理和上下文学习（ICL）能力，并且是迄今为止训练的最大多项式模型，包含32层Transformer和大约十亿个参数。（iii）我们提供了早期的消融研究和加密数据上的延迟分析，为进一步改进铺平了道路。&lt;/p&gt;
&lt;h2 id=&#34;powerformer-efficient-privacy-preserving-transformer-with-batch-rectifier-power--max-function-and-optimized-homomorphic-attention&#34;&gt;Powerformer: Efficient Privacy-Preserving Transformer with Batch Rectifier-Power  Max Function and Optimized Homomorphic Attention
&lt;/h2&gt;&lt;p&gt;&amp;ldquo;Powerformer: 高效隐私保护变换器，采用批量整流器-幂最大函数和优化同态注意力&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;
我们提出了一种高效的非交互式隐私保护变换器推理架构，称为Powerformer。由于softmax是一个非代数运算，之前的研究尝试修改它以使其适应同态加密（HE），但这些方法由于多次引导（bootstrapping）而导致准确度下降或执行时间延长。我们提出用一种基于ReLU的新函数——批量整流器-幂最大（Batch Rectifier-Power max, BRPmax）函数替代softmax，且无需任何不稳定的近似方法，该方法在BERT-Large模型中超越了原始BERT的性能，同时需要的层数更少，从而只需一次引导即可运行。我们还提出了一种专门针对注意力模块的矩阵乘法算法，相比现有的最先进方法，减少了35%到91%的密钥切换次数。我们设计了清晰的端到端基于HE的私有变换器模型实现，我们在BERT-tiny模型上使用RNS-CKKS实现的Powerformer，在单线程CPU上运行时需时503秒，据我们所知，这是首个使用HE的端到端非交互式变换器实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;引言&lt;/strong&gt;
变换器（Transformer）模型是近年来最突出的技术之一，基于变换器的语言模型ChatGPT通过最大化大型语言模型的潜力，推动了传统人工智能能力的边界，实现了与人类的自然互动，带来了重大创新。然而，ChatGPT目前采用的是机器学习即服务（MLaaS）模型，其中客户端将数据发送到服务器，服务器使用其模型进行推理并将结果返回给客户端。这个过程将客户端的数据暴露给服务器，产生了显著的隐私问题。由于这些问题，注重安全的公司对使用ChatGPT持谨慎态度，担心企业数据的潜在泄露，这最终限制了强大的基于变换器的模型的充分利用。为了解决MLaaS环境中的隐私问题，利用同态加密（HE）对加密数据进行推理的隐私保护机器学习（PPML）得到了积极研究 [3, 7, 13, 16, 19, 27]。随着对变换器模型关注的增加，关于变换器安全推理的研究也日益增多 [4, 6, 9, 21, 23, 28, 29]。PPML研究分为交互式PPML（通过客户端与服务器之间的通信进行推理）和非交互式PPML（服务器在没有通信的情况下对加密数据进行推理）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;交互式PPML&lt;/strong&gt; [6, 9, 13, 21, 23, 24, 27] 主要采用使用同态加密（HE）和多方计算（MPC）的方法，这些方法具有相对较快的执行时间。然而，当针对像变换器这样的复杂模型时，这种方法可能会导致通信成本超过几十GB，这在带宽不足的环境中可能成为限制因素。与此相对，&lt;strong&gt;非交互式PPML&lt;/strong&gt; 仅涉及服务器对客户端加密数据进行计算，无需任何通信，客户端只需发送加密数据并接收加密后的最终结果。由于不需要在线通信的优势，许多研究采用了非交互式方法 [3, 7, 10, 16, 19, 28, 29]。本文重点讨论非交互式隐私保护变换器推理。&lt;strong&gt;RNS-CKKS&lt;/strong&gt; [5] 是非交互式变换器推理的良好选择，因为它具有快速执行大量实数运算的优势。在RNS-CKKS上实现高效的非交互式隐私保护变换器推理的两个主要挑战是softmax操作和注意力所需的矩阵乘法。基于HE的变换器研究中最关键的问题是，目前仍没有完整解决基于HE的变换器模型在实践中稳定运行的清晰端到端实现的论文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Softmax函数&lt;/strong&gt;
Softmax函数可以通过以下方程计算，其中减去$x_{\text{max}}$是为了数值稳定性 [15]：
&lt;/p&gt;
$$
y_i = \frac{\exp(x_i - x_{\text{max}})}{\sum_{j=0}^{m-1} \exp(x_j - x_{\text{max}})}
$$&lt;p&gt;
在同态加密（HE）中应用softmax函数的两种最常见方法是：用多项式函数替代softmax函数，或准确地近似softmax函数本身。第一种方法是用多项式替代softmax函数并对模型进行微调，可能改善基于HE的实现中的时间性能，但一个关键问题是，当多项式未能有效替代神经网络中的softmax时，可能会导致准确度下降。例如，Zimerman等人提出的softmax替代方法 [29] 显著降低了准确度，如第2.1节中的实验结果所示。第二种方法则专注于提供快速且准确的近似。例如，最近的研究，如 [28]，使用泰勒级数和Goldschmidt多项式近似方法来近似softmax函数。由于指数函数的高不稳定性，在RNS-CKKS上进行准确计算是困难的。因此，为了稳定性，使用$x_{\text{max}}$进行减法变得不可避免，这进一步需要大量的比较操作，导致在安全推理过程中显著的计算开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;矩阵乘法&lt;/strong&gt;
所有涉及基于同态加密（HE）私有推理的论文都未完全呈现适用于整个变换器网络的端到端矩阵乘法方法。例如，本研究领域的先前研究包括NEXUS [28]和Zimerman等人的研究 [29]。NEXUS提出了一种用于计算多头注意力中查询矩阵（Qi）、键矩阵（Ki）和值矩阵（Vi）的密文-明文乘法算法，以及用于计算$Q_iK_i^T$的密文-密文乘法。然而，后续乘法的同态算法，如与Vi的乘法或输出权重矩阵$W_O$的乘法，在NEXUS中并未提及。此外，NEXUS采用了多种打包方法，如按组件打包、按行打包、按列打包和按对角线打包，但没有详细说明推理过程中打包结构如何变化，以支持非交互式计算。上述打包方法的定义在B.2节中提供。另一方面，Zimerman等人的研究 [29]没有提供详细的实现细节，如同态矩阵乘法算法，也没有提供在加密状态下的实现代码，使得很难复现他们的非交互式实现并分析其效率。因此，未来的基于HE的变换器实现研究必须提出专门适用于HE私有变换器推理的矩阵乘法方法，并提供该方法的端到端实现源代码。&lt;/p&gt;
&lt;p&gt;尽管没有完全解决变换器网络的HE实现问题，但在HE-MPC混合实现中提出了几种适用于注意力模块的矩阵乘法方法。然而，这些矩阵乘法的计算量并未得到优化，或者由于非紧凑实现，产生了大量的中间密文，从而导致了显著的引导操作量。例如，BOLT [23]提出了一种利用列主打包的同态矩阵乘法算法，尽管明文-密文乘法需要2√d个密钥切换，效率相对较高，但密文-密文矩阵乘法仍然需要多达$d \log d + 2d$个密钥切换，相比于Jiang等人提出的著名同态矩阵乘法方法 [10]，这是一个效率较低的实现。另一方面，NEXUS [28]中使用的按行和按列打包方法效率较低，因为它们留下了许多未使用的槽位，我们的分析表明，使用这些打包方法需要大量的引导操作。此外，撇开引导操作的数量不谈，使用NEXUS提出的矩阵乘法算法实现多头注意力时，至少需要6d²次密钥切换。对于我们所针对的变换器模型（d = 128），这会导致过多的密钥切换。虽然Jiang等人提出了一种高效的通用密文-密文矩阵乘法算法，但该方法需要为注意力模块（具有复杂的矩阵操作结构）进行专门化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我们的贡献&lt;/strong&gt;
我们提出了一种高效的端到端私有基于同态加密（HE）的变换器实现，称为PowerFormer，通过解决基于HE实现的两个重要研究问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们引入了一组新的softmax替代函数——批量整流器-幂最大（Batch Rectifier-Power max，BRPmax）函数集，超越了现有的softmax替代方法。这一创新方法通过我们新的训练方法实现了高数值稳定性、与原始BERT模型几乎相同的分类准确性，以及高效的推理运行时性能。&lt;/li&gt;
&lt;li&gt;我们提出了一种针对多头注意力优化的矩阵乘法操作，呈现了一种端到端实现方法，最小化了多头注意力中的密钥切换操作次数和引导操作次数。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;批量整流器-幂最大brpmax方法&#34;&gt;&lt;strong&gt;批量整流器-幂最大（BRPmax）方法&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;我们提出将softmax函数近似为$ \text{ReLU}(x + c)^p / R_d $，其中$c$、$p$和$R_d$是常数，且$c$、$p$是固定的。通过采用批量归一化中的批量方法，运行中的分母$R_d$在训练过程中被学习，并在推理过程中作为常数处理，从而通过乘以常数$1/R_d$来在加密数据上执行操作。ReLU函数通过Lee等人提出的方法 [17, 18] 被精确近似。我们通过数值实验展示了Powered ReLU（整流器-幂）函数有效地替代了指数函数。我们还展示了蒸馏学习中的批量方法解决了动态缩放问题，这是PPML中的常见挑战。&lt;/p&gt;
&lt;h3 id=&#34;优化同态注意力&#34;&gt;&lt;strong&gt;优化同态注意力&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;我们进一步优化了Jiang等人 [10] 提出的矩阵-矩阵乘法算法。对于明文-密文矩阵乘法，我们将密文-密文矩阵乘法算法中的多个明文-密文乘法步骤集成到一个单一的baby-step giant-step求和中，与 [10] 相比，密钥切换次数减少了85%到91%。我们还提出了一种针对查询矩阵和键矩阵乘法优化的分块矩阵乘法算法，密钥切换次数比 [10] 减少了35%。&lt;/p&gt;
&lt;h3 id=&#34;端到端基于he的变换器实现&#34;&gt;&lt;strong&gt;端到端基于HE的变换器实现&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;我们提出了首个端到端非交互式安全变换器推理架构——Powerformer。该架构通过使用多项式近似替代非线性操作（如softmax、GELU和层归一化），同时利用我们高效的矩阵乘法算法。尽管我们的重点是BERT-tiny模型，但该架构可以轻松扩展到其他模型，如BERT-medium和BERT-base。我们已使用RNS-CKKS库Lattigo [1] 完成了Powerformer架构的端到端实现。与仅提供加密数据上执行的单个组件结果的NEXUS不同，我们实现了整个过程并展示了完整的结果。我们的实现仅在单个CPU线程上运行了503秒，比基于单线程性能的NEXUS快至少39倍。鉴于这是单线程实现，我们预计在GPU或硬件加速的情况下，运行时间将减少数十到数百倍 [12, 14]。&lt;/p&gt;
&lt;h2 id=&#34;privformer-privacy-preserving-transformer-with-mpc&#34;&gt;Privformer: Privacy-preserving Transformer with MPC
&lt;/h2&gt;&lt;p&gt;&amp;ldquo;Privformer: 基于多方计算的隐私保护Transformer&amp;rdquo;&lt;/p&gt;
&lt;p&gt;摘要——Transformer 是一种处理序列数据的深度学习架构。Transformer 在多个序列数据分析任务中达到了最先进的水平，其变体，如 BERT 和 GPT-3，已成为解决自然语言处理（NLP）一般任务的事实标准。本文提出了一种三方多方计算（MPC）协议，用于在诚实多数设置下进行 Transformer 的安全推理。当使用现有构建块实现 Transformer 的 MPC 协议时，注意力层是最耗时的部分。注意力机制是 Transformer 的核心组件，能够捕捉和利用输入序列中元素之间的复杂依赖关系。注意力机制调用指数函数 O(S²) 次，这在使用现有 MPC 原语实现 Transformer 时成为主要瓶颈。为了解决这个问题，我们采用了 Performer [11]，这是 Transformer 的一种变体，其中调用指数函数的 sigmoid 函数被 ReLU 函数替代，ReLU 是一种更适合 MPC 的非线性函数。此外，通过引入基于核的注意力矩阵近似方法，并使用随机正交矩阵，我们展示了注意力层可以通过 O(S) 次 ReLU 函数调用来处理。我们通过三方 MPC 对 Transformer 进行端到端实现，研究了该方法的效率。实验评估表明，对于一个输出序列长度为 64 的翻译任务，整个计算过程在局域网环境下大约需要 19 分钟。&lt;/p&gt;
&lt;p&gt;引言——Transformer [49] 在机器学习中广泛应用于自然语言处理（NLP）任务，如机器翻译 [51]、文本摘要 [33]、问答 [8] 等。最初，Transformer 被应用于需要利用输入序列中元素间复杂依赖关系的 NLP 任务（例如，依存句法分析用于确定语法结构）。随后，Transformer 也被应用于视觉问答（VQA）[10]、图像分类 [18]、图像生成 [56] 和蛋白质折叠预测（AlphaFold 2）[24] 等任务。有关 Transformer 及其变体的更多信息，请参见 [48]。在大规模通用数据集（例如，通用文本语料库）上训练的 Transformer 被称为预训练模型（如 BERT、GPT-3），并且已知能够处理多种任务。将预训练模型应用于特定任务时，通常通过迁移学习使用专门为该任务准备的小量训练数据对模型进行微调。通过结合预训练和迁移学习，即使在合理的计算时间内，使用少量的训练数据也能实现高精度的识别性能。&lt;/p&gt;
&lt;p&gt;使用基于 Transformer 的模型的一大难题是其庞大的模型规模。例如，GPT-3 是一个生成句子的语言模型，包含大约 1750 亿个参数。因此，在使用基于 Transformer 的模型时，通常将模型部署在云服务器上，而不是本地计算资源上。使用时，信息流遵循 API 模式；用户将输入数据发送到服务器，服务器使用模型进行处理，并仅将结果返回给用户。从用户的角度来看，数据隐私是云端模型部署中的主要问题之一。在 API 模式下，用户必须将所有数据提交到云服务器。例如，考虑将文件进行云端机器翻译。如果文件内容高度机密，将整个文件发送到云服务器是不可接受的。此外，借助最先进的机器翻译技术，实时口译对话已成为可能。然而，由于同样的原因，当需要保护机密性时，基于云的 API 模式并不现实。隐私问题可以通过将整个模型分发给用户并在用户端执行所有计算来解决。然而，近期的最先进的基于 Transformer 的模型规模极大，需要 GPU 进行处理。在用户端准备如此大规模的计算环境也是不现实的。此外，模型通常需要经过大量的努力和成本进行特定任务的微调。这样的模型是宝贵的信息资产，将这些模型分发给用户可能是不可接受的。&lt;/p&gt;
&lt;p&gt;在过去十年中，已经开发了多种用于处理序列数据的架构，如循环神经网络（RNN）、LSTM 和 Seq2Seq。目前，预训练的 Transformer 变体，如 BERT（Bidirectional Encoder Representations from the Transformers）[17] 和 GPT（Generative Pre-trained Transformers）-3 [6]，被认为是许多 NLP 任务的最先进技术，并被广泛作为事实标准使用。考虑到上述情况，本文旨在开发一种三方多方计算（MPC）协议，用于在诚实多数设置下对 Transformer 进行安全推理，使得能够通过 API 使用部署在云服务器上的 Transformer，同时保护模型和用户的隐私。&lt;/p&gt;
&lt;p&gt;相关工作——在过去五年中，深度学习推理和训练中的隐私保护取得了显著进展。大多数研究依赖于一种或多种隐私保护计算技术，如同态加密 [46]、[22]、秘密共享 [32]、[44]、[13] 以及它们的组合 [45]、[23]、[35]。卷积神经网络（CNN）的隐私保护推理和训练已被深入研究，并通过 CrypTFlow [26]、CrypTFlow 2 [43]、CryptGPU [47]、[39] 等展示了对大规模 ImageNet 数据进行隐私保护计算是可行的。综述论文 [43] 总结了深度学习隐私保护计算技术的最新进展。隐私保护计算技术在序列分析（例如自然语言处理、语音识别、时间序列分析）中的应用，仍然少于在图像识别中的应用。然而，已有一些研究探索了深度神经网络在序列分析中的隐私保护计算，如循环神经网络（RNN）、LSTM、Seq2Seq 和基于 Transformer 的模型。SIRNN [42] 提出了一个二方协议，用于处理序列数据的 RNN 推理。他们引入了高效的通信协议，用于处理通过多方计算（MPC）计算成本高昂的非线性函数，如指数函数、sigmoid 函数、tanh 函数和平方根逆函数，采用查找表和混合位宽的方法。该协议结合了秘密共享和盲传输，保证了半诚实安全性。冯等人提出了使用加法和乘法秘密共享的 n 方协议，用于计算在神经网络中常用的典型非线性函数 sigmoid 函数和 tanh 函数 [19]。使用这些技术，冯等人提出了 PrivLSTM 和 PrivSeq2Seq，分别在半诚实对抗模型中安全地计算 LSTM 和 Seq2Seq 的推理。王等人提出了一种 n 方 MPC 协议，使用加法和二进制秘密共享，在半诚实对抗模型中实现 Transformer 的近似推理 [53]。该工作集中于 Transformer 计算中软最大（softmax）函数评估所占的主导地位；他们提出通过使用 Linformer [52] 和 Nystromformer [55]，这些方法可以近似计算 Transformer，从而减少 softmax 函数的评估时间。该工作的目标和方法与我们的工作相似，但在以下两点上存在显著差异。&lt;/p&gt;
&lt;p&gt;首先，Nystromformer 仅考虑了 Transformer 的编码器部分，而未考虑涉及遮罩注意力的解码器部分。遮罩注意力是处理复杂任务（如机器翻译和句子生成）所必需的组件。正如我们将在 5.2 节中详细讨论的那样，在应用多方计算（MPC）时，特别需要考虑如何将遮罩矩阵应用到注意力矩阵中。其次，该框架假设使用半诚实对手模型，并不支持对抗恶意对手的安全性。我们的协议是在诚实多数设置下工作的。陈等人提出了一种协议 THE-X，用于使用同态加密进行 Transformer 的安全推理 [9]。陈等人通过微调引入了 Transformer 的多项式激活函数近似。给定一个预训练的 Transformer 模型，非多项式函数（如高斯误差线性单元（GELU）、softmax 和层归一化）被多项式替代，整个模型进行了微调，以提高其推理性能。他们的实验评估评估了预测性能，但未报告计算时间和通信带宽。李等人还提出了基于同态（CKKS）加密的隐私保护嵌入，并通过对 BERT 嵌入的加密进行文本分类来展示其性能 [30]。该框架支持 CKKS 加密方案的高效 GPU 实现；然而，只有下游分类（逻辑回归）部分通过同态加密得到保护，整个序列编码部分则没有加密保护。除了 Transformer 的隐私保护计算外，还有一些关于模型隐私和输入隐私的研究。Lang 等人和 Coavoux 等人研究了 Transformer 编码器获得的潜在表示信息泄露的风险，并提出了使用对抗训练的防御方法 [12]，[29]。Lu 等人揭示了使用在联邦学习过程中收集的梯度信息来反转 Transformer 梯度的风险 [34]。Qu 等人研究了相同的私人输入泄露风险，并提出了使用差分隐私的对策 [41]。这些研究的目标与我们的工作是正交的，且这些研究采用了不同的安全模型。&lt;/p&gt;
&lt;p&gt;我们的贡献&lt;/p&gt;
&lt;p&gt;Transformer 编码器层对某些任务（如分类）非常有用。然而，当 Transformer 被要求作为生成模型工作时（例如机器翻译和句子生成），涉及遮罩注意力的解码器部分是必不可少的。在本研究中，我们开发了用于全 Transformer 模型推理的多方计算（MPC）协议，包括编码器层和解码器层。本工作的贡献总结如下：&lt;/p&gt;
&lt;p&gt;我们设计了一个 3 方 MPC 协议，用于在诚实多数设置下对 Transformer 进行安全推理。诚实多数设置下的 MPC 协议由 Araki 等人 [3] 和 Furukawa 等人 [20] 首先提出，并随后应用于 ABY3 [16]、SecureNN [50] 和 FALCON [31] 中的机器学习模型的推理和训练。FALCON 引入的 MPC 协议实现了隐私保护计算，这些计算通常用于深度神经网络，如矩阵乘法、修正线性单元（ReLU）、ReLU 的导数（DReLU）和批量归一化。其中一些协议是通过结合 ABY3 和 SecureNN 中使用的技术设计的。我们的协议也在很大程度上依赖于这些工作的协议。&lt;/p&gt;
&lt;p&gt;我们的技术贡献有三个方面：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;一个用于计算注意力的 MPC 协议，ReLU 注意力，使用 O(S) 次 ReLU 调用，且在 O(1) 轮中完成；&lt;/li&gt;
&lt;li&gt;两个用于遮罩注意力的 MPC 协议：一个在 O(1) 轮中需要 O(S²) 时间，另一个在 O(S) 轮中实现 O(S) 时间复杂度；&lt;/li&gt;
&lt;li&gt;一个新的用于平方根逆运算的 MPC 协议。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;首先，我们提出了一个高效的 MPC 协议——ReLU 注意力，用于计算注意力矩阵，这是 Transformer 编码器层中的核心组件。实现注意力层可以通过简单地结合现有的协议来实现，但这种简单的组合会因以下原因而导致效率低下。通过初步研究，我们揭示了在注意力层中，sigmoid 函数中的指数运算是整个计算的瓶颈（见第 4 节）。更精确地说，给定长度为 S 的输入序列，在 MPC 中，指数运算会在注意力层中调用 O(S²) 次。为了解决这一问题，我们引入了两种技术：（1）我们将注意力层中的 sigmoid 函数替换为 ReLU 函数（ReLU 注意力，见第 5 节），（2）我们使用 Performer [11] 对 Transformer 进行近似，Performer 是注意力的基于核的泛化方法。由于 ReLU 函数对 MPC 更友好，替换 sigmoid 为 ReLU 提高了计算效率。此外，Performer 使用随机正交矩阵对 ReLU 注意力进行近似，从而仅需 O(S) 次 ReLU 调用来处理注意力层。&lt;/p&gt;
&lt;p&gt;其次，我们通过实验研究了两种 MPC 协议处理遮罩注意力层的效率，遮罩注意力层是 Transformer 解码器层中的另一个核心组件。与用于（非遮罩）ReLU 注意力的 MPC 协议不同，处理遮罩注意力的 MPC 即使在近似的情况下，也需要 O(S²) 次 ReLU 调用（见第 5.2.1 节）。这是因为遮罩矩阵需要应用于 S × S 的注意力矩阵，无论是否进行近似。一个解决方法是按顺序应用遮罩到注意力向量，这样 MPC 就不需要处理整个注意力矩阵。通过这种修改，我们可以将 ReLU 的调用次数减少到 O(S) 次；但是，由于顺序遮罩，轮次复杂度增加到 O(S)（见第 5.2.2 节）。考虑到这一轮次和通信复杂度的差异，这两种协议的优越性取决于通信环境。通过实验评估，我们揭示了这些策略在遮罩注意力中的计算效率。&lt;/p&gt;
&lt;p&gt;第三，我们提出了一种新的用于平方根逆运算的 MPC 协议。平方根逆运算包含在批量归一化层或层归一化层中，通常不仅出现在 Transformer 中，也出现在各种神经网络中，并且已经在 SIRNN 和 FALCON 中进行了研究。据我们所知，只有 FALCON [31] 在诚实多数设置下研究了平方根逆运算的 MPC。一些协议的限制是，它们通过近似估算 log2 b，其中 b 是输入，并以明文形式在各方之间共享，这可能泄漏关于私有输入序列的信息（信号的方差）。我们提出了一种用于平方根逆运算的 MPC 协议，该协议不会泄漏任何中间值，并且在诚实多数设置下是安全的。&lt;/p&gt;
&lt;p&gt;本文的结构安排如下：第二节介绍了我们提出的方法所需的基本构件；第三节定义了我们的问题和威胁模型；第四节讨论了通过组合现有 MPC 协议来实现 Transformer 时的问题；第五节提出了 ReLU 注意力和遮罩 ReLU 注意力的 MPC 协议；第六节介绍了平方根逆运算的 MPC 协议；第七节我们构建了 Privformer，这是一个端到端的 MPC 协议，用于计算 Transformer，使用了所提出的构件；第八节讨论了所提出协议的安全性；第九节通过实验展示了这些协议的效率；第十节总结了我们的工作。&lt;/p&gt;
&lt;h2 id=&#34;relus-revival-on-the-entropic-overload-in-normalization-free-large-language-models&#34;&gt;ReLU’s Revival: On the Entropic Overload in Normalization-Free Large Language Models
&lt;/h2&gt;&lt;p&gt;ReLU的复兴：关于无归一化大语言模型中的熵过载问题&lt;/p&gt;
&lt;p&gt;LayerNorm是现代大语言模型（LLM）中的一个关键组件，能够稳定训练并确保优化过程的顺利进行。然而，它在机制可解释性、异常特征抑制、信号传递的真实性以及私密推理的计算和通信复杂性方面带来了显著的挑战。本研究探索了无归一化解码器-only LLM中理想的激活函数。与传统上在基于Transformer的模型中偏好使用GELU不同，我们的实验证据表明了一个相反的趋势：在无LayerNorm的模型中，ReLU显著优于GELU，带来了8.2%的困惑度改进。我们发现GELU存在一个关键问题，即早期层会经历熵过载，导致注意力头的表征能力未得到充分利用。这表明，像GELU这样平滑的激活函数并不适合无LayerNorm架构，而ReLU的几何属性——在输入空间中的专业化和类内选择性——有助于在没有LayerNorm的情况下改善学习动态和信息保留。本研究为优化Transformer架构提供了关键见解，特别是在LayerNorm带来显著挑战的情况下。代码和实现可在relu-revival-normfree获得。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;引言&lt;/strong&gt;
&lt;strong&gt;动机与挑战&lt;/strong&gt;
LayerNorm [1] 是现代大语言模型（LLM）成功的关键架构组件之一，通过对层内特征的输入进行归一化来稳定训练过程。此外，它在增强模型的非线性表征能力方面也起到了重要作用 [2-5]。尽管LayerNorm具有诸多优点，但它在特定场景下会带来一些实际挑战，具体包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;私密推理（PI）&lt;/strong&gt;：PI协议使得在加密数据上进行推理成为可能，且不暴露输入数据，从而确保数据隐私并保护模型权重 [6-14]。混合PI协议在处理LayerNorm中的逆平方根计算时遇到困难，这使得它成为仅次于GELU的第二大开销操作，贡献了22%的总延迟和通信开销 [11]。此外，纯同态加密（HE）PI需要对LayerNorm进行多项式近似，这在具有广泛方差范围的情况下非常具有挑战性 [8]。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;机制可解释性&lt;/strong&gt;：LayerNorm增加了残差流的复杂性，使得分析和理解Transformer模型内部工作变得更加困难 [15]，限制了LLM在要求透明性和可解释性的应用中的适用性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;低精度训练&lt;/strong&gt;：LayerNorm中的可训练参数与异常特征的放大相关，这对LLM量化训练提出挑战，因为它加剧了数值不稳定性并降低了低精度训练下的性能 [16-19]。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;信号传播&lt;/strong&gt;：LayerNorm已被证明对信号传播产生负面影响 [20]。
这些挑战突显了需要无LayerNorm架构的必要性，这样可以保留Transformer的优点，同时避免其缺点。然而，这一转变也带来了新的考虑，尤其是在选择前馈网络（FFN）激活函数时。此前的研究 [20-22] 探讨了设计无归一化LLM的各种架构启发式方法，但去除归一化层对FFN激活函数选择的影响仍未得到充分探索。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;研究洞察与影响&lt;/strong&gt;
本研究超越了以往的工作，深入探讨了无归一化LLM中激活函数的设计选择，提供了关于这些选择如何影响学习动态、内部表征和整体模型性能的新见解。我们的研究揭示了几个关键发现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ReLU在无LayerNorm模型中优于GELU&lt;/strong&gt;：与传统做法相反，我们表明在无LayerNorm的模型中，使用ReLU作为FFN激活函数的模型在困惑度上显著优于使用GELU的模型，困惑度提高了8.2%（见图2和表2）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;具有可学习负斜率的学习动态&lt;/p&gt;
&lt;p&gt;：为了进一步探索，我们在泄漏ReLU激活函数中实验了可学习的负斜率，并使用了两种配置：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;层级配置&lt;/strong&gt;：每一层有一个独立的可学习斜率。最初，较浅的层学习正斜率，而较深的层学习负斜率。但随着训练的进行，所有层的斜率都逐渐收敛到接近零的值（见图3a）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;全局配置&lt;/strong&gt;：所有层共享一个可学习的斜率。该斜率最初为正，然后逐渐收敛到接近零（见图3b）。这些结果表明，无LayerNorm模型本质上更倾向于使用具有零负斜率的ReLU类激活函数。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GELU激活中的熵过载&lt;/strong&gt;：为了深入分析，我们检查了按头的熵值，发现使用GELU激活的无归一化模型中，早期层经历了熵过载，注意力头的大部分达到了接近最大熵水平，这表明注意力头的表征能力未得到充分利用。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;贡献&lt;/strong&gt;
我们的主要贡献如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们通过对无归一化解码器-only模型中激活函数的深入分析，研究了从头开始训练时的学习动态。&lt;/li&gt;
&lt;li&gt;我们从香农熵的角度探索了不同激活函数在基准模型和无归一化模型中对注意力得分分布的影响，为无LayerNorm模型架构设计提供了有价值的见解。&lt;/li&gt;
&lt;li&gt;我们在GPT-2和Pythia [23] 模型上进行了各种上下文大小（128和256）的实验，训练数据来自CodeParrot [24]，并使用了21亿训练token。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;secure-transformer-inference-made-non-interactive&#34;&gt;Secure Transformer Inference Made Non-interactive
&lt;/h2&gt;&lt;p&gt;使得变换器推理安全且非交互式&lt;/p&gt;
&lt;p&gt;随着ChatGPT的普及，安全变换器推理已成为一个重要的研究课题。现有的解决方案通常是交互式的，涉及大量的通信负载以及客户端和服务器之间的多轮交互。在本文中，我们提出了NEXUS，首个用于安全变换器推理的非交互式协议。该协议要求客户端在整个推理过程中只与服务器进行一轮通信：提交加密输入并接收加密结果。NEXUS引入了几个新颖的原语，包括SIMD密文压缩/解压缩、SIMD槽折叠和安全Argmax，使其在通信方面显著超越现有技术，同时保持可比的运行时性能。具体而言，它与BOLT（Oakland ’24）相比，减少了372.5倍的带宽消耗，与Bumblebee（NDSS ’25）相比，减少了53.6倍。此外，它的非交互式特性允许进行最优的硬件加速，GPU版本在运行时实现了42.3倍的加速。这使得NEXUS能够在仅用164MB带宽的情况下，在37.3秒内完成基于BERT的模型推理。&lt;/p&gt;
&lt;h3 id=&#34;引言-2&#34;&gt;引言
&lt;/h3&gt;&lt;p&gt;变换器模型，如GPT [48] 和 BERT [17]，已经彻底改变了人工智能领域。它们在语言翻译、内容生成和问答等多种应用中表现出色。然而，这些应用通常涉及敏感数据的处理，随着时间的推移，用户隐私的担忧也日益增加。最近，OpenAI 开发了 ChatGPT 作为一种在线推理服务，并为开发者提供了一个公共 API，使他们能够通过提交提示或消息轻松访问该平台。虽然这种方式便捷，但它对数据隐私构成了重大风险，因为用户提交的内容有时可能包含私人信息。&lt;/p&gt;
&lt;p&gt;安全推理是一种双方加密协议，旨在使模型推理以一种方式进行，即服务器 S 对客户端 C 提交的输入一无所知，且客户端 C 对服务器 S 的模型一无所知，除非推理结果。过去几年中，许多此类协议已被开发用于卷积神经网络（CNNs）[40]，[28]，[2]，[31]，而一些最新的研究也开始支持基于变换器的模型 [25]，[11]，[27]，[42]，[45]。&lt;/p&gt;
&lt;p&gt;值得注意的是，这些协议中的大多数是交互式的，要求大量的通信成本和客户端与服务器之间的多轮交互。例如，当前安全变换器推理的最先进解决方案 BOLT [45]，记录了单次推理消耗 59.61GB 带宽和 10,509 轮交互。如此庞大的通信开销显著增加了网络延迟，尤其在广域网（WAN）配置中，并使得传统的硬件加速技术，如 GPU 或 FPGA，失去效果。此外，通信负载的大小使得安全推理的财务成本非常高。根据 AWS 的定价标准 [1]，BOLT [45] 每个回复标记的成本为 5.44 美元，这意味着实际部署既昂贵又不可行。&lt;/p&gt;
&lt;p&gt;我们强调建立一种非交互式安全推理模型的重要性，其中客户端 C 只需提交一个加密输入，就能从服务器 S 获得加密的预测结果。对于需要实时响应的场景，现有的安全推理协议，无论是交互式还是非交互式，都无法满足速度要求。然而，非交互式协议在利用硬件加速时，在满足速度要求方面展现了潜力 [16]，[15]，[54]，[56]，[51]，[3]，[33]。在诸如数据仓库和医院诊断等非实时场景中，客户端 C 可以容忍响应的延迟，此时部署非交互式协议是可行的，而交互式协议则不可行。这是因为交互式协议要求客户端 C 的计算资源在等待期间持续占用，妨碍了 C 执行其他任务的能力。&lt;/p&gt;
&lt;h3 id=&#34;在非交互式安全推理的背景下变换器与卷积神经网络cnn模型之间有两个关键区别&#34;&gt;在非交互式安全推理的背景下，变换器与卷积神经网络（CNN）模型之间有两个关键区别。
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;更大规模的矩阵-矩阵乘法&lt;/strong&gt;
之前关于私有神经网络的研究，如Gazalle [31] 和 Cheetah [28]，提出了针对全连接层的安全矩阵-向量乘法的优化协议。然而，变换器需要进行大规模的矩阵-矩阵乘法。之前的研究 [31]，[28]，[25] 通过内积计算乘法，并在矩阵-矩阵乘法的情况下采用稀疏打包来处理生成的密文。由于这种技术，密文中的大部分数据槽经常被浪费，从而引入了额外的通信开销。此外，变换器模型中矩阵的输入维度通常更高，导致需要执行更多的乘法，从而增加了计算成本。因此，开发一种既节省时间又节省空间的新矩阵乘法协议是非常有用的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更高维度的Argmax输入&lt;/strong&gt;
CNN和变换器的最后一层都是Argmax，其输入是一个概率向量，每个条目表示一个输出标签候选的概率（总共有m个标签）。推理输出是概率最高的标签。当前最先进的FHE（全同态加密）算法用于Argmax，由Phoenix（CCS&#39;22）[30] 提出，其计算复杂度为O(m)。在CNN图像分类任务中，m的值通常不大（例如，在ImageNet-1k中，m = 1,000），因此这一度量被认为是可接受的。然而，对于基于变换器的NLP任务，m等于词汇表的大小，BERT中m为30,522，Llama-3-8B中m为128,256。显然，现有的算法不适用于变换器，因此需要一种低复杂度的解决方案。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;a-我们的贡献&#34;&gt;A. 我们的贡献
&lt;/h3&gt;&lt;p&gt;本文提出了NEXUS，这是我们所知的首个用于安全变换器推理的非交互式协议。NEXUS的协议设计从客户端使用RNS-CKKS全同态加密（FHE）加密输入开始，使服务器能够在FHE加密的数据上评估变换器模型。我们的贡献总结如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;高效且优化的通信矩阵乘法&lt;/strong&gt;
许多以前的安全推理协议，如Gazalle [31]、Cheetah [28] 和 Iron [25]，在输出密文中浪费了数据槽，从而引入了不必要的通信开销。BumbleBee [42] 通过密文交织消除了浪费的槽，但需要更多的计算。我们采用了密文压缩和解压缩策略，并利用我们矩阵乘法算法中的单项式特殊性质（参见Section-III.B）来减少通信成本。我们还提出了一种适合摊销的离线-在线计算策略（参见Section-III.C），以减少计算成本。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;高效的Argmax和其他非线性函数评估&lt;/strong&gt;
为了防止成员推理攻击 [53]，[58]，[57]，一种常见的方法是输出Argmax后的logits向量，因为它泄漏关于模型的信息最少 [53]。对于长度为m的输入（BERT中m=30,522，Llama-3-8B中m=128,256），当前最先进的协议 [30] 需要执行m次SGN操作（参见Section-II.C）和m次密文旋转。而我们的方法仅需要(log m + 1)次SGN操作和(log m + 1)次密文旋转，从而大大减少了计算开销。此外，我们还使用RNS-CKKS实现了GELU、Softmax和Layer Normalization等非线性函数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提供了NEXUS在CPU和GPU上的端到端实现&lt;/strong&gt;
图1展示了我们提出的协议与基线相比的改进。总之，与最先进的协议Bumblebee [42]相比，NEXUS（CPU）在计算上快了1.79倍，可以节省98.1%的通信开销，并将财务成本降低了2.38倍。利用我们协议的非交互式特性，我们进一步提供了GPU加速实现。NEXUS（GPU）将推理速度提高了42.3倍，并将财务成本降低了17.2倍，降至每个token仅0.05美元。我们的代码已经开源，地址是 &lt;a class=&#34;link&#34; href=&#34;https://github.com/zju-abclab/NEXUS&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/zju-abclab/NEXUS&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;selective-network-linearization-for-efficient-private-inference&#34;&gt;Selective Network Linearization for Efficient Private Inference
&lt;/h2&gt;&lt;p&gt;选择性网络线性化以实现高效的私密推理&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;
私密推理（Private Inference, PI）允许在加密保护的数据上直接进行推理。尽管这种技术有望解决许多隐私问题，但由于极长的运行时间，其应用仍然十分有限。与明文推理中延迟主要由浮点运算（FLOPs）主导不同，在私密推理中，非线性函数（特别是 ReLU）是主要瓶颈。因此，实用的私密推理需要针对 ReLU 的全新优化方法。为减少 PI 的延迟，我们提出了一种基于梯度的算法，能够选择性地线性化 ReLU，同时保持预测精度。我们在多个标准的私密推理基准上评估了该算法。实验结果表明，与当前最先进的方法相比，在相同 ReLU 数量（50K）的情况下，我们的算法可提高预测精度最多达 4.25%；在相同预测精度（70%）的情况下，延迟可减少最多 2.2 倍。这些改进推进了延迟-精度空间中的帕累托前沿。除了实验结果外，我们还提出了一项“无免费午餐”定理，阐明了在保持预测精度的前提下，网络线性化的可能性及适用条件。公共代码已开放，访问地址为：https://github.com/NYU-DICE-Lab/selective_network_linearization。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;引言&lt;/strong&gt;
基于云的机器学习框架推动了私密推理（Private Inference, PI）的发展。总体而言，私密推理的愿景是使用户能够在云服务提供商拥有的模型上高效地对其数据进行推理，同时保护双方的隐私。在推理之前，用户的数据和服务提供商的模型均通过加密技术进行了加密。&lt;/p&gt;
&lt;p&gt;实现这一愿景的主要难点在于深度网络中的非线性操作。使用密文进行网络中线性操作的私密执行，可以通过秘密共享技术和与输入无关的预处理阶段，使其速度几乎与对相同操作的普通（明文）评估一样快。然而，为了在网络中私密地评估 ReLU，需要使用某种版本的 Yao 的混淆电路（Garbled Circuits, GC），这导致了高昂的延迟成本和存储开销。简单来说，标准的深度网络架构（例如 ResNet）并不适合高效的私密推理，因为它们包含了过多的 ReLU 操作。（参见 Mishra et al., 2020; Ghodsi et al., 2020; 2021; Jha et al., 2021，以及第 2 节的详细讨论。）&lt;/p&gt;
&lt;p&gt;因此，要充分释放快速、准确且私密的神经网络推理的潜力，需要重新设计网络架构，以尽可能减少 ReLU 的数量。针对这一方向，已经涌现了许多努力。早期工作如 MiniONN（Liu et al., 2017）关注于安全协议本身，而最近的工作如 Delphi（Mishra et al., 2020）和 Circa（Ghodsi et al., 2021）则提出用其他激活函数替换 ReLU。另一类研究工作则通过神经架构搜索（NAS）设计 ReLU 高效的网络结构。例如，CryptoNAS（Ghodsi et al., 2020）采用进化式 NAS，而 Sphynx（Cho et al., 2021a）则使用微搜索 NAS。&lt;/p&gt;
&lt;p&gt;本文探索了一条新的路径，适用于用于成像、计算机视觉或其他感知问题的多种架构（特别是具有/不具有残差连接和 ReLU 激活的深度卷积网络）。我们研究了一个我们称为“深度网络线性化”的问题，这指的是在此类网络中精心选择一部分神经元，并消除它们的非线性（即，用恒等或线性操作替换它们的 ReLU 操作），从而在整体性能和 ReLU 操作数量之间实现最小化的权衡。需要注意的是，通过这一过程，我们并未减少网络中的参数数量。由于 ReLU 激活是简单的标量操作，在正常情况下，这一过程在网络训练或测试期间不会显著减少浮点操作（FLOP）的数量。然而，在私密推理场景下，线性化的主要优势在于减少了 GC 计算的数量。&lt;/p&gt;
&lt;p&gt;本文提出了一种简单的基于梯度的算法（我们称之为选择性网络线性化，Selective Network Linearization, SNL）来解决深度网络线性化问题，验证了该算法在多种标准基准上的有效性，并提供了理论依据以阐明其观察到的行为。&lt;/p&gt;
&lt;p&gt;目前，私密推理的最先进方法是 DeepReDuce（Jha et al., 2021）。该方法引入了深度网络中的 ReLU 丢弃（ReLU Dropping）概念，从概念上看，与网络线性化等价。他们提出的算法包含三个阶段：ReLU “筛选”、ReLU “稀疏化”和 ReLU “重塑”，每个阶段都需要多个手动设计选择，并涉及多个超参数。候选架构的搜索空间非常大；将 DeepReDuce 应用于具有 D 个阶段的 ResNet 需要训练 Ω(D) 个不同的网络，并选择准确率最高的那个。此外，DeepReDuce 所做的保留/丢弃决策是分阶段的：要么线性化整个 ReLU 层，要么完全保留整个 ReLU 层不变。&lt;/p&gt;
&lt;p&gt;相比之下，我们提出的技术 SNL 高度自动化，仅涉及极少的超参数。SNL 通过单一的基于梯度的训练过程实现（具体细节将在后文描述），无需在多个网络骨架上进行搜索。此外，SNL 提供了精细的控制，能够细化到像素特征图级别，以决定保留或消除哪些 ReLU。&lt;/p&gt;
&lt;p&gt;SNL 的核心直觉简单明了，并适用于许多深度网络。考虑任何标准架构（例如 ResNet-34），但做一个变化：将所有 ReLU 替换为参数化 ReLU（parametric ReLUs，简称 pReLUs）（He et al., 2015），其中每个 ReLU 都有一个独立的、可训练的斜率参数。此外，每个斜率参数被进一步约束为二值（0 或 1）。定义此网络后，SNL 通过标准训练过程（例如使用 SGD 或 Adam）进行优化，无需其他人工干预。&lt;/p&gt;
&lt;p&gt;在实际实现中需要注意一些关键点。主要挑战在于：(a) 强制 pReLU 的斜率满足二值约束，以及 (b) 确保仅保留少量的 ReLU，即斜率参数向量是稀疏的（或反稀疏的）。借鉴某些网络剪枝算法的方法（Lee et al., 2019；Cho et al., 2021b），我们通过以下方式解决这些困难：在标准训练误差损失中增加一个作用于斜率系数的 $&lt;code&gt;l_1&lt;/code&gt;$ 惩罚项，根据时间表逐渐降低该惩罚的权重，并在最后执行一个二值化步骤，将斜率参数取整为二值。详见第 3 节。&lt;/p&gt;
&lt;p&gt;我们在文献中常用的私密图像分类基准数据集上验证了 SNL，结果表明 SNL 在整个准确率-延迟权衡曲线中对现有所有方法实现了帕累托支配。如图 1 所示，在 CIFAR-100 数据集的上下文中验证了这一点；我们在后文以及附录中提供了针对 CIFAR-10 和 Tiny ImageNet 的额外实验结果（及消融研究），也展示了类似的优势。详见第 4 节和附录。&lt;/p&gt;
&lt;p&gt;深入分析 SNL 的结果揭示了一些有趣的现象：在较高比例上，网络早期层的 ReLU 被优先线性化，而晚期层的 ReLU 则较少被线性化。这与早期 PI 工作（如 DeepReDuce，Jha et al., 2021）的结论一致。这是否暗示了网络学习中的某些基本属性？为了严谨回答这个问题，我们证明了线性化会以牺牲 3 层网络记忆容量为代价的无免费午餐定理。此外，如果网络是收缩性的，即第二层的神经元数量少于第一层（分类问题中常见的情况），那么只有在第二层较少的神经元被线性化时，选择性线性化才能保留原始网络的容量。详见第 5 节。&lt;/p&gt;
&lt;h2 id=&#34;sigma-secure-gpt-inference-with-function-secret-sharing&#34;&gt;Sigma: Secure GPT Inference with Function Secret Sharing
&lt;/h2&gt;&lt;p&gt;Sigma：通过功能秘密共享实现安全的 GPT 推理&lt;/p&gt;
&lt;p&gt;摘要：安全两方计算（2PC）支持安全推理，能够同时保护专有的机器学习（ML）模型及其敏感输入。然而，现有的安全推理解决方案，尤其是针对Transformer模型的解决方案，存在较高的延迟和通信开销。功能秘密共享（FSS）是一种新兴的范式，可通过预处理阶段实现高效的2PC协议。我们提出了Sigma，这是第一个基于FSS的端到端安全Transformer推理系统。通过为复杂的机器学习功能（如Softmax、GeLU和SiLU）构建新的FSS协议，并加速其在GPU上的计算，Sigma将Transformer安全推理的延迟相比现有基于预处理和GPU的最先进方法提高了12至19倍。我们展示了生成预训练Transformer（GPT）模型的首次安全推理。特别地，Sigma在38秒内执行了Meta的Llama2（HuggingFace提供，参数量为130亿），并在1.5秒内完成了GPT2的推理。&lt;/p&gt;
&lt;p&gt;基于Transformer的生成式语言模型近年来因其在各种自然语言任务上的卓越表现（例如，问答、摘要、语言翻译、代码生成等）[20, 21, 76]而受到广泛关注。除了确保模型和输入隐私外，对此类模型进行安全推理还带来了其他有趣的应用场景，例如“提示隐私（prompt privacy）”。人工智能公司正在投入大量精力构建能够产生良好推理结果的提示，并希望保持这些提示的隐秘性。安全推理允许持有专有提示的公司与持有敏感数据的客户，在不向彼此暴露输入的情况下，利用公开的语言模型生成推理结果。&lt;/p&gt;
&lt;p&gt;然而，目前用于安全推理的最先进系统在Transformer模型上表现不尽如人意。我们认为，安全机器学习推理系统必须满足以下需求：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;准确性&lt;/strong&gt; - 即安全推理下的准确性应与明文推理一致；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;安全性&lt;/strong&gt; - 系统应提供标准的两方计算（2PC）安全性；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效率&lt;/strong&gt; - 安全推理的延迟和通信开销应较低；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可扩展性&lt;/strong&gt; - 系统必须能够扩展至具有数十亿参数的模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;然而，我们发现现有系统往往无法满足（甚至多个）这些要求。现有的安全Transformer推理系统包括THE-X [25]、Iron [38] 和 CrypTen [47, 51, 84]（我们将在第8节讨论其他相关工作）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;THE-X&lt;/strong&gt;：通过将基于基本函数（如 exe^x）的复杂非线性替换为简单的非线性（如 (\max(x, 0)\）），牺牲了准确性；此外，还通过暴露中间值牺牲了安全性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Iron&lt;/strong&gt;：在保持准确性和安全性的同时，通信开销巨大，即使是BERT模型也需要超过100 GB的通信量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CrypTen&lt;/strong&gt;：虽然利用GPU加速和预处理提升了效率，但其在线延迟和通信开销仍然显著。此外，由于使用了不安全的本地截断操作，未能提供标准的2PC安全性。而且，由于GPU内存溢出问题，它无法扩展到更大的模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;我们的贡献&lt;/strong&gt;
本文提出了 &lt;strong&gt;Sigma2&lt;/strong&gt; 系统，它在多个维度上推进了基于Transformer模型的安全推理技术的最先进水平。与CrypTen类似，Sigma在带有预处理的两方计算（2PC）模型中运行并利用GPU加速，但其延迟和通信效率提高了一个数量级，同时提供标准的2PC安全性保证。在安全推理中，Sigma通过精确逼近复杂非线性函数，保持模型的推理准确性，并能够高效扩展至具有数十亿参数的GPT模型。&lt;/p&gt;
&lt;p&gt;尽管现有的基于Transformer的安全推理最先进系统 [47, 51] 已经使用GPU加速计算，它们基于秘密共享协议的设计导致了大量通信开销，即使在高带宽环境下，这些开销仍然是性能瓶颈。与传统的基于秘密共享的2PC协议相比，基于函数秘密共享（FSS）的2PC协议通信量显著降低，但计算开销较高 [16, 19]。这促使了许多近期工作 [37, 43, 73, 80] 专注于在预处理模型中设计基于FSS的专用协议，用于简单神经网络的安全推理。例如，Orca [43] 是目前卷积神经网络（CNN）安全推理的最先进系统，它证明了GPU加速可以有效处理FSS的大量计算。然而，Orca仅适用于使用简单非线性（如ReLU和Maxpool）的CNN，对Transformer中大量复杂非线性函数（如Softmax）的处理则带来不可接受的开销（详见7.1.2节）。&lt;/p&gt;
&lt;p&gt;在Sigma中，我们设计了针对Transformer的高效基于FSS的2PC协议。由于Transformer安全推理的延迟主要由复杂非线性（如GeLU、SiLU、Softmax和层归一化 [38]）主导，我们提出了新的FSS协议处理这些操作，并通过GPU加速实现。这些操作需要准确计算各种基本函数（如指数、倒平方根、逆运算等）。之前的工作Pika [80] 使用大型查找表（LUTs）来处理这些函数。尽管这种方法通用，Grotto [73] 指出大型查找表效率低下，并在可能的情况下使用基于自定义样条的协议。Sigma的协议通过减小查找表大小在保持准确性的同时，比Grotto更高效（详见7.1.1节）。例如，对于50位的GeLU值，Pika需要大小为 $2^{50}$ 的查找表，而Sigma仅需大小为 $2^8$ 的查找表，并且在相同威胁模型下总计算量比Grotto降低9倍。&lt;/p&gt;
&lt;p&gt;我们评估了基于GPT [20]、BERT [29] 和 Llama2 [77] 的模型，这些模型广泛应用于下一个单词预测和分类任务。我们的创新协议能够安全且准确地评估具有13亿参数的GPT-Neo模型（“一个由EleutherAI基于GPT-3架构复现设计的Transformer模型”[5]），推理时间为7.2秒。Sigma还支持Meta AI最近发布的Llama2模型，并在Huggingface上可用。Llama2-7B需要23秒，Llama2-13B需要38秒。Sigma运行HuggingFace上的较小GPT2模型（每月下载量数千万次）仅需1.5秒，而运行BERT模型则耗时0.1−4.5秒。总体上，Sigma在安全推理的延迟上比现有最先进技术提高了12.2−19倍。&lt;/p&gt;
&lt;p&gt;为了保证标准的2PC安全性，Sigma摒弃了本地截断操作，转而使用安全的忠实截断。截断广泛用于线性层（如矩阵乘法后的操作）和非线性层。我们提出了一种新的忠实截断协议（详见4.2节），其效率比现有工作 [16] 提高最多30倍。尽管我们的截断比CrypTen中的“几乎免费”的本地截断更昂贵，但由于在GeLU、SiLU和Softmax上的巨大性能提升，Sigma的端到端推理速度仍比CrypTen快10倍以上。&lt;/p&gt;
&lt;p&gt;我们的大规模评估得益于Sigma的前端系统，它允许用户简洁地表达所需的Transformer架构，并使用Sigma针对CPU或GPU优化的协议运行（详见第6节）。针对CPU和GPU的协议设计有所不同，我们均提供支持（详见5.1节）。事实上，即使在CPU上运行的Sigma也比在GPU上运行的CrypTen更快。我们在附录A中讨论了使用Sigma的一些实际考量。Sigma的代码已公开，地址为：https://github.com/mpcmsri/EzPC/tree/master/GPU-MPC/experiments/sigma。&lt;/p&gt;
&lt;h2 id=&#34;the-x-privacy-preserving-transformer-inference-with-homomorphic-encryption&#34;&gt;THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;THE-X: 基于同态加密的隐私保护Transformer推理&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;
随着越来越多的预训练语言模型采用云端部署，隐私问题也迅速增长，主要体现在明文用户数据（如搜索历史、医疗记录、银行账户等）的暴露。云服务用户对Transformer模型的隐私保护推理有着强烈的需求。为了保护隐私，使用同态加密（HE）仅在密文上进行计算是一种具有吸引力的选择。然而，由于Transformer模块中的复杂计算，目前的HE工具尚不支持在密文数据上进行推理。本文提出了THE-X，这是一种针对Transformer的近似方法，能够实现由流行框架开发的预训练模型的隐私保护推理。THE-X提出了一种处理Transformer网络中复杂计算的工作流程，涵盖了所有非多项式函数，如GELU、Softmax和LayerNorm。实验结果表明，我们提出的THE-X能够在加密数据上进行Transformer推理，并适用于不同的下游任务，所有任务在性能上几乎没有下降，同时享有理论上保证的隐私保护优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;引言&lt;/strong&gt;
随着预训练模型在许多自然语言处理（NLP）应用中的革命性进展，如情感分析（Xu et al., 2019a）、问答系统（Yang et al., 2019b）、信息检索（Yang et al., 2019c）和文本生成（Raffel et al., 2020），许多相关技术已经被部署到云端，由工业服务提供商处理来自个人客户、小型企业和大型企业的用户数据。然而，云端预训练技术的便利性也带来了一系列隐私挑战，原因在于用户数据的敏感性。例如，用户请求中的输入文本甚至文本向量表示可能泄露私人信息，进而导致特定用户的身份泄露（Schwartz and Solove, 2011; Zhu and Han, 2020）。这种缺乏隐私保障的情况可能会阻碍注重隐私的用户将数据提供给服务提供商。因此，服务提供商可能面临无法利用用户数据发展模型的困境。此外，数据泄露和其他隐私漏洞可能导致服务提供商面临诉讼、罚款以及声誉损害。这些隐私担忧促使我们提出了THE-X，以实现Transformer模型的隐私保护推理。&lt;/p&gt;
&lt;p&gt;具体而言，我们识别出了预训练模型隐私保护推理中的两个挑战。第一个挑战是如何保护用户的明文数据不被第三方服务提供商访问（例如，诊疗记录或购物历史）。以往的工作应用了差分隐私（DP）（Dwork et al., 2006）及其变体来解决类似的隐私问题——最初用于统计数据库，最近也用于深度学习（DL）（Abadi et al., 2016）和NLP（Qu et al., 2021；Basu et al., 2021b；Fernandes et al., 2019；Lyu et al., 2020；Basu et al., 2021a）。然而，这种解决方案可能遭遇窃听攻击。有少数研究（Zhu and Han, 2020；Zhao et al., 2020）表明，攻击者有可能通过梯度泄露恢复原始数据。此外，隐私保护永远无法理论上保证。第二个挑战是性能问题，近期的研究如TextHide（Huang et al., 2020）和FedNLP（Lin et al., 2021）利用联邦学习（Yang et al., 2019a）在加密数据上训练模型，但代价是显著的性能下降。虽然它们集中关注训练数据的隐私保护，但并未充分探讨隐私保护推理问题。&lt;/p&gt;
&lt;p&gt;为了解决上述问题，我们在图1中描述了一种隐私保护推理的实践，其中一个微调后的语言模型可以通过THE-X转换为云服务模式，并在“闭眼”状态下处理用户数据。在推理过程中，用户查询的内容对Transformer模型来说是匿名的。计算结果也是密文，只有用户的私钥才能解密。此外，我们需要一个理论上保障的加密解决方案，如同态加密（HE）（Gentry, 2009），以便让服务提供商和用户在生产场景中确信隐私安全。HE的语义安全性由基于格的密码学保证，HE对密文的计算结果可以解密为与明文相同的结果，从而避免了性能降低的成本。同态加密的基本思想是在不先解密数据的情况下对加密数据进行计算，这可以在云服务场景中完全确保隐私安全。它允许将用户数据加密后外包到商业云环境进行处理。然而，由于Transformer模型中的复杂操作（例如GELU激活函数），流行的部分同态加密解决方案（仅支持加法或乘法）无法轻松适配到预训练模型的场景中。基于HE Transformer后端（Boemer et al., 2019b,a, 2020），我们设计了一系列近似组件，以完成主流Transformer骨干网络的整个推理流程。我们在GLUE基准（Wang et al., 2019）和CONLL2003任务（Tjong Kim Sang and De Meulder, 2003）上评估了THE-X对BERTtiny的表现。我们的结果表明，THE-X能够实现隐私保护推理，并且平均性能下降仅为1.49%。我们的贡献包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们是首个探索使用HE进行隐私保护Transformer推理的工作。&lt;/li&gt;
&lt;li&gt;我们设计了一种实用且有效的近似工作流程，用于将基于Transformer的模型转换为完全由HE操作组成的函数。&lt;/li&gt;
&lt;li&gt;一系列实验结果确认了我们提出的THE-X近似方法带来的性能损失几乎可以忽略不计。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;-&lt;/p&gt;
</description>
        </item>
        <item>
        <title>第4周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</link>
        <pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/colorful-2609978_1280.jpg" alt="Featured image of post 第4周工作总结" /&gt;&lt;h1 id=&#34;总结&#34;&gt;总结
&lt;/h1&gt;&lt;p&gt;这一周高强度阅读把书都读完了，但是没看代码，后果是有点记不住，等有需要时再去看吧。&lt;/p&gt;
&lt;p&gt;这周看了大模型内容比较多，主要是为了快点看完相关内容找ZD哥确定毕设方向，早点开始，做的好一点。&lt;/p&gt;
&lt;p&gt;周五下午找了ZD哥确定方向，看完方向人有点蒙，可能是大模型看得多，安全那方面看得少，导致我不知道这个方向在干什么，于是开始沉思，思考了半天感觉确实不会，找gpt老师询问，没有得到结果，故有些惆怅，周日又坐在实验室上了一天的软件工程实训，还好马上就结束了，太烦了。今天是周一，先把上周周报做了&lt;/p&gt;
&lt;p&gt;我又重新换了问法，问了gpt老师，这次得到了满意的结果，找到了几篇论文，这周就读这几篇。&lt;/p&gt;
&lt;p&gt;感觉这才是周报，之前的都是书的摘抄。&lt;/p&gt;
&lt;h2 id=&#34;阅读&#34;&gt;阅读
&lt;/h2&gt;&lt;p&gt;LLM预训练数据准备&lt;/p&gt;
&lt;p&gt;transformer模型架构（不知道是不是我本，这个我从不同的地方看了听了好多遍，现在才刚有点感觉）&lt;/p&gt;
&lt;p&gt;Instruction Tuning&lt;/p&gt;
&lt;p&gt;提示学习（实在不行，没有研究天赋就去干点轻松的(bushi)）&lt;/p&gt;
&lt;p&gt;解码与部署&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;后面就是补了两篇差分隐私的文献，读完感觉对要研究的方向还是不清楚，故又找了几篇：&lt;/p&gt;
&lt;h2 id=&#34;计划&#34;&gt;计划
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文综述了在机器学习中实现安全和隐私保护的各种方法，包括使用同态加密和安全多方计算的技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Inference with Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Inference of Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Machine Learning: A Practical Approach&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Outsourced Computation in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Differentially Private Inference in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Towards Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Model Inference: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>transformer模型架构</title>
        <link>https://JiangZhiyu-1024.github.io/p/transformermodelbase/</link>
        <pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/transformermodelbase/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/transformermodelbase/house-7497002_1280.png" alt="Featured image of post transformer模型架构" /&gt;&lt;h1 id=&#34;模型架构&#34;&gt;模型架构
&lt;/h1&gt;&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391.png&#34;
	width=&#34;807&#34;
	height=&#34;689&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391_hu13532172091181392611.png 480w, https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391_hu12135867853004229579.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241029095402391&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;281px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;大语言模型架构配置表&lt;/center&gt;&gt;
&lt;h2 id=&#34;transformer-模型&#34;&gt;🍈Transformer 模型
&lt;/h2&gt;&lt;p&gt;当前主流的大语言模型都是基于Transformer模型进行设计的。Transformer是由多层的多头自注意力（Multi-headSelf-attention）模块堆叠而成的神经网络模型。原始的Transformer 模型由编码器和解码器两个部分构成，而这两个部分实际上可以独立使用，例如基于编码器架构的BERT模型[1]和解码器架构的GPT模型[2]。与BERT等早期的预训练语言模型相比，大语言模型的特点是使用了更长的向量维度、更深的层数，进而包含了更大规模的模型参数，并主要使用解码器架构，对于Transformer 本身的结构与配置改变并不大。&lt;/p&gt;
&lt;h3 id=&#34;输入编码&#34;&gt;🍉输入编码
&lt;/h3&gt;&lt;p&gt;在Transformer 模型中，输入的词元序列(𝒖 = [𝑢1,𝑢2,&amp;hellip;,𝑢𝑇]) 首先经过一个输入嵌入模块（InputEmbeddingModule）转化成词向量序列。具体来说，为了捕获词汇本身的语义信息，每个词元在输入嵌入模块中被映射成为一个可学习的、具有固定维度的词向量$v_t$ ∈$R^H$。由于Transformer的编码器结构本身无法识别序列中元素的顺序，位置编码（PositionEmbedding,PE）被引入来表示序列中的位置信息。给定一个词元$u_t$，位置编码根据其在输入中的绝对位置分配一个固定长度的嵌入向量$p_t$ ∈$R^H$。然后，每个词元对应的词向量和位置向量将直接相加，生成了最终的输入嵌入序列𝑿=[𝒙1,&amp;hellip;,𝒙𝑇]，并且被传入到后续层中：$x_t=v_t+p_t$.&lt;/p&gt;
&lt;p&gt;通过这种建模方法的表示，Transformer 模型可以利用位置编码 𝒑𝑡 建模不同词元的位置信息。由于不同词元的位置编码仅由其位置唯一决定，因此这种位置建模方式被称为绝对位置编码。尽管绝对位置编码能够一定程度上建模位置信息，然而它只能局限于建模训练样本中出现的位置，无法建模训练数据中未出现过的位置，因此极大地限制了它们处理长文本的能力。&lt;/p&gt;
&lt;h3 id=&#34;多头自注意力机制&#34;&gt;🍋多头自注意力机制
&lt;/h3&gt;&lt;p&gt;多头自注意力是Transformer模型的核心创新技术。相比于循环神经网络（Recurrent Neural Network, RNN）和卷积神经网络（ConvolutionalNeuralNetwork,CNN）等传统神经网络，多头自注意力机制能够直接建模任意距离的词元之间的交互关系。作为对比，循环神经网络迭代地利用前一个时刻的状态更新当前时刻的状态，因此在处理较长序列的时候，常常会出现梯度爆炸或者梯度消失的问题。而在卷积神经网络中，只有位于同一个卷积核的窗口中的词元可以直接进行交互，通过堆叠层数来实现远距离词元间信息的交换。&lt;/p&gt;
&lt;p&gt;多头自注意力机制通常由多个自注意力模块组成。在每个自注意力模块中，对于输入的词元序列，将其映射为相应的查询（Query,𝑸）、键（Key,𝑲）和值（Value,𝑽）三个矩阵。然后，对于每个查询，将和所有没有被掩盖的键之间计算点积。这些点积值进一步除以$\sqrt{D}$进行缩放（𝐷是键对应的向量维度），被传入到softmax函数中用于权重的计算。进一步，这些权重将作用于与键相关联的值，通过加权和的形式计算得到最终的输出。在数学上，上述过程可以表示为：
&lt;/p&gt;
$$
Q = XW^Q,
$$$$
K = XW^K,
$$$$
V = XW^V,
$$$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{D}})V.
$$&lt;p&gt;与单头注意力相比，多头注意力机制的主要区别在于它使用了𝐻组结构相同，但映射参数不同的自注意力模块。输入序列首先通过不同的权重矩阵被映射为一组查询、键和值。每组查询、键和值的映射构成一个“头”，并独立地计算自注意力的输出。最后，不同头的输出被拼接在一起，并通过一个权重矩阵$W^O$∈$R^{H \times H}$ 进行映射，产生最终的输出。如下面的公式所示：
&lt;/p&gt;
$$
head_n = Attention(XW^Q_n,XW^K_n,XW^V_n)
$$$$
MHA = Concat(head_1,...,head_N)W^O
$$&lt;p&gt;由上述内容可见，自注意力机制能够直接建模序列中任意两个位置之间的关系，进而有效捕获长程依赖关系，具有更强的序列建模能力。另一个主要的优势是，自注意力的计算过程对于基于硬件的并行优化（如GPU、TPU等）非常友好，因此能够支持大规模参数的高效优化。&lt;/p&gt;
&lt;h3 id=&#34;前馈网络层&#34;&gt;🍏前馈网络层
&lt;/h3&gt;&lt;p&gt;为了学习复杂的函数关系和特征，Transformer 模型引入了一个前馈网络层（Feed Forward Netwok, FFN），对于每个位置的隐藏状态进行非线性变换和特征提取。具体来说，给定输入𝒙，Transformer中的前馈神经网络由两个线性变换和一个非线性激活函数组成：
&lt;/p&gt;
$$
FFN(X) = σ(XW^U + b_1)W^D + b_2
$$&lt;p&gt;
其中$W^U$ ∈ $R^{H \times H}$ 和$W^D$ ∈ $R^{H \times H}$  分别是第一层和第二层的线性变换权重矩阵，$b_1$ ∈ $R^{𝐻^′}$ 和 $b_2$ ∈ $R^H$ 是偏置项，𝜎是激活函数（在原始的Transformer中，采用 ReLU 作为激活函数）。前馈网络层通过激活函数引入了非线性映射变换，提升了 模型的表达能力，从而更好地捕获复杂的交互关系。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440.png&#34;
	width=&#34;650&#34;
	height=&#34;691&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440_hu8196060707081265206.png 480w, https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440_hu16997779109624366145.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241029105634440&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;94&#34;
		data-flex-basis=&#34;225px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;Transformer 架构图&lt;/center&gt;
&lt;h3 id=&#34;编码器&#34;&gt;🍐编码器
&lt;/h3&gt;&lt;p&gt;在Transformer 模型中，编码器（Encoder）的作用是将每个输入词元都编码成一个上下文语义相关的表示向量。编码器结构由多个相同的层堆叠而成，其中每一层都包含多头自注意力模块和前馈网络模块。在注意力和前馈网络后，模型使用层归一化和残差连接来加强模型的训练稳定度。其中，残差连接（Residual Connection）将输入与该层的输出相加，实现了信息在不同层的跳跃传递，从而缓解梯度爆炸和消失的问题。而LayerNorm则对数据进行重新放缩，提升模型的训练稳定性。编码器接受经过位置编码层的词嵌入序列𝑿作为输入，通过多个堆叠的编码器层来建模上下文信息，进而对于整个输入序列进行编码表示。由于输入数据是完全可见的，编码器中的自注意力模块通常采用双向注意力，每个位置的词元表示能够有效融合上下文的语义关系。在编码器-解码器架构中，编码器的输出将作为解码器（Decoder）的输入，进行后续计算。形式化来说，第𝑙层（𝑙∈{1,&amp;hellip;,𝐿}）的编码器的数据处理过程如下所示：
&lt;/p&gt;
$$
X^′_l = LayerNorm(MHA(X_{l-1})+X_{l-1})
$$$$
X_l = LayerNorm(FFN(X^′_l)+X^′_l)
$$&lt;p&gt;其中，$X^′_l$ 和 $X_l$ 分别是该Transformer层的输入和输出，$X^′_l$是该层中输入经过多头注意力模块后的中间表示，LayerNorm表示层归一化。&lt;/p&gt;
&lt;h3 id=&#34;解码器&#34;&gt;🥥解码器
&lt;/h3&gt;&lt;p&gt;Transformer 架构中的解码器基于来自编码器编码后的最后一层的输出表示以及已经由模型生成的词元序列，执行后续的序列生成任务。与编码器不同，解码器需要引入掩码自注意力（MaskedSelf-attention）模块，用来在计算注意力分数的时候掩盖当前位置之后的词，以保证生成目标序列时不依赖于未来的信息。除了建模目标序列的内部关系，解码器还引入了与编码器相关联的多头注意力层，从而关注编码器输出的上下文信息$X_L$。同编码器类似，在每个模块之后，Transformer 解码器也采用了层归一化和残差连接。在经过解码器之后，模型会通过一个全连接层将输出映射到大小为𝑉的目标词汇表的概率分布，并基于某种解码策略生成对应的词元。在训练过程中，解码器可以通过一次前向传播，让每个词元的输出用于预测下一个词元。而在解码过程，解码器需要经过一个逐步的生成过程，将自回归地生成完整的目标序列。解码器的数据流程如下所示：
&lt;/p&gt;
$$
Y^′_l = LayerNorm(MaskedMHA(Y_{l-1})+Y_{l-1})
$$$$
Y^&#34;_l = LayerNorm(CrossMHA(Y^′_l,X_L)+Y^′_l)
$$$$
Y_l = LayerNorm(FFN(Y^&#34;_l)+Y^&#34;_l)
$$&lt;p&gt;其中，$Y_{l-1}$ 和 $Y_l$ 分别是该Transformer 层的输入和输出，$Y^′_l$ 和 $Y^&amp;quot;_l$ 是该层中输入经过掩码多头注意力MaskedMHA和交叉多头注意力CrossMHA模块后的中间表示，LayerNorm 表示层归一化。然后将最后一层的输入𝒀𝐿映射到词表的维度上：
&lt;/p&gt;
$$
O = softmax(W^LY_L)
$$&lt;p&gt;
其中，𝑶 ∈$R^{H \times V}$ 是模型最终的输出，代表下一个词在词表上的概率分布；$W^L$ ∈ $R^{H \times V}$ 是将输入表示映射到词汇表维度的参数矩阵，而$W^LY_L$是概率化前的中间值，通常被称为logits。&lt;/p&gt;
&lt;p&gt;[1]. JacobDevlin et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. In: NAACL-HLT. 2019.&lt;/p&gt;
&lt;p&gt;[2]. Alec Radford et al. “Improving language understanding by generative pre-training”. In: OpenAI Blog (2018).&lt;/p&gt;
</description>
        </item>
        <item>
        <title>第3周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/third-week/</link>
        <pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/third-week/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/third-week/cards-8594729_640.jpg" alt="Featured image of post 第3周工作总结" /&gt;&lt;h1 id=&#34;文献阅读&#34;&gt;文献阅读
&lt;/h1&gt;&lt;h5 id=&#34;每周都完不成上次的任务杂事太多了尽快完成&#34;&gt;每周都完不成上次的任务，杂事太多了，尽快完成：
&lt;/h5&gt;&lt;h2 id=&#34;detecting-formal-thought-disorder-by-deep-contextualized-word-representations&#34;&gt;Detecting formal thought disorder by deep contextualized word representations
&lt;/h2&gt;&lt;p&gt;竟然要收费，不看了，让gpt给讲一下&lt;/p&gt;
&lt;p&gt;这篇论文《通过深度上下文化词表示检测形式思维障碍》研究了如何使用深度学习中的上下文词嵌入模型（如ELMo）来检测精神疾病患者的形式思维障碍（Formal Thought Disorder, FTD）。FTD是一种影响语言表达和思维逻辑的精神疾病症状，常见于精神分裂症患者。&lt;/p&gt;
&lt;p&gt;论文主要利用深度上下文化词嵌入模型，如ELMo，通过生成词在句子中的嵌入表示，捕捉语境中词的多义性和丰富的语义信息。研究团队通过分析患者的语言数据，构建模型来区分正常语言与存在思维障碍的语言表达，从而实现自动化的FTD检测。&lt;/p&gt;
&lt;p&gt;研究的核心贡献是使用更复杂的语义表示（如上下文化词嵌入）来解决传统方法无法有效处理的词义歧义问题。&lt;/p&gt;
&lt;h2 id=&#34;distributed-representations-of-words-and-phrases-and-their-compositionality&#34;&gt;Distributed Representations of Words and Phrases and their Compositionality
&lt;/h2&gt;&lt;p&gt;《Distributed Representations of Words and Phrases and their Compositionality》是由Mikolov等人在2013年提出的一篇重要论文，介绍了&lt;strong&gt;Word2Vec&lt;/strong&gt;模型的改进和创新。这篇文章主要讨论了如何通过分布式词表示（distributed word representations）更好地捕捉单词和短语的语义信息，并且提出了两种核心模型：&lt;strong&gt;CBOW（Continuous Bag of Words）&lt;/strong&gt; 和  &lt;strong&gt;Skip-gram&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这篇论文的主要贡献包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Skip-gram和CBOW模型&lt;/strong&gt;：论文提出了两种用于生成词向量的架构。Skip-gram模型的目标是根据一个词预测其上下文，而CBOW模型则根据上下文词来预测目标词。这两种模型能够有效地捕捉单词之间的语义关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;层次Softmax和负采样&lt;/strong&gt;：为了提高训练大规模语料库的效率，作者引入了&lt;strong&gt;层次Softmax&lt;/strong&gt;和**负采样（negative sampling）**技术，这大幅降低了模型的计算复杂度，使得大规模训练变得可行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;词和短语的组合性&lt;/strong&gt;：论文不仅处理了单词的表示，还处理了短语的表示，通过数据驱动的方式将多词短语映射为词向量，捕捉更复杂的语义结构。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;向量运算反映语义关系&lt;/strong&gt;：Word2Vec生成的词向量能够通过简单的向量运算表达词语之间的关系。例如，向量运算 &lt;code&gt;vec(&amp;quot;King&amp;quot;) - vec(&amp;quot;Man&amp;quot;) + vec(&amp;quot;Woman&amp;quot;) ≈ vec(&amp;quot;Queen&amp;quot;)&lt;/code&gt; 说明了这种分布式表示在捕捉语义关系上的强大能力。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;llm-book&#34;&gt;LLM BOOK
&lt;/h2&gt;&lt;p&gt;找到一本书，准确的来说是一个github仓库，组织者把近些年的LLM相关的东西都整进去了，好全，写的真好，库库看吧那就&lt;/p&gt;
&lt;p&gt;先把资源放在这里：&lt;a class=&#34;link&#34; href=&#34;https://github.com/RUCAIBox/LLMSurvey/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RUCAIBox/LLMSurvey: The official GitHub page for the survey paper &amp;ldquo;A Survey of Large Language Models&amp;rdquo;.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;第一部分-背景与基础知识&#34;&gt;第一部分 背景与基础知识
&lt;/h3&gt;&lt;h4 id=&#34;语言模型的发展历程p16&#34;&gt;语言模型的发展历程（P16）
&lt;/h4&gt;&lt;p&gt;LLM大语言模型那肯定是从LM语言模型来的，语言模型的发展又经历了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;统计语言模型（StatisticalLanguageModel, SLM）&lt;/li&gt;
&lt;li&gt;神经语言模型（NeuralLanguageModel,NLM）&lt;/li&gt;
&lt;li&gt;预训练语言模型（Pre-trainedLanguageModel,PLM）&lt;/li&gt;
&lt;li&gt;大语言模型（LargeLanguageModel, LLM）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;llm的特点p19&#34;&gt;LLM的特点（P19）
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;具有较为丰富的世界知识&lt;/li&gt;
&lt;li&gt;具有较强的通用任务解决能力&lt;/li&gt;
&lt;li&gt;具有较好的复杂任务推理能力&lt;/li&gt;
&lt;li&gt;具有较强的人类指令遵循能力&lt;/li&gt;
&lt;li&gt;具有较好的人类对齐能力&lt;/li&gt;
&lt;li&gt;具有可拓展的工具使用能力&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;大语言模型对于产业应用带来了变革性的技术影响，将会催生一个基于大语言模型的应用生态系统。&lt;/p&gt;
&lt;h3 id=&#34;第二部分-预训练&#34;&gt;第二部分 预训练
&lt;/h3&gt;&lt;p&gt;大语言模型的第一个训练阶段是预训练，预训练语料的规模和质量对于提升大语言模型的能力至关重要。&lt;/p&gt;
&lt;h4 id=&#34;数据来源&#34;&gt;数据来源
&lt;/h4&gt;&lt;p&gt;通用文本数据和专用文本数据。通用文本数据涵盖了网页、书籍和对话文本等。由于通用文本数据规模较大、多样性强且易于获取，大多数大语言模型都会收集大量的通用文本数据，以增强其语言建模能力。此外，为了进一步提升大语言模型在特定专业任务上的表现，人们还将预训练语料的范围扩展至更专业的数据集，如多语数据、科学数据和代码数据等。&lt;/p&gt;
&lt;h5 id=&#34;通用文本数据&#34;&gt;通用文本数据
&lt;/h5&gt;&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026154722061.png&#34;
	width=&#34;792&#34;
	height=&#34;657&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026154722061_hu1725408825362406164.png 480w, https://JiangZhiyu-1024.github.io/p/third-week/image-20241026154722061_hu12406635621069597218.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241026154722061&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;120&#34;
		data-flex-basis=&#34;289px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;现有大语言模型预训练数据中各种数据来源的比例分布图&lt;/center&gt;&gt;
&lt;h5 id=&#34;专用文本数据&#34;&gt;专用文本数据
&lt;/h5&gt;&lt;p&gt;多语文本、 科学文本、代码。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026155259857.png&#34;
	width=&#34;831&#34;
	height=&#34;189&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/third-week/image-20241026155259857_hu14579868356241669728.png 480w, https://JiangZhiyu-1024.github.io/p/third-week/image-20241026155259857_hu13508329583982414925.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241026155259857&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;439&#34;
		data-flex-basis=&#34;1055px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;典型的预训练数据预处理流程图&lt;/center&gt;&gt;
&lt;h4 id=&#34;数据预处理&#34;&gt;数据预处理
&lt;/h4&gt;&lt;p&gt;构建并使用系统化的数据处理框架（如开源库Data-Juicer）&lt;/p&gt;
&lt;p&gt;具体操作（P74）&lt;/p&gt;
&lt;h1 id=&#34;附录&#34;&gt;附录
&lt;/h1&gt;&lt;h2 id=&#34;softmax是啥&#34;&gt;softmax是啥？
&lt;/h2&gt;&lt;h4 id=&#34;softmax是一种常用的函数特别是在多分类任务中用来将一个向量中的元素转换为0到1之间的概率分布其核心作用是&#34;&gt;Softmax是一种常用的函数，特别是在&lt;strong&gt;多分类任务&lt;/strong&gt;中，用来将一个向量中的元素转换为0到1之间的概率分布。其核心作用是：
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;将实数映射为概率分布&lt;/strong&gt;：Softmax 接受一个实数向量，输出一个与输入维度相同的向量，其中每个元素的值表示该元素作为某个类别的概率，且这些概率的和为1。这样就可以根据这些概率来进行分类预测。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;公式&lt;/strong&gt;：Softmax 函数的定义如下：
&lt;/p&gt;
$$
   \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
   $$&lt;p&gt;
其中，\( z_i \) 是输入向量中的某个元素，\( e^{z_i} \) 是该元素的指数值，分母是所有输入元素的指数值的和。这个函数将每个 \( z_i \) 映射为一个在 \( (0,1) \) 之间的概率，并且所有输出的概率之和为1。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;作用机制&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;指数放大效果&lt;/strong&gt;：Softmax 函数通过对输入值取指数（\( e^x \)），会将较大的输入值放大更多，而较小的值则被进一步压缩，突出更大的概率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;归一化&lt;/strong&gt;：Softmax 保证所有输出的概率之和为1，符合概率的定义。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;应用场景&lt;/strong&gt;：在&lt;strong&gt;神经网络的最后一层&lt;/strong&gt;，特别是分类任务中，Softmax 常用于多类分类问题，比如在自然语言处理（NLP）中的词分类、图像识别中的对象分类等。它将网络的输出转化为易于理解的概率值，帮助模型决定输入属于哪个类别。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;举例&#34;&gt;举例
&lt;/h4&gt;&lt;p&gt;假设我们有一个模型输出三个值 \( [2.0, 1.0, 0.1] \)，通过 Softmax 转换后得到的结果是大约 \( [0.71, 0.26, 0.03] \)，其中最大的值0.71表示模型认为输入属于第一个类别的概率最大。&lt;/p&gt;
&lt;p&gt;简言之，Softmax 是一种将输出转换为概率分布的工具，特别适合多分类问题的场景。&lt;/p&gt;
&lt;h2 id=&#34;看了半天论文感觉没有一个时间线很难受不清楚那些论文之间的关系问问gpt&#34;&gt;看了半天论文，感觉没有一个时间线很难受，不清楚那些论文之间的关系，问问gpt：
&lt;/h2&gt;&lt;h3 id=&#34;基础概念与理论&#34;&gt;&lt;strong&gt;基础概念与理论&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;先掌握大模型背后的基本理论，这些知识是理解后续论文的基础：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语言模型的基础：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;经典论文：《Attention Is All You Need》（Transformer）&lt;/li&gt;
&lt;li&gt;相关概念：自注意力机制、序列到序列模型、词嵌入（如Word2Vec、GloVe）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自回归模型与自编码器：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;GPT 系列（GPT, GPT-2, GPT-3）的原理和应用&lt;/li&gt;
&lt;li&gt;BERT 及其衍生模型的预训练与微调方式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;扩展学习与生成式任务：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》&lt;/li&gt;
&lt;li&gt;生成式预训练模型的设计与任务应用（如文本生成、机器翻译）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;模型的训练与优化&#34;&gt;&lt;strong&gt;模型的训练与优化&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;学习如何高效地训练大模型，并且了解模型的优化技术：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;微调与参数高效训练：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Efficient Fine-Tuning Techniques for Large Models: Adapter, LoRA 等&lt;/li&gt;
&lt;li&gt;探索模型压缩技术（知识蒸馏、量化、剪枝等）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对比学习与自监督学习：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;《SimCLR》、《MoCo》 等代表性论文，理解对比学习在大模型中的应用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;模型对齐与安全&#34;&gt;&lt;strong&gt;模型对齐与安全&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;在大模型发展过程中，模型对齐（alignment）和伦理问题变得越来越重要：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型对齐与价值观嵌入：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;《InstructGPT: Training language models to follow instructions with human feedback》&lt;/li&gt;
&lt;li&gt;RLHF（强化学习通过人类反馈）等对齐策略&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型安全与公平性：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;探讨模型偏见问题以及在实际应用中的安全隐患&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;视觉-语言模型与多模态模型&#34;&gt;&lt;strong&gt;视觉-语言模型与多模态模型&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;学习大模型在跨模态任务中的应用，比如视觉-语言任务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CLIP（Contrastive Language-Image Pretraining）&lt;/strong&gt;：理解图像和文本共同表征的训练方法&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DALL-E、Stable Diffusion&lt;/strong&gt;：生成式视觉任务的模型设计与应用&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;视觉语言对话系统&lt;/strong&gt;：如《Visual ChatGPT》&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;前沿研究与新趋势&#34;&gt;&lt;strong&gt;前沿研究与新趋势&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;在深入理解基本模型后，可以关注大模型领域的前沿研究方向：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;大模型与知识图谱的结合&lt;/strong&gt;：如何将知识图谱融入语言模型&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;领域自适应模型&lt;/strong&gt;：如何通过特定领域数据进行模型的进一步微调&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;检索增强生成（RAG）&lt;/strong&gt;：如《RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;实战应用与开源框架&#34;&gt;&lt;strong&gt;实战应用与开源框架&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;阅读开源项目中的技术文档，参与开源项目（如Hugging Face的模型库）&lt;/li&gt;
&lt;li&gt;学习如何应用大模型解决实际问题，并尝试训练和微调自己的模型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样，你可以从基础概念、模型训练到前沿研究和实战应用，逐步深入，既能掌握理论又能结合实践。这个过程可以帮助你筛选出重要的论文和研究方向。&lt;/p&gt;
&lt;h2 id=&#34;对齐是什么意思&#34;&gt;对齐是什么意思？
&lt;/h2&gt;&lt;p&gt;在大型语言模型（LLM）中，“对齐”通常指的是模型的输出与人类意图或价值观的匹配程度。具体来说，它涉及确保模型在生成文本时能够产生符合人类期望的、有益的和安全的内容。对齐的目标是减少模型生成有害、误导或不准确的信息。&lt;/p&gt;
&lt;p&gt;对齐可以通过多种方法实现，包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练数据的选择和清洗&lt;/strong&gt;：使用高质量的数据集进行训练，确保模型接触到的内容符合人类的价值观和期望。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;强化学习&lt;/strong&gt;：利用人类反馈来微调模型，使其在生成内容时更符合人类的意图。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;安全性和伦理性评估&lt;/strong&gt;：在模型发布之前，对其输出进行评估，以识别和修正潜在的偏见或有害内容。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过这些方法，可以提高模型在特定任务或应用场景中的可靠性和安全性。&lt;/p&gt;
&lt;h2 id=&#34;arxiv-论文是什么&#34;&gt;arXiv 论文是什么？
&lt;/h2&gt;&lt;p&gt;arXiv 是一个开放获取的电子预印本存档，主要用于科学研究领域，特别是在物理学、数学、计算机科学、统计学、金融学、生物学和其他相关学科。研究者可以在 arXiv 上免费上传和分享他们的研究论文，通常是在正式出版之前的版本。&lt;/p&gt;
&lt;p&gt;arXiv 的特点包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;快速发布&lt;/strong&gt;：研究者可以迅速分享他们的研究成果，而不必等待同行评审和正式出版的过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;开放获取&lt;/strong&gt;：任何人都可以访问和下载论文，无需订阅或支付费用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;广泛的学科覆盖&lt;/strong&gt;：涵盖多个领域，促进跨学科的交流和合作。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;社区反馈&lt;/strong&gt;：研究者可以在正式发表之前收到来自社区的反馈，帮助他们改进论文。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;arXiv 的论文通常以“预印本”的形式存在，因此它们可能尚未经过同行评审。这意味着在引用这些论文时，研究者需要谨慎，了解其可能的局限性。&lt;/p&gt;
&lt;h2 id=&#34;在arxiv上发还未出版的论文不怕被别人剽窃成果吗&#34;&gt;在arXiv上发还未出版的论文不怕被别人剽窃成果吗？
&lt;/h2&gt;&lt;p&gt;在 arXiv 上发布未出版的论文确实存在被剽窃的风险，但研究者通常认为在 arXiv 上共享研究成果是有益的，原因如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;时间戳和公开记录&lt;/strong&gt;：一旦论文被上传到 arXiv，上传的时间戳可以作为知识产权的证明。这在某种程度上可以保护研究者的原创性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;学术社区的支持&lt;/strong&gt;：arXiv 是一个活跃的学术社区，研究者通常会关注相关领域的论文，并且对剽窃行为持有负面态度。学术界普遍重视诚信，剽窃通常会受到严厉的谴责。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;早期反馈&lt;/strong&gt;：通过发布预印本，研究者可以获得来自同行的反馈，从而改进自己的工作。这也有助于在正式出版之前建立声誉。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;增加曝光率&lt;/strong&gt;：在 arXiv 上发布可以提高研究的可见性，吸引潜在合作者或资助者。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;通用人工智能系统是什么&#34;&gt;通用人工智能系统是什么？
&lt;/h2&gt;&lt;p&gt;通用人工智能系统（AGI, Artificial General Intelligence）指的是一种能够理解、学习和应用知识，完成任何人类所能完成的智力任务的人工智能系统。与当前的狭义人工智能（Narrow AI）相比，后者在特定任务或领域表现出色（如图像识别、自然语言处理等），AGI 的目标是实现一种更广泛的智能，具备跨领域的学习和适应能力。&lt;/p&gt;
&lt;p&gt;AGI 的特点包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;自主学习&lt;/strong&gt;：能够从有限的数据中学习并推理，具备自我改进的能力。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多任务处理&lt;/strong&gt;：能够在多个不同的领域和任务中表现出色，而不局限于特定的应用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;常识推理&lt;/strong&gt;：具备理解和应用常识的能力，能够处理复杂的、不确定的环境。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;灵活性和适应性&lt;/strong&gt;：能够适应新情况和挑战，灵活地应对不同的环境和需求。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;AGI 的发展仍然是一个前沿研究领域，目前尚未实现。许多研究者在探索如何构建这样的系统，同时也讨论其潜在的伦理和社会影响，例如对就业、隐私和安全的影响。&lt;/p&gt;
&lt;h1 id=&#34;思考&#34;&gt;思考
&lt;/h1&gt;&lt;p&gt;随着看的东西多了，和使用chatgpt，我隐隐感觉到，由于chatgpt的语言模型主要是通过预测下一个最可能的词来生成文本，这一过程称为自回归生成（autoregressive generation）。导致gpt经常会顺着你的话去说，即有更小的概论去反对你的话，尽量让你说的话看起来正确，当然，如果你说的是：“太阳从西边升起”，那么他会义无反顾的反驳你。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>第2周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/second-week/</link>
        <pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/second-week/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/second-week/leaves-6729056_640.jpg" alt="Featured image of post 第2周工作总结" /&gt;&lt;h2 id=&#34;文献阅读&#34;&gt;文献阅读
&lt;/h2&gt;&lt;p&gt;看了知乎的科普文章，先从经典的论文开始看：&lt;/p&gt;
&lt;h3 id=&#34;attention-is-all-you-need&#34;&gt;Attention Is All You Need
&lt;/h3&gt;&lt;p&gt;《Attention is All You Need》是由Vaswani等人在2017年提出的一篇重要论文，首次引入了Transformer模型。以下是其主要内容的总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;：传统的序列到序列模型（如RNN和LSTM）在处理长序列时存在瓶颈。Transformer模型通过自注意力机制克服了这些限制，使得模型可以并行处理数据，提升了训练效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型架构&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder-Decoder结构&lt;/strong&gt;：Transformer由两个主要部分组成：编码器和解码器。编码器将输入序列转化为上下文向量，解码器则根据上下文生成输出序列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自注意力机制&lt;/strong&gt;：每个输入的表示都通过自注意力机制与其他输入进行交互，从而捕捉序列中的依赖关系。每个位置的表示通过加权求和得到，权重由输入的相似度计算得到。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多头注意力&lt;/strong&gt;：模型使用多个注意力头并行计算不同的表示，能够更全面地捕捉信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;位置编码&lt;/strong&gt;：由于Transformer没有循环结构，论文引入了位置编码，提供序列中词语的位置信息，以保留词序信息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练与优化&lt;/strong&gt;：Transformer使用了残差连接和层归一化，以提高训练的稳定性和收敛速度。论文中还介绍了基于Adam优化器的训练过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;：Transformer在机器翻译任务上取得了显著的性能提升，并且在其他NLP任务中也展示了强大的通用性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;影响&lt;/strong&gt;：该论文奠定了现代NLP研究的基础，推动了基于Transformer的预训练语言模型（如BERT、GPT等）的发展。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;差分隐私深度学习deep-learning-with-differential-privacy&#34;&gt;差分隐私深度学习(Deep Learning with Differential Privacy)
&lt;/h3&gt;&lt;p&gt;《Deep Learning with Differential Privacy》是由Martin Abadi等人在2016年提出的一篇重要论文，探讨了在深度学习中实现差分隐私的策略。以下是其主要内容的总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;差分隐私&lt;/strong&gt;：一种保护用户数据隐私的技术，旨在确保模型在训练时无法推断出任何单个用户的敏感信息。即使攻击者知道模型和数据集的其余部分，也无法有效地获取关于某个特定用户的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度学习与隐私问题&lt;/strong&gt;：深度学习模型通常依赖大量数据，这使得保护用户隐私变得尤为重要。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型与训练&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;噪声注入&lt;/strong&gt;：通过在模型的梯度更新中添加噪声，可以达到差分隐私的效果。噪声的大小与隐私预算（ε）相关，预算越小，隐私保护越强，但模型的性能可能会受到影响。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Accounting&lt;/strong&gt;：论文提出了一种计算隐私损失的框架，确保在每次训练步骤中监控和控制隐私损失。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算法设计&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DP-SGD（Differentially Private Stochastic Gradient Descent）&lt;/strong&gt;：论文提出了一种改进的随机梯度下降算法，结合了梯度裁剪和噪声注入，以实现差分隐私。这种方法确保在训练过程中，个别数据对模型的影响是被限制的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文通过实验展示了DP-SGD在图像分类任务上的有效性，同时比较了不同噪声级别对模型性能的影响，证明了在保持合理准确性的同时实现差分隐私是可行的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;应用与展望&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文强调了在各类机器学习应用中（如医疗、金融等）保护隐私的重要性，提出将差分隐私技术与深度学习结合是一个有效的解决方案，并鼓励未来的研究在这一领域的进一步探索。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;感觉偏向数学一点，全是公式和证明，好难看懂。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;差分隐私顾名思义就是用来防范差分攻击的
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;加入噪声，改变原来的概率分布
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;hr&gt;
&lt;h3 id=&#34;deep-reinforcement-learning-from-human-preferences&#34;&gt;Deep Reinforcement Learning from Human Preferences
&lt;/h3&gt;&lt;p&gt;《Deep Reinforcement Learning from Human Preferences》是由Christiano等人在2017年提出的一篇重要论文，探讨了如何利用人类偏好来训练强化学习模型。以下是其主要内容的总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;传统的强化学习方法通常依赖于奖励信号来指导学习，但设计有效的奖励函数在复杂任务中往往困难且耗时。&lt;/li&gt;
&lt;li&gt;人类可以提供更直观的偏好信息，这为训练强化学习代理提供了一种新途径。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;方法概述&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;人类偏好收集&lt;/strong&gt;：论文提出了一种方法，通过收集人类对不同行为的偏好来生成奖励信号。人类评估多个策略的表现，从中选择更好的策略，以此来建立奖励模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励模型&lt;/strong&gt;：利用收集到的人类偏好数据，训练一个深度神经网络来预测代理在给定状态下的奖励。这种奖励模型可以用来指导强化学习算法。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练过程&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;深度强化学习&lt;/strong&gt;：在获得奖励模型后，使用深度强化学习算法（如Proximal Policy Optimization，PPO）来训练代理。代理通过与环境交互，不断优化其策略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;迭代学习&lt;/strong&gt;：通过不断收集新的偏好数据并更新奖励模型，实现迭代学习，使得代理能够逐步提升性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文在多个复杂任务（如Atari游戏和模拟环境）中展示了该方法的有效性，代理能够通过学习人类的偏好，超越基于手动设计的奖励函数的表现。&lt;/li&gt;
&lt;li&gt;结果表明，利用人类偏好进行训练能够加速学习过程，并提升代理的最终性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;意义与展望&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该方法展示了人类偏好在强化学习中的潜力，提出了一种新的思路来解决奖励设计问题。&lt;/li&gt;
&lt;li&gt;论文强调了未来研究中结合人类反馈和偏好的重要性，特别是在涉及复杂决策和安全性问题的领域。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;附录&#34;&gt;附录
&lt;/h2&gt;&lt;h3 id=&#34;残差连接residual-connection和层归一化layer-normalization是什么&#34;&gt;残差连接（residual connection）和层归一化（layer normalization）是什么？
&lt;/h3&gt;&lt;p&gt;残差连接（Residual Connection）和 层归一化（Layer Normalization）是深度学习中常用的两种技术，主要用于提高神经网络的训练效率和性能。下面是它们的定义和作用：&lt;/p&gt;
&lt;h4 id=&#34;残差连接residual-connection&#34;&gt;残差连接（Residual Connection）
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：残差连接是一种将输入直接与输出相加的结构，通常在深度神经网络中使用。公式表示为：
&lt;/p&gt;
$$
\text{Output} = \text{Layer}(x) + x
$$&lt;p&gt;
其中 \(x\) 是输入，\(\text{Layer}(x)\) 是通过某个层（如卷积层或全连接层）处理后的输出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;缓解梯度消失问题&lt;/strong&gt;：在非常深的网络中，梯度可能会随着层数增加而消失，导致网络难以训练。残差连接提供了一条捷径，使得梯度可以更容易地通过网络传播。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加速收敛&lt;/strong&gt;：通过引入直接路径，残差连接有助于提高网络的收敛速度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提高模型性能&lt;/strong&gt;：它允许模型学习恒等映射，这使得训练更深的网络成为可能，提升了模型的表现。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;层归一化layer-normalization&#34;&gt;层归一化（Layer Normalization）
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：层归一化是一种对网络中每一层的激活进行归一化的技术，它通过计算每个样本的均值和标准差来调整激活值。层归一化的公式为：
&lt;/p&gt;
$$
\text{LayerNorm}(x) = \frac{x - \mu}{\sigma + \epsilon} \cdot \gamma + \beta
$$&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(x\) 是输入。&lt;/li&gt;
&lt;li&gt;\(\mu\) 是输入的均值。&lt;/li&gt;
&lt;li&gt;\(\sigma\) 是输入的标准差。&lt;/li&gt;
&lt;li&gt;\(\epsilon\) 是一个小常数，避免除以零。&lt;/li&gt;
&lt;li&gt;\(\gamma\) 和 \(\beta\) 是可学习的参数，用于缩放和平移归一化的结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;减少内部协变量偏移&lt;/strong&gt;：层归一化帮助减少模型训练过程中每层输入的变化，使得网络在训练时更加稳定。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提高训练速度&lt;/strong&gt;：通过标准化输入，层归一化可以使得训练过程更加平滑，从而加快收敛速度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;适用于变长序列&lt;/strong&gt;：与批量归一化（Batch Normalization）不同，层归一化可以直接应用于变长的输入序列，适合于RNN和Transformer等模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;残差连接&lt;/strong&gt;主要通过提供捷径来缓解深层网络中的梯度消失问题，并加速训练过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;层归一化&lt;/strong&gt;则通过标准化激活值来提高训练的稳定性和速度。这两者结合使用，能够显著提升深度学习模型的表现。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>第1周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/first-week/</link>
        <pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/first-week/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/first-week/banlan.jpg" alt="Featured image of post 第1周工作总结" /&gt;&lt;h1 id=&#34;工作总结&#34;&gt;&lt;strong&gt;工作总结&lt;/strong&gt;：
&lt;/h1&gt;&lt;h2 id=&#34;文献阅读&#34;&gt;文献阅读
&lt;/h2&gt;&lt;h3 id=&#34;综述类&#34;&gt;综述类：
&lt;/h3&gt;&lt;p&gt;2023年国内大模型发展综述与趋势研判_赵子忠&lt;/p&gt;
&lt;p&gt;AIGC大模型测评综述：使能技术、安全隐患和应对_许志伟&lt;/p&gt;
&lt;p&gt;AI大模型发展综述_张乾君&lt;/p&gt;
&lt;h3 id=&#34;深入&#34;&gt;深入：
&lt;/h3&gt;&lt;hr&gt;
&lt;p&gt;《A Neural Probabilistic Language Model》是Yoshua Bengio等人在2003年发表的一篇重要论文，提出了一种基于神经网络的语言模型方法。这篇论文的主要目标是通过神经网络来改进传统的语言模型。&lt;/p&gt;
&lt;p&gt;传统的语言模型的任务是估计一个词序列的概率：
P(w1,w2,…,wT)
在这种情况下，模型需要根据上下文词来预测当前词的概率。传统的做法一般是通过统计方法（如n元语法模型）来估计这些概率，但这类方法有一个很大的问题：它们无法处理高维数据，尤其是在词汇量非常大的情况下，计算复杂度极高，且泛化能力有限。&lt;/p&gt;
&lt;p&gt;Bengio等人提出了一种新的神经网络语言模型，目的是克服这个问题。这个模型的核心思想是使用一个多层感知器来学习词的分布式表示（即词向量），并基于这些词向量来估计概率。具体过程大致如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;词嵌入&lt;/strong&gt;：每个词通过一个嵌入矩阵被映射为一个连续向量，这种向量的维度比词汇表要小得多。这一步的目的是将离散的词映射到一个连续的低维空间，减少计算复杂度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;神经网络建模&lt;/strong&gt;：利用一个前馈神经网络（通常是多层感知器），根据前面的若干个词的词向量，来预测下一个词的条件概率。通过这个网络，模型可以学习到词与词之间的相似性和上下文关系。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练&lt;/strong&gt;：模型通过最大化训练数据的似然估计来进行训练。具体来说，模型会调整嵌入矩阵和网络的参数，使得预测的概率尽可能接近真实的词序列概率。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这篇论文的创新之处在于提出了“词向量”的概念，并展示了使用神经网络来处理语言建模问题的潜力。这为后来的深度学习在自然语言处理（NLP）中的应用奠定了基础。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;附录&#34;&gt;附录
&lt;/h1&gt;&lt;h2 id=&#34;什么是语言模型&#34;&gt;什么是语言模型
&lt;/h2&gt;&lt;p&gt;语言模型（Language Model, LM）是自然语言处理中的一个核心概念，其主要任务是对一个词序列的概率分布进行建模。简单来说，语言模型的目标是估计一段文本或句子出现的概率，或者根据前面的词来预测下一个词。&lt;/p&gt;
&lt;h3 id=&#34;语言模型的定义&#34;&gt;语言模型的定义
&lt;/h3&gt;&lt;p&gt;给定一个词序列
&lt;/p&gt;
$$
P( w_1, w_2, \dots, w_T )
$$&lt;p&gt;
，语言模型的目标是计算这个序列的联合概率：
&lt;/p&gt;
$$
P(w_1, w_2, \dots, w_T)
$$&lt;p&gt;
这通常可以分解为条件概率的乘积：
&lt;/p&gt;
$$
P(w_1, w_2, \dots, w_T) = P(w_1) \cdot P(w_2 | w_1) \cdot P(w_3 | w_1, w_2) \cdot \dots \cdot P(w_T | w_1, w_2, \dots, w_{T-1})
$$&lt;p&gt;
这意味着每个词的概率取决于它前面的词。一个好的语言模型能够通过学习语料库中的模式，准确地捕捉这些条件概率。&lt;/p&gt;
&lt;h3 id=&#34;语言模型的用途&#34;&gt;语言模型的用途
&lt;/h3&gt;&lt;p&gt;语言模型有广泛的应用场景，包括但不限于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;文本生成&lt;/strong&gt;：语言模型可以用来自动生成自然语言文本。例如，给定一部分句子，模型可以预测并生成合理的下一个词，从而逐步生成完整的句子或段落。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;机器翻译&lt;/strong&gt;：在机器翻译中，语言模型帮助翻译系统生成符合目标语言语法的翻译结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;语音识别&lt;/strong&gt;：语言模型能够帮助语音识别系统，在可能的识别结果中选择最合适的词序列。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;拼写纠正&lt;/strong&gt;：在文本输入或搜索引擎中，语言模型可以用于纠正拼写错误，提供更符合语言习惯的结果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;传统语言模型&#34;&gt;传统语言模型
&lt;/h3&gt;&lt;p&gt;在神经网络语言模型之前，最常用的语言模型是&lt;strong&gt;n元语法模型（n-gram Model）&lt;/strong&gt;。n元语法模型通过假设一个词的概率只与它前面的n-1个词有关来简化问题。举个例子，三元语法模型（trigram model）认为：
&lt;/p&gt;
$$
P(w_T | w_1, w_2, \dots, w_{T-1}) \approx P(w_T | w_{T-2}, w_{T-1})
$$&lt;p&gt;这种简化降低了计算复杂度，但也限制了模型的表达能力，因为它只能捕捉到短距离的上下文信息，无法充分考虑长距离的依赖关系。&lt;/p&gt;
&lt;h3 id=&#34;神经网络语言模型&#34;&gt;神经网络语言模型
&lt;/h3&gt;&lt;p&gt;传统语言模型的一个主要问题是词汇量增大时，模型的计算复杂度随之增加。为了解决这个问题，Yoshua Bengio等人在《A Neural Probabilistic Language Model》论文中提出了使用神经网络进行语言建模的想法。这种神经网络模型通过将词表示为&lt;strong&gt;词向量（word embeddings）&lt;/strong&gt;，并通过深度学习模型来捕捉更复杂的语境和词间关系，从而解决了传统语言模型的许多不足之处。&lt;/p&gt;
&lt;p&gt;总之，语言模型的核心任务就是对自然语言的概率进行建模，并通过学习语言中的统计模式来预测文本。&lt;/p&gt;
&lt;h2 id=&#34;什么是纬度灾难&#34;&gt;什么是纬度灾难？
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;维度灾难&lt;/strong&gt;（Curse of Dimensionality）是指在处理高维数据时，随着数据维度增加，问题的复杂性呈指数级增长，导致很多传统方法在高维空间中变得低效或不可行的现象。&lt;/p&gt;
&lt;p&gt;其影响包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;数据稀疏性&lt;/strong&gt;：在高维空间中，数据点变得非常稀疏，因为数据的总量相对于空间的体积变得极其小，导致模型难以从有限的数据中捕捉到模式和关系。简单来说，随着维度的增加，需要的样本量也随之爆炸性增长，否则模型可能会过拟合或无法泛化。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;计算复杂度增加&lt;/strong&gt;：在高维空间中，计算距离、相似性等操作的复杂度大大增加，使得很多算法在高维情况下效率低下。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型的泛化能力下降&lt;/strong&gt;：由于数据点过于稀疏，模型很难学习到有用的特征，从而可能导致泛化能力下降。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;具体来说，维度灾难对机器学习、统计学、最优化等领域带来了极大的挑战，尤其是当模型需要处理很多特征或变量时。为应对这个问题，研究人员提出了许多方法，如&lt;strong&gt;降维技术&lt;/strong&gt;（如主成分分析PCA）和&lt;strong&gt;分布式表示&lt;/strong&gt;（如词嵌入），以减少有效维度或通过分布式方法来表示复杂的数据结构。&lt;/p&gt;
&lt;h2 id=&#34;有概率为什么要引入似然的概念他们有什么不同&#34;&gt;有概率为什么要引入似然的概念，他们有什么不同？
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;概率&lt;/strong&gt;和&lt;strong&gt;似然&lt;/strong&gt;虽然看起来相似，但在统计学中它们有不同的定义和用途：&lt;/p&gt;
&lt;h3 id=&#34;概率probability&#34;&gt;&lt;strong&gt;概率（Probability）&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;概率是用来描述随机事件发生的可能性。在给定模型参数的情况下，概率告诉我们某个事件或观测结果发生的机会有多大。&lt;/p&gt;
&lt;p&gt;假设我们有一个随机变量 \( X \)，它取某个值 \( x \) 的概率可以表示为 \( P(X = x|\theta) \)，其中 \( \theta \) 是模型的参数。例如，如果我们有一个硬币掷出的模型，参数 \( \theta \) 可以是硬币正面朝上的概率（比如 \( \theta = 0.5 \)）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：给定模型的参数 \( \theta \)，事件 \( X = x \) 发生的概率是多少？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;似然likelihood&#34;&gt;&lt;strong&gt;似然（Likelihood）&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;似然是从&lt;strong&gt;反方向&lt;/strong&gt;来考虑的。它描述的是&lt;strong&gt;在已知观测数据的前提下，模型参数的可能性&lt;/strong&gt;。具体来说，似然是给定观测数据后，模型参数如何使观测数据变得最可能的一个度量。&lt;/p&gt;
&lt;p&gt;假设我们已经观察到数据 \( X = x \)，现在我们想知道，在不同的模型参数 \( \theta \) 下，这个数据出现的可能性有多大。似然可以表示为 \( L(\theta|X = x) \)，或者更直观地写作 \( P(X = x|\theta) \)，虽然数学表达式相同，但意义不同——在这里我们关注的是参数 \( \theta \) 的值使观测到的数据最可能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心问题&lt;/strong&gt;：给定观测到的数据 \( X = x \)，模型参数 \( \theta \) 有多大可能是正确的？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;概率和似然的区别&#34;&gt;概率和似然的区别
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;概率&lt;/strong&gt;：我们知道参数 \( \theta \)，希望知道某个事件 \( X \) 发生的概率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题：&lt;strong&gt;给定参数，事件的概率是多少？&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;似然&lt;/strong&gt;：我们知道事件（观测数据），希望推断出最有可能的参数 \( \theta \)。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题：&lt;strong&gt;给定观测数据，哪个参数最可能是正确的？&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;例子&#34;&gt;例子
&lt;/h3&gt;&lt;h4 id=&#34;抛硬币的例子&#34;&gt;抛硬币的例子
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;概率&lt;/strong&gt;：假设你有一枚硬币，已知它是公平的，即 \( \theta = 0.5 \)，那么掷硬币得到正面的概率是 \( P(\text{正面}) = 0.5 \)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;似然&lt;/strong&gt;：现在，你掷硬币10次，得到8次正面和2次反面。你不知道硬币的公平性（即不知道 \( \theta \) 的值）。你想根据这次实验结果估计硬币正面朝上的概率 \( \theta \)。此时，你需要用&lt;strong&gt;似然&lt;/strong&gt;来衡量在不同 \( \theta \) 值下，观测结果（8次正面，2次反面）出现的可能性有多大，从而找到最有可能的 \( \theta \)。&lt;/p&gt;
&lt;p&gt;似然函数 \( L(\theta|X) \) 可能在某个 \( \theta \) 值处达到最大值，这个 \( \theta \) 就是最能解释观测数据的参数值。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;概率&lt;/strong&gt;用于给定模型参数时预测事件的发生可能性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;似然&lt;/strong&gt;用于在已知观测数据时，推断哪个参数最能解释这些数据。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
