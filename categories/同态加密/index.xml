<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>同态加密 on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/categories/%E5%90%8C%E6%80%81%E5%8A%A0%E5%AF%86/</link>
        <description>Recent content in 同态加密 on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Thu, 02 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/categories/%E5%90%8C%E6%80%81%E5%8A%A0%E5%AF%86/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>MOFHEI Model Optimizing Framework for Fast  and Efficient Homomorphically Encrypted Neural  Network Inference</title>
        <link>https://JiangZhiyu-1024.github.io/p/mofhei-model-optimizing-framework-for-fast-and-efficient-homomorphically-encrypted-neural-network-inference/</link>
        <pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/mofhei-model-optimizing-framework-for-fast-and-efficient-homomorphically-encrypted-neural-network-inference/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/mofhei-model-optimizing-framework-for-fast-and-efficient-homomorphically-encrypted-neural-network-inference/sunset-7133867_1280.jpg" alt="Featured image of post MOFHEI Model Optimizing Framework for Fast  and Efficient Homomorphically Encrypted Neural  Network Inference" /&gt;&lt;h1 id=&#34;mofhei-model-optimizing-framework-for-fast--and-efficient-homomorphically-encrypted-neural--network-inference&#34;&gt;MOFHEI Model Optimizing Framework for Fast  and Efficient Homomorphically Encrypted Neural  Network Inference
&lt;/h1&gt;&lt;p&gt;MOFHEI：用于快速高效同态加密神经网络推理的模型优化框架&lt;/p&gt;
&lt;p&gt;摘要：由于机器学习（ML）在广泛领域中的广泛应用以及数据隐私的重要性，隐私保护机器学习（PPML）解决方案近年来受到广泛关注。其中一种方法基于同态加密（HE），它使我们能够在加密数据上执行机器学习任务。然而，即使使用最先进的HE方案，与明文操作相比，HE操作仍然显著更慢，并且需要大量内存。因此，我们提出了MOFHEI，这是一种优化模型的框架，用于实现基于HE的神经网络推理（称为私有推理，PI）的快速高效化。首先，我们提出了一种基于学习的方法，能够自动将预训练的ML模型转换为兼容HE操作的版本，称为HE友好版本。然后，我们的迭代块剪枝方法根据数据打包方法，以可配置的块形状剪枝模型参数。这使我们能够减少大量高成本的HE操作，从而降低延迟和内存消耗，同时保持模型性能。我们通过对不同模型和各种数据集的广泛实验评估了我们的框架。我们的方法在LeNet上实现了高达98%的剪枝率，减少了高达93%的PI所需HE操作，将延迟和内存需求分别降低了9.63倍和4.04倍，同时几乎没有精度损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关键词&lt;/strong&gt;：隐私保护机器学习，同态加密，私有推理，模型优化，块剪枝&lt;/p&gt;
&lt;h2 id=&#34;i-引言&#34;&gt;&lt;strong&gt;I. 引言&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;由于机器学习（ML）解决方案的卓越性能，它们已被广泛应用于众多领域，以利用其从海量数据中提取复杂模式和洞见的能力 [47]。然而，训练高效的ML模型需要大量的数据集、可观的计算能力以及ML专业知识。为了解决这些问题，云服务提供商提供了机器学习即服务（MLaaS），通过提供精心训练的模型和计算能力，减轻了用户从零开始训练复杂模型的负担。用户只需将数据发送到云服务器，即可享受这种服务 [52]。&lt;/p&gt;
&lt;p&gt;然而，将医疗或财务记录等敏感数据外包给云服务器会引发新的数据隐私问题，并且可能不符合《通用数据保护条例》（GDPR）[1] 和《健康保险可携性与责任法案》（HIPAA）[42] 等法规的要求。隐私保护机器学习（PPML）技术通过采用差分隐私 [54]、联邦学习 [40]、安全多方计算（SMC）[55] 或同态加密（HE）[23] 等技术来解决这一隐私问题。其中，HE允许不可信的第三方直接对加密数据执行某些操作，从而保护数据的机密性。&lt;/p&gt;
&lt;p&gt;与PPML文献中的许多工作 [20]、[28]、[29]、[32]、[3] 一样，我们专注于基于HE的ML模型推理，这些模型是在明文数据上训练的，我们将其称为私有推理（PI）。许多PI解决方案依赖于HE和/或安全多方计算（SMC）协议，如混淆电路（GC）[9]、不可知传输（OT）[13] 或秘密共享（SS）[49]。需要客户端参与的方案称为交互式方案 [29]、[32]，而无需客户端协助的则称为非交互式方案，这类方案通常仅使用HE [39]、[4]、[15]、[3]、[28]。&lt;/p&gt;
&lt;p&gt;交互式方案需要客户端参与部分计算，例如HE无法评估的非多项式函数。这些方案能够在客户端的帮助下执行更广泛的功能，并达到与明文ML模型相当的精度，但它们要求客户端始终在线，并在计算过程中与云端进行多轮通信，导致高通信成本和长运行时间 [29]。此外，这些方案可能容易受到模型提取攻击 [37]。为此，我们专注于非交互式PI，其中所有计算均在加密状态下完成，无需客户端参与；然而，我们的方法同样适用于交互式解决方案。&lt;/p&gt;
&lt;p&gt;由于非交互式解决方案受限于HE支持的操作（通常为加法和乘法），它们只能评估多项式函数。因此，ML模型中涉及非多项式计算的层（如激活层或池化层）需要用可以通过HE操作评估的近似多项式函数代替。在这一模型转换之后，我们称其为HE友好模型。MOFHEI通过我们基于学习的方法自动执行这一步骤，并对给定的训练模型进行相应优化，以保持其性能。&lt;/p&gt;
&lt;p&gt;尽管在过去十年中同态加密（HE）方案在理论和实现方面都有所进步，HE操作依然比其明文操作慢数个数量级，并且需要大量的内存 [5]。随着模型深度和规模的增长，这种明文与HE计算之间的性能差距显著扩大。为了缩小这一差距，我们需要对已训练的模型进行优化。在明文领域中，为了最小化计算开销，一种实用的解决方案是减少模型的冗余参数 [26], [10]。机器学习模型可能存在过参数化，其中包含一些并不直接影响模型性能的参数 [41], [38]。多个研究 [26], [22], [31], [56] 表明，可以通过“剪枝”这些冗余参数实现更高效的计算，而不会显著降低模型精度。&lt;/p&gt;
&lt;p&gt;在机器学习领域，模型剪枝指的是基于特定指标选择一组参数，例如将值小于某个阈值的参数设置为零，然后对剪枝后的模型进行微调以恢复精度损失 [27]。然而，最近的研究 [4], [15] 表明，将传统剪枝方法应用于加密领域几乎无法有效减少HE计算的开销。其根本原因在于这些方法未考虑HE领域的操作方式。&lt;/p&gt;
&lt;p&gt;大多数HE方案（如Cheon-Kim-Kim-Song (CKKS) [17] 和 Brakerski/Fan-Vercauteren (BFV) [50]）支持单指令多数据（SIMD）[51]技术，通过将一组值打包到一个密文/明文的“槽”中来降低HE计算成本。当进行某项操作时，该操作会同时作用于所有槽中的值，而不会增加额外开销。然而，现有的剪枝方法并未考虑SIMD打包的要求，通常会在权重矩阵中任意位置剪枝模型参数。这种随机剪枝无法显著减少高延迟和高内存需求的HE操作数量。原因在于，只要非零元素仍存在于HE操作的打包操作数中，相应的HE操作就无法被跳过，因此无法减少HE操作的开销 [15], [4]。&lt;/p&gt;
&lt;p&gt;例如，对于一个包含加密输入数据的密文和一个向量化权重值的明文之间的HE乘法操作，只有当明文包中的所有元素均为零（即被剪枝）时，该操作才能在私有推理（PI）中被跳过。为了解决这一问题，我们提出了一种与数据打包方法对齐的迭代块剪枝方法。该方法显著减少了HE操作的数量，从而实现了更快、更高效的PI。需要注意的是，虽然我们使用了批量打包方法 [20]，但我们提出的剪枝方法具有通用性，也可应用于其他数据打包技术。&lt;/p&gt;
&lt;p&gt;在本文中，我们的贡献如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们提出了一种基于学习的方法，将给定的预训练模型转换为HE友好版本。该方法通过迭代方式：（1）学习替代激活层的近似多项式函数的系数，(2) 将最大池化层替换为HE兼容的平均池化层，并微调模型以保持其性能。&lt;/li&gt;
&lt;li&gt;我们引入了一种迭代块剪枝方法，该方法考虑加密领域中的数据打包方式，并据此对模型进行剪枝，显著减少所需的HE操作数量，从而实现更快、更经济的PI。&lt;/li&gt;
&lt;li&gt;我们通过对各种模型架构（如LeNet、自编码器）在MNIST [35]、CIFAR-10 [33]、Chest X-Ray [53] 和电网稳定性模拟（EGSS）[8] 数据集上的综合实验，评估了我们剪枝技术在PI中的效率。实验结果表明，即使在高剪枝率下，我们的方法不仅减少了PI的延迟和内存占用，还能在几乎无精度损失或仅有轻微精度损失的情况下完成。&lt;/li&gt;
&lt;li&gt;我们提供了MOFHEI，一个模型优化框架，能够自动将模型转换为HE友好版本，并对其执行迭代剪枝方法，从而在PI中具有更高的实际应用性。源代码可在 &lt;a class=&#34;link&#34; href=&#34;https://github.com/inspire-lab/MOFHEI&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/inspire-lab/MOFHEI&lt;/a&gt; 获取。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本文其余部分组织如下：第二部分讨论了高效PI的最新实践。第三部分介绍了HE的相关背景信息、HE中的打包方法及我们的威胁模型。第四部分详细解释了我们提出的框架。第五部分描述了我们的实验、数据集、超参数、安全措施及实验中使用的资源。第六部分评估了实验结果，与最先进的方案进行了比较，并讨论了优缺点以及未来方向。第七部分总结全文。&lt;/p&gt;
&lt;h2 id=&#34;ii-相关研究&#34;&gt;&lt;strong&gt;II. 相关研究&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;在隐私保护机器学习（PPML）领域中，为了使私有推理（PI）更加高效和实用，研究者们采用了多种技术。Downlin等人 [20] 提出了CryptoNets，这是早期使非交互式PI实用化的研究之一。CryptoNets通过用缩放平均池化替代最大池化，并用平方函数替代激活函数，使模型具有HE友好性，同时通过合并连续的线性层来减少网络深度。他们还提出了批量打包方法（batch packing），该方法被本研究所采用（详见III-B部分）。&lt;/p&gt;
&lt;p&gt;Hesamifard等人 [28] 开发了一个类似于CryptoNets的系统CryptoDL，通过使用Chebyshev多项式 [46] 改进激活函数的低阶多项式近似，并采用BGV完全同态加密（FHE）[11]方案及常数缩放因子来解决中间值过大的问题。&lt;/p&gt;
&lt;p&gt;另一条研究路径是神经架构搜索（NAS），其目标是寻找优化的PPML架构，综合考虑加密操作，减少复杂函数（如ReLU）的使用。尽管我们的研究方法与NAS研究方向互为补充，两者均试图减少模型计算所需的加密操作数。Ghodsi等人 [24] 提出了CryptoNAS，旨在为PI任务定义一个ReLU预算，并在给定预算内识别最有效的网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;量化&lt;/strong&gt;是另一个用于提高PI模型计算效率的技术。与通过删除权重和连接来优化模型的剪枝不同，量化对权重施加特定值限制。Jacob等人 [30] 提出了一种量化方法，用于设备端推理，该方法使用仅基于整数的计算，比浮点推理更高效。他们还开发了一种在量化后保持模型精度的方法。在Faster CryptoNets中，Chou等人 [19] 通过采用 [25] 提出的剪枝个别权重的方法，最小化网络中的乘法数量，并对剩余权重进行量化以提高权重多项式编码的稀疏性。通过将权重值限制为2的幂，作者能够采用更高效的乘法算法 [6]。Podschwadt等人 [44] 提出了一种动态加载和缓存机制，用于卷积神经网络（CNN）中重复结构的值，从而降低内存成本。&lt;/p&gt;
&lt;p&gt;最近一些研究提出了结合HE数据打包的剪枝方法，以提升PI效率。Cai等人 [15] 提出的Hunter通过剪枝以明文向量形式打包的对角权重值，跳过点积函数中计算中间乘法结果所需的排列操作。他们采用了GAZELLE [32]（一种基于SMC的PI解决方案）的打包方法，并将激活函数的计算委托给客户端。然而，我们的工作不依赖客户端，而是使用我们提出的基于学习的方法对激活层进行近似。因此，我们可以在服务器端加密下评估激活函数。此外，Hunter在MNIST数据集上的LeNet剪枝耗时超过1小时，而我们的方法即使在90%以上的剪枝率下也仅需几分钟。&lt;/p&gt;
&lt;p&gt;Ran等人 [45] 提出了一种CNN推理框架，该框架引入了HE组卷积以及组交错编码，通过优化密文内的通道位置，并迭代地剪枝“子块”对应的权重集合，减少了耗时的HE旋转操作的数量，从而降低总体延迟。然而，他们的分组方法、相关编码方式以及卷积组内的特定稀疏模式仅适用于多通道卷积层，且未探讨其他模型层的适用性。我们的剪枝方法更加通用，并且在全连接层和卷积层中均表现良好，无需依赖特定编码方式。&lt;/p&gt;
&lt;p&gt;Aharoni等人 [4] 提出了HE-PEx，按tile-packing方法 [2] 对模型进行剪枝。首先，随机剪枝模型权重，然后通过排列操作将剩余权重重新分布到更少的包中，从而生成更多的全零包，以便剪枝，减少所需的HE操作数量。接着，通过扩展步骤，扩展剩余包中的权重值，以最大限度地利用这些权重来保持模型性能。由于剪枝过程中网络结构的变化，其方法需要在输入和输出数据上执行两次转换操作，这增加了额外的计算成本。然而，我们的方法不需要在输入和输出数据上执行这两个额外的转换操作，也不需要剪枝后的排列和扩展步骤。&lt;/p&gt;
&lt;h2 id=&#34;iii-背景&#34;&gt;III. 背景
&lt;/h2&gt;&lt;p&gt;本节介绍了同态加密（Homomorphic Encryption, HE）、HE 中的打包方法，特别是批量打包（batch packing），以及 HE 方案，并以假定的威胁模型作为总结。&lt;/p&gt;
&lt;h3 id=&#34;a-同态加密&#34;&gt;A. 同态加密
&lt;/h3&gt;&lt;p&gt;同态加密（HE）是一种特殊的加密方式，它允许在加密数据上进行特定的计算。在计算过程中，数据始终保持加密状态，计算结果也是加密的。解密后，基于加密数据计算得到的结果，与在明文数据上进行计算的结果相同。这使 HE 非常适合将计算任务外包给不可信的第三方。&lt;/p&gt;
&lt;p&gt;根据特性（如消息空间、支持的操作和可评估的函数），HE 方案可以分为不同的类别 [7]。功能最强大的 HE 方案是完全同态加密（FHE, Fully Homomorphic Encryption）方案。FHE 方案通过引导（bootstrapping）支持无限制的加法和乘法操作 [23]，但在可评估的函数类型方面仍然受到限制。例如，无法用多项式表达的函数（如比较操作）必须通过近似实现，且通常计算成本较高。&lt;/p&gt;
&lt;p&gt;此外，加密会引入噪声以隐藏数据，这些噪声在解密时会被移除。然而，在加密数据上执行操作会增加噪声水平，当噪声超过某个阈值时，正确的解密将变得不可能。由于乘法引起的噪声增长远高于加法，乘法的次数成为限制因素。在密文上连续可评估的乘法次数称为&lt;strong&gt;乘法深度&lt;/strong&gt;（multiplicative depth）。因此，乘法深度也限制了可评估的函数。&lt;/p&gt;
&lt;p&gt;虽然引导（bootstrapping）可以通过刷新噪声水平来解决此问题，但其计算成本较高。使用分层完全同态加密（leveled FHE）方案可以避免这一问题。这些方案的乘法深度可以通过加密参数进行配置。当函数的乘法深度在计算前已知时，分层 FHE 方案非常有用。&lt;/p&gt;
&lt;h3 id=&#34;b-同态加密中的数据打包&#34;&gt;B. 同态加密中的数据打包
&lt;/h3&gt;&lt;p&gt;大多数同态加密（HE）方案提供单指令多数据（SIMD, Single Instruction Multiple Data）[51] 操作，以缓解 HE 计算对资源的高需求。通过 SIMD，可以将多个值打包到一个密文或明文中。每个打包中的值的数量取决于加密参数，通常在 2¹⁰ 到 2¹⁶ 之间。我们可以将 SIMD 明文/密文视为值的向量。所有 SIMD 明文/密文之间的操作都会逐槽（slot-wise）应用于底层的值，且不会产生额外的开销。这种技术可以优化某些函数的执行。例如，Brutzkus 等人 [14] 和 Lee 等人 [36] 提出了用于高效卷积的 SIMD 打包技术。Aharoni 等人 [2] 提出了一种名为“tile tensor”的数据结构，用于以固定大小的“tile”为单位，在任意形状的张量中进行 SIMD 打包，并对其进行 HE 密文/明文编码，同时展示了基于该方法的二维卷积算法。&lt;/p&gt;
&lt;p&gt;Dowlin 等人 [20] 提出了另一种技术——批处理（batching）或批量打包（batch packing）。在批量打包中，数据被分组为多个实例的批次，并将每个实例的相同特征打包到一个密文中。这意味着密文的数量等于特征的数量。假设有一个包含 n 个实例的批次数据 $X_1, X_2, \dots, X_n$，每个实例 $X_i$; $\forall i \in [1, n]$ 包含 m 个特征 $X_i = [x_{i,1}, x_{i,2}, \dots, x_{i,m}]$。那么，批量打包会生成 m 个密文 $c_1, c_2, \dots, c_m$，其中每个 $C_i = [x_{1,i}, x_{2,i}, \dots, x_{n,i}]$; $\forall i \in [1, m]$。&lt;/p&gt;
&lt;p&gt;尽管这种技术需要更多的密文（因此占用更多内存），并增加了延迟，但它提供了更高的吞吐量。此外，它避免了代价高昂的旋转操作。需要注意的是，我们必须将明文值编码为明文格式。这种编码是必要的，因为只能在密文与密文或密文与明文之间执行操作。在我们的工作中，我们采用了这种打包方法。&lt;/p&gt;
&lt;h3 id=&#34;c-同态加密方案&#34;&gt;C. 同态加密方案
&lt;/h3&gt;&lt;p&gt;大多数现代同态加密（HE）方案基于环学习带错误（Ring Learning With Errors, RLWE）问题的难解性。尽管 BGV [12] 和 BFV [21] 仅支持整数运算，TFHE [18] 仅支持单个位和二进制门运算，但我们在此使用的 CKKS 或 HEAAN [17] 方案支持对实数的近似计算。由于计算是近似的，明文和加密数据的结果会有所不同。然而，这种近似首先出现在最低有效位，并且可以通过加密参数进行控制。可以接受的近似误差大小取决于具体应用场景。&lt;/p&gt;
&lt;h3 id=&#34;d-威胁模型&#34;&gt;D. 威胁模型
&lt;/h3&gt;&lt;p&gt;我们的威胁模型包含三个实体：模型所有者 (O)、云服务器 (S) 和客户端 (C)。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;O&lt;/strong&gt; 拥有训练好的模型，即模型的结构和所有训练参数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;S&lt;/strong&gt; 使用加密数据对训练模型执行推理计算（PI）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;C&lt;/strong&gt; 拥有私有数据，将其加密后发送到云服务器以进行推理计算。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;O 可以以加密或明文的形式将模型发送给 S；如果以加密形式发送，S 不应了解模型的任何信息，除了其架构。C 也不应了解模型的任何信息。在我们的场景中，S 知道模型的参数。我们假设 S 和 C 都是“诚实但好奇”的实体，即：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;C&lt;/strong&gt; 试图通过推理结果提取模型参数；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;S&lt;/strong&gt; 按照 C 的请求计算函数，不偏离预定行为，但也对 C 的机密数据感兴趣。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;iv-提出的框架&#34;&gt;IV. 提出的框架
&lt;/h2&gt;&lt;p&gt;我们假设“原始模型”是一个使用明文数据训练的模型，可能包含以下层：二维卷积（有无填充均可）、二维池化（最大池化、平均池化或最小池化——无填充）、批量归一化、全连接层、激活层和/或 dropout 层。
我们的框架包含两个步骤，用于创建与原始模型具有可比精度的 HE 友好版本（步骤 1 和步骤 2）。最后，在步骤 3 中，我们在该版本的模型上执行推理计算（PI）。以下是对这些步骤的详细说明：&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;a-第一步基于学习的方法将原始预训练模型转换为-he-友好模型&#34;&gt;A. 第一步：基于学习的方法将原始预训练模型转换为 HE 友好模型
&lt;/h3&gt;&lt;p&gt;在这一步中，我们将所有最大池化层逐一转换为与其对应的平均池化层，保持相同的窗口大小和步幅，从最新的池化层（即最靠近输出的）开始。修改较后层的准确性衰减比修改较前层的更容易恢复。因为修改较后层产生的误差会传递到更少的后续层，统计上，它在输出层中产生的传播误差会比修改较前层产生的传播误差低。&lt;/p&gt;
&lt;p&gt;每转换一个池化层，我们仅对后续层进行少量轮次的训练。然后，我们以相对较小的学习率对整个模型进行微调。接下来，与池化层类似，我们转换激活层。从最新的激活层开始，我们将其逐一替换为可训练的多项式（具有预定义的阶数）或不可训练的平方函数。在多项式的情况下，为了获得激活函数的近似多项式，我们首先将多项式系数作为模型的可训练参数，并进行少量轮次的训练，初始权重较小，学习率较低，以避免产生较大的损失值。随后，我们以小学习率对整个模型进行微调。而在平方函数的情况下，仅需执行微调步骤即可。&lt;/p&gt;
&lt;p&gt;需要注意的是，使用相对较小的学习率进行微调是为了避免过拟合。
第一步的输出是一个 HE 友好模型，且其精度$A_{\text{hef}}$ 与原始模型精度 $A_{\text{org}}$ 近似相等。&lt;/p&gt;
&lt;h3 id=&#34;b-第二步迭代块剪枝&#34;&gt;B. 第二步：迭代块剪枝
&lt;/h3&gt;&lt;p&gt;如我们在第一节讨论的，针对 HE 的有效剪枝需要考虑数据的打包方式。我们需要将同一个数据包中的所有值置零，从而跳过该包的计算。这里，我们提出了一种可配置的迭代块剪枝方法，该方法能够以任意块形状对模型权重进行剪枝，以匹配权重值在明文/密文中打包的方式。&lt;/p&gt;
&lt;p&gt;Zhu 等人 [56] 提出了一种过训练剪枝方法，通过使用二值掩码逐渐将低幅值的权重置零，直到达到预定义的目标稀疏度。我们扩展了该方法，定义了一个边界约束，将权重矩阵按照预定义形状（高度和宽度）的块进行划分。如有必要，我们对权重矩阵进行零填充。为了剪枝每一层，我们不是直接使用与权重矩阵大小和形状一致的二值掩码，而是构建与权重矩阵上的块大小一致的二值块掩码。二值块掩码决定了哪些权重块参与前向传播。&lt;/p&gt;
&lt;p&gt;我们定义了一个剪枝计划，从初始稀疏度 sis_i（通常为 0）逐步达到最终稀疏度 sfs_f，整个过程分为 nn 个剪枝步骤，从训练步骤 t0t_0 开始，并以剪枝频率 Δt\Delta t 进行。在每次剪枝步骤中，我们根据每个块权重绝对值的平均值对块进行排序，并将最小的块掩盖为零，直到达到该剪枝步骤所需的稀疏度水平。一旦模型达到目标稀疏度 sfs_f，块掩码将不再更新。&lt;/p&gt;
&lt;p&gt;在反向传播中，前向传播中被掩盖的块内权重会从可训练参数列表中排除，不会被更新。在每轮块掩码更新（剪枝步骤）之间的训练步骤中，模型会通过训练恢复因剪枝引起的精度损失。算法 1 展示了我们提出的迭代块剪枝方法的伪代码。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/mofhei-model-optimizing-framework-for-fast-and-efficient-homomorphically-encrypted-neural-network-inference/image-20250102164119279.png&#34;
	width=&#34;789&#34;
	height=&#34;407&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/mofhei-model-optimizing-framework-for-fast-and-efficient-homomorphically-encrypted-neural-network-inference/image-20250102164119279_hu4490656553328759874.png 480w, https://JiangZhiyu-1024.github.io/p/mofhei-model-optimizing-framework-for-fast-and-efficient-homomorphically-encrypted-neural-network-inference/image-20250102164119279_hu11819874384709406149.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250102164119279&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;193&#34;
		data-flex-basis=&#34;465px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;我们首先展示如何对全连接层进行剪枝，然后解释如何通过一种转换过程将此方法应用于卷积层。由于本研究中使用了批量打包（参见 III-B），对于输出层以外的每个全连接层，我们将块形状选择为 M×1，其中 M 是相应权重矩阵的行数；这相当于对权重矩阵的列进行剪枝。剪去权重矩阵的一整列可以跳过 MM 次同态加法和 M-1 次同态乘法。&lt;/p&gt;
&lt;p&gt;需要强调的是，我们的剪枝方法并不限于按列剪枝，还可以使用任意块形状对权重矩阵进行剪枝。用户可以将我们方法的剪枝块形状与其他打包方法使用的数据编码格式相匹配，以高效地剪枝模型。一旦模型被剪枝，每个可剪枝层都会根据预定义的稀疏度包含一组全为零的权重列。为了简化 HE 侧的计算，我们从权重矩阵中移除这些全为零的列。在全连接层中，这相当于从层中移除一些神经元。&lt;/p&gt;
&lt;p&gt;同样，剪去的列（例如移除偏置）引起的任何精度下降，可以通过对剩余参数的微调来恢复。需要注意的是，从全连接层中移除神经元会影响其输出特征图的形状。为了保持维度一致，下一层权重矩阵中对应于被剪枝层全为零列的所有行也需要被移除（参见图 1b）。这也解释了为什么剪枝后的模型稀疏度显著高于每个可剪枝层的预定义稀疏度。&lt;/p&gt;
&lt;p&gt;图 1 以示意图的形式展示了我们针对全连接层的剪枝方法（按列剪枝）的具体步骤。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/mofhei-model-optimizing-framework-for-fast-and-efficient-homomorphically-encrypted-neural-network-inference/image-20250102164419543.png&#34;
	width=&#34;1603&#34;
	height=&#34;650&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/mofhei-model-optimizing-framework-for-fast-and-efficient-homomorphically-encrypted-neural-network-inference/image-20250102164419543_hu15081137977654305192.png 480w, https://JiangZhiyu-1024.github.io/p/mofhei-model-optimizing-framework-for-fast-and-efficient-homomorphically-encrypted-neural-network-inference/image-20250102164419543_hu3670434003082504870.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250102164419543&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;246&#34;
		data-flex-basis=&#34;591px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;为了使卷积层受益于我们的迭代块剪枝方法，我们通过一种转换技术将二维卷积层转换为等效的全连接层，该操作在本文中称为 &lt;strong&gt;Conv-Dense 操作&lt;/strong&gt;。以下是转换的具体说明：&lt;/p&gt;
&lt;p&gt;假设一个二维卷积层包含 F 个滤波器，其权重矩阵 W 的大小为 $I \times J \times K$，偏置向量 B 为 F 维；该卷积层将一个 K-通道特征图 X 映射到一个 $U \times V \times F$ 的特征图 Y。对于每一个步幅，每个大小为 $ \times J \times K$ 的滤波器与 X 中相同大小的体积块逐元素相乘，生成一个新的体积块；然后将新体积块中的所有值与滤波器的偏置一起相加，得到 Y 中对应的值。&lt;/p&gt;
&lt;p&gt;由于乘法是逐元素进行的，体积块和滤波器可以分别被展平为行向量和列向量，形状为 $1 \times M = I \times J \times K$ 和 $M \times 1$。随后，可以通过点积将它们相乘并加上偏置。这是将二维卷积转换为等效全连接表示的主要思路。&lt;/p&gt;
&lt;p&gt;根据每个方向上的步幅，可以从 X 中提取所有体积块，并广播到大小为 $N = U \times V$ 行、M 列的矩阵 $\bar{X}$，其中每一行代表 X 中对应不同步幅的体积块。同样，可以将权重矩阵 W 中的所有滤波器展平为大小为 $M \times F $的矩阵 $\bar{W}$，其中每一列对应 W 中的一个滤波器。&lt;/p&gt;
&lt;p&gt;二维特征图 Y 的矩阵表示为 $N \times F $的矩阵 $\bar{Y} = \bar{X} \otimes \bar{W} + B$（⊗ 表示点积），这与全连接层中的计算等效；随后可以轻松地将 Yˉ 转换回 Y。通过这种转换技术，我们可以对卷积层应用我们的迭代块剪枝方法。剪除等效全连接表示中 Wˉ 的列相当于剪除转换后卷积层的滤波器。因此，移除 Wˉ 中被剪枝的列实际上是从卷积层中去掉对应的滤波器。&lt;/p&gt;
&lt;p&gt;最后，我们通过再次进行微调来补偿由于滤波器移除导致的精度损失。图 2 展示了当F = 3，I = 2，J = 2，K = 4，U = 2，V = 2，M=2×2×4=16，N=2×2=4 时 Conv-Dense 操作的一个示例。&lt;/p&gt;
&lt;p&gt;第二步的最终输出是一个 HE 友好的剪枝模型，其精度 $A_{\text{prn}} \approx A_{\text{org}}$。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/image-20250102164825284.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250102164825284&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;c-第三步私有推理private-inference-pi&#34;&gt;C. 第三步：私有推理（Private Inference, PI）
&lt;/h3&gt;&lt;p&gt;我们将第二步中生成的 HE 友好型剪枝模型的权重和层配置提取出来，用于执行私有推理（PI）。我们使用 SEAL [48] 提供的 CKKS 基元实现 PI 的算法。由于使用了批量打包（参见 III-B），我们可以将所有操作视为在单个实例上执行。例如，考虑一个具有 128 个单元和 256 个输入的全连接层。该层的权重矩阵 W 尺寸为 128×256。输入批次 X 包含 b 个实例，每个实例有 256 个特征，组成一个 b×256的矩阵。&lt;/p&gt;
&lt;p&gt;这一层的输出通过$Y = X \otimes W^T$ 计算得到（此处为简化，忽略偏置），其中 Y 尺寸为 b×128。使用批量打包时，输入批次 XX 被打包为 256 个密文。这 256 个密文可以视为一个向量。尽管计算方式未变，输入和输出现在以向量形式表示而非矩阵形式。这种方法对沿批次轴进行计算的所有神经网络层都适用。&lt;/p&gt;
&lt;p&gt;为了使这种计算正常运行，我们需要将权重矩阵中的每个值编码为单个明文。在编码时，从权重矩阵中取出一个值，将其重复 b 次后转为一个明文对象。这种方式会生成大量的明文对象。为了加速计算，我们可以通过多任务并行化点积计算。&lt;/p&gt;
&lt;p&gt;例如，如果每个任务计算 Y 中的一个值 $y_k \in Y$，则可以同时运行 128 个并行任务。这种并行化的优势在于它是无锁的，因为没有任务会同时修改相同的资源，也不会读取其他任务修改的资源。&lt;/p&gt;
&lt;h2 id=&#34;v-实验设置&#34;&gt;V. 实验设置
&lt;/h2&gt;&lt;p&gt;为了展示我们所提出方法的高效性，我们设计了七个实验，包括四个分类任务和三个自编码器实验。其中，四个分类实验中有三个是图像分类任务，分别使用 LeNet [34] 及其修改版本（详细信息见图 3）对 MNIST [35]、CIFAR-10 [33] 和胸部 X 光图像数据集 [53]（调整为 64×64 的灰度图像）进行分类。我们选择 LeNet 作为标准基准模型，用以评估我们的方法，其也被其他相关研究所采用 [15]。我们对 LeNet 进行了一些修改，以 (1) 构建一个具有更多样化和复杂层的模型，以及 (2) 增强模型对更复杂的 CIFAR-10 和 X 光图像分类任务的能力。&lt;/p&gt;
&lt;p&gt;最后一个分类实验是一个二分类任务，使用一个定制的全连接模型 (FcNet)（详细信息见图 3）对 &lt;strong&gt;Electrical Grid Stability Simulated (EGSS)&lt;/strong&gt; 数据集 [8] 进行分类。EGSS 是一个包含 10,000 个 14 变量样本的非图像数据集，任务是估算电网的稳定性（即是否稳定）。选择该实验的原因是：(1) 展示我们的方法在现实生活中非图像基础设施管理任务中的基准表现，以及 (2) 证明我们的方法能够处理自定义模型。&lt;/p&gt;
&lt;p&gt;最后三个实验是简单的自编码器 (AE) 网络架构，用于重建任务，这些网络架构参考了最近 Aharouni 等人的工作 [4]（详细信息见图 4）。这些自编码器用于 MNIST 数据集的重建任务。我们选择这些实验是为了：(1) 将我们提出的方法与最新的先进工作（即 HE-PEx [4]）进行比较，以及 (2) 展示我们的方法在不同任务（即重建任务）上的有效性。&lt;/p&gt;
&lt;p&gt;在实验中，我们使用表 I 中总结的超参数开发了原始训练模型。对于每个实验，我们对 HE 友好型模型进行了 10 次剪枝，每次剪枝使用不同的层稀疏率，从 50%、55%、60% 依次递增至 95%。然而，由于篇幅限制，我们仅在表 III 中报告了四种稀疏率下的剪枝和 PI 结果。&lt;/p&gt;
&lt;p&gt;对于 MNIST 和 CIFAR-10 数据集中的 8 位整数图像，我们通过将像素值除以 255，将其归一化到 0 到 1 之间。对于 EGSS 数据集，我们将变量线性缩放到 0 到 1 的范围内。所有实验的模型训练（或微调）的停止条件为：(1) 达到最大训练轮次，或 (2) 验证指标（如损失或准确率）在指定的容忍轮次后没有改进，以先达到的条件为准。此外，为了更平滑的收敛，如果验证指标未改进，每五轮将学习率降低 50%，最低可降至 $1 \times 10^{-7}$。&lt;/p&gt;
&lt;p&gt;在私有推理 (PI) 中，我们采用批量打包 [20] 对数据进行编码和加密，并在所有实验中使用 SEAL [48] 的 CKKS 加密方案，提供 128 位的安全性。我们使用 TensorFlow 模型优化工具包 [43] 实现了我们的迭代块剪枝方法。&lt;/p&gt;
&lt;p&gt;在 HE 友好模型的训练、微调以及剪枝步骤中，我们使用一台 Tesla V100-SXM2 GPU（32 GB 显存）。对于 PI，我们使用两台 52 核 Intel(R) Xeon(R) Gold 6230R CPU（主频 2.10GHz），总计 755 GB 内存。&lt;/p&gt;
&lt;h2 id=&#34;vi评估与讨论&#34;&gt;&lt;strong&gt;VI.评估与讨论&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;我们首先检验基于学习的方法的性能，然后讨论每个实验的结果，以评估我们的迭代块剪枝方法。为与当前最先进的研究进行比较，我们将MNIST-LeNet实验的结果与Hunter论文[15]进行对比，同时将自动编码器实验的结果与HE-PEx论文[4]进行对比。&lt;/p&gt;
&lt;p&gt;表2比较了我们的实验模型在不同数据集上训练的原始版本和HE友好版本，以展示我们提出的基于学习的方法在将训练好的模型转换为HE友好版本时的性能。结果表明，在大多数情况下，我们的方法不会显著降低模型的准确性，这显示了该方法的有效性。此外，实验结果还表明，我们的方法能够在合理的时间内完成模型的转换。&lt;/p&gt;
&lt;p&gt;表3总结了我们七个实验的HE友好模型和十个剪枝模型中四个模型（剪枝稀疏度分别为50%、60%、80%和90%）的结果。正如第一部分所述，我们方法的主要目标是通过减少HE操作的数量来加速隐私推理（PI）。实验结果表明，我们的方法以HE友好模型为基准，通过比较不同稀疏度剪枝模型的HE操作数量及其他因素（如剪枝时间、最大内存需求、PI延迟、准确性或均方误差、剪枝后模型的最终稀疏度）验证了其有效性。此外，对于每个实验模型的可剪枝层（参见图3和图4的标注），我们报告了神经元或滤波器的数量、其减少率及HE操作数量。需要注意的是，HE操作的总数并不等于可剪枝层HE操作的总和，因为每个模型中还有一些不可剪枝层（例如池化层、激活层、输出层等）也包含HE操作。&lt;/p&gt;
&lt;p&gt;此外，图5用示意图展示了七个实验中所有十个层级稀疏度（50%、55%到95%）与以下指标的关系：(1) PI延迟（log10刻度）、(2) 相对于HE友好模型的PI时间减少率、(3) HE操作总数（log10刻度）、(4) HE操作总数减少率、(5) PI内存使用量（log10刻度）、(6) PI内存减少率以及(7) 与基线（即原始模型）相比的性能（准确性或均方误差）变化率。&lt;/p&gt;
&lt;p&gt;将MNIST-LeNet实验的结果与Hunter论文[15]的结果进行比较，在几乎没有精度下降的情况下，我们的方法将HE操作总数减少了69.32%，而Hunter方法仅减少了47.35%。在最高稀疏度且仅降低2%精度的情况下，我们的方法将HE操作数量减少了93.41%，PI运行时间和所需内存分别减少了9.63倍和4.04倍。此外，Hunter方法需要1.2小时来剪枝该模型，而我们的方法在训练过程中完成剪枝，仅需几分钟。Hunter方法通过交互式PI使用较小的加密参数，从而降低了内存需求，但代价是通信成本增加；相比之下，我们的模型优化框架生成了一个优化版本的模型，使我们能够在无需客户交互的情况下高效地在加密下执行PI。&lt;/p&gt;
&lt;p&gt;在X-Ray-LeNet实验中，剪枝后的模型总体稀疏度接近98%，不仅保持了性能，甚至优于基线模型，这表明LeNet在X-Ray数据集上可能存在过度参数化。在最高稀疏度下，HE操作减少了93.42%，PI运行时间和所需内存分别减少了89.62%和74.25%。&lt;/p&gt;
&lt;p&gt;在CIFAR-10-MLeNet实验中，由于资源和选定的加密参数限制，HE友好（未剪枝）模型过于庞大，无法进行计算，仅第一个卷积层就需要超过600 GB的内存。然而，正如表3所示，当该模型剪枝至73%时，精度损失仍可忽略。这使我们能够在剪枝后的CIFAR-10-MLeNet模型上进行PI，而性能几乎没有显著下降，证明我们的剪枝方法使在复杂模型上进行PI变得更加实际。&lt;/p&gt;
&lt;p&gt;在EGSS-FcNet实验中，剪枝至最终稀疏度94%时，模型保持了相同的性能，同时HE操作减少了93.85%。相应地，所需内存和PI延迟分别减少了12倍和8倍。在最高稀疏度下，仅保留了基线模型HE操作的2.88%，这显示了剪枝方法在显著减少HE操作的同时保留模型性能的卓越效果。&lt;/p&gt;
&lt;p&gt;在所有自动编码器中，由于其简单的架构，增加稀疏度会导致性能下降。然而，根据表3和图5，AE3的性能优于其他两个模型，其性能在层级稀疏度达到50%时仍能保持不变。将自动编码器实验的结果与HE-PEx[4]进行比较可以发现，在近似相同的重建均方误差（≈0.03）下，我们的方法将AE3的PI延迟减少了65%，这一结果与HE-PEx[4]在AE3模型上的最佳结果相同。然而，我们的方法在减少内存消耗方面表现更优，将其降低了73.68%，而HE-PEx[4]的降低率为65%。&lt;/p&gt;
&lt;p&gt;此外，HE-PEx[4]的方法需要对剪枝后的模型执行一系列排列和扩展操作，以使其符合数据打包的要求。为适配这些模型结构的变化，他们还需要对模型的输入和输出执行两次转换操作，这增加了额外的复杂性。而我们的方法在训练过程中按需以块形状对模型进行剪枝，使其与数据打包要求一致，因此无需在剪枝后对模型执行额外操作，也无需对输入和输出数据进行任何转换。&lt;/p&gt;
&lt;p&gt;尽管HE-PEx[4]声称其方法可以扩展到卷积层，但他们仅在简单的全连接网络（自动编码器）上评估了该方法。相较之下，我们通过一系列广泛的实验展示了我们的方法同样能够高效地应用于卷积层。&lt;/p&gt;
&lt;h2 id=&#34;vii-结论&#34;&gt;&lt;strong&gt;VII. 结论&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;本研究提出了一种模型优化框架——MOFHEI，用于在同态加密（HE）环境下优化预训练的机器学习（ML）模型，以实现更快、更高效的非交互式隐私推理（PI）。我们的方法能够有效地将ML模型转换为适应HE的版本，并根据HE打包方法对模型进行剪枝。因此，在保持模型准确性的同时，减少了HE操作的数量，从而降低了PI的延迟和内存使用量。&lt;/p&gt;
&lt;p&gt;我们的迭代块剪枝方法适用于卷积层和全连接层，并结合批量打包（batch packing）方法使用。在实验中，我们通过对比应用剪枝方法前后不同稀疏度下HE操作数量、延迟和内存消耗的显著差异，证明了该剪枝技术的有效性。与最先进的研究相比，MOFHEI具有以下优势：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;采用过度训练的方法，剪枝时间更短；&lt;/li&gt;
&lt;li&gt;能够优化复杂网络并支持PI的执行；&lt;/li&gt;
&lt;li&gt;提供基于学习的方法，自动将模型转换为HE友好版本；&lt;/li&gt;
&lt;li&gt;能够与其他打包方法集成；&lt;/li&gt;
&lt;li&gt;无需客户端交互及其通信成本；&lt;/li&gt;
&lt;li&gt;无需对输入输出数据进行任何转换或后剪枝操作（例如，根据数据打包要求进行置换）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;该框架通过提供一种在模型性能、隐私和计算效率之间平衡的解决方案，为隐私保护机器学习（PPML）做出了贡献。未来工作中，我们计划扩展支持的层类型（如循环神经网络），并将剪枝方法与批量打包以外的其他打包方法集成。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>使用同态加密和联邦学习的隐私保护机器学习</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E4%BD%BF%E7%94%A8%E5%90%8C%E6%80%81%E5%8A%A0%E5%AF%86%E5%92%8C%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
        <pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E4%BD%BF%E7%94%A8%E5%90%8C%E6%80%81%E5%8A%A0%E5%AF%86%E5%92%8C%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E4%BD%BF%E7%94%A8%E5%90%8C%E6%80%81%E5%8A%A0%E5%AF%86%E5%92%8C%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/forest-6818683_1280.jpg" alt="Featured image of post 使用同态加密和联邦学习的隐私保护机器学习" /&gt;&lt;h1 id=&#34;使用同态加密和联邦学习的隐私保护机器学习&#34;&gt;使用同态加密和联邦学习的隐私保护机器学习
&lt;/h1&gt;&lt;p&gt;Haokun Fang 和 Quan Qian 是在隐私保护机器学习、同态加密、联邦学习等领域具有一定研究成果的学者。以下是他们的简要介绍：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Haokun Fang&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Haokun Fang 是一位专注于机器学习和加密技术的研究者。他的研究兴趣涵盖了隐私保护机器学习、联邦学习、同态加密等前沿技术。尤其是在如何通过加密技术保障数据隐私的研究领域，Fang 通过提出和改进各种算法，推动了隐私计算技术的发展。他也致力于如何在不暴露数据的情况下进行分布式计算和机器学习。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Quan Qian&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quan Qian 是另一位活跃在机器学习与信息安全领域的学者，专注于隐私保护技术和加密算法。他的研究方向包括同态加密、联邦学习、数据隐私保护以及分布式学习等。他是隐私保护机器学习的领域内的知名学者之一，并在多个高影响力的会议和期刊上发表了相关研究成果。Qian 对于如何将加密技术与机器学习结合，以实现在保护隐私的前提下有效的数据分析，做出了重要贡献。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;两位学者的共同研究方向是将同态加密和联邦学习结合，探索如何在确保数据隐私的情况下，进行高效且可扩展的机器学习模型训练和推理。他们的工作对于推动隐私保护技术的发展具有重要意义。&lt;/p&gt;
&lt;h2 id=&#34;摘要&#34;&gt;摘要：
&lt;/h2&gt;&lt;p&gt;随着机器学习的巨大成功，隐私保护成为了一个重要的关注点。本文提出了一种基于部分同态加密和联邦学习的多方隐私保护机器学习框架，命名为PFMLP。其核心思想是所有学习方仅通过同态加密传输加密的梯度。通过实验，PFMLP训练出的模型在准确率上几乎没有差异，偏差小于1%。考虑到同态加密的计算开销，本文采用了一种改进的Paillier算法，能够加速训练过程25%-28%。此外，本文还详细讨论了加密密钥长度、学习网络结构、学习客户端数量等方面的比较。&lt;/p&gt;
&lt;p&gt;关键词：多方机器学习；隐私保护机器学习；同态加密&lt;/p&gt;
&lt;h2 id=&#34;引言&#34;&gt;引言
&lt;/h2&gt;&lt;p&gt;在大数据时代，数据隐私已成为最重要的问题之一。迄今为止，已经存在许多安全策略和加密算法，旨在确保敏感数据不会受到泄露。此外，其中大多数安全策略假设只有拥有密钥的人才能访问机密数据。然而，随着机器学习，尤其是集中式机器学习的广泛应用，为了训练有效的模型，数据需要被收集并传输到一个中央点。因此，对于那些私人和敏感数据，它将不可避免地面临数据泄露的风险。因此，如何在没有数据泄露的情况下对私有数据集进行机器学习，是共享智能的关键问题。基于隐私保护，具有多方隐私保护的机器学习可以帮助各方用户在确保自身数据安全的前提下，共同学习彼此的数据[1-3]。其中，联邦学习[4,5]是一个典型的例子，它能够在多方计算的背景下解决隐私问题。本文提出了一种基于同态加密的隐私保护机器学习算法，命名为PFMLP。基本上，该模型通过多方隐私保护下的梯度学习共同训练。在每次迭代中，模型通过梯度下降进行优化，并通过传输梯度从其他用户的数据中学习。然而，正如[6]中提到的成员推断攻击，训练中的恶意用户可能会使用明文梯度训练一个影像模型，从而危及其他用户的数据安全。因此，我们引入了同态加密来防御这一攻击，允许在不解密的情况下对加密数据进行计算。此外，同态操作后的解密结果等价于对明文数据的操作[7]。由于在整个同态操作过程中无法识别操作的数据，因此可以保证隐私数据的安全。本文提出的基于同态加密的多方隐私保护机器学习在实际应用中具有广泛的场景。此外，本文的主要贡献如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提供了一种结合同态加密和联邦学习的多方隐私保护机器学习框架，在模型训练过程中实现数据和模型的安全保护。此外，所提出的框架能够在多方联合学习时保持隐私数据的安全。&lt;/li&gt;
&lt;li&gt;验证了通过我们提出的算法训练的模型与传统方法训练的模型在准确率上相似。从MNIST和金属疲劳数据集的实验结果来看，准确率偏差不超过1%。&lt;/li&gt;
&lt;li&gt;关于同态加密的时间开销，分析了不同密钥长度和网络结构的影响，发现随着密钥长度的增加或结构的复杂化，时间开销会增加。性能与安全性水平之间的权衡需要更多关注。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文其余部分的组织结构如下：第2节简要总结了相关工作；第3节从安全性、交互性和网络结构的角度详细讨论了联邦网络算法和Paillier联邦网络算法；第4节展示了实验结果，第5节总结了全文。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作
&lt;/h2&gt;&lt;h3 id=&#34;分布式机器学习&#34;&gt;分布式机器学习
&lt;/h3&gt;&lt;p&gt;分布式机器学习是一种多节点的机器学习，旨在提高性能、增加准确性，并轻松扩展数据到大规模。2013年在NIPS会议上提出了一种分布式机器学习框架[8]，该框架提出了一种状态同步并行模型，解决了普通同步或在海量数据和模型尺寸下训练的问题。2015年，Xing等人提出了一个通用框架，系统地解决了大规模机器学习中的数据和模型并行挑战[9]。Xie等人提出了一种有效的因子广播（SFB）计算模型，在分布式学习大矩阵参数化模型中既有效又高效[10]。Wei等人通过最大化机器间在给定网络带宽下的通信效率，最小化并行误差，同时确保大规模数据并行机器学习应用的理论融合[11]。Kim等人提出了一个分布式框架STRADS，优化了经典分布式机器学习算法的吞吐量[12]。在分布式深度学习中，2012年Jeffrey等人提出了谷歌的第一代深度学习系统Disbelief，并将模型拆分到32个节点进行计算[13]。2013年，分布式机器学习中的数据和模型并行性被引入到深度学习中，并在InfiniBand网络中实现[14]。2014年，Seide等人从理论上比较了分布式SGD（随机梯度下降）训练在模型和数据并行中的效率，并指出增加小批量的大小可以提高数据训练的效率[15,16]。&lt;/p&gt;
&lt;h3 id=&#34;安全多方计算与同态加密&#34;&gt;安全多方计算与同态加密
&lt;/h3&gt;&lt;p&gt;由于分布式机器学习基于中心任务调度，数据对系统是透明的，因此数据隐私无法得到有效保护。一般来说，分布式学习涉及多方计算，通常将复杂或未知的计算过程交给第三方。1986年，Yao提出了基于百万富翁问题的Garbred电路方法，可以用于解决一般问题，包括几乎所有的双方密码问题[17]。此后，1998年，Goldreich提出了安全多方计算（SMPC）的概念[18]。至今，SMPC被视为密码学的一个子领域，能够使分布式各方在不透露自己私有输入和输出的情况下共同计算任意功能。目前，同态加密已成为SMPC中常用的方法。1978年，Rivest等人提出了同态加密的概念，应用于银行业务[19]。作为第一个公钥加密系统，著名的RSA（Rivest-Shamir-Adleman）具有乘法同态性[20,21]。1999年，Paillier算法被发明[22]，由于Paillier满足加法同态性，它已广泛应用于云加密检索、数字拍卖、数字选举及其他隐私保护应用中。2009年，Craig Gentry首次提出了一种基于理想格的全同态加密（FHE）算法，满足加法同态性和乘法同态性[23]。由于FHE具有极高的安全性，它已被广泛应用[24–26]。尤其是在云计算中，同态加密为隐私保护作出了巨大贡献[27]。此外，差分隐私也是一种通过向样本中添加噪声来防止隐私泄漏的隐私保障技术[28–30]。由于引入噪声，当数据量较小时，噪声的影响不可避免地会影响模型训练。如何减少这种影响是一个大挑战。&lt;/p&gt;
&lt;h3 id=&#34;联邦学习&#34;&gt;联邦学习
&lt;/h3&gt;&lt;p&gt;针对数据隐私保护和多方联合学习，2016年谷歌提出了一种机器学习方法，命名为联邦学习[31]。作为一种多方协作的机器学习方法，联邦学习逐渐引起了研究界和工业界的广泛关注[32,33]。最初，联邦学习的目的是帮助Android用户解决本地更新模型的问题。此外，联邦学习可以应用于机器学习的各个领域。2019年，谷歌科学家提到，他们基于TensorFlow构建了一个用于移动设备领域联合学习的可扩展生产系统[34]。此外，2019年还提出了更多相关工作。Wang关注了在数据分布在多个边缘节点时，如何学习模型参数而无需将原始数据发送到中央节点的问题[35]。也有一些工作专注于联邦迁移学习，例如[36]中设计的框架，可以灵活地应用于各种安全多方机器学习。关于性能，[37]提出了一个框架SecureBoost，准确率几乎与五种隐私保护方法相当。联邦学习已广泛应用于各个领域。例如，谷歌设计的Gboard系统实现了键盘输入预测，同时保护隐私并帮助用户提高输入效率[38,39]。在医疗领域，患者的医疗数据是敏感的，因此联邦学习非常有用[40,41]。此外，联邦学习还可应用于自然语言处理[42]和推荐系统[43]等领域。此外，近年来，在隐私保护机器学习方面也有许多值得关注的工作。Zhou等人提出了使用差分隐私保护机器学习中的隐私，并使用SMC减少差分隐私引起的噪声[44,45]。2020年，Zhang等人提出了一种batchcrypt算法，该算法基于FATE框架的优化[46]，它将一批量的量化梯度编码为长整型，然后一次性加密，从而通过减少计算量提高了加密和解密效率。Wei Ou等人提出了一个基于同态加密的贝叶斯机器学习垂直联邦学习系统，该系统能够达到单一联合服务器训练模型的90%的性能[47]。&lt;/p&gt;
&lt;h2 id=&#34;方法与算法&#34;&gt;方法与算法
&lt;/h2&gt;&lt;h3 id=&#34;基于联邦思想的多样本协同学习&#34;&gt;基于联邦思想的多样本协同学习
&lt;/h3&gt;&lt;p&gt;联邦学习的思想是，在数据孤立的情况下，通过在训练过程中中间变量的交互，利用其他方的数据来优化自己的模型，如图1所示。从数据划分的角度，联邦学习可以分为两类：水平联邦学习（样本扩展）和垂直联邦学习（特征扩展）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;水平联邦学习&lt;/strong&gt;：水平联邦学习是通过样本扩展进行的机器学习。假设 \( D \) 表示数据，\( X \) 表示特征，\( Y \) 表示样本，\( I \) 表示数据索引。水平联邦学习可以表示为：&lt;/p&gt;
$$X_i = X_j, Y_i = Y_j, I_i \neq I_j, \forall D_i, D_j, i \neq j \tag{1}$$&lt;p&gt;这表示不同的用户有不同的数据，这些数据可能有交集也可能没有交集。水平联邦学习的主要思想是帮助多个用户使用自己的数据共同训练一个可靠的模型，同时确保数据的隐私和安全。然而，对于样本扩展，所有方的数据需要先对齐，以确保所有参与训练的方具有相同的特征域。这有助于所有方构建相同的模型架构并同步迭代。类似地，对于垂直联邦学习，所有参与者都有不同特征的样本。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;联邦网络算法&#34;&gt;联邦网络算法
&lt;/h3&gt;&lt;p&gt;本文提出的联邦学习网络的主要目标是通过在训练过程中传递中间变量，帮助所有方共同训练相同的模型。考虑到大多数神经网络是通过梯度下降进行训练的，这里我们选择梯度作为其中间变量。尽管梯度不能直接表示所有数据，但它可以表示模型与数据之间的关系，有助于模型的训练。联邦学习网络的架构如图2所示，包含一个计算服务器和多个学习客户端。&lt;/p&gt;
&lt;h4 id=&#34;学习客户端&#34;&gt;学习客户端
&lt;/h4&gt;&lt;p&gt;对于学习客户端，它们拥有自己的私有数据，并且假设所有数据已对齐，它们与其他学习参与者的数据量化维度一致。学习客户端的主要功能包括：与其他客户端初始化相同的初始模型、在本地训练数据、在训练过程中提取梯度、与计算服务器计算梯度、收集服务器的响应、传递结果、更新模型，并反复迭代，直到模型收敛。&lt;/p&gt;
&lt;h4 id=&#34;计算服务器&#34;&gt;计算服务器
&lt;/h4&gt;&lt;p&gt;计算服务器是学习过程中的一个中介平台。其主要功能包括：接收来自多个学习客户端的梯度信息、对梯度进行计算、整合多个模型学到的信息，并将结果分别传输给每个学习客户端。&lt;/p&gt;
&lt;h3 id=&#34;联邦多层感知机算法&#34;&gt;联邦多层感知机算法
&lt;/h3&gt;&lt;p&gt;在这里，我们提出了一种基于传统多层感知机的联邦多层感知机算法（FMLP）。FMLP可以通过共享梯度，在多方数据隔离环境中为每个客户端训练一个简单的模型。多层感知机，也称为深度前馈网络，是一种典型的深度学习模型。其架构示例如图3所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/%E4%BD%BF%E7%94%A8%E5%90%8C%E6%80%81%E5%8A%A0%E5%AF%86%E5%92%8C%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20241114205904687.png&#34;
	width=&#34;616&#34;
	height=&#34;358&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/%E4%BD%BF%E7%94%A8%E5%90%8C%E6%80%81%E5%8A%A0%E5%AF%86%E5%92%8C%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20241114205904687_hu16887854498636176068.png 480w, https://JiangZhiyu-1024.github.io/p/%E4%BD%BF%E7%94%A8%E5%90%8C%E6%80%81%E5%8A%A0%E5%AF%86%E5%92%8C%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20241114205904687_hu9960055567494880987.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241114205904687&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;172&#34;
		data-flex-basis=&#34;412px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;所有涉及算法的参数及其含义如表1所示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;表1. PFMLP算法中的参数及描述&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;参数&lt;/th&gt;
          &lt;th&gt;说明&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;$x$：数据集中的样本&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;$\theta$：模型的参数&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;$f_p$：前馈过程&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4&lt;/td&gt;
          &lt;td&gt;$out$：每次迭代的输出&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;$f^*$：激活函数&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;6&lt;/td&gt;
          &lt;td&gt;$loss$：损失函数&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;7&lt;/td&gt;
          &lt;td&gt;$c$：通过损失函数计算的损失&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;8&lt;/td&gt;
          &lt;td&gt;$e$：最小误差&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;9&lt;/td&gt;
          &lt;td&gt;$bp$：反向传播过程&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;10&lt;/td&gt;
          &lt;td&gt;$grad$：反向传播过程计算的梯度&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;11&lt;/td&gt;
          &lt;td&gt;$lr$：学习率&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;假设模型的参数为 $\theta = {\omega_1, \cdots, \omega_n, b_1, \cdots, b_n}$，训练的学习率为 $lr$。数据集可以表示为 $x = {x_1, \cdots, x_n}$。模型的目标是逼近一个分布 $f^*$。网络的前馈过程是计算训练输出，定义为：&lt;/p&gt;
$$
out = f_p(x, \theta)
$$&lt;p&gt;计算输出与理想值之间距离的损失函数可以定义为：&lt;/p&gt;
$$
c = loss(f^*(x), out) \tag{3}
$$&lt;p&gt;反向传播的功能是计算梯度，并将其从损失函数向后传播，帮助网络根据梯度调整参数，从而减少输出值与理想值之间的误差。反向传播过程可以定义为：&lt;/p&gt;
$$
grad = bp(x, \theta, c) \tag{4}
$$&lt;p&gt;模型更新过程是根据反向传播得到的梯度调整网络参数，可以表示为：&lt;/p&gt;
$$
\theta&#39; = \theta - lr \cdot grad \tag{5}
$$&lt;p&gt;通过联邦网络实现的多层感知机（MLP），我们可以得到一个联邦多层感知机（FMLP）。然后，MLP 模型的副本存储在每个学习客户端的本地内存中。它包含一个输入层，具有 $x$ 个单元，$n$ 个隐藏层，每个隐藏层有 $y$ 个单元，以及一个输出层，具有 $z$ 个单元。$x$ 的大小取决于输入数据的特征维度，$z$ 的大小取决于网络所需的输出，具体依赖于实际应用的目标输出。&lt;/p&gt;
&lt;p&gt;计算服务器的主要功能是融合梯度数据，帮助模型加速梯度下降，同时学习来自每个客户端的数据。在模型更新之前，每个学习客户端将梯度传递给计算服务器进行模型训练。此外，计算服务器整合所有客户端的梯度数据，并返回计算得到的新梯度给每个客户端用于模型更新。最后，当每个客户端的损失小于 $e$ 时，模型收敛。此外，所有客户端都可以获得相同的联邦模型。FMLP的具体步骤见算法1。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法 1 联邦多层感知机&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入：数据集 $x$&lt;br&gt;
输出：模型 $\theta_{final}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化模型参数 $\theta$&lt;/li&gt;
&lt;li&gt;对于每次迭代 $i$，执行以下步骤：&lt;br&gt;
3. 前馈传播：$out_i = f_p(x_i, \theta_i)$&lt;br&gt;
4. 计算损失：$c_i = loss(f^*(x_i), out_i)$&lt;br&gt;
5. 如果 $c_i &amp;lt; e$，则&lt;br&gt;
6. 跳出循环&lt;br&gt;
7. 否则&lt;br&gt;
8. 反向传播：$grad_i = bp(x_i, \theta_i, c_i)$&lt;br&gt;
9. 将梯度发送给计算服务器并获取新梯度&lt;br&gt;
10. 更新：$\theta_{i+1} = \theta_i - lr \cdot grad_{new}$&lt;br&gt;
11. 结束&lt;/li&gt;
&lt;li&gt;结束循环&lt;/li&gt;
&lt;li&gt;返回带有参数 $\theta_{final}$ 的模型&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;paillier-联邦网络&#34;&gt;Paillier 联邦网络
&lt;/h3&gt;&lt;p&gt;本文提出的联邦网络允许多个方在数据孤立的情况下进行协同机器学习。然而，在实际情况中，攻击者所需要的不仅仅是参与者提供的数据，还有多个方训练的最终模型。根据Shokri等人在2017年提出的成员推断攻击，攻击者可以入侵服务器，并从服务器中的数据推断出若干个影子模型。基于集成学习的思想，攻击者可以最终通过这些影子模型得到一个与实际合作训练的模型相似的预测。换句话说，在这种情况下，联邦模型只能解决数据安全问题，而不能解决模型安全问题。因此，为了保证模型的安全性，可以将同态加密引入联邦学习中。&lt;/p&gt;
&lt;p&gt;此外，同态加密的核心思想是，在对明文 $a$ 进行加密得到密文 $c$ 后，在密文空间内对 $c$ 执行某些操作的结果，相当于在明文空间内对 $a$ 执行相同操作的结果。加密操作可以表示为：&lt;/p&gt;
$$
E(a) \oplus E(b) = E(a \otimes b) \tag{6}
$$&lt;p&gt;在公式 (6) 中，$E$ 表示加密算法，$a$ 和 $b$ 表示两个不同的明文，$\oplus$ 和 $\otimes$ 表示操作符。如果操作是乘法操作，那么同态加密满足乘法同态性，例如 RSA 算法 [20]。如果操作是加法操作，那么同态加密算法满足加法同态性。Paillier 算法是最著名的一种 [22]。此外，如果算法同时满足加法和乘法同态性，那么该加密算法满足完全同态性 [23]。由于在多层感知机（MLP）中我们需要对梯度数据进行求和，因此可以使用 Paillier 算法进行同态加密。&lt;/p&gt;
&lt;h4 id=&#34;paillier-算法&#34;&gt;&lt;strong&gt;Paillier 算法&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;如上所述，Paillier 加密是一种部分同态加密，满足加法同态性。它可以分为三部分：密钥生成、加密和解密。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;密钥生成&lt;/strong&gt;：首先，选择两个足够大的素数 $p$ 和 $q$，它们的长度相等，并且满足 $\text{gcd}(p \cdot q, (p - 1) \cdot (q - 1)) = 1$。然后，计算 $n$ 和 $\lambda$，如下所示：&lt;/li&gt;
&lt;/ul&gt;
$$
n = p \cdot q \tag{7}
$$$$
\lambda = \text{lcm}(p - 1, q - 1) \tag{8}
$$&lt;p&gt;接着，随机选择一个整数 $g$，使得 $g \in Z^*_{n^2}$，从而使得 $n$ 能够整除 $g$ 的阶。然后，定义 $L(x)$ 来计算 $\mu$，如下所示：&lt;/p&gt;
$$
L(x) = \frac{x - 1}{n} \tag{9}
$$$$
\mu = (L(g^\lambda \mod n^2))^{-1} \mod n \tag{10}
$$&lt;p&gt;到此为止，我们可以得到公钥为 $(n, g)$，私钥为 $(\lambda, \mu)$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;加密&lt;/strong&gt;：假设明文为 $m$，密文为 $c$，使用公钥进行加密的过程可以表示为：&lt;/li&gt;
&lt;/ul&gt;
$$
c = g^m \cdot r^n \mod n^2 \tag{11}
$$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;解密&lt;/strong&gt;：使用私钥解密密文 $c$，得到明文 $m$ 的过程为：&lt;/li&gt;
&lt;/ul&gt;
$$
m = L(c^\lambda \mod n^2) \cdot \mu \mod n \tag{12}
$$&lt;h4 id=&#34;改进的-paillier-算法&#34;&gt;&lt;strong&gt;改进的 Paillier 算法&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;然而，由于 Paillier 算法在进行加密和解密时的高复杂度，这将影响网络训练的效率。因此，我们使用了改进版本的 Paillier，并且在 [48] 中详细证明了优化的正确性和效率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;密钥生成&lt;/strong&gt;：使用 $\alpha$ 作为除数，如果 $\lambda$ 替换了私钥中的 $\lambda$ 位置，我们可以修改公钥中的 $g$，并确保 $g$ 的阶为 $\alpha n$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;加密&lt;/strong&gt;：假设明文为 $m$，密文为 $c$，$r$ 为随机正整数，并且满足 $r &amp;lt; \alpha$。改进的加密过程可以表示为：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
$$
c = g^m \cdot (g^n)^r \mod n^2 \tag{13}
$$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;解密&lt;/strong&gt;：解密过程可以表示为：&lt;/li&gt;
&lt;/ul&gt;
$$
m = L(c^\alpha \mod n^2) \cdot L(g^\alpha \mod n^2)^{-1} \mod n \tag{14}
$$&lt;p&gt;从上述算法可以看出，使用 $\alpha$ 代替 $\lambda$ 的最大优势在于解密过程中。幂运算的次数从 $2 \cdot \lambda$ 次减少为 $2 \cdot \alpha$ 次。由于 $\alpha$ 是 $\lambda$ 的除数，时间开销显著减少。原生 Paillier 算法的计算复杂度为 $O(|n|^3)$，而改进后的 Paillier 算法的计算复杂度为 $O(|n|^2|\alpha|)$ [49]。&lt;/p&gt;
&lt;h4 id=&#34;paillier-联邦网络的架构&#34;&gt;&lt;strong&gt;Paillier 联邦网络的架构&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;在这里，我们使用 Paillier 加密来保护梯度数据。因此，即使攻击者破坏了计算服务器，他们也无法从每个学习客户端获取梯度数据的具体信息。此外，攻击者无法利用这些加密的梯度数据来训练影子模型。由于 Paillier 加密需要密钥对，为了生成和管理密钥对，我们在算法中加入了一个密钥管理中心（KMC）。Paillier 联邦网络的架构如图 4 所示，它包括 KMC、计算服务器和多个学习客户端。&lt;/p&gt;
&lt;h3 id=&#34;paillier-联邦多层感知机-pfmlp&#34;&gt;&lt;strong&gt;Paillier 联邦多层感知机 (PFMLP)&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;PFMLP 的基本结构与 FMLP 非常相似。由于 PFMLP 需要与 KMC 进行交互，学习客户端在训练开始之前应该向 KMC 发送请求。KMC 确认每个参与者都在线，然后生成密钥对并将其返回给学习客户端。收到密钥对后，每个学习客户端基于加密数据进行多方机器学习。PFMLP 的流程图如图 5 所示。与 FMLP 相比，PFMLP 增加了三个部分：(1) 学习客户端的加密和解密操作；(2) 计算服务器的同态操作；(3) 密钥管理中心（KMC）中的密钥对生成和分发。在 PFMLP 中，包含学习客户端、计算服务器和 KMC。学习客户端的算法如下所示：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法 2：学习客户端中的 PFMLP&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入：数据集 x&lt;br&gt;
输出：模型 θ_f inal&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;向 KMC 请求密钥对&lt;/li&gt;
&lt;li&gt;初始化模型参数 θ&lt;/li&gt;
&lt;li&gt;对于每次迭代 i:
&lt;ol&gt;
&lt;li&gt;前向传播：$out_i = f_p(x_i, \theta_i)$&lt;/li&gt;
&lt;li&gt;计算损失：$c_i = loss(f^*(x_i), out_i)$&lt;/li&gt;
&lt;li&gt;如果 $c_i &amp;lt; e$，则
&lt;ol&gt;
&lt;li&gt;退出&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;否则：
&lt;ol&gt;
&lt;li&gt;反向传播：$grad_i = bp(x_i, \theta_i, c_i)$&lt;/li&gt;
&lt;li&gt;使用客户端 i 的公钥加密梯度：$Enc(grad_i) = EncPaillier(Publickey, grad_i)$&lt;/li&gt;
&lt;li&gt;将加密的梯度 $Enc(grad_i)$ 发送给计算服务器并接收：$Enc(grad_i^{new})$&lt;/li&gt;
&lt;li&gt;使用客户端 i 的私钥解密梯度：$grad_i = DecPaillier(Privatekey, Enc(grad_i))$&lt;/li&gt;
&lt;li&gt;更新：$\theta_{i+1} = \theta_i - lr \cdot grad_{new}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;结束&lt;/li&gt;
&lt;li&gt;返回具有参数 θ_f inal 的模型&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;学习客户端在每次学习迭代计算完梯度后，并不会立即更新本地模型。它会对梯度数据进行同态加密，并将其传输给计算服务器，随后等待服务器在进行同态操作后返回新的加密梯度数据。在解密阶段，一旦客户端解密了新的加密梯度数据，它就可以使用新的梯度更新每个学习客户端的本地模型。因此，新的梯度隐式地包含了其他客户端的私有数据，从而间接保护了数据隐私。&lt;/p&gt;
&lt;p&gt;由于 PFMLP 对梯度数据执行 Paillier 加密，即使计算服务器被黑客攻破，泄露的数据也仅显示加密后的梯度数据 $Enc(grad)$。因此，可以避免推理攻击的威胁。KMC 的算法如算法 3 所示。此外，KMC 的主要功能是生成和分发密钥对。也就是说，当它收到来自学习客户端的请求时，它会生成一个密钥对并将其分发给客户端。&lt;/p&gt;
&lt;p&gt;算法 3 KMC 中的 PFMLP&lt;br&gt;
输入：请求&lt;br&gt;
输出：密钥对&lt;/p&gt;
&lt;p&gt;1: 在监听客户端请求时循环&lt;br&gt;
2: 如果收到来自客户端的请求，则&lt;br&gt;
3: 生成一个密钥对；&lt;br&gt;
4: 将密钥对返回给学习客户端；&lt;br&gt;
5: 结束条件&lt;br&gt;
6: 结束循环&lt;/p&gt;
&lt;p&gt;算法 4 计算服务器中的 PFMLP&lt;br&gt;
输入：请求&lt;br&gt;
输出：梯度数据&lt;/p&gt;
&lt;p&gt;1: 在监听来自客户端的请求时循环&lt;br&gt;
2: 初始化梯度数据；&lt;br&gt;
3: 如果收到请求，则&lt;br&gt;
4: 将加密数据 Enc(data) 推送到队列；&lt;br&gt;
5: 如果请求的数量 == 学习客户端的数量，则&lt;br&gt;
6: 对于每个学习客户端的请求：&lt;br&gt;
7: 将梯度数据更新为：GradientData = GradientData ⊕ Enc(datai)；&lt;br&gt;
8: 结束循环&lt;br&gt;
9: 将梯度数据返回给每个客户端；&lt;br&gt;
10: 退出循环；&lt;br&gt;
11: 结束条件&lt;br&gt;
12: 结束条件&lt;br&gt;
13: 结束循环&lt;/p&gt;
&lt;h3 id=&#34;算法安全性分析&#34;&gt;算法安全性分析
&lt;/h3&gt;&lt;p&gt;在 PFMLP 中，密钥管理中心仅负责密钥生成，并且不能访问任何数据。对于密钥管理中心，它甚至不知道客户端用密钥加密了哪些数据，因此它无法与其他方串通非法访问数据。计算服务器接收到的数据是客户端加密的密文，所有操作都是同态操作，没有解密过程。这意味着，在计算服务器上，所有数据都是加密格式的，因此即使服务器被攻破，也无法获得明文数据。学习客户端从密钥管理中心获取密钥对，然后将加密的梯度数据发送到计算服务器；计算服务器计算完成后，将仍然是加密格式的结果返回给客户端。在整个过程中，客户端无法访问其他客户端的数据。参与的唯一数据是上传的数据和返回的结果，它们都是加密格式的，这可以确保数据的安全性。如果攻击者想通过攻击计算服务器或通信通道来获取数据，他/她只能得到密文。由于我们可以在每次迭代中更换密钥对，即使攻击者足够幸运能够破解几轮训练结果，他/她也无法获得最终结果。即使攻击者是参与者，由于上述的客户端安全分析，他/她也无法从其他客户端获取数据。&lt;/p&gt;
&lt;h2 id=&#34;实验与结果分析&#34;&gt;实验与结果分析
&lt;/h2&gt;&lt;h2 id=&#34;实验数据集与环境&#34;&gt;实验数据集与环境
&lt;/h2&gt;&lt;p&gt;本实验使用了两个数据集进行验证：MNIST数据集和金属疲劳强度数据集。对于MNIST手写数字数据集[50]，它包含60,000个训练样本和10,000个测试样本。此外，神经网络模型包括784个输入层单元、两个默认64个单元的隐藏层和一个包含10个输出单元的输出层。关于金属疲劳数据，它只有437条记录，来自NIMS MatNavi开放数据集[51]。MatNavi是全球最大的材料数据库之一，涵盖了聚合物、陶瓷、合金、超导材料、复合材料和扩散数据库等。这里，我们从MatNavi中选择了437条金属疲劳强度数据，用于建立回归模型，测试不同金属（如碳钢、低合金钢、渗碳钢和弹簧钢）在不同测试条件下（如不同组件、轧制产品特性和后续热处理）下的表现。每条金属疲劳数据包含15维特征和1维标签。根据疲劳数据集，我们将其分为四类，如表2所示。实验中使用的模型结构包括一个15个单元（15维）的输入层、三个64单元的隐藏层和一个包含四个单元的输出层。PFMLP的网络结构如表3所示。&lt;/p&gt;
&lt;p&gt;表2. 用于多分类任务的疲劳数据集。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;数据集&lt;/th&gt;
          &lt;th&gt;数据范围&lt;/th&gt;
          &lt;th&gt;数据量&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;疲劳&lt;/td&gt;
          &lt;td&gt;[200, 400)&lt;/td&gt;
          &lt;td&gt;56&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;[400, 500)&lt;/td&gt;
          &lt;td&gt;147&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;[500, 600)&lt;/td&gt;
          &lt;td&gt;148&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;[600, ∞)&lt;/td&gt;
          &lt;td&gt;86&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;表3. PFMLP的网络结构。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;数据集&lt;/th&gt;
          &lt;th&gt;输入层&lt;/th&gt;
          &lt;th&gt;隐藏层&lt;/th&gt;
          &lt;th&gt;输出层&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;MNIST&lt;/td&gt;
          &lt;td&gt;784（单位）&lt;/td&gt;
          &lt;td&gt;2（层）· 64（单位）&lt;/td&gt;
          &lt;td&gt;10（单位）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;疲劳&lt;/td&gt;
          &lt;td&gt;15（单位）&lt;/td&gt;
          &lt;td&gt;3（层）· 64（单位）&lt;/td&gt;
          &lt;td&gt;4（单位）&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这里，$D^{Dataset}_n$ 表示数据集的第 n 个数据。为了评估 PFMLP 算法及其优化方法，设计了几组对比实验，从三个角度进行比较：(1) 联邦多层感知器与单节点多层感知器的预测准确性；(2) 使用不同密钥长度进行模型训练的时间消耗；(3) 使用不同大小的隐藏层单元进行模型训练的时间消耗；(4) 不同数量的学习客户端对模型性能的影响。实验环境为 Windows 10，Python 3.6，scikit-learn 0.21.3 和 phe 1.4.0。我们在局域网中部署了计算服务器、KMC 和多个客户端，并通过 Socket 建立了机器之间的通信。具体的网络部署如图6所示。&lt;/p&gt;
&lt;h3 id=&#34;精度比较&#34;&gt;精度比较
&lt;/h3&gt;&lt;p&gt;为了进行比较，PFMLP 和 MLP 算法在相同的网络结构下进行模型训练，并使用相同的数据集进行学习。假设有两个学习客户端，我们将每个数据集分成两个子集，并将它们分配给两个学习客户端。对于 MNIST 数据集，我们选择前 4000 个数据作为训练集 Dmnist，然后将这 4000 个数据分别分为两个部分：Dmnist I = {Dmnist 1, · · · , Dmnist 200}，Dmnist II = {Dmnist 201, · · · , Dmnist 400}。测试数据使用 MNIST 提供的 10,000 个测试集。同时，我们从金属疲劳强度数据集中选择 400 个数据，并将它们分为两个相等的子集进行模型训练。假设原始数据为 DFatigue，将原始数据随机化，记为 DFatigue′ = random(DFatigue)。两个子数据集为 DFatigue′ I = {DFatigue′ 1, · · · , DFatigue′ 200}，DFatigue′ II = {DFatigue′ 201, · · · , DFatigue′ 400}。此外，我们将 70% 作为训练集，剩余的 30% 作为测试集。实验结果如表 4 所示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;表 4. MLP 和 PFMLP 在两个数据集上的预测精度比较结果&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;数据集&lt;/th&gt;
          &lt;th&gt;数据子集&lt;/th&gt;
          &lt;th&gt;算法&lt;/th&gt;
          &lt;th&gt;精度&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;MNIST&lt;/td&gt;
          &lt;td&gt;Dmnist 1&lt;/td&gt;
          &lt;td&gt;MLP&lt;/td&gt;
          &lt;td&gt;0.8333&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Dmnist 2&lt;/td&gt;
          &lt;td&gt;MLP&lt;/td&gt;
          &lt;td&gt;0.9033&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Dmnist&lt;/td&gt;
          &lt;td&gt;MLP&lt;/td&gt;
          &lt;td&gt;0.9245&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Dmnist 1&lt;/td&gt;
          &lt;td&gt;PFMLP&lt;/td&gt;
          &lt;td&gt;0.9252&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;Dmnist 2&lt;/td&gt;
          &lt;td&gt;PFMLP&lt;/td&gt;
          &lt;td&gt;0.9252&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Fatigue&lt;/td&gt;
          &lt;td&gt;DFatigue′ 1&lt;/td&gt;
          &lt;td&gt;MLP&lt;/td&gt;
          &lt;td&gt;0.9013&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;DFatigue′ 2&lt;/td&gt;
          &lt;td&gt;MLP&lt;/td&gt;
          &lt;td&gt;0.7833&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;DFatigue′&lt;/td&gt;
          &lt;td&gt;MLP&lt;/td&gt;
          &lt;td&gt;0.8583&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;DFatigue′ 1&lt;/td&gt;
          &lt;td&gt;PFMLP&lt;/td&gt;
          &lt;td&gt;0.8833&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;DFatigue′ 2&lt;/td&gt;
          &lt;td&gt;PFMLP&lt;/td&gt;
          &lt;td&gt;0.8167&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;DFatigue′&lt;/td&gt;
          &lt;td&gt;PFMLP&lt;/td&gt;
          &lt;td&gt;0.8500&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;从表 4 可以看出，PFMLP 训练的模型比本地 MLP 的模型更准确。PFMLP 训练的最终模型几乎等于或甚至优于使用每个客户端所有数据训练的 MLP 模型。在 MNIST 数据集上的实验表明，PFMLP 训练的模型在测试集上的准确率为 0.9252，而使用所有训练集数据的 MLP 训练的模型准确率为 0.9245，PFMLP 的准确率比 MLP 高出 0.007。&lt;/p&gt;
&lt;p&gt;对于金属疲劳强度数据集，由于每个客户端上的模型都是从相同的 PFMLP 中学习的，我们可以根据测试集的数量对两个实验的结果进行加权平均，从而得到最终的预测精度为 0.85。与学习了所有数据的 MLP 模型精度为 0.858 相比，精度仅下降了 0.008。因此，从两个数据集的实验结果来看，PFMLP 算法能够训练出一个与 MLP 在多个客户端上的所有数据上几乎相同精度的模型。详细结果如图 7 所示。&lt;/p&gt;
&lt;h3 id=&#34;不同密钥长度下模型训练时间对比&#34;&gt;不同密钥长度下模型训练时间对比
&lt;/h3&gt;&lt;p&gt;由于成员推断攻击的威胁，明文传输梯度数据可能被恶意用户利用来训练自己的影子模型，从而侵犯其他客户端的隐私数据安全。为此，PFMLP 使用了 Paillier 同态加密。此外，在梯度数据传输过程中执行加密，并且在计算服务器中进行同态操作，确保即使服务器存在安全漏洞，加密的梯度数据也不会泄露。&lt;/p&gt;
&lt;p&gt;在 Paillier 中，密钥长度是影响安全级别的一个重要因素。通常，密钥长度越长，安全性越高。然而，使用较长的密钥也会增加生成密文的时间开销。对于 MNIST 和金属疲劳数据集，进行了三次对比实验。模型结构固定，不同的密钥长度是模型训练时间开销的核心因素。表 5 显示了详细信息。&lt;/p&gt;
&lt;p&gt;从表 5 中可以看出，在两个数据集中，Paillier 密钥长度的影响与时间消耗成正比。同时，如图 8 和图 9 所示，这是 PFMLP 在两个数据集上每轮学习训练时间的折线图。由于在每轮中都需要对梯度数据进行加密和解密，并将加密后的梯度数据传输到计算服务器进行进一步操作，因此，随着密钥长度的增加，每轮训练的时间开销也增加，这一现象是合理的。因此，我们可以选择一个适当的密钥长度，作为安全性和时间性能之间的折衷。此外，为了提高安全级别，我们可以在每轮训练中更新密钥。这样，即使某一轮的密钥被破解，也不会影响整体训练过程的安全性，从而实现更高的数据安全级别。&lt;/p&gt;
&lt;p&gt;从上述实验中可以看出，在相同模型和相同密钥长度下，4000 条数据每轮需要 358.14 秒，8000 条数据需要 733.69 秒，12000 条数据需要 1284.06 秒。因此，时间开销与加密数据的数量呈正相关。我们使用改进的 Paillier 算法在 MNIST 数据集上进行实验，并比较了相同梯度数据在相同轮次中的加密和解密时间开销，结果如表 6 所示。&lt;/p&gt;
&lt;p&gt;表 6. 不同密钥长度对每轮迭代时间的影响&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;算法&lt;/th&gt;
          &lt;th&gt;密钥长度（位）&lt;/th&gt;
          &lt;th&gt;时间（秒）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Paillier&lt;/td&gt;
          &lt;td&gt;128&lt;/td&gt;
          &lt;td&gt;1068.38&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;256&lt;/td&gt;
          &lt;td&gt;6411.23&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;512&lt;/td&gt;
          &lt;td&gt;35,930.83&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;改进版 Paillier&lt;/td&gt;
          &lt;td&gt;128&lt;/td&gt;
          &lt;td&gt;779.37&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;256&lt;/td&gt;
          &lt;td&gt;4716.37&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;512&lt;/td&gt;
          &lt;td&gt;26,148.56&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;从表 6 可以看出，与原版 Paillier 算法相比，改进版 Paillier 在加密和解密性能上显著提高，提升幅度接近 25% 至 28%。&lt;/p&gt;
&lt;h3 id=&#34;隐藏层大小对训练性能的影响比较&#34;&gt;隐藏层大小对训练性能的影响比较
&lt;/h3&gt;&lt;p&gt;对于神经网络，每一层的大小会影响前向传播和反向传播的时间性能。一般来说，网络大小与训练时间呈正相关。在这里，我们设计了几组关于两个数据集的比较实验，结果如表 7 所示。具体的每轮训练时间开销如图 10 和图 11 所示。&lt;/p&gt;
&lt;p&gt;表 7. 不同隐藏层大小对总训练时间的影响&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;数据集&lt;/th&gt;
          &lt;th&gt;隐藏层大小（单元数）&lt;/th&gt;
          &lt;th&gt;时间（秒）&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;MNIST&lt;/td&gt;
          &lt;td&gt;2 · 64&lt;/td&gt;
          &lt;td&gt;12,033.25&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;2 · 128&lt;/td&gt;
          &lt;td&gt;23,981.02&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;2 · 256&lt;/td&gt;
          &lt;td&gt;47,702.87&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Fatigue&lt;/td&gt;
          &lt;td&gt;3 · 64&lt;/td&gt;
          &lt;td&gt;2615.42&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;3 · 128&lt;/td&gt;
          &lt;td&gt;6941.04&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;3 · 256&lt;/td&gt;
          &lt;td&gt;21,782.07&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;因此，由于算法需要对梯度矩阵进行加密，并且隐藏层单元数越多，时间开销会成比例增加。此外，随着隐藏层单元数的增加，网络中需要传输的数据量也会增加。为了减少 PFMLP 算法的时间开销，在保证准确性的前提下，应尽可能减少隐藏层的数量和每个隐藏层的单元数。&lt;/p&gt;
&lt;h3 id=&#34;不同数量学习客户端对训练准确度和时间开销的影响&#34;&gt;不同数量学习客户端对训练准确度和时间开销的影响
&lt;/h3&gt;&lt;p&gt;PFMLP 支持多方机器学习。此外，理论上，随着客户端数量的增加，学习算法应该保证在模型训练过程中获得相似的准确度，并且减少时间开销。在此，我们设计了一项比较实验，分别在单节点（MLP）、两个客户端（2-Client-PFMLP）和四个客户端（4-Client-PFMLP）上对金属疲劳强度数据集进行实验。实验结果如表 8 所示。这里，本地准确度是指模型在本地测试数据集上的预测准确率；逻辑准确度是指每个客户端上的平均准确率。详细结果见图 12。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;表 8. 不同学习客户端数量下，PFMLP 在金属疲劳数据集上的准确度&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;算法&lt;/th&gt;
          &lt;th&gt;数据集&lt;/th&gt;
          &lt;th&gt;本地准确度&lt;/th&gt;
          &lt;th&gt;逻辑准确度&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;MLP&lt;/td&gt;
          &lt;td&gt;DFatigue′ 0−400&lt;/td&gt;
          &lt;td&gt;0.858&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;MLP&lt;/td&gt;
          &lt;td&gt;DFatigue′ 0−200&lt;/td&gt;
          &lt;td&gt;0.833&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;MLP&lt;/td&gt;
          &lt;td&gt;DFatigue′ 200−400&lt;/td&gt;
          &lt;td&gt;0.783&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2-Client-PFMLP&lt;/td&gt;
          &lt;td&gt;DFatigue′ 0−200&lt;/td&gt;
          &lt;td&gt;0.867&lt;/td&gt;
          &lt;td&gt;0.850&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2-Client-PFMLP&lt;/td&gt;
          &lt;td&gt;DFatigue′ 200−400&lt;/td&gt;
          &lt;td&gt;0.833&lt;/td&gt;
          &lt;td&gt;0.850&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;MLP&lt;/td&gt;
          &lt;td&gt;DFatigue′ 0−100&lt;/td&gt;
          &lt;td&gt;0.767&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;MLP&lt;/td&gt;
          &lt;td&gt;DFatigue′ 100−200&lt;/td&gt;
          &lt;td&gt;0.933&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;MLP&lt;/td&gt;
          &lt;td&gt;DFatigue′ 200−300&lt;/td&gt;
          &lt;td&gt;0.800&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;MLP&lt;/td&gt;
          &lt;td&gt;DFatigue′ 300−400&lt;/td&gt;
          &lt;td&gt;0.600&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4-Client-PFMLP&lt;/td&gt;
          &lt;td&gt;DFatigue′ 0−100&lt;/td&gt;
          &lt;td&gt;0.833&lt;/td&gt;
          &lt;td&gt;0.850&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4-Client-PFMLP&lt;/td&gt;
          &lt;td&gt;DFatigue′ 100−200&lt;/td&gt;
          &lt;td&gt;0.967&lt;/td&gt;
          &lt;td&gt;0.850&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4-Client-PFMLP&lt;/td&gt;
          &lt;td&gt;DFatigue′ 200−300&lt;/td&gt;
          &lt;td&gt;0.867&lt;/td&gt;
          &lt;td&gt;0.850&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4-Client-PFMLP&lt;/td&gt;
          &lt;td&gt;DFatigue′ 300−400&lt;/td&gt;
          &lt;td&gt;0.733&lt;/td&gt;
          &lt;td&gt;0.850&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;从图 12 可以看出，多客户端 PFMLP 算法显著提高了预测准确度。两种和四种学习客户端的逻辑准确率几乎相同。与划分数据后的本地训练相比，PFMLP 训练的本地准确率有所提高，尤其在极端情况下，准确度得到了放大。例如，在一个 4-Client-PFMLP 实验中，最后一个学习客户端显然有离群数据，而 PFMLP 的准确率比 MLP 高出 13.3%。第二个学习客户端的本地准确率达到了 93.3%，而使用 PFMLP 后仍提高了 3.4%。&lt;/p&gt;
&lt;p&gt;此外，从表 8 可以看出，客户端数量对 PFMLP 训练模型的性能几乎没有影响。这些模型的性能接近于通过收集所有参与者数据的单一 MLP 训练的模型。同时，由于 N-Client-PFMLP 的核心思想是基于批次扩展，一旦每个客户端的数据量较少，他们每轮学习的批次大小也会更小。因此，训练过程的时间开销将会减少。&lt;/p&gt;
&lt;h2 id=&#34;结论与未来工作&#34;&gt;结论与未来工作
&lt;/h2&gt;&lt;p&gt;本文提出的多方隐私保护机器学习方法，通过结合同态加密和联邦学习，能够帮助多个用户在不泄露自己私密数据的情况下进行机器学习。特别是在隐私数据保护方面，该算法能够在数据孤立的情况下训练通用模型。PFMLP 算法的实验结果表明，使用 PFMLP 训练的模型效果与在单台机器上使用所有数据训练的模型相似。所有参与方只需传输梯度数据，梯度融合则在中央计算服务器上通过同态操作完成。学习模型会根据经过同态操作后的新梯度数据进行更新。然而，同态加密不可避免地会引发一些性能问题，例如加密和解密过程的额外开销，这将大大影响训练效率。此外，网络结构、加密/解密密钥长度和密钥替换频率等因素也会影响最终的性能。&lt;/p&gt;
&lt;p&gt;关于未来的工作，首先，应考虑更加强大和可扩展的联邦学习方法，例如将特征分配到不同客户端的纵向联邦学习算法。其次，高效的同态加密算法将加速学习性能。最后，应该更加关注更加稳健的隐私保护学习算法，包括混合算法、防恶意攻击客户端算法等。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
