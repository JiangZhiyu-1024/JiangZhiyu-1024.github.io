<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Transformer on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/categories/transformer/</link>
        <description>Recent content in Transformer on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Tue, 03 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/categories/transformer/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>BumbleBee 大型变换器模型的安全双方推理框架</title>
        <link>https://JiangZhiyu-1024.github.io/p/bumblebee-%E5%A4%A7%E5%9E%8B%E5%8F%98%E6%8D%A2%E5%99%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%89%E5%85%A8%E5%8F%8C%E6%96%B9%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/</link>
        <pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/bumblebee-%E5%A4%A7%E5%9E%8B%E5%8F%98%E6%8D%A2%E5%99%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%89%E5%85%A8%E5%8F%8C%E6%96%B9%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/bumblebee-%E5%A4%A7%E5%9E%8B%E5%8F%98%E6%8D%A2%E5%99%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%89%E5%85%A8%E5%8F%8C%E6%96%B9%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/cottage-2955582_1280.jpg" alt="Featured image of post BumbleBee 大型变换器模型的安全双方推理框架" /&gt;&lt;h1 id=&#34;bumblebee-大型变换器模型的安全双方推理框架&#34;&gt;&lt;strong&gt;BumbleBee: 大型变换器模型的安全双方推理框架&lt;/strong&gt;
&lt;/h1&gt;&lt;h2 id=&#34;摘要&#34;&gt;&lt;strong&gt;摘要&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;基于大型变换器模型的技术已在自然语言处理、计算机视觉等许多实际任务中实现了最先进的性能。然而，随着它们处理的数据和任务敏感性增加，隐私问题在模型部署过程中成为了一个主要关注点。在这项工作中，我们关注于在两方设置下进行私密推理的场景，其中一方持有私密输入，另一方持有模型。我们介绍了BumbleBee，一个快速且适合通信的双方私密变换器推理系统。我们的贡献主要有三点：首先，我们提出了优化的矩阵乘法协议，相较于以往技术，显著降低了80%-90%的通信成本。其次，我们开发了一种针对变换器模型中非线性激活函数的高效协议构建方法。所提出的激活协议在处理速度上实现了显著提升，同时与之前的两种方法相比，通信成本降低了80%-95%。最后，我们对五个变换器模型进行了广泛的基准测试。BumbleBee通过评估LLaMA-7B模型展示了其能力，使用CPU生成一个token大约需要14分钟。我们的结果进一步表明，BumbleBee在性能上超越了Iron（NeurIPS22）一个数量级，并且比BOLT（Oakland24）快三倍，同时通信成本仅为BOLT的十分之一。&lt;/p&gt;
&lt;h2 id=&#34;引言&#34;&gt;&lt;strong&gt;引言&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;基于大型变换器模型，如BERT [19]、GPT [60] 和 ViT [22]，在许多实际任务中实现了最先进（SOTA）的性能，包括人员重识别 [47]、语音助手 [14] 和代码自动补全 [75]。随着变换器模型处理的数据和任务越来越敏感，隐私问题已成为模型部署中的主要关注点之一。私密推理旨在保护模型权重免受用户的干扰，同时确保服务器无法获取用户私密输入的信息。许多近期的研究引入了基于安全多方计算（MPC）[8]、[27]的加密框架，以实现深度学习模型（如卷积神经网络（CNN）[36]、[2]、[41]、[62]）和变换器模型的私密推理[48]、[33]、[79]、[73]。尽管安全双方计算（2PC）可以高效地在几分钟内推理CNN，但在变换器模型上的私密推理则带来了新的挑战，特别是在通信开销方面。例如，[33]、[30]等研究表明，对12层BERT模型的单次私密推理可能需要高达90 GB的通信。此外，[73，表1]报告称，12层ViT模型的单次私密推理可能需要交换约262 GB的消息。因此，这些通信密集型方法对高带宽的需求不可或缺。我们总结了开发高效、通信友好的2PC框架，用于大规模变换器私密评估的两个主要挑战。&lt;/p&gt;
&lt;h4 id=&#34;大规模矩阵的乘法&#34;&gt;&lt;strong&gt;大规模矩阵的乘法&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;变换器模型的推理可能涉及数百次大矩阵的乘法。例如，自然语言处理（NLP）变换器使用嵌入表将一组单词查询转换为数值表示。嵌入表查找可以定义为矩阵乘法 $EV$，其中 $E$ 的每一行是一个对应于输入查询中每个单词索引的独热向量。换句话说，这种乘法的维度可以解析为：单词数 × 词汇表大小 × 嵌入大小，这比卷积神经网络（CNN）中的矩阵要大得多。由于相对于NLP变换器而言，视觉变换器通常具有更大的嵌入大小，因此也可能涉及较大的矩阵。现有的大多数加密协议用于私密矩阵乘法，都依赖于“不可知转移”（Oblivious Transfer，OT）[18]、[57]、[59] 或同态加密（Homomorphic Encryption，HE）[41]、[12]、[36]、[35]。然而，这两种方法各有其局限性。基于OT的私密矩阵乘法方法计算时间较少，但需要传输大量消息；另一方面，基于HE的方法计算量显著增加，但比OT方法更加适合通信。第一个挑战是开发一种既快速又通信友好的矩阵乘法协议。&lt;/p&gt;
&lt;h4 id=&#34;更复杂的激活函数&#34;&gt;&lt;strong&gt;更复杂的激活函数&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;与CNN中使用的简单ReLU激活函数不同，变换器模型包含复杂的激活函数，如softmax、Gaussian Error Linear Unit（GeLU）和Sigmoid Linear Unit（SiLU）。这些激活函数的计算需要基本的数学函数，如指数运算、除法和双曲函数。尽管研究人员已经为这些基本函数开发了特定的协议[11]、[61]、[62]、[44]，但直接在变换器模型中使用这些协议仍然不切实际。主要原因是变换器模型中的激活数量非常庞大。例如，GPT2模型[15]的单次推理需要计算大约 3.9 × 10⁶ 次点对点的GeLU激活。我们的第二个挑战是为这些复杂的激活函数设计高效的2PC协议。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A. 技术细节&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;高效的线性函数协议&lt;/strong&gt;
我们提出了一种叫做“不可知线性变换”（Oblivious Linear Transformation，OLT）的原语。我们将OLT描述为一个双方协议，其中两方分别提供私密矩阵 $Q \in \mathbb{Z}_2^{* 2l}$ 和 $V \in \mathbb{Z}_2^{* 2l}$，并生成它们之间的共享 $QV$。使用OLT，我们可以在模 $2l$ 的环上实现两个加法共享矩阵的乘法。基于同态加密（HE）的方法[36]、[54]提供了一个很好的起点。这些方法的一个重要局限性是显著的通信开销，这来自于输出密文的“稀疏”格式。更具体地说，[36]、[54]中的每个输出密文加密了一个长向量。然而，乘法结果只需要该向量中的一小部分元素。为了进行解密，仍然需要传输整个加密向量。为了克服这一不足，我们提出了一种压缩过程，通过同态方式将加密向量中不必要的条目归零，从而将多个“稀疏”向量合并为一个“密集”向量。因此，通信开销减少，因为需要发送的密文数量变少。与之前的密文压缩方法[13]相比，我们的压缩过程快了大约50倍。总体而言，我们观察到与[36]、[54]相比，通信成本减少了80%-90%。&lt;/p&gt;
&lt;p&gt;除了矩阵乘法，点对点乘法 $x_0 \cdot y_0, x_1 \cdot y_1, \dots$ 也是变换器推理中的一个重要计算。许多基于同态加密的点对点乘法协议[18]、[63]需要设置一个相对较大的明文模数$t$。例如，[18]、[63]都设置HE参数 $t &amp;gt; 22l+40$ 来加密来自 $\mathbb{Z}_2^l$ 的值。在这项工作中，我们提出了一种模数提升函数，用于统一底层HE的秘密共享模数和明文模数。提升函数使我们能够在HE密文上执行模 $2l$ 的算术运算。这使我们能够选择较小的HE参数，即 $t \approx 22l$。我们的实验结果表明，在[63]中应用提升函数后，性能提高了1.3倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为变换器中的激活函数构建高效准确协议的框架&lt;/strong&gt;
我们首先提出了一个通用框架，用于构建高效且精确的2PC协议，适用于许多变换器模型中使用的激活函数。这些激活函数具有一个共同的特性：在靠近原点的短区间内它们相对平滑，且在两侧几乎是线性的。考虑到这一特性，我们提出使用一个或两个低度多项式来逼近激活函数在短区间内的表现，并在两侧使用恒等函数。例如，我们建议用两个低度的多项式 $P(x)$ 和 $Q(x)$ 来逼近SiLU函数，这两个多项式最小化最小二乘误差，如下所示：
&lt;/p&gt;
$$
\text{SiLU}(x) \approx 
\begin{cases} 
-10^{-5} &amp; \text{if } x &lt; -8 \\
P(x) &amp; \text{if } -8 &lt; x \leq -4 \\
Q(x) &amp; \text{if } -4 &lt; x \leq 4 \\
x - 10^{-5} &amp; \text{if } x &gt; 4 
\end{cases}
$$&lt;p&gt;
接着，我们基于两个关键的见解提出了优化方法。首先，我们利用激活函数的平滑性来提高效率。例如，对于输入 $x = -3.98$，即便使用（错误的）第二段，也能得到类似的结果，即 $P(-3.98) \approx Q(-3.98)$。这种平滑性为我们设计一种高效的评估方法提供了空间，尽管会引入轻微的误差。其次，我们提出了优化措施，以提高在对相同输入点评估多个多项式时的摊销效率。具体来说，我们优化后的激活协议比当前在[61]、[53]中详细描述的数值方法快了9到20倍，通信成本减少了80%到95%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;B. 贡献&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;总结来说，我们做出了三项关键贡献：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们开发了一个基于同态加密（HE）的矩阵乘法协议，特别针对模 $2l$ 运算进行了优化，以提升通信效率。事实上，我们的协议已被证明与现有的基于HE的方法相比，通信成本减少了80%。此外，我们的协议还具有高速度。例如，私密矩阵乘法操作（维度为 $128 \times 768 \times 768$）可以在1秒钟内完成，使用一个中等规格的云实例。&lt;/li&gt;
&lt;li&gt;我们实现了所有提出的协议，并开发了一个新的MPC后端，称为BumbleBee，并将其集成到SPU库[53]中。为了进行比较，我们还基于SPU库实现了一个基准版本，使用了几种最先进的2PC协议。通过简单的对比，图1展示了与基准版本相比，提出的协议所带来的改进。总的来说，我们在SIGMA[30]的基础上减少了84%的通信成本，在BOLT[58]的基础上减少了90%，在Iron[33]的基础上减少了92%。&lt;/li&gt;
&lt;li&gt;BumbleBee使得私密变换器推理变得易于使用。现有的工作[48]、[33]、[58]仅考虑了BERT系列模型。与之对比，我们成功地在5个预训练变换器模型上运行了BumbleBee，利用了HuggingFace网站上提供的模型权重和Python程序，包括BERT-base、BERT-large、GPT2-base、LLaMA-7B和ViT-base。我们还在四个公开数据集上评估了BumbleBee的准确性。所有实验都使用了我们提出的协议，而不是通过模拟进行的。我们提供了可重现的实现，网址为：https://github.com/AntCPLab/OpenBumbleBee。我们的方法为精确且可行的私密变换器推理提供了有力证据，即使在不改变神经网络结构的情况下，也能实现这一目标。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;II. 基础知识&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A. 符号约定&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们用 $x = y$ 表示 $x$ 等于 $y$，用 $x := y$ 表示将 $y$ 的值赋给变量 $x$。对于一个交互协议 $\Pi$，我们用 $JxK \gets \Pi$ 表示协议的执行。我们用 $[n]$ 来表示集合 ${0, \dots, n - 1}$，其中 $n \in \mathbb{N}$。对于一个集合 $D$，$x \in R^D$ 表示 $x$ 是从 $D$ 中均匀随机采样的。我们使用 $\lceil \cdot \rceil$、$\lfloor \cdot \rfloor$ 和 $\lfloor \cdot \rceil$ 来分别表示天花板函数、地板函数和四舍五入函数。逻辑与和异或分别表示为 $\land$ 和 $\oplus$。令 $1{P}$ 表示指示函数，当谓词 $P$ 为真时，$1{P}$ 为 1，$P$ 为假时，$1{P}$ 为 0。我们用带有“帽”符号的小写字母，例如 $\hat{a}$，来表示多项式，用 $\hat{a}[j]$ 表示多项式 $\hat{a}$ 的第 $j$ 个系数。我们用点符号 $\cdot$，例如 $\hat{a} \cdot \hat{b}$，来表示多项式的乘法。我们用 $Z_q = Z \cap [0, q)$ 来表示 $q \geq 2$ 时的整数模 $q$。同余式 $x \equiv y \mod 2^l$ 将简写为 $x \equiv_l y$。对于一个二次幂数 $N$，和 $q &amp;gt; 0$，我们写 $R_q$ 来表示整数多项式环 $R_q = Z_q[X] / (X^N + 1)$。我们用粗体字母如 $\mathbf{a}$、$\mathbf{M}$ 来表示向量和矩阵，用 $a[j]$ 来表示向量 $\mathbf{a}$ 的第 $j$ 个分量，用 $M[j, i]$ 来表示矩阵 $\mathbf{M}$ 的 $(j, i)$ 项。Hadamard 乘积表示为 $\mathbf{a} \odot \mathbf{b}$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;B. 加密原语&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1.&lt;strong&gt;加法秘密共享&lt;/strong&gt;：本文中我们使用 2-out-of-2 加法秘密共享方案，基于环 $Z_{2^l}$。一个 $l$ 位（$l \geq 2$）的值 $x$ 被加法共享为 $JxK_0$ 和 $JxK_1$，其中 $JxK_l$ 是由参与方 $P_l$ 持有的 $x$ 的随机份额。我们可以将两个共享值的乘法写作：
&lt;/p&gt;
$$
JxK⋅JyK≡l(JxK0+JxK1)⋅(JyK0+JyK1)≡lJxK0JyK0+JxK1JyK1+JxK0JyK1+JxK1JyK0JxK \cdot JyK \equiv_l (JxK_0 + JxK_1) \cdot (JyK_0 + JyK_1)\equiv_l JxK_0JyK_0 + JxK_1JyK_1 + JxK_0JyK_1 + JxK_1JyK_0
$$&lt;p&gt;
其中，混合项 $JxK_0 JyK_1$ 和 $JxK_1 JyK_0$ 是通过同态加密计算的。对于一个实值 $x ̃ \in \mathbb{R}$，我们首先将其编码为一个定点值 $x = \lfloor x ̃ 2^f \rfloor \in [-2^{l-1}, 2^{l-1})$，并在指定精度 $f &amp;gt; 0$ 下进行秘密共享。我们用 $J·; f K$ 明确表示 $f$ 位定点精度的共享值。当 $l = 1$ 时，我们使用 $JzK_B$ 表示布尔共享。此外，当所有权与上下文无关时，我们省略下标，仅写 $JxK$ 或 $JzK_B$。&lt;/p&gt;
&lt;p&gt;2.&lt;strong&gt;遗忘传输&lt;/strong&gt;：我们依赖于遗忘传输（OT）来进行非线性计算。在一般的 1-out-of-2 OT 协议（$2^1$-OT）中，发送方输入两个长度为 $l$ 位的消息 $m_0$ 和 $m_1$，接收方输入一个选择位 $c \in {0, 1}$。协议结束时，接收方学习到消息 $m_c$，而发送方则什么也不知道。当发送方的消息相关时，相关 OT（COT）在通信上更为高效 [6]。在我们的加法 COT 中，发送方输入一个函数 $f(x) = x + \Delta$，其中 $\Delta \in Z_{2^l}$，接收方输入选择位 $c$。协议结束时，发送方学到 $x \in Z_{2^l}$，而接收方学到 $x + c \cdot \Delta \in Z_{2^l}$。在本工作中，我们使用 Ferret 协议 [77] 来实现低通信量的 COT。&lt;/p&gt;
&lt;p&gt;3.&lt;strong&gt;基于格的加法同态加密&lt;/strong&gt;：同态加密（HE）允许在不知晓解密密钥的情况下，计算函数 $F(x)$ 的加密结果。在本研究中，我们使用基于环学习与误差（RLWE）的同态加密方案 [52]。RLWE方案由一组公共参数 $HE.pp = {N, q, t}$ 定义。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;KeyGen&lt;/strong&gt;：生成RLWE密钥对 $(sk, pk)$，其中私钥 $sk \in R_q$，公钥 $pk \in R_{q^2}$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加密&lt;/strong&gt;：RLWE密文表示为多项式元组 $(\hat{b}, \hat{a}) \in R_{q^2}$。我们使用 $RLWE_{q,t}  pk(\hat{m})$ 来表示在公钥 $pk$ 下加密的 $m \in R_t$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加法 (⊞)&lt;/strong&gt;：给定两个RLWE密文 $ct_0 = (\hat{b}_0, \hat{a}_0)$ 和 $ct_1 = (\hat{b}_1, \hat{a}_1)$，它们分别加密了 $m_0, m_1 \in R_t$，并使用相同的密钥，则操作 $ct_0 \oplus ct_1$ 计算 RLWE 元组 $(\hat{b}_0 + \hat{b}_1, \hat{a}_0 + \hat{a}_1) \in R_q^2$，并且该元组可以解密为 $\hat{m}_0 + \hat{m}_1 \mod R_t$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;乘法 (⊠)&lt;/strong&gt;：给定一个RLWE密文 $ct = (\hat{b}, \hat{a})$，它加密了 $m \in R_t$，以及一个普通多项式 $\hat{c} \in R_t$，则操作 $ct \otimes \hat{c}$ 计算元组 $(\hat{b} \cdot \hat{c}, \hat{a} \cdot \hat{c}) \in R_{q^2}$，并且该元组可以解密为$ \hat{m} \cdot \hat{c} \mod R_t$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SIMD 编码&lt;/strong&gt;：通过选择一个素数 $t$，使得 $t \equiv 1 \mod 2N$，SIMD 技术 [66] 允许将 $v, u \in Z_t^N$ 的 N 个元素向量转换为多项式 $\hat{v}, \hat{u} \in R_t$。多项式 $\hat{v} \cdot \hat{u}$ 可以解码为按点相乘 $u \odot v \mod t$。在私有评估的上下文中，SIMD 技术可以通过一个因子 $1/N$ 来摊销按点相乘的成本。我们用 $\hat{v} := SIMD(v)$ 来表示SIMD编码，并用 $SIMD^{-1}(\cdot)$ 表示解码函数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自同态变换&lt;/strong&gt;：给定一个RLWE密文 $ct \in R_q^2$，它加密了一个多项式 $\hat{m}(X) \in R_t$，以及一个奇数 $g \in [1, 2N)$，操作 $ct&amp;rsquo; := Auto(ct, g)$ 计算新的密文 $ct&amp;rsquo; \in R_q^2$，解密后得到 $m&amp;rsquo;(X) = \hat{m}(X^g) \in R_t$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;C. &lt;strong&gt;基于Transformer的模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;许多现代语言预处理器，如GPT [15] 和 BERT [19]，由一个输入嵌入层和多个Transformer层组成 [70]。类似地，视觉Transformer（ViTs）[22], [5] 也采用了类似的架构，不过它们没有输入嵌入层。基于Transformer的模型主要有两个计算模块：多头注意力机制和前馈神经网络。此外，层归一化（Layer Normalization）用于两个连续层之间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多头注意力&lt;/strong&gt;：一个注意力机制 $Attention(Q, K, V)$ 计算的是 $softmax(QK^\top + M)V$，它可以被描述为将一个查询 $Q$ 和一组键值对 $(K, V)$ 映射为一个加权和。这里，$Q, K, V$ 是输入矩阵的不同线性变换。多头注意力的变体计算 $H$ 个并行的注意力：$Attention(Q_j, K_j, V_j)$，其中 $j \in [H]$，然后将这些 $H$ 个结果矩阵拼接起来。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;层归一化&lt;/strong&gt;：对于一个向量 $x \in \mathbb{R}^d$，令 $\mu = \frac{1}{d} \sum_{j} x[j] \in \mathbb{R}$ 和 $\sigma = \sum_{j \in [d]} (x[j] - \mu)^2 \in \mathbb{R}$。层归一化表示为：&lt;/p&gt;
&lt;p&gt;$LayerNorm(x) = \gamma \cdot \frac{x - \mu}{\sigma} + \beta$&lt;/p&gt;
&lt;p&gt;其中 $\gamma, \beta \in \mathbb{R}$ 是两个超参数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;前馈神经网络&lt;/strong&gt;：前馈神经网络通常包括两个线性变换，并在它们之间应用激活函数，即：&lt;/p&gt;
&lt;p&gt;$FFN(X) = W_0 \cdot F(W_1 \cdot X)$&lt;/p&gt;
&lt;p&gt;其中，$F: \mathbb{R}^* \to \mathbb{R}^*$ 是一个激活函数，如 GeLU 或 SiLU。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;D. 威胁模型与私密推理&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;类似于之前的工作 [41], [55], [62], [36], [3]，我们针对一个静态且半诚实的概率多项式时间（PPT）对手，遵循理想/现实世界范式 [10]。也就是说，我们考虑一个计算能力有限的对手，在协议执行开始时腐化了其中一个方，并遵循协议规范，但试图学习有关诚实方输入的附加信息。BumbleBee 调用了几个小规模私密计算的子协议，概要见表 I。为了简化协议描述和安全性证明，我们使用混合模型描述 BumbleBee。一个调用功能 $F$ 的协议称为在“$F$-混合模型”中。此外，BumbleBee 中某些函数为了更好的效率进行了近似计算。根据定义 [25]，如果一个协议的近似计算揭示的输入信息不比 $F$ 本身更多，则该协议构成了 $F$ 的私密近似。&lt;/p&gt;
&lt;p&gt;在私密 2PC 推理（图 2）中，服务器 $S$ 持有一个 transformer 模型，而客户端 $C$ 向该模型发送查询，如一段文本。假设 $S$ 和 $C$ 均为半诚实模型，BumbleBee 使得 $C$ 只能得知两项信息：transformer 的架构和推理结果。$S$ 要么可以得知结果，要么什么都不能得知，具体取决于应用场景。关于 $C$ 的私密输入和 $S$ 的模型权重的所有其他信息应保持秘密。威胁模型的正式定义见附录。&lt;/p&gt;
&lt;h2 id=&#34;安全矩阵乘法&#34;&gt;安全矩阵乘法
&lt;/h2&gt;&lt;p&gt;在私有变换器推理中，我们需要两种类型的矩阵乘法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;共享矩阵与明文矩阵的乘法。例如，对于每个Transformer模块（参见图2），我们计算共享输入矩阵（由前一个模块计算得出）与服务器的明文权重矩阵之间的乘法。&lt;/li&gt;
&lt;li&gt;注意力机制内部两个秘密共享矩阵的乘法。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们提出了一种原语，称为&amp;quot;隐式线性变换&amp;quot;（Oblivious Linear Transformation，OLT），用于实现这两种矩阵乘法。我们将OLT描述为一种二方协议，分别从两个方获取两个私有矩阵Q和V，并生成它们之间的共享矩阵JQVK。通过OLT原语，我们可以使用一次OLT执行计算共享矩阵JQK和明文矩阵V之间的乘法。另一方面，对于两个共享矩阵的乘法，我们需要进行两次OLT。也就是说，我们通过两个OLT计算两个混合项JQK1 · JVK0和JQK0 · JVK1。&lt;/p&gt;
&lt;h3 id=&#34;基于同态加密he的olt现有方法&#34;&gt;基于同态加密（HE）的OLT现有方法
&lt;/h3&gt;&lt;p&gt;我们的出发点是许多研究中使用的基于同态加密（HE）的OLT方法，例如[54]、[36]、[33]（以下简称为KRDY风格）。KRDY需要两个函数：
πlhs : Zkw×mw  2l 7→ R2l 和 πrhs : Zmw ×nw  2l 7→ R2l，用于将矩阵编码为可以使用RLWE加密的多项式系数。假设 1 ≤ kw · mw · nw ≤ N，这两个函数定义如下：
qˆ := πlhs(Q) 和 vˆ := πrhs(V)，使得
(1) qˆ[0] = Q[i, j]，当 i = 0 且 j = 0 时，
qˆ[N + i · kw · mw − j] = −Q[i, j]，当 i = 0 且 j ∈ [mw]/{0} 时，
qˆ[i · kw · mw − j] = Q[i, j]，当 i &amp;gt; 0 且 j ∈ [mw] 时，
vˆ[k · mw + j] = V[j, k]，当 j ∈ [mw] 且 k ∈ [nw] 时。
qˆ 和 vˆ 的所有其他系数设置为0。
乘积多项式 qˆ · vˆ 直接给出结果矩阵 QV 的一些系数，如以下命题所示。&lt;/p&gt;
&lt;p&gt;命题 1. [54，改编] 假设 1 ≤ kw · mw · nw ≤ N。给定两个多项式 qˆ = πlhs(Q)，vˆ = πrhs(V) ∈ R2l，乘法 U ≡l Q · V 可以通过在环 R2l 上计算多项式的乘积 uˆ = qˆ · vˆ 来评估。即 U[i, k] = uˆ[i · mw · nw + k · mw]，对于所有 i ∈ [kw]，k ∈ [nw]。&lt;/p&gt;
&lt;p&gt;图3提供了πlhs和πrhs编码方法背后的基本思想的简单示意图。在OLT的背景下，我们让P0使用P0的密钥将同态加密的RLWEpk0(πlhs(Q))发送给P1。然后，P1可以通过单次同态乘法，即RLWEpk0(πlhs(Q))⊠πrhs(V)，同态地评估矩阵乘法 Q · V。为了将加密后的矩阵转换为算术共享，P0和P1接着共同调用FH2A功能（参见表I）。当矩阵形状 k·m·n &amp;gt; N 时，我们可以将其拆分成更小的子矩阵，并将KRDY应用于每对对应的子矩阵。我们将划分窗口表示为(kw, mw, nw)。&lt;/p&gt;
&lt;p&gt;在通信成本方面，KRDY需要交换 O(min(km / kwmw, mn / mwnw) + kn / kwnw) 个密文。与[55]、[62]中使用的其他基于HE的OLT方法相比，KRDY OLT不需要任何旋转（在HE中旋转操作非常昂贵），因此相对高效。然而，KRDY OLT的结果中有许多“无用”的系数。根据命题1，结果多项式uˆ中只有kwnw个系数是“有用的”，而N个系数中KRDY仍然需要传输整个uˆ的RLWE密文进行解密，这可能会浪费通信资源。为了提供一个例子，我们考虑一个在变换器模型中常用的维度集合，如k = 16，m = 768，n = 3072。在这种情况下，KRDY协议可能需要交换约25MB的密文。这可能是一个可观的通信开销，因为变换器模型通常包含数百个这样的超大规模矩阵。接下来的部分将详细讨论减少这种通信负担的方法。&lt;/p&gt;
&lt;h3 id=&#34;通过密文打包的首次尝试&#34;&gt;通过密文打包的首次尝试
&lt;/h3&gt;&lt;p&gt;我们的首次尝试是将PackLWEs [13]过程应用于KRDY，我们将其称为KRDY+。PackLWEs过程允许我们从多个RLWE密文中选择任意系数，并通过执行同态自同构将它们组合成一个单一的RLWE密文。在KRDY+的情况下，我们可以将密文数量从 O(kn/(kwnw)) 减少到 O(kn/N)，代价是 O(kn) 的同态自同构操作。再次考虑矩阵形状 (k, m, n) = (16, 768, 3072)。现在，KRDY+交换大约2.9 MB的密文，相较于KRDY的25 MB，这是一个显著的减少。然而，在HE中，自同构操作非常缓慢，实际上对于具有较大kn的矩阵，KRDY+使用的自同构可能比同态乘法本身更为计算密集。&lt;/p&gt;
&lt;h3 id=&#34;密文交织优化&#34;&gt;密文交织优化
&lt;/h3&gt;&lt;p&gt;我们在矩阵乘法的背景下提出了针对PackLWEs过程[13]的专门优化。其主要思路是，与其按照PackLWEs过程逐个“挑选”有用的系数，我们更倾向于“清理”结果密文中的无用系数，然后将它们合并成一个单一的密文。我们简要描述清理过程。让我们以 N = 8 和 aˆ(X) = ∑ᵢ=0⁷ aiXi 为例。根据定义，自同构Auto(ˆa, 9)生成 ∑ᵢ=0⁷ aiXi·9，这相当于 ∑ᵢ=0³ a2iX2i − ∑ᵢ=0³ a2i+1X2i+1 mod X8 + 1。在这里，奇数位置的系数符号被翻转。因此，评估 ˆa + Auto(ˆa, N + 1) 会消除奇数索引的系数，并将偶数索引的系数加倍。更一般地，令我们计算 ˆa + Auto(ˆa, N/2j + 1) 对于任何 N 的2的幂因子。再次以N = 8为例，我们考虑 2j = 2。Auto(ˆa, 8/2 + 1) 等于 ∑ᵢ=0⁷ aiXi·5 ≡ a0 − a2X2 + a4X4 − a6X6 + ∑ᵢ≠0 mod 2 aiXi·5。简而言之，2j倍数位置的系数符号被翻转。假设对于所有 i ≠ 0 mod 2j 的 ˆa[i] 系数已经为零。那么，ˆa + Auto(ˆa, N/2j + 1) 会消除2j的奇数倍位置的系数，同时加倍偶数倍位置的系数。我们定义一个函数 ZeroGap(ˆa, 2r)，它重复公式 r 次 ˆa := aˆ + Auto(ˆa, N/2j + 1) 对于 j = 0, 1, · · · , r − 1。执行这些操作后，结果是一个多项式，其中只有在2r倍数位置的系数是非零的。然而，这些系数被2r的倍数缩放。为了修正这个缩放，我们首先将多项式乘以逆缩放因子 2−r mod q。这隐式地要求使用奇数模数q，这在使用模数 q ≡ 1 mod 2N 的基于RLWE的HE中通常是满足的。此外，ZeroGap过程要求两个需要的系数之间的间隔是2的幂数。这等同于πlhs和πrhs编码中的分区窗口 mw。请注意，只要满足 1 ≤ kwmwnw ≤ N，可以自由选择这个分区窗口。&lt;/p&gt;
&lt;p&gt;有了ZeroGap过程，我们提出了我们的关键贡献之一，即InterLeave过程。InterLeave过程将2r个多项式交织成一个单一的多项式，输入系数以2r步的跨度排列。我们首先给出InterLeave的简单版本，如图4所示。在这个简单版本中，我们分别对每个输入多项式应用ZeroGap过程（步骤2到步骤5）来清理无关系数。然后，我们通过简单的旋转-求和计算（步骤6到步骤8）得到最终结果。我们想强调的是，密文域中的系数旋转可以比SIMD旋转[32]更高效地实现。事实上，图4计算了 O(r · 2r) 次自同构操作，将 N 个系数合并成一个多项式。图5给出了一个InterLeave（简单版本）的玩具示例。&lt;/p&gt;
&lt;p&gt;最终版本。我们现在展示图6中的优化版本，它将图4中的复杂度从 O(r · 2r) 次自同构操作降低到了 O(2r) 次自同构操作。我们将展示这个优化版本的直觉。直观上，利用自同构函数的双线性性质，将两个具有相同索引的自同构操作合并为一个自同构操作是合理的。即，对于任何有效的索引 g，两个自同构操作的和 Auto(ˆa, g) + Auto(ˆb, g) 等于对多项式和的自同构操作。然而，应用相同自同构索引的多项式在求和步骤（图4的步骤8）之前，需要先进行不同单位的右移（即图4的步骤6）。为了解决这个问题，我们利用以下方程。即，当 ˆb[i] = 0 对于所有 i ≠ 0 mod 2j 时，&lt;/p&gt;
$$
Auto(ˆa,N/2j+1)+Auto(ˆb,N/2j+1)⋅X2j=Auto(ˆa−ˆb⋅X2j,N/2j+1)Auto(ˆa, N/2j + 1) + Auto(ˆb, N/2j + 1) \cdot X^{2j} = Auto(ˆa - ˆb \cdot X^{2j}, N/2j + 1)
$$&lt;p&gt;
回想一下，自同构操作 Auto(·, N/2j + 1) 会翻转位于 2j 的奇数倍位置的系数，而保持偶数倍位置的系数不变。同时，通过乘以 $X^{2j}$ 对 ˆb 进行右移，只是改变了 ˆb 的系数位置的奇偶性——从 2j 的奇数倍位置移到偶数倍位置，反之亦然。因此，自同构-然后-右移的模式
&lt;/p&gt;
$$
Auto(ˆb,N/2j+1)⋅X2jAuto(ˆb, N/2j + 1) \cdot X^{2j}
$$&lt;p&gt;
等价于取反-右移-然后自同构的方式
&lt;/p&gt;
$$
Auto(−ˆb⋅X2j,N/2j+1)Auto(-ˆb \cdot X^{2j}, N/2j + 1)
$$&lt;p&gt;
这样，我们就可以利用双线性特性，将具有相同索引的两个自同构操作合并为一个自同构操作。&lt;/p&gt;
&lt;h3 id=&#34;提出的olt协议&#34;&gt;提出的OLT协议
&lt;/h3&gt;&lt;p&gt;我们现在在算法1中描述我们的OLT协议。前三个步骤与KRDY风格的协议 [54]，[36] 类似。具体来说，我们将一个大矩阵分割成子块，并使用 πlhs 和 πrhs 将每个块编码为多项式。然而，在我们的方案中，我们故意选择了一个2的幂次方的分区窗口 mw。我们协议与他们的主要区别在于，在步骤4中使用了 InterLeave 操作，以减少RLWE密文的数量，从 O(kn/(kwnw)) 降低到 O(kn/N )。解析交织多项式并构造结果矩阵的过程在附录的图8中进行了说明。为了在计算和通信开销之间取得良好的平衡，选择合适的分区窗口 mw 是至关重要的。较小的 mw 值可能会在步骤1中过度增加通信开销，而较大的 mw 值会导致步骤4中更多的同态自同构操作。给定矩阵形状 (k, m, n) 和分区窗口 (kw, mw, nw)，步骤1中发送的密文数量为 n1 := m′·min(k′, n′)。执行 InterLeave 所需的同态自同构操作总数为 n2 := ⌈k′n′/mw⌉mw。交织后，步骤5中发送的密文数量为 n3 := ⌈ k′n′ / mw ⌉。为了选择合适的分区窗口，我们可以通过最小化以下目标函数来优化：&lt;/p&gt;
&lt;p&gt;$\ argmin_{kw, mw, nw} ; PC \cdot n2 + PB \cdot (n1 + n3)$&lt;/p&gt;
&lt;p&gt;其中，mw 必须是2的幂，并且满足 $1 \leq kw \cdot mw \cdot nw \leq N$ 的约束。这里，PC 表示计算一个同态自同构操作的成本，PB 表示发送一个密文的成本。较小的 PC/PB 比率可能表明计算能力强大或带宽受限的情景。在这种情况下，我们倾向于选择较大的 mw。相反，如果带宽充足，可以选择较小的 mw 值，以减轻密文交织所需的时间。&lt;/p&gt;
&lt;p&gt;复杂度。根据我们的经验结果，选择 mw ≈ √N 是一个可行的选择。然后，算法1中的协议需要大约 O(kn/√N) 次同态自同构操作用于密文交织。相比之下，KRDY+ 基准协议需要 O(kn) 次同态自同构操作。举个例子，设 mw = 26 且 N = 213。在这种设置下，我们协议的密文压缩时间大约是 KRDY+ 密文压缩时间的 1/26 ≈ 1.6%。KRDY+ 和我们协议的通信开销都是 O(kn/N) 个密文。请注意，BOLT 的主模OLT可能根据矩阵维度 [58] 需要更少的同态自同构操作 O(pk²m²n/N²)。&lt;/p&gt;
&lt;p&gt;安全性。我们仅增加了一个密文压缩步骤来减少通信开销，我们发送的信息是 KRDY 协议的子集，因此我们的安全级别与其相同。定理1的证明参考了 [54]，[36]。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法 1 提议的不可知线性变换 \( $ \Pi_{OLT} $\)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入：&lt;/strong&gt;&lt;br&gt;
发送方 \( S \)：&lt;br&gt;
&lt;/p&gt;
$$ Q \in \mathbb{Z}^{k \times m} $$$$ 2^l $$&lt;p&gt; 和 \( sk \)。&lt;br&gt;
接收方 \( R \)：&lt;br&gt;
&lt;/p&gt;
$$ V \in \mathbb{Z}^{m \times n} $$$$ 2^l $$&lt;p&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出：&lt;/strong&gt;&lt;br&gt;
&lt;/p&gt;
$$ JUK $$&lt;p&gt;，使得&lt;br&gt;
&lt;/p&gt;
$$ U \equiv_l Q \cdot V $$&lt;p&gt;&lt;strong&gt;公共参数：&lt;/strong&gt;&lt;br&gt;
&lt;/p&gt;
$$ pp = (\text{HE.pp}, pk, (kw, mw, nw)) $$&lt;ul&gt;
&lt;li&gt;大小 \( mw \) 是一个 2 的幂，且&lt;br&gt;
$$ 1 \leq kw \cdot mw \cdot nw \leq N $$&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
$$ k&#39; = \lceil \frac{k}{kw} \rceil $$&lt;p&gt;，&lt;br&gt;
&lt;/p&gt;
$$ m&#39; = \lceil \frac{m}{mw} \rceil $$&lt;p&gt;，&lt;br&gt;
&lt;/p&gt;
$$ n&#39; = \lceil \frac{n}{nw} \rceil $$&lt;p&gt;，和&lt;br&gt;
&lt;/p&gt;
$$ \tilde{m} = \lceil \frac{k&#39; \cdot n&#39;}{mw} \rceil $$&lt;ul&gt;
&lt;li&gt;注意：如果&lt;br&gt;
$$ k&#39; &gt; n&#39; $$，则交换发送方和接收方的角色。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;步骤：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;发送方 \( S \) 首先将矩阵 \( Q \) 划分为块矩阵&lt;br&gt;
&lt;/p&gt;
$$ Q_{\alpha, \beta} \in \mathbb{Z}^{kw \times mw} $$&lt;p&gt; \( 2^l \)。然后，\( S \) 对每个块矩阵编码为多项式&lt;br&gt;
&lt;/p&gt;
$$ \hat{q}_{\alpha, \beta} := \pi_{\text{lhs}}(Q_{\alpha, \beta}) $$&lt;p&gt;&lt;br&gt;
其中 \( \alpha \in [k&#39;] \) 且 \( \beta \in [m&#39;] \)。之后，\( S \) 将每个编码后的密文&lt;br&gt;
&lt;/p&gt;
$$ ct&#39;_{\alpha, \beta} := \text{RLWE}_{q, 2l}^{pk} (\hat{q}_{\alpha, \beta}) $$&lt;p&gt;&lt;br&gt;
发送给 \( R \)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接收方 \( R \) 首先将矩阵 \( V \) 划分为块矩阵&lt;br&gt;
&lt;/p&gt;
$$ V_{\beta, \gamma} \in \mathbb{Z}^{mw \times nw} $$&lt;p&gt; \( 2^l \)。然后，\( R \) 对每个块矩阵编码为多项式&lt;br&gt;
&lt;/p&gt;
$$ \hat{v}_{\beta, \gamma} := \pi_{\text{rhs}}(V_{\beta, \gamma}) $$&lt;p&gt;&lt;br&gt;
其中 \( \beta \in [m&#39;] \) 且 \( \gamma \in [n&#39;] \)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在接收到来自 \( S \) 的&lt;br&gt;
&lt;/p&gt;
$$ \{ct&#39;_{\alpha, \beta}\} $$&lt;p&gt; 后，\( R \) 计算一个 RLWE 密文向量 \( c \)，其中&lt;br&gt;
&lt;/p&gt;
$$ c[\alpha \cdot n&#39; + \gamma] := \bigoplus_{\beta \in [m&#39;]} ct&#39;_{\alpha, \beta} \odot \hat{v}_{\beta, \alpha} $$&lt;p&gt;&lt;br&gt;
对于 \( \alpha \in [k&#39;] \)，\( \gamma \in [n&#39;] \)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为了将 \( k&#39; \cdot n&#39; \) 个密文压缩为 \( \tilde{m} \) 个密文而不触及需要的系数，\( R \) 在密文向量 \( c \) 的子向量上运行 InterLeave 操作。例如&lt;br&gt;
&lt;/p&gt;
$$ \tilde{c}[\theta] := \text{InterLeave}([c[\theta \cdot mw], c[\theta \cdot mw + 1], \cdots \mid \{z\} mw]) $$&lt;p&gt;&lt;br&gt;
对于 \( \theta \in [\tilde{m}] \)。&lt;br&gt;
▷ 当 \( k&#39; \cdot n&#39; \) 无法被 \( mw \) 整除时，填充零。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于每个密文 \( \tilde{c}[i] \) 中的密文，运行&lt;br&gt;
&lt;/p&gt;
$$ \hat{c}_{i, 0}, \hat{c}_{i, 1} \leftarrow \text{FH2A}(\tilde{c}[i]) $$&lt;p&gt;&lt;br&gt;
其中 \( S \) 获得 \( \hat{c}_{i, 0} \in R^{2l} \)，\( R \) 获得 \( \hat{c}_{i, 1} \in R^{2l} \)。之后，\( S \) 和 \( R \) 都可以通过附录中的本地过程&lt;br&gt;
&lt;/p&gt;
$$ JUK_l := \text{ParseMat}(\hat{c}_{0, l}, \hat{c}_{1, l}, \cdots) $$&lt;p&gt;&lt;br&gt;
来获取各自的分享。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;定理 1. 算法 1 中的协议 ΠOLT 在 FH2A 混合模型下，能够在半诚实对手存在的情况下，私密地实现 OLT 功能。&lt;/p&gt;
&lt;p&gt;E. 进一步优化&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;批量矩阵乘法：回顾一下，变换器模型中的每个多头注意力涉及 H &amp;gt; 1 个并行的矩阵乘法，例如，Qj · Kj，其中 j ∈ [H]。当这些结果矩阵较小时，即 |Qj · Kj| ≤ N/2 时，我们倾向于应用 InterLeave 对批量乘法的密文进行压缩。简而言之，批量乘法的 O(H) 个 RLWE 密文将进一步打包成 O((H · kn)/N) 个密文。&lt;/li&gt;
&lt;li&gt;动态压缩策略：我们采用一种动态的密文压缩策略，使我们能够在减少通信成本和增加计算成本之间取得平衡。具体来说，当只有一个 RLWE 密文需要发送时，即 ⌈k/kw⌉ · ⌈n/nw⌉ = 1，我们不使用任何密文压缩。另外，当 kn ≪ N 时，我们使用 PackLWEs 而不是 InterLeave，因为前者在小规模情况下可以运行得更快。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;基于-rlwe-的更快批量-ole&#34;&gt;基于 RLWE 的更快批量 OLE
&lt;/h2&gt;&lt;p&gt;除了矩阵乘法，变换器模型中还需要标量乘法（或按批次处理的 Hadamard 乘积）。我们使用符号 Batch OLE（bOLE）[17] 来描述一个二方计算协议，该协议接收一个发送方 S 的向量 x 和接收方 R 的向量 y，并生成它们 Hadamard 乘积的秘密共享 Jx ⊙ yK。在私有推理的背景下，我们证明了带误差的 bOLE 变体（bOLEe）是足够的。我们可以使用更小的 RLWE 参数构建更高效的 bOLEe 协议。我们的 bOLEe 可能会在最终输出中引入最低有效位（LSB）误差。然而，由于在多方计算（MPC）中使用了定点表示，这些 LSB 误差将通过后续的截断被移除，因此我们的方法不会影响整体准确性。与 [63] 中基于 RLWE 的 bOLE 类似，我们还应用了 SIMD 技术 [66] 来实现 bOLE，但具有更好的摊销效率。在 [63] 中，发送方 S 对其私有输入 x ∈ ZN 2l 进行预处理为 SIMD(x)，并将密文 RLWEq,t S (SIMD(x)) 发送给接收方 R。然后，R 回应密文 RLWEq,t S (SIMD(x)) ⊠ SIMD(y) ⊞ SIMD(r) 给 S，r 是一个掩码向量，r ∈ ZN 。实际上，这里的算术运算 x⊙y+r 是在模 t 下进行的，尽管 x 和 y 中的值来自环 Z2l 。由于模 t 与 2l 不可整除，掩码向量 r 应从更大的环中采样以提供统计安全性。具体来说，[63] 从 ZN 22l+σ 中采样 r，以提供 σ 位的统计安全性。此外，为了防止加法溢出，他们需要设置一个较大的明文模 t &amp;gt; 22l+σ+1。我们展示了如何避免这种额外的 σ 位开销，代价是引入 1 位的 LSB 误差。基本上，我们使用提升函数在质数明文模 t 上引入一个中间层的 2l 模。
Lift(x) : ZN 2l → ZN t 通过 ⌊ t / 2l · x⌉ mod t
Down(y) : ZN t → ZN 2l 通过 ⌊ 2l / t · y⌉ mod 2l&lt;/p&gt;
&lt;p&gt;命题 2. 如果 t &amp;gt; 22l，则对于任意 x, y ∈ Z2l，有 Down(Lift(x)·y mod t) ≡l x·y。也就是说，我们可以在质数模 t 上进行模 2l 运算。&lt;/p&gt;
&lt;p&gt;证明. 只需证明误差项可以被舍入为零。Lift(x) · y mod t 可以写为 t / 2l · ((x · y mod 2l) + re · y，其中 re 是舍入误差，|re| ≤ 1/2。然后，误差项被舍入为 ⌊ 2l / t · (re · y)⌉ = 0，当 t &amp;gt; 22l 且 y &amp;lt; 2l 时。
现在，我们可以使用信息学随机掩码 r ∈ ZtN 代替统计掩码。结果中可能会引入 1 位误差。&lt;/p&gt;
&lt;p&gt;命题 3. 设 u′ := Down(Lift(x) · y − r mod t)，v′ := Down(r)，其中 x, y ∈ Z2l 且 r ∈ Zt。如果 t &amp;gt; 22l 且 r 在 Zt 上均匀分布，则 u′ + v′ ≡l x · y + e，其中 e ∈ {0, ±1}。&lt;/p&gt;
&lt;p&gt;算法 2 带误差的 bOLE 协议 ΠbOLEe&lt;/p&gt;
&lt;p&gt;输入：
发送方 S：x ∈ ZN 2l ，私钥 sk。
接收方 R：y ∈ ZN 2l 。
公共参数 pp = {N, t}，其中 t = 1 mod 2N 为质数，且 t &amp;gt; 22l，以及公共密钥 pk。&lt;/p&gt;
&lt;p&gt;输出：
JzK ∈ ZN 2l，使得 ∥z − x ⊙ y mod 2l∥∞ ≤ 1。&lt;/p&gt;
&lt;p&gt;步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;S 将 RLWEq,t pk (xˆ) 发送给 R，其中 xˆ := SIMD(Lift(x))。&lt;/li&gt;
&lt;li&gt;R 计算 yˆ := SIMD(y)。&lt;/li&gt;
&lt;li&gt;收到密文 RLWEq,t sk (xˆ) 后，R 计算 ct := RLWEq,t pk (xˆ) ⊠ yˆ。&lt;/li&gt;
&lt;li&gt;JuˆK ← FH2A(ct) 将其转换为算术共享。假设 S 的共享为 JuˆK0 ∈ Rt，R 的共享为 JuˆK1 ∈ Rt。请注意，我们让 FH2A 函数来捕捉电路隐私（参见备注 1）。&lt;/li&gt;
&lt;li&gt;S 输出 Down(SIMD−1(JuˆK0))。&lt;/li&gt;
&lt;li&gt;R 输出 Down(SIMD−1(JuˆK1))。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;该证明基本遵循 [36, 完整版本，附录 C] 中的类似论证。&lt;/p&gt;
&lt;p&gt;定理 2. 算法 2 中的协议 ΠbOLEe 在 FH2A 混合模型下，能够在半诚实对手存在的情况下，私密地实现 bOLEe 功能（参见表 I）。
定理 2 的正确性简单地遵循 SIMD 打包和命题 3。对于具体的改进，我们的 bOLEe 协议在 l = 64 时需要 t ≈ 2128，而 [63] 的方法需要更大的 t ≈ 2168。我们的实证结果表明，我们的协议比他们的协议快约 1.3 倍。&lt;/p&gt;
&lt;h2 id=&#34;激活函数协议&#34;&gt;激活函数协议
&lt;/h2&gt;&lt;p&gt;我们首先描述用于 GeLU 函数的协议。本节中描述的方法和优化也可以应用于其他函数，如 SiLU 和 ELU（见附录 D）。&lt;/p&gt;
&lt;p&gt;A. 高斯误差线性单元（GeLU）&lt;/p&gt;
&lt;p&gt;GeLU 函数的常见定义是：&lt;/p&gt;
&lt;p&gt;$GeLU(x)=0.5x(1+tanh⁡(2π(x+0.044715x3))).\text{GeLU}(x) = 0.5x(1 + \tanh\left(\frac{\sqrt{2}}{\pi}(x + 0.044715x^3)\right)).$&lt;/p&gt;
&lt;p&gt;遵循现有的研究工作 [58]、[49]、[24]、[21]，我们使用分段函数来近似 GeLU 函数，如下所示：
&lt;/p&gt;
$$
\text{Seg4GeLU}(x) = 
\begin{cases}
-\epsilon &amp; \text{if } x &lt; -5 \\
P_3(x) &amp; \text{if } -5 \leq x \leq -1.97 \\
P_6(x) &amp; \text{if } -1.97 \leq x \leq 3 \\
x - \epsilon &amp; \text{if } x &gt; 3
\end{cases}
$$&lt;p&gt;
其中 $P_b(x)$ 是近似 GeLU 函数的 b 次多项式，定义在一个较短的区间上。例如，设定 $\epsilon = 10^{-5}$，我们绘制了 Seg4GeLU 如图 7a 所示。从图中可以看到，Seg4GeLU 很好地近似了 GeLU 函数。&lt;/p&gt;
&lt;p&gt;我们在算法 3 中的 GeLU 协议基本上遵循（2），使用 Fless 和 Fmux 进行分支选择。为了进一步提高效率，我们讨论并引入了三个独立的优化，这些优化在之前的研究工作 [58]、[49]、[24]、[21] 中未提及。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;近似分支选择&lt;/strong&gt;：第一个优化利用了激活函数的平滑性。具体来说，我们首先找到近似多项式 $P_3(x)$ 和 $P_6(x)$，使得 $P_3(x) \approx P_6(x)$，特别是在枢轴点附近，例如在我们的案例中，$x = -1.97$。公式（2）中的选择可以通过一个小于协议 $1{x &amp;lt; y}$ 来私密地实现，其中 $x, y \in Z_{2l}$。基于这些观察，我们建议计算小于位时忽略一些低位 f′ 位，以减少通信开销。这是一个常见的优化方法，用于在固定点值的秘密共享上执行比较。然而，确保正确的近似多项式至关重要，因为超出枢轴点的波动可能会破坏近似。经验表明，近似的分段选择有助于将算法 3 中 GeLU 协议的通信开销减少 5%。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;批量（近似）分支选择&lt;/strong&gt;：我们的第二个优化是针对 [46] 和 [36] 中基于 OT 的 MSB 协议。该协议使用公式 $MSB(JxK_0) \oplus MSB(JxK_1) \oplus 1{(JxK_0 \mod 2^{l-1}) + (JxK_1 \mod 2^{l-1}) \geq 2^{l-1}}$ 计算算术共享 $JxK$ 的 MSB，其中最后一位的计算需要一个 $M_1$-OT2。因此，对于公式（2）中的分支选择，我们需要 3 次 $M_1$-OT2 调用。请注意，这些“批量”比较是通过一个秘密共享和多个明文阈值进行的。通过这种安排，我们可以将 3 次 $M_1$-OT2 调用合并为一次 $M_1$-OT6，即在 6 位消息上进行 1-of-M OT。尽管这种 OT 组合可能不会显著减少通信开销，但根据我们的实验，它可以将 GeLU 的计算时间减少 35%。这是因为在使用 Ferret OT 时，调用一次 $M_1$-OT2 和调用一次 $M_1$-OT6 的运行时间相似。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优化多项式评估&lt;/strong&gt;：对于公式（2）中的 $P_3(x)$ 和 $P_6(x)$ 的评估，我们建议使用更快的平方协议 $\Pi_{\text{square}}$ 来计算所有偶次幂项，如 $x^2$、$x^4$ 和 $x^6$。实际上，在 2PC 中，执行平方操作的成本是标准乘法操作的一半。此外，我们还提出通过减少一半的通信成本来优化奇次幂项的计算。举个例子，考虑给定 $JxK$ 和 $Jx^2K$ 时，如何计算 $Jx^3K$。我们可以使用两次 bOLE 调用来计算 $Jx^3K$，即 $FbOLE(JxK_0, Jx^2K_1)$ 和 $FbOLE(Jx^2K_0, JxK_1)$。对于算法 2 中的 bOLE 构造，P0 将 SIMD(Lift(JxK_0)) 和 SIMD(Lift(Jx^2K_0)) 的密文发送给 P1。我们注意到，当计算 $Jx^2K$ 和 $Jx^4K$ 时，这两个密文已经被发送给 P1。因此，为了计算立方项，玩家可以跳过算法 2 中的步骤 1，按照其余步骤相同地执行。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;权衡。在我们的评估中，我们证明了这三种优化可以使时间减少35%，通信减少7%。需要注意的是，这些改进伴随着近似误差的增加。具体来说，我们的GeLU评估的平均ULP误差约为11，而[58]的平均ULP误差为4。此外，我们在四个transformer模型中测试了提出的GeLU协议，结果表明推理精度的降级非常小，不超过1%。我们认为这是精度和效率之间的合理权衡。&lt;/p&gt;
&lt;p&gt;B. &lt;strong&gt;Softmax&lt;/strong&gt;
为了数值稳定性[29，第4章]，softmax函数通常计算为：
&lt;/p&gt;
$$
softmax(x)[i]=exp⁡(x[i]−xˉ)∑jexp⁡(x[j]−xˉ),\text{softmax}(x)[i] = \frac{\exp(x[i] - \bar{x})}{\sum_j \exp(x[j] - \bar{x})},
$$&lt;p&gt;
其中 $\bar{x}$ 是输入向量 $x$ 的最大元素。对于二维矩阵，我们对其每一行向量应用公式(3)。值得注意的是，公式(3)中的所有指数操作的输入都是负数。我们利用负操作数来加速私有softmax。具体来说，我们通过简单的裁剪分支来近似指数操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 $x \in [T_{\text{exp}}, 0]$，我们有 $\exp(x) \approx 1 + \frac{x}{2^n}$；&lt;/li&gt;
&lt;li&gt;对于 $x &amp;lt; T_{\text{exp}}$，则 $\exp(x) \approx 0$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于裁剪范围 $T_{\text{exp}}$，我们简单地设置 $T_{\text{exp}}$ 使得 $\exp(T_{\text{exp}}) \approx 2^{-f}$，其中 $f$ 是定点精度。假设我们设置 $f = 18$，那么我们设置 $T_{\text{exp}} = -13$，因为 $\exp(-13) &amp;lt; 2^{-18}$。当 $T_{\text{exp}}$ 固定后，我们可以经验性地设置泰勒展开的次数 $n$。例如，在我们的实验中，对于 $T_{\text{exp}} = -13$，我们设置 $n = 6$，以使平均误差在 $2^{-10}$ 之内。此外，我们还使用上一节提出的近似“小于”操作进行分支选择，因为负输入的指数操作也是平滑的。通过调用 $F_{\text{tnrunc}}$ 可以实现除以 $2^n$，而 $2^n$ 的幂运算是通过一系列的 $\Pi_{\text{square}}$ 来计算的。&lt;/p&gt;
&lt;p&gt;C. &lt;strong&gt;混合位宽评估&lt;/strong&gt;
在运行我们的激活协议之前，我们可以先将共享值切换到较小的环 $l&amp;rsquo; &amp;lt; l$ 以节省通信开销。大环 $Z_{2^l}$ 到小环 $Z_{2^{l&amp;rsquo;}}$ 的共享转换可以通过局部设置 $JxK_l \mod 2^{l&amp;rsquo;}$ 来完成。反方向的转换，我们结合了[61]中的环扩展协议和[16]中的启发式优化。在我们的实现中，每次从 $Z_{2^{l&amp;rsquo;}}$ 到 $Z_{2^l}$ 的转换将在两轮中交换大约 $O(2^{(l - l&amp;rsquo;)})$ 位。这个混合位宽评估的主要问题是乘法引起的溢出。例如，双精度定点 $2^f$ 可能已经大于 $l&amp;rsquo;$。为了解决这个问题，我们必须做出两个妥协：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用额外一次对 $F_{\text{trunc}}$ 的调用来降低激活函数中的定点精度；&lt;/li&gt;
&lt;li&gt;缩小近似区间或增加分支数，以便可以使用较低次数的多项式。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最近的工作[58]在狭窄区间内使用低次多项式近似GeLU，即
&lt;/p&gt;
$$
GeLU(x)≈P4(x)=0.5x+∑i=04ci∣x∣iforx∈[−2.7,2.7].\text{GeLU}(x) \approx P_4(x) = 0.5x + \sum_{i=0}^{4} c_i |x|^i \quad \text{for} \quad x \in [-2.7, 2.7].
$$&lt;p&gt;
在我们的一些实验中，我们对GeLU应用了这个 $P_4(x)$，但对于SiLU函数，我们仍然需要使用两个多项式。&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;&lt;strong&gt;RELATED WORK&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Low Communication OLT&lt;/strong&gt;
许多利用同态SIMD [66] 技术的工作可以直接用于OLT功能，并具有相对较小的通信开销，例如 [32]、[12]、[39]、[35] 等。SIMD技术要求使用质数明文模数 $t$，而不是秘密共享中使用的 $2^l$ 模数。可以利用中国剩余定理接受来自 $Z_{2^l}$ 的秘密共享，但这会使得同态加密侧的计算和通信开销大大增加。另一种构建低通信OLT的策略是使用向量隐式线性评估（VOLE）[9]、[77]、[7]，正如CipherGPT [34]中提出的。然而，这种基于VOLE的方法要求较大的矩阵维度 $k \approx 10^7$ 才能实现低（摊销）通信，因此只适用于自回归变换器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Non-linear Functions&lt;/strong&gt;
对于softmax函数，[45]、[68] 也使用泰勒级数 $(1 + x/2^n)^{2^n}$ 来近似指数运算，只是他们没有应用范围裁剪。例如，CryptGPU根据对一些数据集的检查经验性地设置了 $n = 9$。SiRNN [61] 使用查找表进行初步猜测，并通过几次牛顿迭代进行修正。这些方法提供了精确的指数运算，但在2PC中计算开销较大。[80] 使用另一种数值方法近似softmax函数。然而，它可能需要超过64轮乘法，这使得它在通信上非常密集。Kelkar等人[42]提出了一种新的2PC指数协议，但由于失败概率，它的局限性在于必须使用较大的环 $l \approx 128$ 或严格约束输入域（例如，$|x| \leq 5$）。[49] 通过加扰电路[78]使用12个一次多项式来近似激活函数。最近的研究方法也[20]、[21]、[24]、[58] 使用多个低次多项式来近似GeLU函数。然而，这些方法都没有考虑GeLU函数的平滑性以及批量比较来减少分支选择的开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Private Transformer Inference (2PC)&lt;/strong&gt;
Iron [33]、BOLT [58] 和 CipherGPT [34] 是为变换器设计的2PC推理框架。这三个框架都大量复用了SiRNN框架 [61]、[23] 中基于OT的协议来评估激活函数。BOLT [58] 被认为是安全的双方变换器推理的最先进框架。BumbleBee与BOLT共享一些设计元素，例如非线性函数的分段近似和使用同态加密（HE）的私有矩阵乘法。然而，BumbleBee在通信效率方面超越了BOLT（见图1）。BumbleBee优于BOLT的主要原因总结如下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;共享类型一致的乘法&lt;/strong&gt;
BOLT的矩阵乘法协议在质数模数$p$上运行，而他们的非线性协议则在环模数$2^l$上运行。在私有推理过程中，它们必须在不同的共享类型之间来回切换。为了集成环到质数的转换（例如，参考[42]），BOLT需要一种特殊的共享乘法，其中输出的位宽可以大于输入的位宽。具体来说，BOLT复用了EzPC框架[23]、[61]中的基于OT的协议来实现这种不均匀的共享乘法。相比之下，在BumbleBee中，矩阵乘法协议和共享乘法协议都在环模数$2^l$上运行。这些共享类型一致的乘法使我们能够基于HE构建更高效的共享乘法协议。现有的工作如[63]已经表明，基于HE的共享乘法协议在通信方面比基于OT的协议高效5到6倍。需要注意的是，我们进一步优化了[63]的通信开销，提升了1.3倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;平方操作的成本是标准乘法的一半&lt;/strong&gt;
为了私密评估分段函数，我们需要对秘密共享输入$JxK$评估低次多项式。BOLT采用霍纳法则来评估多项式。例如，BOLT评估一个四次多项式如下：&lt;/p&gt;
&lt;p&gt;$P4(JxK)=(((((a4⋅JxK+a3)⋅JxK)+a2)⋅JxK)+a1)⋅JxK+a0,P_4(JxK) = (((((a_4 \cdot JxK + a_3) \cdot JxK) + a_2) \cdot JxK) + a_1) \cdot JxK + a_0$,&lt;/p&gt;
&lt;p&gt;这需要3次标准的共享乘法。BOLT通过Motzkin多项式预处理将标准乘法的次数减少到2次。相比之下，在BumbleBee中，我们倾向于利用平方操作，因为执行平方操作的成本是标准乘法操作的一半。例如，我们需要两个平方操作（用于二次项和四次项）以及“一半”的标准乘法（用于三次项）来评估四次多项式，更不用说更高效的共享乘法协议了。我们评估低次多项式的方式也比BOLT使用的霍纳法则更高效，尤其是在私有计算的背景下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lower Communication OT&lt;/strong&gt;
最后一个优势来自于OT协议的具体选择。具体来说，BOLT选择使用IKNP OT [38]作为其基础OT协议，而我们则利用Ferret OT [77]。Ferret OT的通信开销比IKNP OT小，但代价是更多的本地计算。我们在工程上进行了努力，将Ferret OT集成到多核CPU中，以最小化其运行时间，包括跨多个线程的同步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;从上述总结来看&lt;/strong&gt;，我们强调了支持环模数的矩阵乘法协议的重要性。正是我们乘法协议中的共享类型一致性，使得BumbleBee能够显著超越BOLT。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;私有变换器推理 ((2+1)-PC)&lt;/strong&gt;
最近的研究，如[48]、[31]、[67]、[30]，已经探讨了双方（例如S和C）可以访问受信任第三方（TTP）帮助的场景。这些方法通常将私有计算过程分为两个独立的阶段：初步预处理阶段，随后是评估阶段。值得注意的是，在预处理阶段，TTP负责生成并分发双方随后在评估阶段使用的所有相关随机性。参照[67]的命名，我们将这种TTP辅助的计算过程称为(2+1)-PC，以便与我们的2PC设置清晰区分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;私有变换器推理 (3PC)&lt;/strong&gt;
PrivFormer [3] 和 PUMA [21] 是为变换器设计的两种基于三方设置 [4]、[56] 的私有推理框架。具体来说，PrivFormer通过使用MPC友好的替代方案（即ReLU注意力机制）替代了softmax注意力，但这需要对模型进行微调。PUMA与BumbleBee共享一些设计元素，如GeLU的分段近似和指数函数的泰勒近似。简而言之，这些3PC方法不使用OT和HE，但通信开销比我们的方案要大。其他相关工作[79]、[48]、[3]考虑了使用不同近似结构的替代模型，这些模型在MPC中更易计算。然而，已有研究表明粗略的近似可能会显著降低模型的准确性[43]，因此他们的工作需要进行模型微调。BumbleBee的一个优势是它不需要任何模型微调；相反，我们的重点是在给定的预训练变换器模型上进行私有推理。值得注意的是，我们的方法也可以适配到他们的模型中，从而实现更好的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VII. 评估&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型与数据集&lt;/strong&gt;
我们在5个变换器模型上评估BumbleBee，包括四个NLP模型，即BERT-base、BERT-large [19]、GPT2-base [15]、LLaMA-7B [69]，以及一个计算机视觉模型ViT-base [74]、[22]。这些模型通过三个超参数进行参数化：块数B、表示的维度D和头数H。我们直接复用了来自公开源的训练模型。为了展示BumbleBee的有效性，我们在4个数据集上进行了私有推理，包括来自GLUE基准的CoLA、RTE和QNLI（用于NLP任务），以及用于图像分类的ImageNet-1k [64]。具体来说，ImageNet-1k数据集是一个1000个不同类别的分类任务，而来自GLUE基准的三个数据集是二分类任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;评估指标&lt;/strong&gt;
在2PC设置下，我们不区分“离线”和“在线”成本，如一些先前的(2+1)-PC工作所做的那样[55]、[30]。我们报告端到端的运行时间，包括通过网络传输密文的时间。但我们没有包括从硬盘加载模型所花费的时间。我们测量了总通信量，包括双方发送的所有消息。我们使用1GB = 210MB = 230字节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;实验环境&lt;/strong&gt;
本文描述的实验主要在两台阿里巴巴云实例（ecs.g7.16xlarge）上进行，这些实例配备了64个vCPU，主频为2.70GHz，内存为256GB。我们尽可能使用多线程。为了模拟不同的网络条件，我们通过Linux中的流量控制命令操控了云实例之间的带宽。具体而言，我们在两种网络设置下进行了基准测试：局域网（LAN），带宽为1Gbps（单向1Gbps），延迟为0.5ms，以及广域网（WAN），带宽为400Mbps，延迟为4ms。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;具体参数&lt;/strong&gt;
我们将秘密共享的l设置为64，将定点精度f设置为18。在对GeLU和指数运算进行混合环优化时，我们将l′设置为32，f′设置为12。具体来说，我们对BERT-base、GPT2-base和ViT-base模型应用了GeLU和指数运算的混合环优化，而对于其他较大的模型，我们仅对GeLU/SiLU激活进行优化。我们扩展了Yacl库中的Ferret实现[1]，以支持各种应用级的OT类型，例如M 1-OT6。对于RLWE，我们使用SEAL库[65]并借助[37]加速在Intel CPU上的计算。对于近似小于运算，我们在50位输入上评估了MSB协议，以进行(2)中的区段选择。对于近似指数运算，我们将Texp设置为-14，并使用n = 6。有关实现参数的更多细节，请参见附录。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;可用性&lt;/strong&gt;
我们提供了可复现的实现，网址为：https://github.com/AntCPLab/OpenBumbleBee。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>transformer模型架构</title>
        <link>https://JiangZhiyu-1024.github.io/p/transformermodelbase/</link>
        <pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/transformermodelbase/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/transformermodelbase/house-7497002_1280.png" alt="Featured image of post transformer模型架构" /&gt;&lt;h1 id=&#34;模型架构&#34;&gt;模型架构
&lt;/h1&gt;&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391.png&#34;
	width=&#34;807&#34;
	height=&#34;689&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391_hu13532172091181392611.png 480w, https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391_hu12135867853004229579.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241029095402391&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;281px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;大语言模型架构配置表&lt;/center&gt;&gt;
&lt;h2 id=&#34;transformer-模型&#34;&gt;🍈Transformer 模型
&lt;/h2&gt;&lt;p&gt;当前主流的大语言模型都是基于Transformer模型进行设计的。Transformer是由多层的多头自注意力（Multi-headSelf-attention）模块堆叠而成的神经网络模型。原始的Transformer 模型由编码器和解码器两个部分构成，而这两个部分实际上可以独立使用，例如基于编码器架构的BERT模型[1]和解码器架构的GPT模型[2]。与BERT等早期的预训练语言模型相比，大语言模型的特点是使用了更长的向量维度、更深的层数，进而包含了更大规模的模型参数，并主要使用解码器架构，对于Transformer 本身的结构与配置改变并不大。&lt;/p&gt;
&lt;h3 id=&#34;输入编码&#34;&gt;🍉输入编码
&lt;/h3&gt;&lt;p&gt;在Transformer 模型中，输入的词元序列(𝒖 = [𝑢1,𝑢2,&amp;hellip;,𝑢𝑇]) 首先经过一个输入嵌入模块（InputEmbeddingModule）转化成词向量序列。具体来说，为了捕获词汇本身的语义信息，每个词元在输入嵌入模块中被映射成为一个可学习的、具有固定维度的词向量$v_t$ ∈$R^H$。由于Transformer的编码器结构本身无法识别序列中元素的顺序，位置编码（PositionEmbedding,PE）被引入来表示序列中的位置信息。给定一个词元$u_t$，位置编码根据其在输入中的绝对位置分配一个固定长度的嵌入向量$p_t$ ∈$R^H$。然后，每个词元对应的词向量和位置向量将直接相加，生成了最终的输入嵌入序列𝑿=[𝒙1,&amp;hellip;,𝒙𝑇]，并且被传入到后续层中：$x_t=v_t+p_t$.&lt;/p&gt;
&lt;p&gt;通过这种建模方法的表示，Transformer 模型可以利用位置编码 𝒑𝑡 建模不同词元的位置信息。由于不同词元的位置编码仅由其位置唯一决定，因此这种位置建模方式被称为绝对位置编码。尽管绝对位置编码能够一定程度上建模位置信息，然而它只能局限于建模训练样本中出现的位置，无法建模训练数据中未出现过的位置，因此极大地限制了它们处理长文本的能力。&lt;/p&gt;
&lt;h3 id=&#34;多头自注意力机制&#34;&gt;🍋多头自注意力机制
&lt;/h3&gt;&lt;p&gt;多头自注意力是Transformer模型的核心创新技术。相比于循环神经网络（Recurrent Neural Network, RNN）和卷积神经网络（ConvolutionalNeuralNetwork,CNN）等传统神经网络，多头自注意力机制能够直接建模任意距离的词元之间的交互关系。作为对比，循环神经网络迭代地利用前一个时刻的状态更新当前时刻的状态，因此在处理较长序列的时候，常常会出现梯度爆炸或者梯度消失的问题。而在卷积神经网络中，只有位于同一个卷积核的窗口中的词元可以直接进行交互，通过堆叠层数来实现远距离词元间信息的交换。&lt;/p&gt;
&lt;p&gt;多头自注意力机制通常由多个自注意力模块组成。在每个自注意力模块中，对于输入的词元序列，将其映射为相应的查询（Query,𝑸）、键（Key,𝑲）和值（Value,𝑽）三个矩阵。然后，对于每个查询，将和所有没有被掩盖的键之间计算点积。这些点积值进一步除以$\sqrt{D}$进行缩放（𝐷是键对应的向量维度），被传入到softmax函数中用于权重的计算。进一步，这些权重将作用于与键相关联的值，通过加权和的形式计算得到最终的输出。在数学上，上述过程可以表示为：
&lt;/p&gt;
$$
Q = XW^Q,
$$$$
K = XW^K,
$$$$
V = XW^V,
$$$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{D}})V.
$$&lt;p&gt;与单头注意力相比，多头注意力机制的主要区别在于它使用了𝐻组结构相同，但映射参数不同的自注意力模块。输入序列首先通过不同的权重矩阵被映射为一组查询、键和值。每组查询、键和值的映射构成一个“头”，并独立地计算自注意力的输出。最后，不同头的输出被拼接在一起，并通过一个权重矩阵$W^O$∈$R^{H \times H}$ 进行映射，产生最终的输出。如下面的公式所示：
&lt;/p&gt;
$$
head_n = Attention(XW^Q_n,XW^K_n,XW^V_n)
$$$$
MHA = Concat(head_1,...,head_N)W^O
$$&lt;p&gt;由上述内容可见，自注意力机制能够直接建模序列中任意两个位置之间的关系，进而有效捕获长程依赖关系，具有更强的序列建模能力。另一个主要的优势是，自注意力的计算过程对于基于硬件的并行优化（如GPU、TPU等）非常友好，因此能够支持大规模参数的高效优化。&lt;/p&gt;
&lt;h3 id=&#34;前馈网络层&#34;&gt;🍏前馈网络层
&lt;/h3&gt;&lt;p&gt;为了学习复杂的函数关系和特征，Transformer 模型引入了一个前馈网络层（Feed Forward Netwok, FFN），对于每个位置的隐藏状态进行非线性变换和特征提取。具体来说，给定输入𝒙，Transformer中的前馈神经网络由两个线性变换和一个非线性激活函数组成：
&lt;/p&gt;
$$
FFN(X) = σ(XW^U + b_1)W^D + b_2
$$&lt;p&gt;
其中$W^U$ ∈ $R^{H \times H}$ 和$W^D$ ∈ $R^{H \times H}$  分别是第一层和第二层的线性变换权重矩阵，$b_1$ ∈ $R^{𝐻^′}$ 和 $b_2$ ∈ $R^H$ 是偏置项，𝜎是激活函数（在原始的Transformer中，采用 ReLU 作为激活函数）。前馈网络层通过激活函数引入了非线性映射变换，提升了 模型的表达能力，从而更好地捕获复杂的交互关系。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440.png&#34;
	width=&#34;650&#34;
	height=&#34;691&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440_hu8196060707081265206.png 480w, https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440_hu16997779109624366145.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241029105634440&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;94&#34;
		data-flex-basis=&#34;225px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;Transformer 架构图&lt;/center&gt;
&lt;h3 id=&#34;编码器&#34;&gt;🍐编码器
&lt;/h3&gt;&lt;p&gt;在Transformer 模型中，编码器（Encoder）的作用是将每个输入词元都编码成一个上下文语义相关的表示向量。编码器结构由多个相同的层堆叠而成，其中每一层都包含多头自注意力模块和前馈网络模块。在注意力和前馈网络后，模型使用层归一化和残差连接来加强模型的训练稳定度。其中，残差连接（Residual Connection）将输入与该层的输出相加，实现了信息在不同层的跳跃传递，从而缓解梯度爆炸和消失的问题。而LayerNorm则对数据进行重新放缩，提升模型的训练稳定性。编码器接受经过位置编码层的词嵌入序列𝑿作为输入，通过多个堆叠的编码器层来建模上下文信息，进而对于整个输入序列进行编码表示。由于输入数据是完全可见的，编码器中的自注意力模块通常采用双向注意力，每个位置的词元表示能够有效融合上下文的语义关系。在编码器-解码器架构中，编码器的输出将作为解码器（Decoder）的输入，进行后续计算。形式化来说，第𝑙层（𝑙∈{1,&amp;hellip;,𝐿}）的编码器的数据处理过程如下所示：
&lt;/p&gt;
$$
X^′_l = LayerNorm(MHA(X_{l-1})+X_{l-1})
$$$$
X_l = LayerNorm(FFN(X^′_l)+X^′_l)
$$&lt;p&gt;其中，$X^′_l$ 和 $X_l$ 分别是该Transformer层的输入和输出，$X^′_l$是该层中输入经过多头注意力模块后的中间表示，LayerNorm表示层归一化。&lt;/p&gt;
&lt;h3 id=&#34;解码器&#34;&gt;🥥解码器
&lt;/h3&gt;&lt;p&gt;Transformer 架构中的解码器基于来自编码器编码后的最后一层的输出表示以及已经由模型生成的词元序列，执行后续的序列生成任务。与编码器不同，解码器需要引入掩码自注意力（MaskedSelf-attention）模块，用来在计算注意力分数的时候掩盖当前位置之后的词，以保证生成目标序列时不依赖于未来的信息。除了建模目标序列的内部关系，解码器还引入了与编码器相关联的多头注意力层，从而关注编码器输出的上下文信息$X_L$。同编码器类似，在每个模块之后，Transformer 解码器也采用了层归一化和残差连接。在经过解码器之后，模型会通过一个全连接层将输出映射到大小为𝑉的目标词汇表的概率分布，并基于某种解码策略生成对应的词元。在训练过程中，解码器可以通过一次前向传播，让每个词元的输出用于预测下一个词元。而在解码过程，解码器需要经过一个逐步的生成过程，将自回归地生成完整的目标序列。解码器的数据流程如下所示：
&lt;/p&gt;
$$
Y^′_l = LayerNorm(MaskedMHA(Y_{l-1})+Y_{l-1})
$$$$
Y^&#34;_l = LayerNorm(CrossMHA(Y^′_l,X_L)+Y^′_l)
$$$$
Y_l = LayerNorm(FFN(Y^&#34;_l)+Y^&#34;_l)
$$&lt;p&gt;其中，$Y_{l-1}$ 和 $Y_l$ 分别是该Transformer 层的输入和输出，$Y^′_l$ 和 $Y^&amp;quot;_l$ 是该层中输入经过掩码多头注意力MaskedMHA和交叉多头注意力CrossMHA模块后的中间表示，LayerNorm 表示层归一化。然后将最后一层的输入𝒀𝐿映射到词表的维度上：
&lt;/p&gt;
$$
O = softmax(W^LY_L)
$$&lt;p&gt;
其中，𝑶 ∈$R^{H \times V}$ 是模型最终的输出，代表下一个词在词表上的概率分布；$W^L$ ∈ $R^{H \times V}$ 是将输入表示映射到词汇表维度的参数矩阵，而$W^LY_L$是概率化前的中间值，通常被称为logits。&lt;/p&gt;
&lt;p&gt;[1]. JacobDevlin et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. In: NAACL-HLT. 2019.&lt;/p&gt;
&lt;p&gt;[2]. Alec Radford et al. “Improving language understanding by generative pre-training”. In: OpenAI Blog (2018).&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
