<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>DifferentialPrivacy on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/categories/differentialprivacy/</link>
        <description>Recent content in DifferentialPrivacy on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Mon, 04 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/categories/differentialprivacy/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>差分隐私：研究结果综述</title>
        <link>https://JiangZhiyu-1024.github.io/p/differential-privacy-a-survey-of-results/</link>
        <pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/differential-privacy-a-survey-of-results/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/differential-privacy-a-survey-of-results/astronomy-1867616_1920.jpg" alt="Featured image of post 差分隐私：研究结果综述" /&gt;&lt;h1 id=&#34;differential-privacy-a-survey-of-results&#34;&gt;Differential Privacy: A Survey of Results
&lt;/h1&gt;&lt;p&gt;差分隐私：研究结果综述&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要。&lt;/strong&gt; 在过去的五年中，一种新的隐私保护数据分析方法取得了成果。这种方法与统计学、数据库、理论和密码学领域的许多相关文献（但并非全部）不同之处在于，它定义了一个形式化的、普遍适用的隐私保障，并且所提出的数据分析技术被严格证明满足这一保障。出现的关键隐私保障是差分隐私。粗略来说，这确保了（几乎可以量化地）通过加入一个统计数据库而不会承担风险。在本次综述中，我们回顾了差分隐私的定义以及实现它的两种基本技术。然后，我们展示了这些技术的一些有趣应用，提出了三个具体任务的算法和关于差分隐私学习的三个一般结果。&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;🍦1. Introduction
&lt;/h2&gt;&lt;p&gt;隐私保护的数据分析也被称为统计披露控制、推断控制、隐私保护数据挖掘和私密数据分析。我们的主要动机场景是统计数据库。统计量是从样本中计算出的数量。假设一个受信任且值得信赖的管理者从大量受访者（样本）中收集敏感信息，目的是学习（并向公众发布）关于潜在群体的统计事实。问题在于如何在不侵犯个别受访者隐私的情况下发布统计信息。有两种设置：在非交互式设置中，管理者计算并发布一些统计信息，数据不会再被进一步使用。隐私问题可能会影响管理者发布的精确答案，甚至影响发布的统计量的集合。请注意，由于数据不会再被使用，管理者可以在统计信息发布后销毁数据（和自己）。在交互式设置中，管理者处于用户和数据库之间。用户提出的查询和/或对这些查询的响应，可能会被管理者修改，以保护受访者的隐私。数据不能被销毁，管理者必须在数据库的整个生命周期中保持在场。当然，任何交互式解决方案都能产生一个非交互式解决方案，前提是查询在此之前已知：管理者可以模拟一个互动，其中这些已知查询被提出，并发布结果记录。关于这个问题的文献丰富，主要来自统计学界（见，例如，[10, 14, 27, 28, 29, 38, 40, 26, 39]以及有关表格数据的受控发布、列联表和单元抑制的文献），也来自计算机科学的各种分支，如算法、数据库理论和密码学，例如在[3, 4, 21, 22, 23, 33, 34, 41, 48]，[1, 24, 25, 31]，以及[6, 9, 11, 12, 13, 18, 7, 19]中；有关1989年前领域总结的文献，见调查[2]。本次综述关于差分隐私。粗略来说，差分隐私确保删除或添加单个数据库项不会（实质性地）影响任何分析的结果。因此，加入数据库不会产生风险，提供了一种在分布信息可能导致泄露的情况下进行严格数学处理的方法。我们将首先描述三种针对特定、不相关的数据分析任务的差分隐私算法。然后，我们将提出三条关于当需要保护个别数据项隐私时的计算学习的一般结果。这在学习理论文献中通常不是一个关注点，标志着一条新研究方向的出现。&lt;/p&gt;
&lt;h2 id=&#34;2differential-privacy&#34;&gt;🍧2.Differential Privacy
&lt;/h2&gt;&lt;p&gt;在后续中，随机化函数 K 是管理者在发布信息时应用的算法。因此，输入是数据集，输出是发布的信息或记录。我们不需要区分交互式和非交互式设置。可以将数据库视为一组行。如果数据库 D1 和 D2 仅在一个元素上有所不同，我们称其为 D1 和 D2 的关系：一个是另一个的真子集，而较大的数据库仅包含一行附加的记录。&lt;/p&gt;
$$
\Pr[K(D_1) \in S] \leq \exp(ε) \times \Pr[K(D_2) \in S]
$$&lt;p&gt;
所涉及的概率是基于 K 的抛硬币过程。&lt;/p&gt;
&lt;p&gt;满足此定义的机制 K 解决了任何参与者对个人信息泄露的担忧：即使参与者从数据集中删除了她的数据，输出（因此输出的后果）也不会变得显著更可能或更不可能。例如，如果数据库被保险提供者查询，以决定是否为某个特定个体投保，那么该个体数据在数据库中的存在或缺失不会显著影响她获得保险的机会。因此，差分隐私是一种全面的保障。这也是一种非常强的保障，因为它是关于机制行为的统计特性，因此独立于对手/用户的计算能力和附加信息。差分隐私并不是隐私的绝对保障。事实上，Dwork 和 Naor 已经证明，任何具有非平凡效用的统计数据库都会妥协隐私的自然定义。然而，在一个已决定某些数据库的好处大于其成本的社会中，差分隐私确保参与这些社会有益的数据库仅会承担有限的额外风险。&lt;/p&gt;
&lt;p&gt;定义 1 中的参数是公开的。这个参数的选择基本上是一个社会性的问题，超出了本文的讨论范围。也就是说，我们倾向于将其视为，比如，0.01、0.1，或者在某些情况下，ln 2 或 ln 3。如果某个不良事件发生的概率非常小，可能可以接受将其增加 2 或 3 倍；而如果这个概率已经被认为接近不可接受，那么增加 $e^{0.01} ≈ 1.01$ 可能是可以容忍的，而增加 e，甚至仅仅是 $e^{0.1}$，可能就是不可接受的。&lt;/p&gt;
&lt;p&gt;定义 1 讨论了机制 K 的行为，并独立于对手或用户可能拥有的任何附加知识。因此，满足该定义的机制在保护数据库中个别行的隐私时，即使对手知道数据库中的其他所有行，也能做到这一点。&lt;/p&gt;
&lt;p&gt;定义 1 也可以扩展到群体隐私（以及个体对数据库贡献多于一行的情况）。一组 c 个参与者可能担心他们的集体数据泄露信息，即使单个参与者的数据不会。根据这个定义，我们可以将任何概率的扩展限制在最多 exp(c)，对于小的 c，这可能是可以容忍的。当然，统计数据库的目的是披露有关大型群体的汇总信息（同时保护个体隐私），因此我们应该预期随着群体规模的增加，隐私界限会逐渐减弱。&lt;/p&gt;
&lt;h2 id=&#34;3achieving-differential-privacy-in-statistical-databases&#34;&gt;🍨3.Achieving Differential Privacy in Statistical Databases
&lt;/h2&gt;&lt;p&gt;在统计数据库中实现差分隐私&lt;/p&gt;
&lt;p&gt;我们将介绍一种由 Dwork、McSherry、Nissim 和 Smith 提出的交互机制 K，适用于连续值查询的情况。在本节中，查询是一个将数据库映射到（向量形式的）实数的函数。例如，查询“计数 P”统计数据库中具有属性 P 的行数。当查询为函数 f，而数据库为 X 时，真实答案是值 f(X)。机制 K 向真实答案添加适当选择的随机噪声，以生成我们所称的响应。通过用真实答案的嘈杂版本进行回应来保护隐私的想法并不新鲜，但这种方法非常微妙。例如，如果噪声是关于原点对称的，并且同样的问题被多次询问，则响应可能会被平均，从而抵消噪声。我们必须考虑这些因素。&lt;/p&gt;
$$
Δf = \underset{\text{ D1, D2}}{\max} \| f(D_1) - f(D_2) \|_1
$$&lt;p&gt;
对于所有D1和D2，它们最多只在一个元素上不同。&lt;/p&gt;
&lt;p&gt;特别地，当 k = 1 时，函数 f 的敏感性是两个仅在一个元素上不同的数据库所可能取的值之间的最大差异。对于许多类型的查询，Δf 会非常小。特别是，上述讨论的简单计数查询（‘有多少行具有属性 P?’）的 Δf = 1。我们的技术在 Δf 较小时效果最佳——引入最少的噪声。请注意，敏感性仅是函数的属性，与数据库无关。敏感性本质上捕捉了需要由管理员生成的加性噪声隐藏的差异程度（即在两个仅在一个元素上不同的数据库上，f 的值之间的差异）。&lt;/p&gt;
&lt;p&gt;标准差为$\sqrt2Δf /ε$的缩放对称指数分布，记作$\text{Lap}(\Delta f / \epsilon)$ ，其在 \(x\) 处的质量与 $\exp(-|x|(\epsilon/\Delta f))$成正比。更准确地说，令$b = \Delta f/\epsilon$ 。概率密度函数为&lt;/p&gt;
$$
p(x) = \frac{1}{2b} \exp\left(-\frac{|x|}{b}\right)
$$&lt;p&gt;累积分布函数为&lt;/p&gt;
$$
D(x) = \frac{1}{2}\left(1 + \text{sgn}(x)\left(1 - \exp\left(\frac{|x|}{b}\right)\right)\right) 
$$&lt;p&gt;在查询函数 \(f\) 上，隐私机制 \(K\) 的响应为&lt;/p&gt;
$$
f(X) + (\text{Lap}(\Delta f/\epsilon))^k
$$&lt;p&gt;即独立地向 $f(x)$的每个$k$ 个分量添加分布为 $\text{Lap}(\Delta f / \epsilon)$的噪声。注意，减少已知的公共参数 $\epsilon$ 会使 $\text{Lap}(\Delta f / \epsilon)$曲线变平，从而产生更大的期望噪声幅度。当 $\epsilon$ 固定时，具有高敏感性的函数 f 会产生更平坦的曲线，同样导致更高的期望噪声幅度。&lt;/p&gt;
$$
\exp(-|f(D_1) - r|\frac{\Delta f}{\epsilon})
$$&lt;p&gt;成正比，$D_2$亦然。应用三角不等式，我们得到的比值最多为&lt;/p&gt;
$$
\exp\left(-|f(D_1) - f(D_2)|\frac{\Delta f}{\epsilon}\right)
$$&lt;p&gt;根据敏感性的定义，$|f(D_1) - f(D_2)| \leq \Delta f$，因此比值被限制为&lt;/p&gt;
$$
\exp(-\epsilon)
$$&lt;p&gt;从而实现了$\epsilon$差分隐私。显而易见，通过对每个查询运行 $K$，使噪声分布为&lt;/p&gt;
$$
\text{Lap}\left(\sum_{i} \Delta f_i/ \epsilon\right)
$$&lt;p&gt;可以为任何（自适应选择的）查询序列 $f_1, \ldots, f_d$实现 $\epsilon$差分隐私。换句话说，每个答案的质量随着查询的敏感性总和而下降。有趣的是，有时可以做到比这更好。大致来说，重要的是&lt;/p&gt;
$$
\Delta = ||(f_1(D_1), f_2(D_1), \ldots, f_d(D_1)) - (f_1(D_2), f_2(D_2), \ldots, f_d(D_2))||_1 
$$&lt;p&gt;的最大可能值。由于查询的潜在自适应选择，陈述的精确表述需要一些谨慎。有关全面的处理，请参见 [19]。我们在此对非自适应情况声明定理，视查询序列$f_1, f_2, \ldots, f_d$（具有各自的元数 $k_1, \ldots, k_d$）为一个单一的 $k = \sum_{i=1}^{d} k_i$元查询 $f$，并回顾对于任意$k$的定义 2。&lt;/p&gt;
&lt;p&gt;定理 1 ([19])：对于 $f : D \to \mathbb{R}^k$，机制 $K_f$ 向每个输出项独立添加服从分布 $\text{Lap}\left(\frac{\Delta f}{\epsilon}\right)$ 的噪声，具有 $\epsilon$-差分隐私。&lt;/p&gt;
&lt;p&gt;机制 $K$ 的上述描述在对不敏感查询时具有出色的准确性。特别地，确保差分隐私所需的噪声仅依赖于函数的敏感性和参数 $\epsilon$。这两者都与数据库的内容和行数无关。因此，如果数据库非常大，由差分隐私机制引入的许多典型查询的错误相对较小。我们可以将 $K$ 看作分析师与数据之间的差分隐私保护接口。这提示了一种隐私保护数据分析的一般方法：寻找需要少量不敏感查询的算法。参见例如 [7, 8, 32]。事实上，甚至计数查询也极其强大，允许对许多标准数据挖掘任务进行准确和差分隐私计算，包括主成分分析、$k$-均值聚类、分离超平面的感知机学习以及生成 ID3 决策树 [7]，以及（邻近的）半空间学习 [8]（见下面的第 4.3 节）。在定理 1 的许多应用中，尤其引人关注的是直方图查询类。直方图查询是将数据库行的域任意划分为不相交的“单元”，其真实答案是描述每个单元中数据库行数的计数集。虽然具有 $k$ 个单元的直方图查询可以视为 $k$ 个独立的计数查询，但添加或删除单个数据库行最多只能影响整个 $k$ 元组计数的一个位置（即与添加（删除）行的单元对应的计数）；此外，该单元的计数最多受到 1 的影响，因此根据定义 2，每个直方图查询的敏感性为 1。许多数据分析实际上就是直方图；因此，令人鼓舞的是，复杂的直方图并不需要每个单元中有大方差，而是需要非常小的方差。&lt;/p&gt;
&lt;h3 id=&#34;31-当噪声毫无意义时&#34;&gt;🍩3.1 当噪声毫无意义时
&lt;/h3&gt;&lt;p&gt;在某些任务中，添加噪声毫无意义。例如，函数 $f$ 可能将数据库映射到字符串、策略或树。在最近的一篇论文中，McSherry 和 Talwar 解决了在保持 $\epsilon$-差分隐私的同时优化此类函数输出的问题 [35]。假设策展人持有一个数据库 $X$，目标是生成一个对象 $y$。简而言之，他们的指数机制工作如下：假设存在一个效用函数 $u(X, y)$，用于衡量在数据库为 $X$ 时输出 $y$ 的质量。例如，如果数据库保存了个人在拍卖中为数字商品所评估的价值，那么当价格设为 $y$ 时，$u(X, y)$ 可能是这些评估的收入。拍卖是噪声毫无意义的一个很好的例子，因为稍微定得高一点的价格可能会阻止许多投标者购买。McSherry 和 Talwar 的指数机制以与 $ \exp(- u(X, y)/2)$ 成比例的概率输出 $y$。这确保了 $\Delta u$-差分隐私，或在 $\Delta u \leq 1$ 时的 $\epsilon$-差分隐私。这里，$\Delta u$ 的定义与之前略有不同；它是通过改变单行数据而引起的 $u$ 值的最大可能变化（与添加或删除一行数据不同；这两者的差异最多为二的倍数）；参见 [35]。通过这种方法，McSherry 和 Talwar 得到近似真实的拍卖，并实现几乎最优的售价。粗略来说，这表明参与者不能通过虚报其评估值来大幅降低其支付的价格。有趣的是，他们展示了差分隐私的简单组合可以用于获取拍卖，在这些拍卖中，任何 $c$ 个代理的合作组都不能通过提交除真实评估以外的出价显著提高其效用。这类似于上述备注 1 的情况，在该情况下，组合被用来为 $c$ 个个体的群体获取隐私。&lt;/p&gt;
&lt;h2 id=&#34;4-特定任务的算法&#34;&gt;🍪4 特定任务的算法
&lt;/h2&gt;&lt;p&gt;在本节中，我们描述了针对三个不相关任务的差分隐私算法。&lt;/p&gt;
&lt;h3 id=&#34;41-统计数据推断&#34;&gt;🎂4.1 统计数据推断
&lt;/h3&gt;&lt;p&gt;本节的结果来自 Dwork 和 Nissim [18]。考虑一个场景，其中数据库中的每个元素由一组 $k$ 个布尔属性 $\alpha_1, \ldots, \alpha_k$ 描述，且行是从某个底层分布中独立采样得到的，分布在 ${0, 1}^k$ 上。设 $1 \leq t \leq \frac{k}{2}$ 为整数。这里的目标是利用关于任何属性值组合的发生率的信息，以学习任意两个属性值组合的发生率。尽管我们使用“查询”一词，但这些查询在此之前都是已知的，且机制将是非交互式的。从大约 $\binom{k}{2}$ 个发布的信息中，可以计算所有 $\binom{k}{2} \cdot 2^2$ 个小项的发生率的近似值。这将允许数据分析师近似计算所有 $2^{\binom{k}{2}}$ 个长度为 2 的二元小项的概率，前提是初始近似值足够准确。我们将概率与发生率等同，因此概率空间是在数据库中的行上。固定任意一组属性。这些属性值的所有可能设置的发生率可以通过具有 $2$ 个单元的直方图来描述，直方图的敏感性为 $1$，因此我们将处理一个整体敏感性与 $O(k)$ 成比例的查询序列（实际上，它会比这个差一个因子 $t$，下面会讨论）。设 $\alpha$ 和 $\beta$ 为属性。如果在概率上 $\alpha$ 蕴含 $\beta$，则表示在给定 $\alpha$ 的情况下 $\beta$ 的条件概率超过 $\beta$ 的无条件概率。测量概率中的蕴含能力对数据挖掘至关重要。注意，由于 $\Pr[\beta]$ 可以通过计数查询简单地估计，因此测量概率中的蕴含问题归结为获得 $\Pr[\beta | \alpha]$ 的良好估计。此外，一旦我们可以估计 $\Pr[\beta | \alpha]$、$\Pr[\beta]$ 和 $\Pr[\alpha]$，就可以使用贝叶斯规则和德摩根定律来确定任何布尔函数的统计数据。例如，$\Pr[\alpha \land \beta] = \Pr[\alpha] \Pr[\beta | \alpha]$，因此如果我们对两个乘数的估计在加性 $\eta$ 内，我们就得到了乘积的估计，其准确度在 $3\eta$ 内。作为朝向非交互式解决方案的第一步，考虑交互式情况，假设我们对 $\Pr[\alpha]$ 和 $\Pr[\beta]$ 有良好的估计。确定 $\Pr[\beta | \alpha]$ 的关键是找到一个 $\alpha$ 的重集，即一个集合 $q \subseteq [n]$，使得 $\alpha$ 的发生率至少比预期高出一个标准差，然后确定该重集上 $\beta$ 的发生率是否高于 $\beta$ 的整体发生率。更具体地，可以测试该条件发生率是否高于给定阈值，然后使用二分搜索找到“正确”的阈值。找到重集很容易，因为随机选择的 $[n]$ 的子集在超过预期的 $\alpha$ 的发生率至少一个标准差的概率是常数。为了“模拟”交互式情况，策展人选择一些随机子集，并为每个子集发布（带噪声的）关于该子集中 $\alpha$ 和 $\beta$ 的发生率的估计。以高概率（取决于 $t$），至少有一个子集对 $\alpha$ 是重的。将所有部分结合起来：对于 $t$ 个随机子集，策展人发布所有 $m = \binom{k}{2}$ 个字面联结的发生率的良好近似值。具体而言，我们要求以至少 $1 - \frac{\delta}{m^2}$ 的概率计算的条件概率 $\Pr[\alpha | \beta]$ 的准确度在 $\frac{\eta}{3m^2}$ 内，其中 $\alpha$ 和 $\beta$ 现在是字面的最小项。这确保了以至少 $1 - \delta$ 的概率，所有计算的条件概率在 $\frac{\eta}{3m^2}$ 内准确，因此所有长度为 $2$ 的最小项的估计概率在 $\frac{\eta}{m^2}$ 内准确。数字 $t$ 相当大，并且依赖于许多因素，包括差分隐私参数以及 $\eta$、$\delta$、$k$ 和 $t$。文献 [18] 的分析表明，当 $\eta$ 和 $\delta$ 是常数时，这种方法将查询数量从 $O\left(\binom{k}{2}\right)$（每个 2 元组的一个直方图）减少到 $O(24 k^2 \log k)$。注意有趣的权衡：我们需要依赖于 $m^2$ 的准确度以避免进行 $m^2$ 次查询。当数据库足够大时，这种权衡是可以实现的。&lt;/p&gt;
&lt;h3 id=&#34;42--contingency-表的发布&#34;&gt;🍰4.2  contingency 表的发布
&lt;/h3&gt;&lt;p&gt;本节的结果来自 Barak、Chaudhuri、Dwork、Kale、McSherry 和 Talwar [5]。 contingency 表是一个计数表。在人口普查或其他调查的背景下，我们将个人的数据视为数据库中的一行。我们不假设行是相互独立的。目前，每一行由 $k$ 位描述 $k$ 个二元属性 $a_1, \ldots, a_k$ 的值。严格来说，contingency 表是一个 $\mathbb{R}^{2^k}$ 中的向量，描述对于 $k$ 个属性的每种设置，数据库中具有该属性值设置的行数。换句话说，它是一个具有 $2^k$ 个单元的直方图。通常情况下，contingency 表本身不会被发布，因为当 $k$ 较大时，它可能会很稀疏。相反，对于不同的属性子集，数据策展人会发布 contingency 表在每个子集上的投影，即每个可能设置的计数，这些较小的计数表称为边际，每个边际由属性的子集命名。由 $j$ 个属性（$j \leq k$）命名的边际称为 $j$-way 边际。数据策展人通常会发布许多低阶边际集合，以揭示许多不同且可能重叠的属性集合之间的关联。由于 contingency 表是一个直方图，我们可以向 contingency 表的每个单元添加与 $-1$ 成比例的独立生成的噪声，以获得一个差分隐私的（非整数且不一定非负的）表。我们稍后将讨论完整性和非负性的问题。目前，我们只需注意，从这个带噪声的表中可以直接计算任何所需的边际，并且不同边际之间的一致性是显而易见的。然而，这种方法的一个缺点是，尽管 contingency 表的每个单元中的噪声相对较小，但计算出的边际中的噪声可能较大。例如，描述属性 $a_1$ 的 1-way 表的方差为 $2^{k-1} - 2$。我们认为这不可接受，尤其是当 $n \gg 2^k$ 时。边际也是直方图。第二种方法，在低阶边际的（常见）情况下噪声较小，但不提供边际之间的一致性，其工作原理如下。设 $C$ 为要发布的边际集合。我们可以考虑一个函数 $f$，当应用于数据库时，生成所需的边际。现在，应用定理 1，选择这个 $f$（对每个表中的每个单元独立添加噪声），其敏感性 $\Delta f = |C|$。当 $n$（数据库中的行数）与 $|C|/\epsilon$ 相比很大时，这也会产生良好的准确性。因此，如果独立随机化每个（表中每个）单元导致的小表与表之间的不一致不成问题，并且用户能接受偶尔出现负值和通常非整数的单元计数，那么我们就可以完成这个过程。我们对这些隐私增强技术的产物——不一致性、负值和非整数——没有哲学或数学上的反对，但在实践中，它们可能会造成问题。例如，单元计数可能作为其他程序（可能是现成的程序）的输入，而这些程序预期正整数，从而导致类型不匹配。不一致性，更不用说负值，可能会让普通用户感到困惑，例如美国 FactFinder 网站的普通用户。接下来，我们概述 Barak 等人的主要工作步骤 [5]。&lt;/p&gt;
&lt;p&gt;移动到傅里叶域。当添加噪声时，两个自然的解决方案出现了：向源表的条目添加噪声（这是我们的第一个提议；当 $k$ 较大时，准确性较差），或向报告的边际添加噪声（我们的第二个提议；不一致性受到影响）。第三种方法是将数据转换到傅里叶域。这只是一个基底的变化。如果我们计算所有 $2^k$ 的傅里叶系数，我们将获得整个一致性表的非冗余编码。如果我们对傅里叶系数进行扰动，然后再转换回 contingency 表域，我们将得到一个（不同的，可能是非整数的，可能是负的）contingency 表，其“距离”（例如，$l_2$ 距离）与原始表的差距由扰动的大小决定。转到傅里叶域的优点是，如果只需要一组边际 $C$，那么我们不需要完整的傅里叶系数集。例如，如果 $C$ 是所有 3-way 边际的集合，那么我们只需要权重最多为 3 的傅里叶系数，数量为 $\binom{k}{3} + \binom{k}{2} + k + 1$。这将转化为一个噪声更小的边际集合。用于计算边际 $C$ 的傅里叶系数形成了数据集的一个模型，捕获了从边际集 $C$ 中可以学习的所有信息。按照定理 1 的指示对这些系数添加噪声，然后再转换回 contingency 表域，得到一个生成合成数据集的程序，确保差分隐私，同时在很大程度上（且可测量地）捕获模型中的信息。这是一个生成具有可证明差分隐私的合成数据的具体方法示例。傅里叶系数准确地描述了边际所需的信息。通过准确测量所需的信息，Barak 等人使用 [19] 的技术添加尽可能少的噪声。此外，傅里叶基底由于根据属性值集的自然分解而特别有吸引力。通过注意到计算低阶边际时不需要额外的傅里叶系数，并且使用更少的噪声系数，可以在给定边际的子边际（即，低阶边际）上施加比定理 4 中更紧的界限，从而通过减少方差提高准确性。使用线性规划和舍入。Barak 等人 [5] 使用线性规划来获得一个非负的但可能是非整数的数据集，具有（几乎）给定的傅里叶系数，然后对结果进行舍入以获得整数解。有趣的是，从线性程序获得的边际与噪声测量的边际之间的“距离”并不比原始数据的真实边际更远（在 [5] 中被精确定义）。因此，通过一致性的施加所引入的额外误差不超过隐私机制本身所引入的误差。&lt;/p&gt;
&lt;h4 id=&#34;符号和预备知识&#34;&gt;🧁符号和预备知识。
&lt;/h4&gt;&lt;p&gt;回想一下，设 $k$ 表示（布尔）属性的数量，我们可以将数据集视为一个向量 $x \in \mathbb{R}^{2^k}$，以属性元组为索引。对于每个 $\alpha \in {0, 1}^k$，数量 $x_\alpha$ 是具有该属性设置的数据元素的数量。我们令 $n = |x|_1$ 表示数据集中元组或行的总数。对于任意 $\alpha \in {0, 1}^k$，我们使用 $|\alpha|_1$ 表示非零位置的数量。我们写作 $\beta \preceq \alpha$，表示 $\alpha, \beta \in {0, 1}^k$，如果 $\alpha$ 中的每个零位置在 $\beta$ 中也是零。&lt;/p&gt;
&lt;h4 id=&#34;边际算子&#34;&gt;🥧边际算子
&lt;/h4&gt;$$
(C_\alpha(x))_\beta = \sum_{\gamma : \gamma \land \alpha = \beta} x_\gamma \tag{3}
$$&lt;p&gt;注意，算子 $C_\alpha$ 对于所有 $\alpha$ 是线性的。&lt;/p&gt;
&lt;p&gt;定理 2. ${f_\alpha}$ 形式构成了 $\mathbb{R}^{2^k}$ 的正交归一基。因此，可以将任何边际写成与相关的傅里叶系数的小和：&lt;/p&gt;
$$
C_\beta x = \sum_{\alpha \preceq \beta} \langle f_\alpha, x \rangle C_\beta f_\alpha. \tag{4}
$$&lt;p&gt;系数 $\langle f_\alpha, x \rangle$ 是从 $x$ 计算 $C_\beta x$ 的必要和充分数据。&lt;/p&gt;
&lt;p&gt;定理 3 ([5]). 设 $B \subseteq {0, 1}^k$ 描述一组傅里叶基向量。释放集合 $\phi_\beta = \langle f_\beta, x \rangle + \text{Lap}(|B|/2^{k/2})$ 对于 $\beta \in B$ 保护 $\epsilon$-差分隐私。&lt;/p&gt;
&lt;p&gt;证明：每个元组对每个输出坐标贡献正好是 ±$1/2^{k/2}$，因此该 $|B|$ 个输出的 $L_1$ 灵敏度至多为 $|B|/2^{k/2}$。根据定理 1，添加标准差为 $|B|/2^{k/2}$ 的对称指数噪声给出 $\epsilon$-差分隐私。&lt;/p&gt;
&lt;p&gt;备注：为了理解规模，我们可以通过随机添加或删除 $|B|2^/$ 个数据集中的个体来实现对每个坐标的类似扰动，这个数量可能远小于 $n$。&lt;/p&gt;
&lt;p&gt;将步骤整合在一起。为了计算一组边际分布 A，我们需要所有在 A 的向下闭包下的 Fourier 系数 fβ。边际分布（A ⊆ {0, 1}^k，D）的步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;令 B 为 A 在向下闭包下的集合。&lt;/li&gt;
&lt;li&gt;对于 β ∈ B，计算 $$ \phi_\beta = \langle f_\beta, D \rangle + \text{Lap}\left(\frac{|B|}{2^{k/2}}\right) $$。&lt;/li&gt;
&lt;li&gt;在以下线性规划中求解 wα，并四舍五入到最接近的整数权重 w&amp;rsquo;α：
&lt;ul&gt;
&lt;li&gt;最小化 $$ b $$&lt;/li&gt;
&lt;li&gt;约束条件：
&lt;ul&gt;
&lt;li&gt;$$ w_\alpha \geq 0 \quad \forall \alpha $$&lt;/li&gt;
&lt;li&gt;$$ \phi_\beta - \sum_\alpha w_\alpha f_\beta^\alpha \leq b \quad \forall \beta \in B $$&lt;/li&gt;
&lt;li&gt;$$ \phi_\beta - \sum_\alpha w_\alpha f_\beta^\alpha \geq -b \quad \forall \beta \in B $$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;使用列联表 w&amp;rsquo;α，计算并返回边际分布 A。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;定理 4 ([5])。使用边际分布 $\text{Marginals}(A) $的记号，以概率  $1 - \delta $，对于所有  $\alpha \in A$ ，有&lt;/p&gt;
$$
\|C_\alpha x - C_\alpha w&#39;\|_1 \leq 2\|\alpha\|_1 \frac{2|B| \log(|B|/\delta)}{ + |B|}. \quad (5)
$$&lt;p&gt;当 \( k \) 较大时，线性规划的时间复杂度是 $O(2^k) $。当 \( k \) 很大时，这并不令人满意。然而，令人惊讶的是，通过在回到数据域之前向第一个 Fourier 系数添加相对较小的量，可以实现非负性（但不是整数性）。不需要线性规划，并且引入的误差非常小。因此，如果在 $O(2^k)$ 上的多项式时间开销不可接受，而可以接受非整数性，那么该方法效果很好。我们注意到，在该工作的初步实现中，非整数性并不是问题，因为计数总是被转换为百分比。&lt;/p&gt;
&lt;h3 id=&#34;43-学习邻近半空间&#34;&gt;🍫4.3 学习（邻近）半空间
&lt;/h3&gt;&lt;p&gt;我们以一个例子结束本节，该例子受到学习理论中问题的启发，出现在Blum、Ligett和Roth即将发表的论文中[8]。其目标是提供一种对半空间查询的非交互式解决方案。从高层次来看，他们的方法是发布信息，以（近似）回答一大组“规范”查询，保证对于任何（可能是非规范的）该类型的查询，都存在一个“邻近”的规范查询。因此，数据分析师可以获得与感兴趣的查询在某种意义上接近的答案。在[8]中，查询是$\mathbb{R}^d$中的半空间查询，具体定义如下。在整个这一节中，我们采用[8]中的假设，即数据库点已缩放到单位球体上。&lt;/p&gt;
&lt;p&gt;定义 3。给定一个数据库$D \subset \mathbb{R}^d$ 和单位长度向量 $y \in \mathbb{R}^d$，半空间查询$H_y$ 定义为&lt;/p&gt;
$$
H_y(D) = \frac{|\{x \in D : \sum_{i=1}^{d} x_i \cdot y_i \geq 0\}|}{|D|}.
$$&lt;p&gt;注意，半空间查询可以通过两个计数查询来估计：“$|D|$  是多少？”和“$|{x \in D : \sum_{i=1}^{d} x_i \cdot y_i \geq 0}|$是多少？”因此，半空间查询的灵敏度最多为 2。半空间查询$H_{y_1}  和  H_{y_2}$ 之间的距离定义为它们之间角度的正弦，记作$\sin(y_1, y_2) $ 。考虑到这一点，Blum、Ligett 和 Roth 的算法确保了以下效用概念：&lt;/p&gt;
&lt;p&gt;定义 4 ([8])。数据库机制 \( A \) 对于某个度量 \( d \) 下的查询类 \( C \) 是 )-有用的，如果以概率$1 - \delta$，对于每个$Q \in C$和每个数据库 \( D \)，都有&lt;/p&gt;
$$
|Q(A(D)) - Q&#39;(D)| \leq \epsilon
$$&lt;p&gt;对于某个$Q&amp;rsquo; \in C$ 满足$d(Q, Q&amp;rsquo;) \leq \gamma$。注意，接近的是查询，而不是（不一定是）它们的答案。给定一个半空间查询$H_{y_1} $ ，下面的算法将输出一个值 \( v \)，使得&lt;/p&gt;
$$
|v - H_{y_2}(D)| &lt; \epsilon
$$&lt;p&gt;对于某个与$H_{y_1}$ $\gamma$ -接近的$H_{y_2}$。等价地，算法任意计数或不计数满足$\cos(x, y_1) \leq \gamma$的点$ x \in D$ 。Blum 等人指出，$\gamma$的作用类似于机器学习中的边际概念，并且即使$H_{y_1}$ 和 $H_{y_2}$是$\gamma$-接近的，也并不意味着查询$H_{y_1}(D)$和$H_{y_2}(D)$ 的真实答案是接近的，除非大多数数据点位于$H_{y_1}$和 $H_{y_2}$ 的$\gamma$  边际之外。&lt;/p&gt;
&lt;p&gt;定义 5 ([8])。一个半空间查询$H_y$是$b$ -离散化的，如果对于每个$i \in [d]$，$ y_i$ 可以用 \( b \) 位来表示。令$C_b$ 为所有 \( b \)-离散化半空间在$\mathbb{R}^d$  中的集合。&lt;/p&gt;
&lt;p&gt;考虑一个特定的 \( k &lt; d \) 维子空间，该子空间由一个随机的 $d \times k$矩阵 \( M \) 定义，其元素独立且均匀地从 \(\{-1, 1\}\) 中选择。考虑投影&lt;/p&gt;
$$
P_M(x) = \frac{1}{\sqrt{k}} x \cdot M,
$$&lt;p&gt;它将数据库点投影到子空间，并将它们重新缩放到单位球体。对于半空间查询$H_y$ ，投影$P_M(H_y)$ 只是由投影$P_M(y)$ 定义的 \( k \) 维半空间查询。关键的事实是，对于随机选择的 \( M \)，将数据库点 \( x \) 和半空间查询指定符 \( y \) 投影到一起，极不可能显著改变它们之间的角度：&lt;/p&gt;
&lt;p&gt;定理 5（约翰逊-林登斯特劳斯定理）。考虑将点 \( x \) 和半空间$H_y$ 投影到由投影矩阵 \( M \) 定义的随机 \( k \) 维子空间中。则有&lt;/p&gt;
$$
\Pr\left[| \cos(x, H_y) - \cos(P_M(x), H_{P_M(y)})| \geq \frac{\gamma}{4}\right] \leq 2e^{-\left(\frac{\gamma}{16}\right)^2 - \left(\frac{\gamma}{16}\right)^3} \frac{k}{4}.
$$&lt;p&gt;子空间的维度 \( k \) 被选择为使得投影一个点和一个半空间的角度变化超过$ \frac{\gamma}{4}$的概率最多为$\frac{1}{4}$ 。这导致&lt;/p&gt;
$$
k \geq \frac{4 \ln(8/\delta)}{\left(\frac{\gamma}{16}\right)^2 - \left(\frac{\gamma}{16}\right)^3}.
$$&lt;p&gt;因此，查询$H_y$ \(  \) 的答案可以通过对投影半空间查询的隐私保护估计来估计，整体准确性可以通过选择 \( m \) 个投影矩阵来提高； \( x \) 和 \( y \) 之间的角度将通过 \( m \) 组投影结果的中位数来估计。当然，如果目标只是响应少量半空间查询，经过投影过程就没有意义，更不用说进行多次投影了。但文献[8]的目标更为雄心勃勃：为（非离散化的）半空间查询提供一个$(\epsilon, \delta, \gamma)$-有用的非交互机制；这正是低维度发挥作用的地方。算法选择 \( m \) 个投影矩阵，其中 \( m \) 取决于离散化参数 \( b \)、维度 \( d \) 和失败概率 $\delta$（更具体地说，$m \in O(\ln(1/\delta) + \ln(bd))$。对于每个随机子空间（由投影矩阵 \( M \) 定义），算法选择一个“标准”半空间的网$N_M$ （由子空间中的标准向量定义），使得对于每个向量$y \in \mathbb{R}^k$ ，都有一个邻近的标准向量，具体而言，距离（诱导的正弦）最多为$\frac{3}{4}\gamma$ 。所需的标准向量数量为$O\left(\frac{1}{\gamma^{k-1}}\right)$。对于每个标准向量，策展人发布投影半空间查询的隐私保护估计。该机制是非交互式的，策展人将不再发挥进一步的作用。为了处理任意查询 \( y \)，分析师从一个空的多重集合开始。对于每个 \( m \) 个投影 \( M \)，分析师找到与$P_M(y)$最近的向量$\hat{y} \in N_M $，并将该半空间查询的答案添加到多重集合中。算法输出这些 \( m \) 个值的中位数。&lt;/p&gt;
&lt;p&gt;定理 6 ([8])。设&lt;/p&gt;
$$
n \geq \log(1/\delta) + \log m + (k - 1) \log(1/\gamma) + m O\left(\frac{1}{\gamma}\right) k - 1 2\alpha.
$$$$ \alpha $$$$ (\epsilon, \gamma, \delta) $$$$ n $$&lt;p&gt; 的数据库。该算法的运行时间为多项式级别，具体为&lt;/p&gt;
$$
\text{poly}(\log(1/\delta), 1/\epsilon, 1/\alpha, b, d)
$$$$ \gamma $$&lt;p&gt;。&lt;/p&gt;
&lt;h2 id=&#34;5-一般学习理论结果&#34;&gt;🍬5 一般学习理论结果
&lt;/h2&gt;&lt;p&gt;我们简要概述三个关于在交互模型中可以私密学习的常规结果。首先，我们介绍Blum、Dwork、McSherry和Nissim的结果，表明在统计查询学习模型中可学习的任何内容也可以通过交互方式有效地私密学习[7]。然后，我们转向忽略计算问题的结果，表明McSherry和Talwar的指数机制[35]可以用于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;私密学习任何可PAC学习的内容[32]；&lt;/li&gt;
&lt;li&gt;为任何具有多项式VC维度的函数类 $$ C $$ 生成一个差分隐私的“合成数据库”，该数据库对 $$ C $$ 中的任何查询给出“良好”的答案[8]。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在这种情况下使用指数机制的工作归功于Kasiviswanathan、Lee、Nissim、Raskhodnikova和Smith[32]。&lt;/p&gt;
&lt;h3 id=&#34;51-模拟统计查询模型&#34;&gt;🍭5.1 模拟统计查询模型
&lt;/h3&gt;$$ f_1, \ldots, f_k $$$$ \tau_1, \ldots, \tau_k $$$$ 1 \leq i \leq k $$$$ f_i $$$$ \tau_i $$$$ f_i $$$$ \delta $$$$ c $$$$ 1 - \delta $$&lt;p&gt;。&lt;/p&gt;
$$ k $$$$ f_1, \ldots, f_k $$$$ \tau = \min\{\tau_1, \ldots, \tau_k\} $$$$ n $$$$ f_i $$$$ p_i $$$$ f_i $$$$ n $$$$ k $$$$ b = k/\epsilon $$$$ \tau = \rho/n $$$$ \rho $$$$ p_i $$$$ \rho $$$$ \tau $$$$ \rho $$$$ \text{Lap}(k/\epsilon) $$$$ \rho $$$$ \delta/k $$$$ x &lt; 0 $$$$ x $$$$ \delta/2k $$&lt;p&gt;：&lt;/p&gt;
$$
\frac{1}{2} e^{-|x|/b} &lt; \frac{\delta}{2k} 
$$$$ |x| &gt; b \ln(k/\delta) $$$$ |x| &gt; (k/\epsilon) \ln(k/\delta) $$$$ \rho &gt; (k/\epsilon) \ln(k/\delta) $$$$ \rho/n &lt; \tau $$$$ n &gt; \rho/\tau $$$$ K $$&lt;p&gt; 引入的噪声；即，它假设&lt;/p&gt;
$$
\frac{1}{n} \sum_{i=1}^{n} f_j(d_i) = \Pr_{x \in D}[f_j(x)], \quad 1 \leq j \leq k, 
$$$$ D $$$$ D $$$$ f_j $$$$ 1 \leq j \leq k $$$$ \tau $$$$ 1 - \delta $$$$ n &gt; \tau^{-2} \log(k/\delta) $$$$ \tau $$$$ \tau/2 $$$$ n $$&lt;p&gt; 的最大下界，可以处理这两种类型的错误。&lt;/p&gt;
&lt;h3 id=&#34;52-私密pac学习&#34;&gt;🍮5.2 私密PAC学习
&lt;/h3&gt;$$ t: X \rightarrow Y $$$$ X $$$$ Y $$$$ D $$$$ h: X \rightarrow Y $$&lt;p&gt;，使得错误率较小，定义为：&lt;/p&gt;
$$
\text{error}(h) = \Pr_{x \in R^D}[t(x) = h(x)].
$$$$ \alpha $$$$ \beta $$$$ C $$$$ 1 - \beta $$$$ \alpha $$$$ C $$$$ C $$$$ \text{OPT} = \min_{c \in C}\{\text{error}(c)\} $$&lt;p&gt;，我们希望&lt;/p&gt;
$$
\Pr[\text{error}(h) \leq \text{OPT} + \alpha] \geq 1 - \beta,
$$&lt;p&gt;其中概率是关于学习者看到的样本和学习者的随机性的。这个过程称为不可知学习。&lt;/p&gt;
$$ d $$$$ t: X_d \rightarrow Y_d $$$$ X_d $$$$ D $$$$ Z \in D^n $$$$ D $$$$ n $$$$ Z $$$$ n $$$$ (x_i, y_i) $$$$ y_i = t(x_i) $$$$ 1 \leq i \leq n $$$$ C $$$$ \epsilon $$&lt;p&gt;-差分隐私。&lt;/p&gt;
$$ C $$$$ \epsilon $$$$ H = C $$$$ n \in O(\log |C|d + \log(1/\beta) \cdot \max\{1/\alpha, 1/\alpha^2\}) $$&lt;p&gt;。学习者可能不是高效的。&lt;/p&gt;
&lt;p&gt;该定理是通过McSherry和Talwar的指数机制证明的，效用函数为&lt;/p&gt;
$$
u(Z, h) = -| \{ i : t(x_i) = h(x_i) \} | 
$$$$ Z \in (X \times Y)^n $$$$ h \in H_d $$$$ u $$$$ \exp(u(Z, c)/2) $$$$ c \in H_d $$$$ u $$&lt;p&gt; 的小敏感度。准确性（以高概率低错误）稍微难以论证；证明直观上源于输出的概率随着错误分类数量的增加而呈指数下降。&lt;/p&gt;
&lt;h3 id=&#34;53-差分隐私的多项式-vc-维度类查询&#34;&gt;🍯5.3 差分隐私的多项式 VC 维度类查询
&lt;/h3&gt;$$ C $$&lt;p&gt; 中所有查询的“合理”准确回答。读者应该对概念类的 Vapnick-Chervonenkis (VC) 维度有所了解。粗略来说，VC 维度是衡量类中概念复杂度的一种度量。&lt;/p&gt;
$$ A $$$$ C $$$$ Q \in C $$$$ D $$$$ \hat{D} = A(D) $$&lt;p&gt;，有&lt;/p&gt;
$$
|\ Q(\hat{D}) - Q(D)| \leq \gamma.
$$$$ C $$$$ n $$$$ D \in ({0, 1}^d)^n $$$$ n $$$$ C $$$$ \epsilon $$$$ \delta $$$$ \hat{D} $$$$ C $$$$ \epsilon $$$$ m = O\left(\frac{\text{VC dim}(C)}{\gamma^2}\right) $$$$ d $$&lt;p&gt; 元组。它是根据指数机制选择的，使用的效用函数为&lt;/p&gt;
$$
u(D, \hat{D}) = - \max_{h \in C} \left| h(D) - \frac{n}{m} h(\hat{D}) \right| .
$$$$ C $$$$ D \subset \{0, 1\}^d $$$$ |D| \geq O\left( \frac{d \cdot \text{VC dim}(C)}{\gamma^3} + \frac{\log(1/\delta)}{\gamma} \right) $$$$ \hat{D} $$$$ \epsilon $$$$ C $$$$ \gamma $$$$ |\hat{D}| &lt; |D| $$&lt;p&gt; 的事实，即匹配任何查询的数据库条目的数量会成比例地较小。&lt;/p&gt;
&lt;h2 id=&#34;6-总结&#34;&gt;🍼6 总结
&lt;/h2&gt;&lt;p&gt;在本节中，我们探讨了隐私机制如何根据数据库上应用的查询序列的复杂性增加噪声。尽管使用高斯噪声可以在一定程度上缓解这一问题，但Dinur和Nissim [13]（参见[17, 20]）所开始的研究表明，这种增加是必不可少的。Dinur和Nissim的结果在很大程度上推动了机制K的开发以及本综述中倡导的整个交互方法。为了更精确地分析现实攻击，并更好地理解未能提供$\epsilon$-差分隐私在实践中可能意味着什么，我们需要进一步研究以改善这些结果，或者确认这种改进是不可能的，从而理解如何在除非常大、“互联网规模”的数据集之外有效利用这些技术。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>第4周工作总结</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</link>
        <pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E7%AC%AC4%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/colorful-2609978_1280.jpg" alt="Featured image of post 第4周工作总结" /&gt;&lt;h1 id=&#34;总结&#34;&gt;总结
&lt;/h1&gt;&lt;p&gt;这一周高强度阅读把书都读完了，但是没看代码，后果是有点记不住，等有需要时再去看吧。&lt;/p&gt;
&lt;p&gt;这周看了大模型内容比较多，主要是为了快点看完相关内容找ZD哥确定毕设方向，早点开始，做的好一点。&lt;/p&gt;
&lt;p&gt;周五下午找了ZD哥确定方向，看完方向人有点蒙，可能是大模型看得多，安全那方面看得少，导致我不知道这个方向在干什么，于是开始沉思，思考了半天感觉确实不会，找gpt老师询问，没有得到结果，故有些惆怅，周日又坐在实验室上了一天的软件工程实训，还好马上就结束了，太烦了。今天是周一，先把上周周报做了&lt;/p&gt;
&lt;p&gt;我又重新换了问法，问了gpt老师，这次得到了满意的结果，找到了几篇论文，这周就读这几篇。&lt;/p&gt;
&lt;p&gt;感觉这才是周报，之前的都是书的摘抄。&lt;/p&gt;
&lt;h2 id=&#34;阅读&#34;&gt;阅读
&lt;/h2&gt;&lt;p&gt;LLM预训练数据准备&lt;/p&gt;
&lt;p&gt;transformer模型架构（不知道是不是我本，这个我从不同的地方看了听了好多遍，现在才刚有点感觉）&lt;/p&gt;
&lt;p&gt;Instruction Tuning&lt;/p&gt;
&lt;p&gt;提示学习（实在不行，没有研究天赋就去干点轻松的(bushi)）&lt;/p&gt;
&lt;p&gt;解码与部署&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;后面就是补了两篇差分隐私的文献，读完感觉对要研究的方向还是不清楚，故又找了几篇：&lt;/p&gt;
&lt;h2 id=&#34;计划&#34;&gt;计划
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文综述了在机器学习中实现安全和隐私保护的各种方法，包括使用同态加密和安全多方计算的技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Deep Learning via Additively Homomorphic Encryption&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该论文研究了通过可加同态加密实现隐私保护的深度学习推理，探讨了如何在保留数据隐私的同时利用深度学习模型进行推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文介绍了在联邦学习框架中使用加密数据的方案，虽然主要聚焦于模型训练，但也讨论了如何在推理阶段保护数据隐私。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Inference with Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了如何在不暴露数据的情况下，安全地使用深度学习模型进行推理，涉及多种加密和隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Homomorphic Encryption for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了同态加密在隐私保护机器学习中的应用，特别是在数据持有方希望使用外部模型进行推理的情况下。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Inference of Deep Learning Models&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文提出了一种框架，用于在保护隐私的情况下安全地进行深度学习模型的推理，涵盖了多个隐私保护技术。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Privacy-Preserving Machine Learning: A Practical Approach&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文讨论了一种实用的方法，通过使用加密技术保护数据隐私，同时实现机器学习模型的推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure Outsourced Computation in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇论文探讨了如何将机器学习任务外包给持有模型的方，同时保证数据的隐私不被泄露。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Differentially Private Inference in Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文介绍了如何在机器学习模型推理过程中实现差分隐私，保证输出结果不泄露输入数据的信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Towards Secure and Private Machine Learning: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这篇综述论文涵盖了各种保护数据隐私的技术，包括同态加密和安全多方计算在推理中的应用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Federated Learning with Encrypted Data for Privacy-Preserving Machine Learning&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该研究探讨了在联邦学习框架下如何处理加密数据，适用于在不暴露数据的情况下进行模型推理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Secure and Private Model Inference: A Survey&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文回顾了与安全和隐私相关的模型推理方法，强调在使用外部模型时的隐私保护需求。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Differential Privacy</title>
        <link>https://JiangZhiyu-1024.github.io/p/differential_privacy/</link>
        <pubDate>Sat, 02 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/differential_privacy/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/differential_privacy/cosmos-6680031_1280.jpg" alt="Featured image of post Differential Privacy" /&gt;&lt;h1 id=&#34;differential-privacy--a-primer-for-the-perplexed&#34;&gt;❤️Differential Privacy – A Primer for the Perplexed
&lt;/h1&gt;&lt;p&gt;差分隐私：给困惑者的入门指南论文笔记&lt;/p&gt;
&lt;p&gt;差分隐私是一种针对隐私保护数据分析的隐私目标定义。自其提出以来，差分隐私数据分析一直是激烈的算法和理论研究的主题。不幸的是，部分文献存在对这一定义的基本误解。在这篇简短的邀请性说明中，我们阐明并回应了这些常见错误中的最重要的几种。&lt;/p&gt;
&lt;h2 id=&#34;错误-1将定义与特定算法混淆&#34;&gt;💖错误 1：将定义与特定算法混淆
&lt;/h2&gt;&lt;h4 id=&#34;原文&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;差分隐私是一个定义。它是一个数学保证，可以通过一个发布数据集统计信息的算法来满足。许多不同的算法都可以满足这一定义。接下来，我们将假设有一个包含 n 行的数据库 X，每一行的数据属于单个个体。如果每个个体的数据被描述为集合 U 的一个元素，那么我们可以将数据集视为 U 中的一个多重集（即每个元素可以出现多次的集合）。设 D 表示可能的数据集空间。存在一个算法 M，它以数据库和（可选）查询作为输入，并返回响应。我们将这种算法称为机制。我们将在后面陈述差分隐私的定义。不过，现在我们注意到，如果一个随机机制 M 在所有数据库上具有相同的输出分布，那么它的输出就不会泄露任何关于其输入的信息。实际上，这等同于以下陈述，其中 D 是所有可能数据库的集合，Q 是所有可能查询的集合：&lt;/p&gt;
$$
\frac{\Pr[M(X, q) \in S]}{\Pr[M(X&#39;, q) \in S]} = e^0 = 1
$$&lt;p&gt;
在分子和分母中，概率空间都是基于机制所做的随机选择。需要注意的是，确定性机制如果忽略其输入，也可以满足这一属性。从方程式1可以推导出，如果产生了输出y，这并不会透露关于数据库的信息。方程式1是绝对的：如果一个算法具有这个属性，那么无论观察者——即看到输出y的数据分析师知道或能访问什么，这个属性都成立。如果观察者已经知道数据库是X，或者数据库包含了本文作者的医疗数据，甚至观察者知道数据库中所有镰状细胞状态位的模2总和，这些都不能改变机制的行为保证。与满足方程式1的机制进行交互不会泄露关于数据库的任何进一步信息。&lt;/p&gt;
&lt;h4 id=&#34;解读&#34;&gt;解读
&lt;/h4&gt;&lt;p&gt;作者引入了“机制”的概念，指的是一个接受数据库和查询作为输入的算法，并返回结果。重要的是，作者提到如果一个随机机制在所有数据库上都有相同的输出分布，那么该机制不会泄露关于输入的任何信息，这是一种非常强的隐私保护保证。这为后续对差分隐私定义的详细阐述奠定了基础，并强调了实现隐私保护的机制设计的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;符号解释&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;X 和 X′：代表两个可能的数据库。&lt;/li&gt;
&lt;li&gt;D：所有可能的数据库的集合。&lt;/li&gt;
&lt;li&gt;q：表示某个查询。&lt;/li&gt;
&lt;li&gt;S：表示机制 M 的输出范围中的一个子集。&lt;/li&gt;
&lt;li&gt;Pr⁡[M(X,q)∈S]：表示在数据库 X 上执行查询 q 时，输出结果落在集合 S 中的概率。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;条件意义&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该条件表明，对于任意两个数据库 X 和 X′，它们的输出落在集合 S 中的概率比是 e0=1，这意味着两者的概率是相等的。&lt;/li&gt;
&lt;li&gt;这个特性确保了任何一个个体的存在或缺失对查询结果的影响是不可察觉的，从而有效保护个体的隐私。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;原文-1&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;这是香农（Shannon）在加密上下文中提出的完美保密的定义。这不是一个算法，而是对随机算法概率分布的条件；任何满足这一条件的算法在信息论意义上都不会泄露关于数据的信息。两个数据库X和X′之间的对称差（symmetric difference），记作X⊕X′，是指出现在X或X′中但不在它们交集中的元素或行的集合。例如，如果X′是X的一个子集，且X中仅包含一行不在X′中，那么对称差的基数为1。（由于X和X′可以是多重集合，对称差实际上要复杂一些：如果一个项在一个数据库中比另一个数据库多出现k次，那么它在对称差中出现k次）。我们现在准备阐述差分隐私的定义。它的形式与方程1完全相同，只是现在我们允许释放一些信息：&lt;/p&gt;
$$
\frac{\Pr[M(X, q) \in S]}{\Pr[M(X&#39;, q) \in S]} = e^{ε|X Θ X&#39;|}
$$&lt;p&gt;
在分子和分母中，概率空间是基于机制所做的随机选择。&lt;/p&gt;
&lt;p&gt;这个定义与完美保密的定义（方程1）有一个显著的不同：在概率比率的界限中，e的指数是ε|XX′|而不是0。&lt;/p&gt;
&lt;p&gt;这里的符号长这样，但我敲不出来，不知道这是啥东西：&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/differential_privacy/image-20241102103704588.png&#34;
	width=&#34;129&#34;
	height=&#34;32&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/differential_privacy/image-20241102103704588_hu15975875244688783122.png 480w, https://JiangZhiyu-1024.github.io/p/differential_privacy/image-20241102103704588_hu16610540167326729282.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241102103704588&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;403&#34;
		data-flex-basis=&#34;967px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;如果一个算法满足定义1，那么这一点不受观察者（看到输出y的数据分析师）所知道或能够访问的内容的影响。如果观察者已经知道数据库是X，或者数据库包含了本文作者的医疗数据，甚至如果观察者知道数据库中所有镰形细胞状态位的模2和，这都不会改变机制的行为保证。对这个定义的一个常见解释是，无论观察者（或攻击者）事先知道什么，差分隐私算法的输出几乎不会指示任何特定用户的数据是否被包含在数据集中。我们稍后会再讨论这一点。现在我们总结：这个定义不是一个算法，而是许多不同算法所满足的条件。需要注意的是，以这些术语来表述隐私，作为可以通过多种方式满足的要求，提供了一个框架，使得人们可以研究算法，比较它们的隐私保证，并理解它们对隐私的联合影响。我们相信，这是隐私科学方法中的必要一步。”&lt;/p&gt;
&lt;h4 id=&#34;解读-1&#34;&gt;解读
&lt;/h4&gt;&lt;p&gt;这段话讨论了差分隐私的定义与完美保密之间的差异，强调了差分隐私算法在不同背景下的行为保证，即使观察者掌握了一些先验知识，也无法推断出特定用户数据的包含情况。作者指出，这种定义不仅可以适用于多种算法，还提供了一个研究框架，使得对隐私保护的算法进行比较和理解成为可能。这种方法被认为是科学研究隐私的必要步骤，显示了隐私保护的复杂性和重要性。&lt;/p&gt;
&lt;h2 id=&#34;错误2混淆输出与生成它们的过程&#34;&gt;💗错误2：混淆输出与生成它们的过程
&lt;/h2&gt;&lt;h4 id=&#34;原文-2&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;考虑著名的1位一次性密码机制，它提供完美的保密性。该随机机制的输入是一个位b ∈ {0, 1}。该机制选择一个新的随机位p ∈R {0, 1}，并输出b + p mod 2。”&lt;/p&gt;
$$
\forall X, X&#39; \in D, \forall q \in Q, \forall S \subseteq \text{Range}(M) :\frac{\Pr[M(X, q) \in S]}{\Pr[M(X&#39;, q) \in S]} = 1
$$&lt;p&gt;
请注意，尽管机制实际上是使用其输入进行计算的，因此在某种正式的操作意义上它并不忽略其输入，但效果是相同的：输出分布是相同的，与数据库无关。如上所述，这意味着机制的输出不提供关于数据库的任何信息。这在以下四种情况下都是正确的：输出为0且数据库为0，输出为1且数据库为0，输出为0且数据库为1，或者输出为1且数据库为1。在所有四种情况下，输出对数据库没有任何可学习的信息。确保这一点的是生成输出的过程。因此，我们不说“安全”或“保护隐私”的输出；相反，我们认为这些输出是根据保护隐私的过程生成的。1位一次性密码是一种提供完美隐私的过程。所有完美私密的机制都是差分隐私的，但差分隐私机制的类别中包含一些不会提供完美隐私的机制；相反，它们提供关于数据库的一些非平凡信息，这在我们希望进行保护隐私的数据分析时是合理的。随机响应就是这样一种机制[War65]。考虑以下1位随机响应机制。与1位一次性密码机制一样，可能的数据库集是D = {0, 1}。设X ∈ {0, 1}为实际数据库。然而，该机制的步骤如下：如果p = 1，则翻转第二个硬币c ∈R {0, 1}，并公布结果。如果p = 0，则发布X。&lt;/p&gt;
&lt;p&gt;声明2：1位随机响应机制是(ln 3)/2-差分隐私的。&lt;/p&gt;
&lt;p&gt;证明：固定d ∈ {0, 1}。没有损失的一般性，设X = d，X′ = 1 − d。在输入X时，以概率3/4输出d。另一方面，在输入X′时，以概率1/4输出d。两者的比率是3。对称差为2，得出((ln 3)/2)-差分隐私。请注意，对于给定数据库并不存在“好输出”或“坏输出”的概念。尽管如此，发布的值确实给出了两个1位数据库哪个更可能的一点线索，但随机性提供了不确定性。与往常一样，我们指出，1位随机响应机制保持其差分隐私属性，独立于任何看到输出的人对数据库或其与其他数据库的连接所知的内容。&lt;/p&gt;
&lt;h4 id=&#34;解读-2&#34;&gt;解读
&lt;/h4&gt;&lt;p&gt;这段文字首先声称1位随机响应机制具有特定的差分隐私级别，即(ln 3)/2。接下来，作者通过一个简单的证明来支持这一声明，设定输入数据库X和X&amp;rsquo;，并计算在这两种情况下输出d的概率。通过比较输出概率，可以得出比率为3，这意味着在一定条件下，一个数据库比另一个数据库更可能生成特定输出。这里提到的对称差反映了两个数据库之间的差异。作者进一步解释，尽管随机响应机制的输出可能提供了关于数据库的信息，但并不存在绝对的“好”或“坏”输出，随机性引入了不确定性，从而保护了隐私。此外，强调了该机制的差分隐私属性与观察者所知的信息无关，确保了其隐私保护的强度。&lt;/p&gt;
&lt;h2 id=&#34;错误3将特定差分隐私算法的低效用与差分隐私本身的失败混淆&#34;&gt;💟错误3：将特定差分隐私算法的低效用与差分隐私本身的失败混淆。
&lt;/h2&gt;&lt;p&gt;低效用指的是某个具体的差分隐私算法在提供隐私保护的同时，可能导致输出结果的质量或信息量下降。然而，这并不意味着差分隐私本身是一种失败的概念或方法。相反，某个特定算法的效用差可能源于设计、实现或数据特性等因素，而不是差分隐私的理论基础。&lt;/p&gt;
&lt;h4 id=&#34;原文-3&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;正如我们所指出的，1位完美一次性密码是一种差分隐私算法。更具体地说，对于所有ε ≥ 0，1位一次性密码算法是ε-差分隐私的。它也不提供关于数据库的信息。这并不意味着‘差分隐私不能起作用’或没有差分隐私算法能够产生有用的输出。实际上，我们也看到1位随机响应是(ln 3)/2-差分隐私的。从效用的角度来看，它是一个比一次性密码更好的(ln 3)/2-差分隐私算法。特别是，众所周知，假设随机响应调查中的所有n个受访者都正确遵循指示，人们可以从他们的回答中推导出承认特定行为的受访者数量的估计，其预期的不准确性（或扭曲）在√n的数量级上。实际上，存在一些(ln 3)/2-差分隐私算法，其准确性更高，即能在预期扭曲独立于n的情况下给出估计计数。当ε = (ln 3)/2时，拉普拉斯机制[DMNS06]发布的统计数据具有正确的均值和标准差√2(√3/2) = √3/2，而不是随机响应中标准差为Θ(√n)。因此，特定差分隐私算法的局限性不一定适用于所有差分隐私算法。任何声称差分隐私无法实现给定准确性目标的说法，必须附有证明，说明差分隐私——关于概率分布比率的具体要求——与该目标不一致。也就是说，必须表明没有任何差分隐私算法能够实现给定的准确性水平。这是相当棘手的，我们对于什么是可能的、什么是不可能的直觉往往是错误的。例如，自然会猜测上述提到的拉普拉斯机制基本上是在差分隐私下回答“统计”类型问题的最佳选择。然而，在需要同时计算多个统计数据的情况下，存在更复杂的算法（例如[BLR08]）表现得更好。尽管存在这些困难，还是有一些已发表的结果证明了差分隐私算法的非平凡局限性（见[GRS08，HT09]以获取特别优雅的例子）。&lt;/p&gt;
&lt;h4 id=&#34;解读-3&#34;&gt;解读
&lt;/h4&gt;&lt;p&gt;这段话探讨了差分隐私算法的有效性和局限性。作者首先确认了1位完美一次性密码是差分隐私的，并强调这种算法提供的信息量为零。接着，作者指出这并不意味着差分隐私本身无效，实际上，1位随机响应机制在效用上表现更好，且具有可接受的差分隐私级别。通过提供具体的例子，作者说明在一些情况下，差分隐私算法可以有效地估计特定行为的发生数量，并且存在一些算法可以在准确性方面表现得更好。&lt;/p&gt;
&lt;p&gt;作者进一步指出，对于声称差分隐私无法达到特定准确性目标的说法，必须提供充分的证明，表明差分隐私的定义与该目标之间的不一致性。强调了解决差分隐私算法的局限性并非易事，且我们的直觉可能会误导我们。作者还提到，即使拉普拉斯机制在某些情况下看似最佳，仍可能存在更复杂且效果更好的算法。最后，作者提到一些研究已证明了差分隐私算法的非平凡局限性，表明在这一领域还有很多需要探索的内容。&lt;/p&gt;
&lt;h2 id=&#34;差分隐私的杂项做和不做&#34;&gt;💝“差分隐私的杂项‘做’和‘不做’”
&lt;/h2&gt;&lt;p&gt;这部分内容通常会列出在实施和理解差分隐私时应遵循的一些基本原则和误区。以下是一些可能的“做”和“不做”的示例：&lt;/p&gt;
&lt;h5 id=&#34;做does&#34;&gt;做（Does）：
&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;确保输入的隐私&lt;/strong&gt;：在设计差分隐私机制时，要确保能够有效保护用户数据的隐私。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;选择合适的噪声机制&lt;/strong&gt;：使用合适的噪声添加方法（如拉普拉斯噪声或高斯噪声）来达到所需的差分隐私级别。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;进行严格的分析&lt;/strong&gt;：对算法进行严格的数学分析，以确认其满足差分隐私的要求。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;理解隐私损失&lt;/strong&gt;：在实际应用中，清楚了解不同参数（如ε）的选择对隐私保护和数据效用的影响。&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;不做does-not&#34;&gt;不做（Does Not）：
&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;不要忽视算法的效用&lt;/strong&gt;：在追求差分隐私的同时，不能完全忽略算法的效用，确保输出仍然有用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不要混淆差分隐私和其他隐私保护技术&lt;/strong&gt;：差分隐私是一个特定的框架，与其他隐私保护方法（如匿名化）有所不同。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不要低估数据特性&lt;/strong&gt;：在实施差分隐私时，忽视数据本身的特性可能会影响隐私保护效果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不要期望完美的隐私保护&lt;/strong&gt;：差分隐私提供的是概率性保证，而非绝对安全，因此要有合理的期望。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些“做”和“不做”可以帮助研究人员和工程师更好地理解和实施差分隐私，避免常见的误区和误解。&lt;/p&gt;
&lt;h4 id=&#34;原文-4&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;我们重申迄今为止的主要观点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;差分隐私是一个定义，而不是一个算法。&lt;/li&gt;
&lt;li&gt;对于任何特定的数据库，没有输出是‘好’或‘坏’的。隐私（完美隐私或差分隐私）是通过生成输出的过程获得的。&lt;/li&gt;
&lt;li&gt;某个特定的ε-差分隐私机制未能提供足够准确的答案，并不意味着差分隐私与该目标不兼容。 在最后一节中，我们将对此进行进一步阐述，并进一步解释差分隐私，解决文献中关于差分隐私所提供或不提供的若干误解。差分隐私通常是在提供给数据分析的敏感记录的背景下进行解释的。隐私保证是，当个体选择加入或退出输入数据库时，输出的分布，因此从这些输出得出的结论的分布，最多只会变化一个（通常较小的）乘法因子。因此，参与其中几乎没有任何危害；关于您、您的数据或更广泛的世界得出的任何结论，几乎与不使用您的数据时得出的结论一样可能。”&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;事实：差分隐私并不保证您认为的秘密保持秘密。差分隐私所保证的是，您参与调查不会泄露您在调查中贡献的具体信息。得出的结论可能会反映出关于您的统计信息。一个旨在发现某种疾病早期指示的健康调查可能会产生强有力甚至结论性的结果；这些结论适用于您并不证明差分隐私的违反。事实上，您甚至可能没有参与调查（再次强调，差分隐私确保无论您是否参与调查，这些结论性结果几乎以相同的概率得出）。特别是，如果调查显示特定私人属性与公共属性之间有很强的相关性，这并不构成差分隐私的违反，因为这种相关性几乎同样的概率会在有无任何受访者的情况下被观察到。我们最喜欢的寓言是：假设一个统计数据库教我们吸烟导致癌症。已知吸烟者艾丽斯受到了伤害，因为她的保险费上涨。无论艾丽斯是否在数据库中，她受到伤害的可能性基本上是相同的。艾丽斯也得到了帮助：因为这项研究，她参加了戒烟项目。差分隐私的设计旨在确保艾丽斯和其他人都有动力参与这些社会有益的研究。&lt;/p&gt;
&lt;h4 id=&#34;解读-4&#34;&gt;解读
&lt;/h4&gt;&lt;p&gt;这段文字强调了差分隐私的局限性，并澄清了一些常见误解。作者指出，尽管差分隐私保护了个人在参与调查时的具体贡献不被泄露，但这并不意味着与个人相关的秘密会保持秘密。调查得出的统计结论可能仍然会反映出个人的某些特征，即使这些特征并没有直接在调查中披露。&lt;/p&gt;
&lt;p&gt;接着，作者用一个例子来说明这一点：即使吸烟导致癌症的结论在某个数据库中被证实，个体（如艾丽斯）因其吸烟习惯而遭受的后果并不违反差分隐私，因为这一结论可以在几乎相同的概率下得出，而与个体是否参与调查无关。作者强调，差分隐私的设计目的是鼓励个人参与对社会有益的研究，尽管参与者可能面临一些与其私人生活相关的后果。&lt;/p&gt;
&lt;p&gt;整体而言，这段文字意在提醒读者，差分隐私虽能保护个体数据的具体性，但并不消除由统计结果引发的潜在影响，反而要让人们更积极地参与能够产生社会价值的研究。&lt;/p&gt;
&lt;h4 id=&#34;原文-5&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;在讨论差分隐私时，有一种观点认为差分隐私的定义在涉及多个参与者或多个数据记录时变得不再有效或合理。这种说法的误解主要源于对差分隐私如何处理多个数据库或多个记录的理解不足。&lt;/p&gt;
&lt;h4 id=&#34;解读-5&#34;&gt;解读
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;差分隐私的扩展性&lt;/strong&gt;：差分隐私的定义本质上是针对单个记录的，但它可以扩展到多个记录和多个参与者的情况。在这种情况下，算法仍然可以通过添加适当的噪声来保证输出的隐私性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;统计特性&lt;/strong&gt;：在处理多个记录时，差分隐私并不失去其意义，因为差分隐私关注的是如何在统计分析中保持个体数据的隐私。当多个人的数据被聚合时，差分隐私确保了即使某个特定个体的记录被包含或排除，输出的变化仍然受到限制。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;应用实例&lt;/strong&gt;：在实际应用中，差分隐私已经被成功地应用于大规模数据集和多个参与者的场景中，如大型调查和机器学习模型的训练。在这些情况下，差分隐私提供了一种框架，使得即使存在多个数据记录，仍然可以有效地保护个体隐私。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;误解来源&lt;/strong&gt;：认为“差分隐私对于多于一个记录没有意义”的观点，往往源于对其基本原理和统计保证的误解。实际上，差分隐私的设计目的就是为了解决在涉及多个数据记录时的隐私问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;原文-6&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;（奇怪的是）人们常常声称，由于概率的最大值为1，因此ε值大于1的情况几乎没有意义。具体选择这个值的来源并不清楚，因为exp(1) = e与概率的最大值并没有特别的关系，尽管大ε值在根本上是没有意义的这种观点是可以理解的。确实，较大的ε值会导致隐私保障的意义远不如较小的ε值。然而，它们仍然提供了数学上有意义的保证。考虑AOL发布的搜索日志事件，其中一名个人通过她的搜索记录被识别。假设在未包含她的数据的情况下，纽约时报记者准确描述西尔玛·阿诺德的私密担忧和关切的事件概率是十亿分之一。在这种情况下，形式为“允许您的数据被纳入将使您遭遇尴尬的风险增加e^12 &amp;lt; 164,000倍”的保证仍然使得披露的可能性非常小。&lt;/p&gt;
&lt;h4 id=&#34;解读-6&#34;&gt;解读：
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;对ε的理解&lt;/strong&gt;：作者指出，虽然有些人认为ε值大于1没有意义，但这并不准确。尽管较大的ε值确实会导致隐私保障的有效性降低，数学上它们仍然提供了某种保证。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;例子说明&lt;/strong&gt;：通过AOL搜索日志的例子，作者强调了大概率事件和隐私泄露之间的关系。即使在极小的概率下，允许数据被使用仍然可能导致一定程度的风险增加，然而这种增加并不一定会导致信息泄露。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;概率与风险&lt;/strong&gt;：作者指出，虽然较大的ε值可能在直观上让人觉得隐私保护效果不佳，但其数学意义仍然存在。在某些情况下，即使是大ε值的风险增加也可能非常小，意味着泄露的可能性仍然是微乎其微的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对风险的理解&lt;/strong&gt;：最后，作者提到即使风险增加的倍数看起来很大，但在具体情境中，泄露事件的概率可能依然很小，这意味着参与者仍然可以相对安全地共享数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;原文-7&#34;&gt;原文
&lt;/h4&gt;&lt;p&gt;错误：差分隐私不能工作，因为没有人能够解释如何设置ε的值。差分隐私方法的一大优点是能够保持个体在特定数据库中所遭受的累积隐私损失的量化衡量。这进一步强调了差分隐私研究的强大之处：我们很好地理解那些数据在多个独立操作数据库中的个体所遭受的累积隐私损失，即使对手可以访问所有这些数据库。此外，差分隐私研究发展出技术来显著减少在可以协调回答的情况下的隐私损失，这也是其强大之处。再者，差分隐私研究者已经确立这种协调对启用高复杂度查询至关重要，且无法替代。这种研究方向——在差分隐私中，而非其他私人数据分析方法——使我们能够通过ε来理解和减轻隐私损失。是的，这项研究尚不完整。是的，以下形式的定理似乎令人感到限制重重：&lt;/p&gt;
&lt;p&gt;如果一个个体参与了10,000个对抗性选择的数据库，并且我们希望确保她的累积隐私损失在概率至少为1 - e^(-32)的情况下被限制在e^1以内，那么每个数据库的ε值设定为1/801差分隐私是足够的。&lt;/p&gt;
&lt;p&gt;但我们还有什么其他方法可以找到理解如何放松对最坏情况对手保护的起点？我们还能如何测量这样做的影响？还有什么其他技术可以证明这样的主张？从更哲学的层面考虑，可以将其比作时间：一个人的一生中只有那么多小时，一旦这些时间耗尽，你就会死亡。（这有时比某人得知关于你私密信息的情况更糟。）然而，我们作为一个社会找到了一些方法来为个体的时间确定价值，而这一过程的基本部分就是能够量化地衡量时间。对于隐私的价值，确实需要新的思考，但我们相信，现在能够做到这一点是一件非常好的事情；每一个有意义的隐私保障都应该是量化的。&lt;/p&gt;
&lt;h4 id=&#34;解读-7&#34;&gt;解读
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;放松隐私保护的思考&lt;/strong&gt;：作者提到在理解如何放宽对最坏情况对手的保护时，探讨不同的方法和工具是必要的。这表明在设计隐私保护机制时，需要权衡保护程度和实际应用的灵活性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;量化的必要性&lt;/strong&gt;：通过量化隐私保护的效果，作者认为可以更好地理解和测量隐私保护的影响。这种量化方法有助于设计出既能有效保护隐私又能满足实际需求的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间的比喻&lt;/strong&gt;：作者通过将隐私的价值与时间的价值进行比较，强调了对隐私进行量化的复杂性。就像时间的价值在社会中得到认可并进行量化一样，隐私的价值也应该被理解和衡量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;隐私价值的新思考&lt;/strong&gt;：最后，作者提到在隐私价值方面需要新的思考方式，指出这种新思考不仅是必要的，而且是积极的。这种思考方式可以推动隐私保护的理论和实践发展，使其更加科学和有效。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
