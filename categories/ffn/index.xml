<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>FFN on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/categories/ffn/</link>
        <description>Recent content in FFN on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Tue, 29 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/categories/ffn/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>transformer模型架构</title>
        <link>https://JiangZhiyu-1024.github.io/p/transformermodelbase/</link>
        <pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/transformermodelbase/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/transformermodelbase/house-7497002_1280.png" alt="Featured image of post transformer模型架构" /&gt;&lt;h1 id=&#34;模型架构&#34;&gt;模型架构
&lt;/h1&gt;&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391.png&#34;
	width=&#34;807&#34;
	height=&#34;689&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391_hu13532172091181392611.png 480w, https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391_hu12135867853004229579.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241029095402391&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;281px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;大语言模型架构配置表&lt;/center&gt;&gt;
&lt;h2 id=&#34;transformer-模型&#34;&gt;🍈Transformer 模型
&lt;/h2&gt;&lt;p&gt;当前主流的大语言模型都是基于Transformer模型进行设计的。Transformer是由多层的多头自注意力（Multi-headSelf-attention）模块堆叠而成的神经网络模型。原始的Transformer 模型由编码器和解码器两个部分构成，而这两个部分实际上可以独立使用，例如基于编码器架构的BERT模型[1]和解码器架构的GPT模型[2]。与BERT等早期的预训练语言模型相比，大语言模型的特点是使用了更长的向量维度、更深的层数，进而包含了更大规模的模型参数，并主要使用解码器架构，对于Transformer 本身的结构与配置改变并不大。&lt;/p&gt;
&lt;h3 id=&#34;输入编码&#34;&gt;🍉输入编码
&lt;/h3&gt;&lt;p&gt;在Transformer 模型中，输入的词元序列(𝒖 = [𝑢1,𝑢2,&amp;hellip;,𝑢𝑇]) 首先经过一个输入嵌入模块（InputEmbeddingModule）转化成词向量序列。具体来说，为了捕获词汇本身的语义信息，每个词元在输入嵌入模块中被映射成为一个可学习的、具有固定维度的词向量$v_t$ ∈$R^H$。由于Transformer的编码器结构本身无法识别序列中元素的顺序，位置编码（PositionEmbedding,PE）被引入来表示序列中的位置信息。给定一个词元$u_t$，位置编码根据其在输入中的绝对位置分配一个固定长度的嵌入向量$p_t$ ∈$R^H$。然后，每个词元对应的词向量和位置向量将直接相加，生成了最终的输入嵌入序列𝑿=[𝒙1,&amp;hellip;,𝒙𝑇]，并且被传入到后续层中：$x_t=v_t+p_t$.&lt;/p&gt;
&lt;p&gt;通过这种建模方法的表示，Transformer 模型可以利用位置编码 𝒑𝑡 建模不同词元的位置信息。由于不同词元的位置编码仅由其位置唯一决定，因此这种位置建模方式被称为绝对位置编码。尽管绝对位置编码能够一定程度上建模位置信息，然而它只能局限于建模训练样本中出现的位置，无法建模训练数据中未出现过的位置，因此极大地限制了它们处理长文本的能力。&lt;/p&gt;
&lt;h3 id=&#34;多头自注意力机制&#34;&gt;🍋多头自注意力机制
&lt;/h3&gt;&lt;p&gt;多头自注意力是Transformer模型的核心创新技术。相比于循环神经网络（Recurrent Neural Network, RNN）和卷积神经网络（ConvolutionalNeuralNetwork,CNN）等传统神经网络，多头自注意力机制能够直接建模任意距离的词元之间的交互关系。作为对比，循环神经网络迭代地利用前一个时刻的状态更新当前时刻的状态，因此在处理较长序列的时候，常常会出现梯度爆炸或者梯度消失的问题。而在卷积神经网络中，只有位于同一个卷积核的窗口中的词元可以直接进行交互，通过堆叠层数来实现远距离词元间信息的交换。&lt;/p&gt;
$$
Q = XW^Q,
$$$$
K = XW^K,
$$$$
V = XW^V,
$$$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{D}})V.
$$$$
head_n = Attention(XW^Q_n,XW^K_n,XW^V_n)
$$$$
MHA = Concat(head_1,...,head_N)W^O
$$&lt;p&gt;由上述内容可见，自注意力机制能够直接建模序列中任意两个位置之间的关系，进而有效捕获长程依赖关系，具有更强的序列建模能力。另一个主要的优势是，自注意力的计算过程对于基于硬件的并行优化（如GPU、TPU等）非常友好，因此能够支持大规模参数的高效优化。&lt;/p&gt;
&lt;h3 id=&#34;前馈网络层&#34;&gt;🍏前馈网络层
&lt;/h3&gt;$$
FFN(X) = σ(XW^U + b_1)W^D + b_2
$$&lt;p&gt;
其中$W^U$ ∈ $R^{H \times H}$ 和$W^D$ ∈ $R^{H \times H}$  分别是第一层和第二层的线性变换权重矩阵，$b_1$ ∈ $R^{𝐻^′}$ 和 $b_2$ ∈ $R^H$ 是偏置项，𝜎是激活函数（在原始的Transformer中，采用 ReLU 作为激活函数）。前馈网络层通过激活函数引入了非线性映射变换，提升了 模型的表达能力，从而更好地捕获复杂的交互关系。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440.png&#34;
	width=&#34;650&#34;
	height=&#34;691&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440_hu8196060707081265206.png 480w, https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440_hu16997779109624366145.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241029105634440&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;94&#34;
		data-flex-basis=&#34;225px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;Transformer 架构图&lt;/center&gt;
&lt;h3 id=&#34;编码器&#34;&gt;🍐编码器
&lt;/h3&gt;$$
X^′_l = LayerNorm(MHA(X_{l-1})+X_{l-1})
$$$$
X_l = LayerNorm(FFN(X^′_l)+X^′_l)
$$&lt;p&gt;其中，$X^′_l$ 和 $X_l$ 分别是该Transformer层的输入和输出，$X^′_l$是该层中输入经过多头注意力模块后的中间表示，LayerNorm表示层归一化。&lt;/p&gt;
&lt;h3 id=&#34;解码器&#34;&gt;🥥解码器
&lt;/h3&gt;$$
Y^′_l = LayerNorm(MaskedMHA(Y_{l-1})+Y_{l-1})
$$$$
Y^&#34;_l = LayerNorm(CrossMHA(Y^′_l,X_L)+Y^′_l)
$$$$
Y_l = LayerNorm(FFN(Y^&#34;_l)+Y^&#34;_l)
$$$$
O = softmax(W^LY_L)
$$&lt;p&gt;
其中，𝑶 ∈$R^{H \times V}$ 是模型最终的输出，代表下一个词在词表上的概率分布；$W^L$ ∈ $R^{H \times V}$ 是将输入表示映射到词汇表维度的参数矩阵，而$W^LY_L$是概率化前的中间值，通常被称为logits。&lt;/p&gt;
&lt;p&gt;[1]. JacobDevlin et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. In: NAACL-HLT. 2019.&lt;/p&gt;
&lt;p&gt;[2]. Alec Radford et al. “Improving language understanding by generative pre-training”. In: OpenAI Blog (2018).&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
