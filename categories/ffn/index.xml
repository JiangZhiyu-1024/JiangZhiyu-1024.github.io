<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>FFN on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/categories/ffn/</link>
        <description>Recent content in FFN on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Tue, 29 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/categories/ffn/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>transformeræ¨¡å‹æ¶æ„</title>
        <link>https://JiangZhiyu-1024.github.io/p/transformermodelbase/</link>
        <pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/transformermodelbase/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/transformermodelbase/house-7497002_1280.png" alt="Featured image of post transformeræ¨¡å‹æ¶æ„" /&gt;&lt;h1 id=&#34;æ¨¡å‹æ¶æ„&#34;&gt;æ¨¡å‹æ¶æ„
&lt;/h1&gt;&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391.png&#34;
	width=&#34;807&#34;
	height=&#34;689&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391_hu13532172091181392611.png 480w, https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029095402391_hu12135867853004229579.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241029095402391&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;281px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;å¤§è¯­è¨€æ¨¡å‹æ¶æ„é…ç½®è¡¨&lt;/center&gt;&gt;
&lt;h2 id=&#34;transformer-æ¨¡å‹&#34;&gt;ğŸˆTransformer æ¨¡å‹
&lt;/h2&gt;&lt;p&gt;å½“å‰ä¸»æµçš„å¤§è¯­è¨€æ¨¡å‹éƒ½æ˜¯åŸºäºTransformeræ¨¡å‹è¿›è¡Œè®¾è®¡çš„ã€‚Transformeræ˜¯ç”±å¤šå±‚çš„å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-headSelf-attentionï¼‰æ¨¡å—å †å è€Œæˆçš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚åŸå§‹çš„Transformer æ¨¡å‹ç”±ç¼–ç å™¨å’Œè§£ç å™¨ä¸¤ä¸ªéƒ¨åˆ†æ„æˆï¼Œè€Œè¿™ä¸¤ä¸ªéƒ¨åˆ†å®é™…ä¸Šå¯ä»¥ç‹¬ç«‹ä½¿ç”¨ï¼Œä¾‹å¦‚åŸºäºç¼–ç å™¨æ¶æ„çš„BERTæ¨¡å‹[1]å’Œè§£ç å™¨æ¶æ„çš„GPTæ¨¡å‹[2]ã€‚ä¸BERTç­‰æ—©æœŸçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œå¤§è¯­è¨€æ¨¡å‹çš„ç‰¹ç‚¹æ˜¯ä½¿ç”¨äº†æ›´é•¿çš„å‘é‡ç»´åº¦ã€æ›´æ·±çš„å±‚æ•°ï¼Œè¿›è€ŒåŒ…å«äº†æ›´å¤§è§„æ¨¡çš„æ¨¡å‹å‚æ•°ï¼Œå¹¶ä¸»è¦ä½¿ç”¨è§£ç å™¨æ¶æ„ï¼Œå¯¹äºTransformer æœ¬èº«çš„ç»“æ„ä¸é…ç½®æ”¹å˜å¹¶ä¸å¤§ã€‚&lt;/p&gt;
&lt;h3 id=&#34;è¾“å…¥ç¼–ç &#34;&gt;ğŸ‰è¾“å…¥ç¼–ç 
&lt;/h3&gt;&lt;p&gt;åœ¨Transformer æ¨¡å‹ä¸­ï¼Œè¾“å…¥çš„è¯å…ƒåºåˆ—(ğ’– = [ğ‘¢1,ğ‘¢2,&amp;hellip;,ğ‘¢ğ‘‡]) é¦–å…ˆç»è¿‡ä¸€ä¸ªè¾“å…¥åµŒå…¥æ¨¡å—ï¼ˆInputEmbeddingModuleï¼‰è½¬åŒ–æˆè¯å‘é‡åºåˆ—ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†æ•è·è¯æ±‡æœ¬èº«çš„è¯­ä¹‰ä¿¡æ¯ï¼Œæ¯ä¸ªè¯å…ƒåœ¨è¾“å…¥åµŒå…¥æ¨¡å—ä¸­è¢«æ˜ å°„æˆä¸ºä¸€ä¸ªå¯å­¦ä¹ çš„ã€å…·æœ‰å›ºå®šç»´åº¦çš„è¯å‘é‡$v_t$ âˆˆ$R^H$ã€‚ç”±äºTransformerçš„ç¼–ç å™¨ç»“æ„æœ¬èº«æ— æ³•è¯†åˆ«åºåˆ—ä¸­å…ƒç´ çš„é¡ºåºï¼Œä½ç½®ç¼–ç ï¼ˆPositionEmbedding,PEï¼‰è¢«å¼•å…¥æ¥è¡¨ç¤ºåºåˆ—ä¸­çš„ä½ç½®ä¿¡æ¯ã€‚ç»™å®šä¸€ä¸ªè¯å…ƒ$u_t$ï¼Œä½ç½®ç¼–ç æ ¹æ®å…¶åœ¨è¾“å…¥ä¸­çš„ç»å¯¹ä½ç½®åˆ†é…ä¸€ä¸ªå›ºå®šé•¿åº¦çš„åµŒå…¥å‘é‡$p_t$ âˆˆ$R^H$ã€‚ç„¶åï¼Œæ¯ä¸ªè¯å…ƒå¯¹åº”çš„è¯å‘é‡å’Œä½ç½®å‘é‡å°†ç›´æ¥ç›¸åŠ ï¼Œç”Ÿæˆäº†æœ€ç»ˆçš„è¾“å…¥åµŒå…¥åºåˆ—ğ‘¿=[ğ’™1,&amp;hellip;,ğ’™ğ‘‡]ï¼Œå¹¶ä¸”è¢«ä¼ å…¥åˆ°åç»­å±‚ä¸­ï¼š$x_t=v_t+p_t$.&lt;/p&gt;
&lt;p&gt;é€šè¿‡è¿™ç§å»ºæ¨¡æ–¹æ³•çš„è¡¨ç¤ºï¼ŒTransformer æ¨¡å‹å¯ä»¥åˆ©ç”¨ä½ç½®ç¼–ç  ğ’‘ğ‘¡ å»ºæ¨¡ä¸åŒè¯å…ƒçš„ä½ç½®ä¿¡æ¯ã€‚ç”±äºä¸åŒè¯å…ƒçš„ä½ç½®ç¼–ç ä»…ç”±å…¶ä½ç½®å”¯ä¸€å†³å®šï¼Œå› æ­¤è¿™ç§ä½ç½®å»ºæ¨¡æ–¹å¼è¢«ç§°ä¸ºç»å¯¹ä½ç½®ç¼–ç ã€‚å°½ç®¡ç»å¯¹ä½ç½®ç¼–ç èƒ½å¤Ÿä¸€å®šç¨‹åº¦ä¸Šå»ºæ¨¡ä½ç½®ä¿¡æ¯ï¼Œç„¶è€Œå®ƒåªèƒ½å±€é™äºå»ºæ¨¡è®­ç»ƒæ ·æœ¬ä¸­å‡ºç°çš„ä½ç½®ï¼Œæ— æ³•å»ºæ¨¡è®­ç»ƒæ•°æ®ä¸­æœªå‡ºç°è¿‡çš„ä½ç½®ï¼Œå› æ­¤æå¤§åœ°é™åˆ¶äº†å®ƒä»¬å¤„ç†é•¿æ–‡æœ¬çš„èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;h3 id=&#34;å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶&#34;&gt;ğŸ‹å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
&lt;/h3&gt;&lt;p&gt;å¤šå¤´è‡ªæ³¨æ„åŠ›æ˜¯Transformeræ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°æŠ€æœ¯ã€‚ç›¸æ¯”äºå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Network, RNNï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvolutionalNeuralNetwork,CNNï¼‰ç­‰ä¼ ç»Ÿç¥ç»ç½‘ç»œï¼Œå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿç›´æ¥å»ºæ¨¡ä»»æ„è·ç¦»çš„è¯å…ƒä¹‹é—´çš„äº¤äº’å…³ç³»ã€‚ä½œä¸ºå¯¹æ¯”ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œè¿­ä»£åœ°åˆ©ç”¨å‰ä¸€ä¸ªæ—¶åˆ»çš„çŠ¶æ€æ›´æ–°å½“å‰æ—¶åˆ»çš„çŠ¶æ€ï¼Œå› æ­¤åœ¨å¤„ç†è¾ƒé•¿åºåˆ—çš„æ—¶å€™ï¼Œå¸¸å¸¸ä¼šå‡ºç°æ¢¯åº¦çˆ†ç‚¸æˆ–è€…æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚è€Œåœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼Œåªæœ‰ä½äºåŒä¸€ä¸ªå·ç§¯æ ¸çš„çª—å£ä¸­çš„è¯å…ƒå¯ä»¥ç›´æ¥è¿›è¡Œäº¤äº’ï¼Œé€šè¿‡å †å å±‚æ•°æ¥å®ç°è¿œè·ç¦»è¯å…ƒé—´ä¿¡æ¯çš„äº¤æ¢ã€‚&lt;/p&gt;
$$
Q = XW^Q,
$$$$
K = XW^K,
$$$$
V = XW^V,
$$$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{D}})V.
$$$$
head_n = Attention(XW^Q_n,XW^K_n,XW^V_n)
$$$$
MHA = Concat(head_1,...,head_N)W^O
$$&lt;p&gt;ç”±ä¸Šè¿°å†…å®¹å¯è§ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿç›´æ¥å»ºæ¨¡åºåˆ—ä¸­ä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„å…³ç³»ï¼Œè¿›è€Œæœ‰æ•ˆæ•è·é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œå…·æœ‰æ›´å¼ºçš„åºåˆ—å»ºæ¨¡èƒ½åŠ›ã€‚å¦ä¸€ä¸ªä¸»è¦çš„ä¼˜åŠ¿æ˜¯ï¼Œè‡ªæ³¨æ„åŠ›çš„è®¡ç®—è¿‡ç¨‹å¯¹äºåŸºäºç¡¬ä»¶çš„å¹¶è¡Œä¼˜åŒ–ï¼ˆå¦‚GPUã€TPUç­‰ï¼‰éå¸¸å‹å¥½ï¼Œå› æ­¤èƒ½å¤Ÿæ”¯æŒå¤§è§„æ¨¡å‚æ•°çš„é«˜æ•ˆä¼˜åŒ–ã€‚&lt;/p&gt;
&lt;h3 id=&#34;å‰é¦ˆç½‘ç»œå±‚&#34;&gt;ğŸå‰é¦ˆç½‘ç»œå±‚
&lt;/h3&gt;$$
FFN(X) = Ïƒ(XW^U + b_1)W^D + b_2
$$&lt;p&gt;
å…¶ä¸­$W^U$ âˆˆ $R^{H \times H}$ å’Œ$W^D$ âˆˆ $R^{H \times H}$  åˆ†åˆ«æ˜¯ç¬¬ä¸€å±‚å’Œç¬¬äºŒå±‚çš„çº¿æ€§å˜æ¢æƒé‡çŸ©é˜µï¼Œ$b_1$ âˆˆ $R^{ğ»^â€²}$ å’Œ $b_2$ âˆˆ $R^H$ æ˜¯åç½®é¡¹ï¼Œğœæ˜¯æ¿€æ´»å‡½æ•°ï¼ˆåœ¨åŸå§‹çš„Transformerä¸­ï¼Œé‡‡ç”¨ ReLU ä½œä¸ºæ¿€æ´»å‡½æ•°ï¼‰ã€‚å‰é¦ˆç½‘ç»œå±‚é€šè¿‡æ¿€æ´»å‡½æ•°å¼•å…¥äº†éçº¿æ€§æ˜ å°„å˜æ¢ï¼Œæå‡äº† æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä»è€Œæ›´å¥½åœ°æ•è·å¤æ‚çš„äº¤äº’å…³ç³»ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440.png&#34;
	width=&#34;650&#34;
	height=&#34;691&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440_hu8196060707081265206.png 480w, https://JiangZhiyu-1024.github.io/p/transformermodelbase/image-20241029105634440_hu16997779109624366145.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241029105634440&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;94&#34;
		data-flex-basis=&#34;225px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;Transformer æ¶æ„å›¾&lt;/center&gt;
&lt;h3 id=&#34;ç¼–ç å™¨&#34;&gt;ğŸç¼–ç å™¨
&lt;/h3&gt;$$
X^â€²_l = LayerNorm(MHA(X_{l-1})+X_{l-1})
$$$$
X_l = LayerNorm(FFN(X^â€²_l)+X^â€²_l)
$$&lt;p&gt;å…¶ä¸­ï¼Œ$X^â€²_l$ å’Œ $X_l$ åˆ†åˆ«æ˜¯è¯¥Transformerå±‚çš„è¾“å…¥å’Œè¾“å‡ºï¼Œ$X^â€²_l$æ˜¯è¯¥å±‚ä¸­è¾“å…¥ç»è¿‡å¤šå¤´æ³¨æ„åŠ›æ¨¡å—åçš„ä¸­é—´è¡¨ç¤ºï¼ŒLayerNormè¡¨ç¤ºå±‚å½’ä¸€åŒ–ã€‚&lt;/p&gt;
&lt;h3 id=&#34;è§£ç å™¨&#34;&gt;ğŸ¥¥è§£ç å™¨
&lt;/h3&gt;$$
Y^â€²_l = LayerNorm(MaskedMHA(Y_{l-1})+Y_{l-1})
$$$$
Y^&#34;_l = LayerNorm(CrossMHA(Y^â€²_l,X_L)+Y^â€²_l)
$$$$
Y_l = LayerNorm(FFN(Y^&#34;_l)+Y^&#34;_l)
$$$$
O = softmax(W^LY_L)
$$&lt;p&gt;
å…¶ä¸­ï¼Œğ‘¶ âˆˆ$R^{H \times V}$ æ˜¯æ¨¡å‹æœ€ç»ˆçš„è¾“å‡ºï¼Œä»£è¡¨ä¸‹ä¸€ä¸ªè¯åœ¨è¯è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼›$W^L$ âˆˆ $R^{H \times V}$ æ˜¯å°†è¾“å…¥è¡¨ç¤ºæ˜ å°„åˆ°è¯æ±‡è¡¨ç»´åº¦çš„å‚æ•°çŸ©é˜µï¼Œè€Œ$W^LY_L$æ˜¯æ¦‚ç‡åŒ–å‰çš„ä¸­é—´å€¼ï¼Œé€šå¸¸è¢«ç§°ä¸ºlogitsã€‚&lt;/p&gt;
&lt;p&gt;[1]. JacobDevlin et al. â€œBERT: Pre-training of Deep Bidirectional Transformers for Language Understandingâ€. In: NAACL-HLT. 2019.&lt;/p&gt;
&lt;p&gt;[2]. Alec Radford et al. â€œImproving language understanding by generative pre-trainingâ€. In: OpenAI Blog (2018).&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
