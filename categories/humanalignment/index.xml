<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>HumanAlignment on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/categories/humanalignment/</link>
        <description>Recent content in HumanAlignment on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Thu, 31 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/categories/humanalignment/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>HumanAlignment</title>
        <link>https://JiangZhiyu-1024.github.io/p/humanalignment/</link>
        <pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/humanalignment/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/humanalignment/flowers-7382926_1920.jpg" alt="Featured image of post HumanAlignment" /&gt;&lt;h1 id=&#34;人类对齐&#34;&gt;🍁人类对齐
&lt;/h1&gt;&lt;p&gt;在大语言模型的学习过程中，如何确保大语言模型的行为与人类价值观、人类真实意图和社会伦理相一致成为了一个关键研究问题，通常称这一研究问题为人类对齐（HumanAlignment）。&lt;/p&gt;
&lt;h2 id=&#34;人类对齐的背景与标准&#34;&gt;🍂人类对齐的背景与标准
&lt;/h2&gt;&lt;h3 id=&#34;背景&#34;&gt;🌰背景
&lt;/h3&gt;&lt;p&gt;尽管大语言模型在下游任务中表现出优秀的性能，这些模型有时会出现错误或具有危害性的行为，例如无法正确遵循指令、生成虚假信息、以及产生有害、有误导性以及带有偏见的表达。在大语言模型的预训练和有监督微调的过程中，主要训练目标是根据上下文内容来预测下一个词元。但是，这一过程并未充分考虑人类的价值观或偏好，可能导致大语言模型从数据中学习到不符合人类期望的生成模式。为了规避这些潜在风险，研究人员提出了“人类对齐”这一关键概念，旨在保证大语言模型的行为与人类期望和价值观相一致[1,2]。&lt;/p&gt;
&lt;p&gt;与预训练和指令微调不同，人类对齐需引入全新的评估标准，如有用性、诚实性和无害性。 为了更直观地理解人类对齐对于大语言模型的重要性，例1对比了同一个语言模型在对齐前后对于相同输入的不同输出。在这个例子当中，输入的问题刻意包含了具有误导性的逻辑关系，即“土地价格”和“有污染的产业”是有直接关系的。因此，在经过人类价值观对齐之前的大语言模型会被输入中的错误逻辑所引导，产生了带有偏见的建议“农村地区更适合发展污染较严重的产业”。在经济生产中，发展有污染的产业需要综合考虑多方面的因素，不能仅仅因为土地价格更为便宜就认为适合发展相关产业。对齐前的大语言模型给出了一个错误的观点，不符合人类价值观，违背了无害性的原则。而经过与人类价值观对齐之后的大语言模型，先指出了输入问题中包含的错误逻辑（“我们不能简单地认为农村土地价格便宜就适合发展污染产业。”），并且给出了正确且合理的做法。对齐后的大语言模型的回复符合有用性和无害性，与人类价值观和偏好相符。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/humanalignment/image-20241031145634528.png&#34;
	width=&#34;770&#34;
	height=&#34;610&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/humanalignment/image-20241031145634528_hu8661545991163250632.png 480w, https://JiangZhiyu-1024.github.io/p/humanalignment/image-20241031145634528_hu13213345832109560578.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241031145634528&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;126&#34;
		data-flex-basis=&#34;302px&#34;
	
&gt;&lt;/p&gt;
&lt;center&gt;例1  大语言模型（YuLan）对于相同输入在对齐前后的不同输出&lt;/center&gt;
&lt;p&gt;思考：&lt;/p&gt;
&lt;p&gt;之前我就遇到过类似的情况，在前面的文章中我也提到过，LLM似乎就是倾向于认同人类的提问（就算那是错误的），到这里明白了，原来是没有经过对齐。那么我就有新的问题了，对齐也是要训练的吧，而且得要人来准备数据，这就又是一个耗费人力的工程了。&lt;/p&gt;
&lt;h3 id=&#34;对齐标准&#34;&gt;🍠对齐标准
&lt;/h3&gt;&lt;p&gt;人类对齐是一个较为抽象的概念，难以直接进行形式化建模，关于对齐的定义和标准也存在不同的观点。这里主要围绕三个具有代表性的对齐标准展开讨论，分别是有用性（Helpfulness）、诚实性（Honesty）和无害性（Harmlessness），这三种对齐标准已被现有的大语言模型对齐研究广泛使用[1,3]。下面具体介绍这三个代表性的对齐标准。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有用性. 在实际应用中，大语言模型需要提供有用的信息，能够准确完成任务，正确理解上下文，并展现出一定的创造性与多样性。模型应尽量以简洁、高效的方式协助用户完成任务。当任务描述存在歧义或涉及背景信息时，模型应具备主动询问并获取任务相关信息的能力，同时具有一定的敏感度、洞察力和审慎态度。由于用户意图的多样性，有用性这一对齐标准仍然难以进行统一的定义与刻画，需要根据不同的用户进行确定。&lt;/li&gt;
&lt;li&gt;诚实性. 模型的输出应具备真实性和客观性，不应夸大或歪曲事实，避免产生误导性陈述，并能够应对输入的多样性和复杂性。在人机交互过程中，大语言模型应向用户提供准确内容，还应适当表达对于输出信息的不确定性程度，以避免任何形式的误导。本质上，这要求模型了解自身的能力和知识水平。与有用性和无害性相比，诚实性是一个更为客观的标准，对人类标注的依赖相对较少。&lt;/li&gt;
&lt;li&gt;无害性. 大语言模型应避免生成可能引发潜在负面影响或危害的内容。在处理敏感主题时，模型应遵循道德标准和社会价值观，从而消除冒犯性与歧视性。此外，模型需要能够检测到具有恶意目的的查询请求。当模型被诱导执行危险行为（如犯罪行为）时，应直接予以拒绝。然而，何种行为被视为有害，很大程度上取决于大语言模型的使用者、用户问题类型以及使用大语言模型的背景。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上述三种通用的对齐标准较为宽泛，因此许多研究针对性地给出了一些更为细化的对齐标准，以更全面地规范大语言模型的输出。例如，行为对齐要求人工智能系统能够做出符合人类期望的行为；在此基础上，意图对齐则进一步要求大语言模型在意图和行为上都要与人类期望保持一致，这涉及到哲学、心理学以及技术细节上的多重挑战；道德对齐要求语言模型应避免涉及非法、不道德或有害的话题，在回应中优先考虑用户安全、道德准绳和行为边界。这些对齐标准在本质上与前述三个标准是相似的，研究人员可以根据任务的特定需求进行调整。&lt;/p&gt;
&lt;p&gt;在实践中，红队攻击（RedTeaming）技术也被广泛运用，通过人工或自动 化的手段，以对抗方式探测大语言模型，诱导其生成有害输出，并据此针对性地 调整大语言模型，以避免产生此类有害输出[4]。&lt;/p&gt;
&lt;h2 id=&#34;基于人类反馈的强化学习&#34;&gt;🍊基于人类反馈的强化学习
&lt;/h2&gt;&lt;p&gt;由于对齐标准难以通过形式化的优化目标进行建模，因此研究人员提出了基于人类反馈的强化学习（Reinforcement Learning from Human Feedback,RLHF），引入人类反馈对大语言模型的行为进行指导。&lt;/p&gt;
&lt;h3 id=&#34;rlhf-概述&#34;&gt;🥮RLHF 概述
&lt;/h3&gt;&lt;p&gt;RLHF首先需要收集人类对于不同模型输出的偏好，然后使用收集到的人类反馈数据训练奖励模型，最后基于奖励模型使用强化学习算法（例如Proximal Policy Optimization,PPO[5]）微调大语言模型。这种将人类反馈纳入大语言模型训练过程的方法已成为实现人类对齐的主要技术途径之一。&lt;/p&gt;
&lt;h4 id=&#34;rlhf算法系统&#34;&gt;🎑RLHF算法系统
&lt;/h4&gt;&lt;p&gt;RLHF 算法系统主要包括三个关键组成部分：需要与人类价值观对齐的模型、基于人类反馈数据学习的奖励模型以及用于训练大语言模型的强化学习算法。&lt;/p&gt;
&lt;p&gt;具体来说，待对齐模型一般指的是经过预训练、具备一定通用能力的大语言模型。然而，这些模型并没有与人类价值观对齐，在下游任务中可能表现出不合适甚至有害的行为。&lt;/p&gt;
&lt;p&gt;奖励模型的作用是为强化学习过程提供指导信号，反映了人类对于语言模型生成文本的偏好，通常以标量值的形式呈现。奖励模型既可以采用人类偏好数据对已有的语言模型继续微调，也可以基于人类偏好数据重新训练一个新的语言模型。&lt;/p&gt;
&lt;p&gt;在训练过程中，基于奖励模型提供的反馈信号，RLHF使用特定的强化学习算法进行大语言模型的训练。目前，PPO算法[5] 是一种被广泛用于人类对齐的强化学习算法。&lt;/p&gt;
&lt;h4 id=&#34;rlhf的关键步骤&#34;&gt;🥧RLHF的关键步骤
&lt;/h4&gt;&lt;p&gt;监督微调. 为了让待对齐语言模型具有较好的指令遵循能力，通常需要收集高质量的指令数据进行监督微调。指令数据一般包括任务描述和示例输出，可以由人类标注员针对特定任务编写，也可以由大语言模型自动生成。&lt;/p&gt;
&lt;p&gt;奖励模型训练.第二步是使用人类反馈数据训练奖励模型。具体来说，首先使用语言模型针对任务指令生成一定数量的候选输出。随后，邀请标注员对于输出文本进行偏好标注，这个标注过程可以采用多种形式，其中最常用的是对候选文本进行排序标注，这样可以有效减少多个标注员之间的不一致情况。进一步，使用人工标注的偏好数据进行奖励模型的训练，使其能够建模人类偏好。&lt;/p&gt;
&lt;p&gt;强化学习训练. 在这一步骤中，语言模型对齐被转化为一个强化学习问题。具体来说，待对齐语言模型担任策略实施者的角色（称为策略模型），它接收提示作为输入并返回输出文本，其动作空间是词汇表中的所有词元，状态指的是当前已生成的词元序列。&lt;/p&gt;
&lt;h3 id=&#34;人类反馈数据的收集&#34;&gt;🔥人类反馈数据的收集
&lt;/h3&gt;&lt;p&gt;在预训练阶段，大语言模型通过语言建模目标在大规模无标注语料库上进行训练。然而，这一过程无法直接反映人类对于大语言模型输出的主观和定性偏好（本书中称为“人类反馈”）。为了实现有效的人类对齐，需要使用高质量的人类反馈数据（不是高质量人类，哈哈哈）对大语言模型进行针对性的微调。&lt;/p&gt;
&lt;h4 id=&#34;人类反馈形式&#34;&gt;🧡人类反馈形式
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;基于评分的人类反馈.最直接的标注方式是根据预设的标准邀请标注人员对于大语言模型的输出进行打分，从而作为模型输出质量的判断。&lt;/li&gt;
&lt;li&gt;基于排序的人类反馈.排序是一种比较典型的人类偏好标注形式。最简单的方式是标注人员根据自身偏好对于大语言模型的输出进行全排序。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;奖励模型的训练&#34;&gt;🏮奖励模型的训练
&lt;/h3&gt;&lt;p&gt;由于RLHF的训练过程中需要依赖大量的人类偏好数据进行学习，因此很难在训练过程中要求人类标注者实时提供偏好反馈。为此，我们需要训练一个模型来替代人类在RLHF训练过程中实时提供反馈，这个模型被称为奖励模型。&lt;/p&gt;
&lt;p&gt;思考：&lt;/p&gt;
&lt;p&gt;这里倒是解决我的疑问了，可以训练一个奖励模型干这个事。&lt;/p&gt;
&lt;h2 id=&#34;非强化学习的对齐方法&#34;&gt;🀨非强化学习的对齐方法
&lt;/h2&gt;&lt;p&gt;尽管RLHF已被证明是一种较为有效的语言模型对齐技术，但是它也存在一些局限性。首先，在RLHF的训练过程中，需要同时维护和更新多个模型，这些模型包括策略模型、奖励模型、参考模型以及评价模型。这不仅会占用大量的内存资源，而且整个算法的执行过程也相对复杂。此外，RLHF中常用的近端策略优化算法在优化过程中的稳定性欠佳，对超参数的取值较为敏感，这进一步增加了模型训练的难度和不确定性。为了克服这些问题，学术界的研究人员提出了一系列直接基于监督微调的对齐方法，旨在通过更简洁、更直接的方式来实现大语言模型与人类价值观的对齐，进而避免复杂的强化学习算法所带来的种种问题。&lt;/p&gt;
&lt;h3 id=&#34;代表性监督对齐算法dpo&#34;&gt;☕代表性监督对齐算法DPO
&lt;/h3&gt;&lt;p&gt;直接偏好优化（DirectPreference Optimization, DPO）是一种不需要强化学习的对齐算法。由于去除了复杂的强化学习算法，DPO可以通过与有监督微调相似的复杂度实现模型对齐，不再需要在训练过程中针对大语言模型进行采样，同时超参数的选择更加容易。&lt;/p&gt;
&lt;h2 id=&#34;参考文献&#34;&gt;🌆参考文献
&lt;/h2&gt;&lt;p&gt;[1]. Long Ouyang et al. “Training language models to follow instructions with human feed back”. In: arXiv preprint arXiv:2203.02155 (2022).&lt;/p&gt;
&lt;p&gt;[2]. Daniel M. Ziegler et al. “Fine-Tuning Language Models from Human Preferences”. In: arXiv preprint arXiv:1909.08593 (2019).&lt;/p&gt;
&lt;p&gt;[3]. Amelia Glaese et al. “Improving alignment of dialogue agents via targeted human judge ments”. In: arXiv preprint arXiv:2209.14375 (2022).&lt;/p&gt;
&lt;p&gt;[4]. Ethan Perez et al. “Red Teaming Language Models with Language Models”. In: EMNLP. 2022.&lt;/p&gt;
&lt;p&gt;[5]. John Schulman et al. “Proximal policy optimization algorithms”. In: arXiv preprint arXi v:1707.06347 (2017).&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
