<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>PPML on Zion Blaze</title>
        <link>https://JiangZhiyu-1024.github.io/categories/ppml/</link>
        <description>Recent content in PPML on Zion Blaze</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Zion Blaze</copyright>
        <lastBuildDate>Mon, 11 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://JiangZhiyu-1024.github.io/categories/ppml/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>端到端隐私保护的多机构医学影像深度学习</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E7%AB%AF%E5%88%B0%E7%AB%AF%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E7%9A%84%E5%A4%9A%E6%9C%BA%E6%9E%84%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
        <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E7%AB%AF%E5%88%B0%E7%AB%AF%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E7%9A%84%E5%A4%9A%E6%9C%BA%E6%9E%84%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E7%AB%AF%E5%88%B0%E7%AB%AF%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E7%9A%84%E5%A4%9A%E6%9C%BA%E6%9E%84%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/fog-7482180_1280.jpg" alt="Featured image of post 端到端隐私保护的多机构医学影像深度学习" /&gt;&lt;h1 id=&#34;端到端隐私保护的多机构医学影像深度学习&#34;&gt;端到端隐私保护的多机构医学影像深度学习
&lt;/h1&gt;&lt;p&gt;使用大型、多国数据集来构建高性能医学影像人工智能系统需要隐私保护机器学习的创新，以便模型能够在不需要数据传输的情况下对敏感数据进行训练。我们在此介绍&lt;strong&gt;PriMIA&lt;/strong&gt;（隐私保护的医学影像分析），这是一个免费、开源的软件框架，用于医学影像数据的差分隐私、加密聚合的联邦学习和加密推理。我们在实际案例中测试了PriMIA，通过一个专家级深度卷积神经网络对儿童胸部X光片进行分类，所得到的模型分类性能与在本地非安全环境中训练的模型相当。我们从理论和实证上评估了该框架的性能和隐私保障，并展示了所提供的保护措施如何防止通过基于梯度的模型反演攻击来重构可用数据。最后，我们在端到端的加密远程推理场景中成功应用了训练模型，利用安全多方计算来防止数据和模型的泄露（还好有最后一句，不然差点又不看这个论文了）。&lt;/p&gt;
&lt;p&gt;人工智能（AI）和机器学习（ML）在生物医学数据分析中的快速发展最近带来了令人鼓舞的成果，展示了AI系统能够在多种情境下协助临床医生，例如在医学影像中进行癌症的早期检测。这类系统正逐步超越概念验证阶段，预计将在未来几年实现广泛应用，正如专利申请数量和监管审批不断增加所显示的那样。高性能AI系统的共同特征是需要大量多样的数据集来训练ML模型，这通常通过数据所有者自愿共享数据以及多机构或多国数据集的积累来实现。通常情况下，患者数据会在原始机构进行匿名化或假名化处理，然后传输至分析和模型训练的场所（即集中式数据共享）。然而，匿名化已被证明不足以抵御再识别攻击。因此，大规模的患者数据收集、聚合和传输从法律和伦理角度来说至关重要。此外，控制个人健康数据的存储、传输和使用是患者的一项基本权利。集中式数据共享在实际中几乎剥夺了这种控制权，导致数据主权的丧失。此外，匿名化数据一旦传输，就难以进行追溯性的更正或增强，例如引入后来获得的额外临床信息。&lt;/p&gt;
&lt;p&gt;尽管存在这些顾虑，对数据驱动解决方案的需求不断增加，预计将进一步推动健康相关数据的收集，不仅来自医学影像数据集、临床记录和医院患者数据，还包括例如通过可穿戴健康传感器和移动设备收集的数据。因此，需要创新的解决方案来平衡数据使用与隐私保护。安全且隐私保护的机器学习（PPML）旨在保护数据的安全、隐私和保密性，同时仍然允许从数据中得出有用的结论或将其用于模型开发。实际中，PPML使得即便在本地数据有限的低信任环境中也能进行先进的模型开发。这种环境在医学领域很常见，因为数据所有者无法依赖其他方的隐私和保密合规性。PPML还可以为模型所有者提供保障，确保其模型在使用过程中不会被修改、盗用或滥用，例如通过加密保护。通过缓解资产保护的顾虑，这为可持续的协作模型开发和商业部署奠定了基础。&lt;/p&gt;
&lt;h2 id=&#34;先前研究的证据&#34;&gt;先前研究的证据
&lt;/h2&gt;&lt;p&gt;近期研究显示了PPML在生物医学科学，尤其是医学影像中的实用性。例如，联邦学习（FL）是一种基于将机器学习模型分发给数据所有者（即计算节点）进行分布式训练的去中心化计算技术，而非将数据集集中收集。该方法被提议用于促进跨国协作，同时避免数据传输。在COVID-19疫情背景下，FL被用于保留数据主权并执行数据仓库的本地治理政策。在医学影像领域，最近的研究表明，基于脑肿瘤分割或乳腺密度分类的深度学习模型的联邦训练，其表现与本地训练相当，并且能够更广泛地纳入多样化数据源，提升了模型的泛化能力。&lt;/p&gt;
&lt;p&gt;然而，FL本身并非完全的隐私保护技术。先前研究表明，反演攻击能够通过模型权重或梯度更新重构图像，甚至具有令人惊讶的视觉细节。此外，在“推理即服务”的场景下，模型暴露给不可信的第三方可能导致模型被滥用或直接盗用。因此，FL必须结合其他隐私增强技术才能真正保护隐私。例如，通过安全聚合（SecAgg）模型权重或梯度更新，或差分隐私（DP），可以防止数据集重构攻击，而在模型推理过程中使用安全多方计算（SMPC）协议可以保护正在使用的模型。我们在先前的工作中对这些技术进行了概述。&lt;/p&gt;
&lt;h2 id=&#34;目标与贡献&#34;&gt;目标与贡献
&lt;/h2&gt;&lt;p&gt;PPML在医学影像中的临床应用需要开发安全与隐私框架，并在复杂的临床任务中进行验证。我们在此介绍PriMIA，这是一个免费、开源的框架，用于医学影像的端到端隐私保护去中心化深度学习。我们的框架结合了差分隐私的联邦模型训练、模型更新的加密聚合以及加密的远程推理。我们的贡献包括以下创新：&lt;/p&gt;
&lt;p&gt;我们展示了在公共互联网环境下，通过PriMIA的隐私增强技术辅助联邦学习（FL），对具有临床挑战性的儿童胸部X光片分类任务进行深度卷积神经网络（CNN）训练的过程。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们的框架兼容多种医学影像数据格式，用户配置简单，并在FL训练中引入了功能性改进（加权梯度下降/联邦平均、丰富的数据增强、本地提前停止、联邦范围的超参数优化、DP数据集统计交换），提升了灵活性、可用性、安全性和性能。&lt;/li&gt;
&lt;li&gt;我们比较了采用与不采用隐私增强技术训练的模型、在聚合数据集上集中训练的模型、在数据子集上个性化训练的模型以及在未见的真实数据集上与专家放射科医生的分类表现，评估医学影像研究中的各种典型情境。&lt;/li&gt;
&lt;li&gt;我们评估了框架的理论和实证隐私与安全保障，并提供了针对多种训练场景下模型的先进梯度反演攻击的应用示例。&lt;/li&gt;
&lt;li&gt;最后，我们展示了在安全的“推理即服务”场景中使用训练模型的案例，实现了数据和模型在明文中不被泄露，并展示了我们SMPC协议在推理延迟方面的改进。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;库功能&#34;&gt;库功能
&lt;/h2&gt;&lt;p&gt;PriMIA是作为PySyft/PyGrid开源PPML工具生态系统的扩展开发的。PySyft（https://github.com/OpenMined/PySyft）是一个Python框架，允许远程执行机器学习任务（例如，张量操作），并通过与常见的机器学习框架（如PyTorch）接口进行加密深度学习。PyGrid提供了服务器/客户端功能，用于在服务器和边缘计算设备上部署此类工作流。有关这些框架所提供的通用功能的详细描述，请参见我们之前的工作。PriMIA基于这些功能，针对医学影像特定应用进行了扩展，原生支持医学影像数据格式，如DICOM，并能够在任意模态和维度的医学数据集上进行操作（例如，计算机断层扫描、X光、超声和磁共振成像）。除了上述PPML技术外，它还提供了解决医学影像分析工作流中常见挑战的方案，例如数据集不平衡、先进的图像增强、全联邦超参数调优功能。此外，它还提供了一个可访问的用户界面，支持从用户机器上的本地实验到远程计算节点上的分布式训练的应用，以促进在医学联盟中应用PPML最佳实践。库的源代码、文档和公开可用的数据可以在https://doi.org/10.5281/zenodo.454559918上找到。&lt;/p&gt;
&lt;h2 id=&#34;案例研究系统设计与威胁模型&#34;&gt;案例研究、系统设计与威胁模型
&lt;/h2&gt;&lt;p&gt;我们通过在公共互联网的云计算节点上训练一个包含1110万个参数的ResNet18 CNN模型，展示了PriMIA在临床数据上的应用案例。该模型在由Kermany等人最初提出的儿童肺炎数据集上进行训练，目的是将儿童胸部X光片分类为以下三类之一：正常（无感染迹象）、病毒性肺炎或细菌性肺炎。肺炎是导致儿童死亡的主要原因之一。胸部X光检查常规用于鉴别诊断和治疗选择，但分类儿童胸部X光片具有挑战性。该案例研究的设置基于以下真实场景：&lt;/p&gt;
&lt;h2 id=&#34;fl训练阶段&#34;&gt;FL训练阶段
&lt;/h2&gt;&lt;p&gt;三个医院的联合体希望训练一个深度学习模型，用于胸部X光片分类。由于它们自身既没有足够的数据，也没有足够的专业知识来在这些数据上训练模型，因此它们寻求模型开发者的支持，以在中央服务器上协调训练。在训练阶段，我们将持有患者数据的医院称为数据所有者。在本文中，我们使用“模型”一词来指代深度神经网络的结构和参数。我们假设训练阶段采用先诚实后好奇的威胁模型，如前所述。这里，参与者信任彼此不会主动破坏学习协议，避免有意降低效用，例如主动提供对抗性输入或低质量数据（诚实）。然而，假设个别参与者及其合谋团体会主动尝试从其他参与者的数据中提取私人信息（好奇）。我们的框架的隐私增强技术旨在防止这种行为，我们将在后续部分详细描述。简而言之，差分隐私梯度下降将差分隐私的保证属性扩展到深度神经网络训练中。具体来说，它限制了数据集中单个患者的最坏隐私损失，并提供了针对模型反演/重构攻击的隐私保障，这些攻击可能发生在联邦参与者或在推理时对模型所有者进行攻击。PriMIA为每个FL节点实现了差分隐私（局部差分隐私），以提供患者级的隐私保证。每个节点的隐私预算使用Rényi差分隐私会计师进行。SMPC允许各方在不泄露各自贡献的情况下共同计算函数。在训练过程中，它用于安全地平均网络权重更新（SecAgg）。SecAgg使用基于SPDZ协议的加法秘密共享。训练阶段如图1所示，最终所有参与者都持有完全训练好的最终模型副本。&lt;/p&gt;
&lt;h2 id=&#34;远程推理阶段&#34;&gt;远程推理阶段
&lt;/h2&gt;&lt;p&gt;一旦模型完全训练完成，就可以用于远程推理。在我们的案例研究中，我们假设一个不同的数据所有者——在远程位置的医生，持有一些患者数据，并希望从模型中获得推理结果以协助诊断。推理服务由模型所有者通过互联网提供。数据和模型所有者之间不信任彼此，并希望他们的数据和模型保持私密。PriMIA的SMPC协议保证了推理阶段模型和数据的加密安全性。我们在之前的工作中描述的AriaNN框架被用来适应端到端加密推理。一个常见的SMPC技术是利用提前生成的加密安全随机数（加密原语）（即所谓的离线阶段），以加速某些计算。提供这些原语的可信系统（例如硬件设备）被称为加密提供者，它不参与实际的推理过程（在线阶段），也永远不会接触到任何一方的数据。实际上，加密原语的“储备”可以提前提供给协议参与者，在多个推理过程中使用。加密推理过程总结如图2所示。&lt;/p&gt;
&lt;h3 id=&#34;分类性能&#34;&gt;分类性能：
&lt;/h3&gt;&lt;p&gt;我们训练了不使用SecAgg或DP的FL模型（DP-/SecAgg-），仅使用SecAgg的模型（DP-/SecAgg+）和同时使用这两种技术的模型（DP+/SecAgg+）。此外，我们还在单台机器上对整个数据集进行了训练（集中训练），并对数据所有者的单独数据子集进行了个性化训练。集中训练的模型代表了引言中描述的集中数据共享场景。个性化模型则代表了每个机构仅基于其自身数据进行训练的情况，这是当前医学影像研究工作流程中的典型做法。FL旨在通过使模型训练效果优于个性化训练，并且理想情况下达到与集中训练模型相当的效果。我们在验证集上测试了这些模型的分类性能，并与两名专家放射科医师在测试集1（145张图像）上的分类表现进行了比较，同时还与测试集2（345张图像）上的临床实际数据进行了对比。我们使用了准确度、敏感性/特异性（召回率）、受试者工作特征曲线下的面积（ROC-AUC）和马修斯相关系数（MCC）27作为评估指标。详细信息请参见方法部分。模型和专家在数据集上的分类性能见表1。未使用SecAgg和DP的FL模型表现最好，与集中训练模型之间没有显著的统计差异。加入SecAgg后，模型性能略微下降，但未达到统计显著性。无论是FL模型还是集中训练模型，在性能上均显著优于人工观察者。DP训练过程（ε = 6.0，δ = 1.9 × 10^-4，α值（发散度阶数）为4.4）显著降低了模型性能，但该模型的表现仍与人工观察者相当，并在测试集1和2的样本外数据上保持了稳定的表现。我们注意到，ε值代表训练结束时所消耗的总隐私预算。仅在数据所有者的个别数据子集上训练的个性化模型仅在验证数据上表现相当，但在测试集1和2的样本外数据上表现显著较差，表明其泛化能力较差。这些结果的统计评估以及评分者间/模型间的一致性度量可以在补充材料2部分和补充表格1、2中找到。&lt;/p&gt;
&lt;h3 id=&#34;训练和推理性能基准测试&#34;&gt;训练和推理性能基准测试：
&lt;/h3&gt;&lt;p&gt;为了评估PriMIA隐私增强技术对性能的影响，我们在多种场景中对训练和推理性能进行了基准测试，如图3所示。训练时长是以固定批量大小计算的每批次平均时间，以便将其与数据集大小解耦。与本地训练相比，FL由于网络通信的开销会导致性能下降，加入SecAgg和DP后，性能下降更加明显，当同时使用SecAgg和DP时，训练时间是原来的三倍。大型神经网络架构由于需要更多的网络传输时间，因此需要更长的训练时间，这也为我们研究中使用ResNet18架构而非更大ResNet架构提供了依据。增加更多的工作节点会导致时间线性增加，尤其是在使用SecAgg时，因该协议的通信开销。然而，由于每轮操作的数量较少，该协议对多方的扩展性较好：对扩展性的线性回归分析得出t(w) = 0.57w + 2.61，其中t表示时间（秒），w表示工作节点的数量（R² = 0.98，p &amp;lt; 0.001，N = 100个样本，每个工作节点数量测试）。没有SecAgg时，训练时间几乎是常数。对于较大的数据集大小，批次训练时间保持不变，表明训练时间仅依赖于数据集大小，在其他条件相同的情况下。最后，我们基于功能秘密共享（FSS）协议28对加密推理实现进行了基准测试，该协议在执行比较操作、最大池化和批量归一化层时相较于广泛使用的SecureNN29提供了更高的效率。使用FSS进行加密推理显著减少了推理时间，特别是在高延迟设置中，FSS相比SecureNN提供了更好的性能。实现细节可以在方法部分找到，统计评估可以在补充材料3部分找到。&lt;/p&gt;
&lt;h3 id=&#34;模型反演攻击&#34;&gt;模型反演攻击：
&lt;/h3&gt;&lt;p&gt;先前的研究13,30表明，模型反演攻击能够重建特征或整个数据集记录（在我们的案例中是胸部X光图像），因此在FL设置中构成患者隐私的威胁。为了说明使用和不使用PriMIA提供的隐私增强技术训练的模型的易受攻击性，我们利用了改进的深度梯度泄漏攻击31,32，并进行了如方法部分所述的小修改。我们选择这种方法是因为它是第一个被证明对我们的案例研究中使用的ResNet18架构高度有效的技术。图4展示了胸部X光案例研究的示例结果。&lt;/p&gt;
&lt;p&gt;我们使用逐像素均方误差（MSE）、信噪比（SNR）和Fréchet初始距离（FID）指标来量化攻击成功率。实证评估表明，攻击的成功率高度依赖于梯度更新的L2范数和所使用的批量大小。为了生成最佳情况下的基准攻击，我们在训练开始时，以批量大小为1攻击集中训练模型，此时损失幅度（因此梯度范数）最高。对于我们案例研究中使用SecAgg的FL模型，攻击未成功，可能是因为其有效批量大小为600。与DP的隐私保障一致，当使用DP训练时，攻击未能成功。结果表明，即使模型在本地被攻击，或未使用SecAgg时，DP也能抵消攻击，这些结果可见于补充材料5部分和补充图2。为了进一步强调在医疗影像设置中隐私相关攻击的高风险，以及因此在协作模型训练中隐私增强技术的重要性，我们在公开的MedNIST数据集上进行了额外实验，并能够在未使用DP的情况下恢复揭示敏感患者属性的图像。启用DP时无法恢复任何图像（图5）。关于攻击的更多细节和统计评估可以在方法部分和补充材料4、6部分找到。&lt;/p&gt;
&lt;p&gt;讨论：我们介绍了PriMIA，这是一个用于医疗影像隐私保护FL和加密推理的开源框架。我们展示了在儿科胸部X光图像分类这一具有挑战性的临床任务中，如何进行去中心化的协作训练，并训练出了一个专家级深度卷积神经网络。此外，我们还展示了端到端加密推理，可以用于安全的诊断服务，无需披露机密数据或暴露模型。我们的工作是向医疗影像工作流程中实现下一代隐私保护方法的第一步。它适用于多机构研究和企业模型开发环境，能够保护数据治理和患者健康数据的主权。我们的框架可以用于推理即服务场景，在这些场景中，诊断支持可以远程提供，同时提供隐私、机密性和资产保护的理论和实证保障。PriMIA代表了我们以前工作17的针对性进化，面向医疗保健领域的部署。虽然我们在所展示的案例研究中专注于分类任务，但PriMIA高度可适应多种医疗影像分析工作流程，支持不同的网络架构、数据集等。在补充材料7部分和补充图3中，我们展示了一个额外的案例研究，聚焦于腹部计算机断层扫描的语义分割，以展示这一灵活性。&lt;/p&gt;
&lt;h3 id=&#34;模型分类性能&#34;&gt;模型分类性能：
&lt;/h3&gt;&lt;p&gt;最近的研究评估了数据质量（过于同质/独立同分布的数据与过于异质的数据）和分布式系统拓扑对联邦模型性能的影响，例如模型对样本外数据的泛化能力。在我们的案例研究中，使用FL训练的模型与集中训练模型表现相当，类似于参考文献5，并且优于人类观察者。仅在数据子集上训练的模型（个性化模型）在样本外数据上的表现显著下降。由于个性化模型训练是大多数单中心医疗影像研究中的标准做法，这一发现提醒我们，通过FL包含来自多个来源的大量更具多样性的数据，可以训练出具有更好泛化性能的模型，这也是当前最佳实践所要求的33。DP模型训练能够提供客观的隐私保障，并增强对模型反演攻击的抵抗力30,32。使用DP会降低模型性能，但其表现仍与人类观察者相当。与此同时，所选模型实现的DP保障（ε = 6）仅为中等水平。这一现象（隐私与效用的权衡）是深度学习与DP领域中一个众所周知的观察结果。例如，先前的研究23在CIFAR-10数据集上达到了大约8的ε值，另一项研究报告了ε值在6.9到8.48之间34。这两项研究也报告了最终模型性能的下降。我们认为改进DP模型训练的方法是未来研究的一个有前景的方向。&lt;/p&gt;
&lt;h3 id=&#34;fl功能改进&#34;&gt;FL功能改进：
&lt;/h3&gt;&lt;p&gt;为了提高框架的可用性、灵活性以及FL模型性能，我们的框架包括以下功能改进。（1）除了采用最近显示出改进收敛结果的Adam优化器形式的自适应客户端优化外，我们还包括了一系列先进的图像增强技术，包括MixUp，已被证明具有增强隐私保护的属性36。（2）我们实施了技术来解决节点之间（本地早停）以及数据集类别之间（类别加权梯度下降和联邦平均37）的数据量不平衡问题。（3）我们提供了在整个联盟中进行集中协调超参数优化的功能，使用的是树状Parzen估计器算法38。展示我们使用超参数选择框架搜索最优FL模型的实验数据可以在补充材料1部分和补充图1中找到。上述所有训练优化都在节点本地实施，并且不会对隐私保障产生负面影响。然而，在使用DP时，超参数调优必须考虑，因为它依赖于多次训练重复。&lt;/p&gt;
&lt;h3 id=&#34;关于隐私增强技术的讨论&#34;&gt;关于隐私增强技术的讨论：
&lt;/h3&gt;&lt;p&gt;在FL过程中引入提供可证明隐私和安全保障的方法是向广泛实施隐私保护AI技术迈出的关键一步8。我们在攻击实验中成功重建未保护模型的图像，强调了此类攻击对患者隐私的风险，这一点也在之前的研究中有所讨论6,39。DP训练在遭遇来自联盟成员或推理过程中的攻击时提供了客观的隐私保障，并不限于我们示例中使用的基于梯度的反演攻击。利用SMPC的SecAgg即使在最多n−1个成员串通的情况下，也仅将聚合模型更新披露给各方。这种我们提出的DP安全聚合数据集统计（均值和标准差）的方法可以保护FL参与者免受数据泄露，特别是在模型构建中包含非影像数据时（例如临床记录，其中如年龄等特征的均值代表敏感信息）。最后，加密推理不会向任何一方泄露数据或模型信息。与完全同态加密协议40（依赖于基于密钥的加密技术）相比，后者在神经网络训练和推理中的实现受到加密过程的计算复杂性和由于函数近似（例如激活函数）带来的性能下降的限制，通信开销传统上一直是SMPC的限制因素。在我们最近的工作中，我们引入了AriaNN26，这是一种利用函数秘密共享（FSS）28并基于SPDZ25构建的SMPC协议。它代表了一种替代协议，如SecureNN29或Falcon41，并通过一次通信回合计算私有比较。这使得FSS比其他SMPC协议在通信上更加高效，特别是在各方地理位置远离且通信延迟高时，例如我们研究中展示的在公共网络上执行推理的情况。通过本案例，我们确认了在其他数据集上获得的结果：在高延迟环境下，安全推理从FSS协议中获得的好处成比例更大。因此，我们建议在诚实但好奇的环境下，当希望减少延迟时，使用FSS而非SecureNN。&lt;/p&gt;
&lt;h2 id=&#34;与先前工作的比较&#34;&gt;与先前工作的比较：
&lt;/h2&gt;&lt;p&gt;当前有几项研究旨在将隐私保护机器学习（PPML）技术引入生物医学影像领域：Silva等人42提出了一种面向生物医学的前端FL框架，但未考虑DP、SecAgg或加密推理。Xu及其同事（https://bit.ly/3pl5dD1）提供了一个使用同态加密进行SecAgg的FL框架，但未使用DP或提供加密推理能力。Sheller等人43展示了一个基于分割的FL应用案例，但没有评估DP、SecAgg或加密推理选项。Li等人44也展示了一个FL分割任务。他们的DP实现依赖于一种替代技术（稀疏向量），且框架未提供安全聚合或加密推理功能。Lu及其同事45的研究展示了带有DP的FL，但他们的应用案例集中于病理切片，未使用SecAgg或提供加密推理能力。Li等人46使用了DP，但假设了固定的敏感性并未进行隐私分析，他们的框架不提供SecAgg或加密推理。&lt;/p&gt;
&lt;h2 id=&#34;局限性&#34;&gt;局限性：
&lt;/h2&gt;&lt;p&gt;我们考虑了以下几点局限性。部署我们的系统的计算要求很高，尽管我们提出了协议改进，但加密推理带来的延迟仍然远高于未加密推理。当前的远程执行环境仅提供实验性的图形处理单元（GPU）支持，计划在即将发布的版本中提供完整支持。FL模型的成功在很大程度上依赖于节点上的高质量数据。数据的审计和整理、量化单个数据集对模型的贡献或检测局部过拟合的方法仍在研究中47。我们的库设计用于诚实但好奇的环境，我们认为这代表了医疗联盟中的标准。因此，尽管我们提供了全面的隐私保护措施，但没有针对恶意贡献低质量或对抗性数据到FL过程中的具体反制措施，也未验证/保证数据所有者推理设置中使用的模型是所承诺的模型。此外，我们指出，关于理论威胁模型的讨论是一种抽象层次，无法完全代表现实生活中的复杂情况。例如，威胁建模通常是在代表整个医院的FL参与者层面进行的，但这不能考虑到为这些医院工作的每个个体及其具体动机。类似地，关于FL中参与者报酬或模型所有权的问题超出了我们当前研究的范围。未来的研究需要进一步阐明这些细节。最后，如上所述，使用DP会导致模型隐私和效用之间的直接权衡。未来的工作需要通过改进隐私分析和训练技术来解决这一权衡，因为当前研究的隐私保障（包括我们研究中大约6.0的ε值）尚不够严格，不能被认为是普遍适用的。&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论：
&lt;/h2&gt;&lt;p&gt;我们提出了一个免费的开源软件框架，用于隐私保护的FL和医疗影像数据的端到端加密推理，并在一个具有临床意义的实际案例研究中展示了该框架。进一步的研究和开发将促进我们框架的更大规模部署，验证我们在不同跨机构数据上的发现，并推动PPML技术在医疗保健及其他领域的广泛应用。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>安全与隐私机器学习：技术与应用的综述</title>
        <link>https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/</link>
        <pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/</guid>
        <description>&lt;img src="https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/leaves-4574860_1280.jpg" alt="Featured image of post 安全与隐私机器学习：技术与应用的综述" /&gt;&lt;h1 id=&#34;安全与隐私机器学习技术与应用的综述&#34;&gt;☕安全与隐私机器学习：技术与应用的综述
&lt;/h1&gt;&lt;h2 id=&#34;abstract&#34;&gt;🥛Abstract
&lt;/h2&gt;&lt;p&gt;机器学习（ML）中的隐私违规可能导致歧视和身份盗窃等严重后果。随着近年来用于训练模型的敏感数据越来越多，保护隐私的机器学习方法的必要性变得更加重要。本综述研究全面回顾了隐私保护机器学习（PPML）的最新方法，如安全多方计算、同态加密和差分隐私。我们还评估了PPML的缺点，包括可扩展性、计算效率以及隐私与实用性之间的权衡。最后，我们识别了PPML中的开放问题和未来研究方向，包括新兴趋势、挑战和机会。本文旨在为对PPML领域感兴趣的研究人员和从业者提供有价值的资源。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;🍵Introduction
&lt;/h2&gt;&lt;p&gt;在当今的生活中，计算能力几乎与我们生活的每个方面都有互动。这种对计算机的高度依赖可能导致严重的隐私违规。计算机使用的主要趋势之一是机器学习（ML）。根据（Burkov, 2019），机器学习是“一个研究领域，专注于设计和开发能够从数据中学习和进行预测的算法，而不需要显式编程。换句话说，它涉及开发计算模型，这些模型可以根据从数据中获得的反馈自动改进其在特定任务上的表现。”机器学习正被用于解决医疗、金融等多个领域的许多生活问题。这项技术使用包含特征的数据集来构建所需的知识，以便得到正确的答案。有一种说法认为，训练阶段使用的数据记录越多，测试阶段的准确性就越高（Hey, Tansley, &amp;amp; Tolle, 2009）。从这个角度来看，科学家们开始请求与他们研究领域相关的数据，以便将其应用于机器学习模型。如果以医疗保健为例，根据健康保险流通与问责法案（HIPAA）规定，这些信息应通过建立国家标准来保护个人健康信息的隐私和安全，规范受保护健康信息（PHI）的使用和披露。 当这些数据被共享给科学家用于实验时，许多对策被考虑在内，例如隐藏个人身份信息（PII），然而，主要问题是：这些对策是否足以保护个人身份信息和受保护健康信息？&lt;/p&gt;
&lt;p&gt;有多位研究人员思考过这个问题；（Kim, Kim, &amp;amp; Kim, 2017）发现，在韩国存在通过居民注册号（RRN）去匿名化医疗数据的可能性，RRN是每位韩国居民的独特标识符。作者指出，利用公开可获取的信息，RRN可以被高精度地重新识别。此外，（Sweeney, 2002）声称，个人的出生日期、性别和邮政编码这三项信息可以用于唯一识别美国87%的人口。仅仅使用数据匿名化技术来保护个人身份信息是不够的。从这个角度来看，隐私保护机器学习（PPML）领域应运而生。&lt;/p&gt;
&lt;p&gt;隐私保护机器学习（PPML）是一种开发机器学习算法和模型的技术，旨在在推理和训练阶段保护敏感数据的隐私。在传统的机器学习中，模型通常使用已收集、整合并用于多种应用的数据进行训练。然而，当使用敏感数据（如财务或个人健康记录）时，这种方法可能会引发隐私问题。PPML通过创建允许在不危及基础数据隐私的情况下部署和训练机器学习模型的方法来解决这个问题。&lt;/p&gt;
&lt;p&gt;隐私保护机器学习（PPML）方法大致可以分为两类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;差分隐私。这种方法通过向数据中引入随机噪声来隐蔽用户身份并阻止私人信息泄露。&lt;/li&gt;
&lt;li&gt;安全多方计算（SMC）。多个参与方可以在不泄露任何私人数据的情况下共同训练机器学习模型。数据以一种允许每个参与方对数据进行计算而不直接访问数据的方式进行交换和加密。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在这项工作中，将讨论PPML的一个新颖概述。这项综述可以帮助读者找到研究空白，并提出与该领域相关的未来工作。本文的结构将如下：第二部分将介绍有助于理解将要讨论的主题的背景术语；第三部分将讨论用于PPML的技术；第四部分将讨论与该主题相关的最新论文；第五部分将讨论该主题的挑战；最后第六部分是结论。&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;🍾Background
&lt;/h2&gt;&lt;p&gt;在本节中，将提到所需的基础概念。本节中的术语对于理解我们研究的主题概念将是有用的。&lt;/p&gt;
&lt;h3 id=&#34;机器学习模型&#34;&gt;🍷机器学习模型。
&lt;/h3&gt;&lt;p&gt;机器学习（ML）被视为人工智能（AI）领域的一部分。它旨在构建一种方法，帮助机器在许多问题中做出决策。机器学习算法可以分为两个主要领域：监督学习和无监督学习算法。这两类之间的主要区别在于是否有来自所研究领域的专家提供的标签。如果算法在学习过程中需要带有标签的数据集，那么该算法被视为监督学习。一般来说，机器学习算法可以检测所用数据集特征中的模式，并基于这些模式对新案例进行预测。&lt;/p&gt;
&lt;h3 id=&#34;隐私保护机器学习ppml中的加密&#34;&gt;🍸隐私保护机器学习（PPML）中的加密。
&lt;/h3&gt;&lt;p&gt;通过促进私密数据的安全交换、存储和处理，密码学在隐私保护机器学习（PPML）中至关重要。为加密数据并对其进行计算而不向未经授权的方披露其内容，使用了同态加密、安全多方计算和秘密共享等密码学技术。同态加密允许对加密数据进行计算，而无需先解密，从而确保在计算过程中不会泄露敏感信息。安全多方计算允许多个参与方共同计算一个函数，而不向彼此披露数据。秘密共享将数据拆分成多个部分，并以一种方式分发到不同的参与方中，只有授权的参与方可以访问和重构原始数据。这些密码学技术使得开发能够保护敏感数据隐私的PPML算法成为可能，同时仍然能够提取有用的见解和知识。图1展示了同态加密在机器学习过程中的概述（Olzak, 2022）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241104204332300.png&#34;
	width=&#34;1049&#34;
	height=&#34;612&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241104204332300_hu1550586218636423810.png 480w, https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241104204332300_hu13796497834075906721.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241104204332300&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;171&#34;
		data-flex-basis=&#34;411px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;差分隐私&#34;&gt;🍹差分隐私
&lt;/h3&gt;&lt;p&gt;差分隐私提供了数据分析中隐私保证的精确规范。它确保在分析结果中不会泄露任何数据集参与者的私人信息。&lt;/p&gt;
&lt;p&gt;“隐私损失”和“隐私预算”的概念可以用来解释差分隐私的思想。当一个人的数据被添加到或从数据集中删除时，机制的输出不会发生显著改变，则该机制被称为具有差分隐私。数学上，随机算法或机制被称为满足ε-差分隐私，如果对于任何一对由于包含或排除单个个体数据而有所不同的数据集D和D&amp;rsquo;，以及对于任何可能的输出集合S，以下不等式成立：
&lt;/p&gt;
$$
Pr [M(D) ∈ S] ≤ exp(ε) Pr [M(D′) ∈ S]
$$&lt;p&gt;
在这个方程中，Pr [M(D) ∈ S] 表示应用于数据集 D 的机制 M 输出结果在集合 S 中的概率。ε 参数控制提供的隐私保护程度，其中较小的 ε 值意味着更强的隐私保证。&lt;/p&gt;
&lt;p&gt;为了用一个例子说明这一点，考虑一个公司希望发布其员工平均收入的汇总统计数据，同时保护个人隐私的场景。通过应用差分隐私，公司可以以受控的方式向真实的平均收入添加随机噪声。&lt;/p&gt;
&lt;p&gt;假设员工的真实平均收入为50,000土耳其里拉。如果没有差分隐私，攻击者可能通过将发布的平均收入与外部信息进行比较，来确定某位员工的薪水。然而，通过应用差分隐私，公司向平均收入添加了随机噪声。例如，报告的平均收入可能是50,200土耳其里拉或49,800土耳其里拉，且概率相等。这种额外的随机性确保了攻击者无法根据发布的统计数据可靠地推断任何个别员工的确切收入。&lt;/p&gt;
&lt;p&gt;通过仔细控制添加的噪声量（由 ε 参数确定），差分隐私实现了个人隐私保护与提供准确汇总信息之间的平衡。较小的 ε 值提供更强的隐私保证，但可能引入更多噪声，从而降低发布数据的准确性，而较大的 ε 值可能提供较少的隐私保护，但会产生更准确的结果。如需了解有关差分隐私的更多信息，请参阅 (Dwork, 2006)。&lt;/p&gt;
&lt;h3 id=&#34;机器学习的隐私保护技术&#34;&gt;🍺机器学习的隐私保护技术
&lt;/h3&gt;&lt;p&gt;鉴于机器学习中使用的模型和算法通常依赖于大量敏感数据，隐私在隐私保护机器学习（PPML）中是一个关键挑战。PPML的目标是创建能够从敏感数据中学习的机器学习模型，而不泄露提供数据的人员的任何敏感信息。在处理应保密的机密信息时，例如财务或个人健康数据，这一点尤其重要。为了确保隐私，机器学习中通常采用多种隐私保护策略。其中一些机制包括数据匿名化、加密和访问限制。数据匿名化是一种在使用数据进行机器学习之前去除个人身份信息的过程。去除姓名、地址和其他可能用于识别特定个人的数据就是一种实现方式。加密则是在数据中进行编码，只有获得适当权限的人才能访问。通过访问控制措施限制谁可以访问数据以及如何访问。这些控制措施对于确保私人信息得到保护以及在整个机器学习过程中保持敏感数据的隐私至关重要。&lt;/p&gt;
&lt;p&gt;除了这些工具，隐私保护机器学习（PPML）还需要在模型推理和训练过程中对数据处理和共享协议进行仔细设计。这些协议的设计方式对防止敏感数据泄露或不当使用至关重要。为了防止重新识别个人，可能会采用差分隐私技术，例如通过对数据进行增强。多个参与方可以通过安全多方计算，在不互相披露个人信息的情况下，共同计算一个函数。数据被分成多个片段，并通过秘密共享在多个参与方之间传输，这样只有授权方才能访问并重新组装原始数据。这些加密方法使得可以创建能够保护敏感数据隐私的机器学习算法，同时仍然能够提取有价值的信息。为了保持对算法的信任，并确保个人的私密信息不被泄露，机器学习中的隐私保护至关重要。&lt;/p&gt;
&lt;h2 id=&#34;ppml中的相关工作&#34;&gt;🥃PPML中的相关工作
&lt;/h2&gt;&lt;p&gt;在本节中，将讨论学术领域在PPML（隐私保护机器学习）方面的研究工作。我们选择了该领域的重要研究论文，旨在研究它们如何解决研究问题。&lt;/p&gt;
&lt;h4 id=&#34;al-rubaie-2019&#34;&gt;🥤Al-Rubaie, 2019
&lt;/h4&gt;&lt;p&gt;本文可以被视为开始研究PPML主题的良好资源。作者提供了关于机器学习中隐私面临的威胁和挑战的概述，以及在机器学习模型的训练和推理阶段保护隐私的解决方案和技术。&lt;/p&gt;
&lt;p&gt;首先，他们讨论了在机器学习模型中应用额外对策以增强隐私的重要性。他们展示了几种可能的未经授权访问或滥用共享数据的场景。根据他们的分类，机器学习过程中可以涉及三方：数据拥有者、处理者以及接收结果的人。当这些参与者属于同一方或同一人时，能够实现最高的安全性，然而并非所有情况都是如此。他们提到了五种隐私泄露的威胁等级，这些威胁包括：私密数据的明文存储、重构攻击、模型反演攻击、成员推断攻击和再识别攻击。&lt;/p&gt;
&lt;p&gt;在私密数据的明文存储中，数据在转移到计算部分时以明文形式存储在存储设备中，这可能会影响隐私，尤其是在数据存储被攻破或遭遇不忠诚员工的内部威胁时。专家建议仅发送必要的特征，而不是发送整个数据行，但这种解决方案会导致第二种威胁——重构攻击。获取提取特征的知识或元数据有助于重建原始数据，从而可能导致隐私泄露。&lt;/p&gt;
&lt;p&gt;在模型反演攻击中，攻击者通过使用机器学习模型的输出，生成与用于构建该模型的特征向量相似的向量。这些攻击利用了作为响应返回的置信度数据。知道某个成员或参与者的数据是否被用于训练阶段，会导致第四种威胁——成员推断攻击。为保护参与者在构建数据集过程中的隐私，提出了一种解决方案，即删除与个人身份信息（PII）相关的任何特征或列。然而，根据第五种威胁——再识别攻击，攻击者可以通过将收集到的数据集与其他数据集结合，来恢复这些被遗漏的数据。&lt;/p&gt;
&lt;p&gt;接着，研究讨论了几种保护机器学习隐私的方法和解决方案，包括联邦学习、同态加密、安全多方计算和差分隐私。每种方法都进行了详细阐述，包括其基本原理、优点以及潜在应用。&lt;/p&gt;
&lt;p&gt;研究还涵盖了PPML中的问题和权衡，以及处理敏感数据时出现的伦理和法律问题。这些问题包括对模型准确性、计算时间和通信开销的影响。&lt;/p&gt;
&lt;p&gt;该论文提供了对PPML领域最新进展的全面且易于理解的总结，并强调了该领域需要进一步研究和发展的必要性，以应对机器学习中隐私保护不断变化的风险和需求。&lt;/p&gt;
&lt;h4 id=&#34;liu-guo-lam--zhao-2022&#34;&gt;🧃Liu, Guo, Lam, &amp;amp; Zhao, 2022
&lt;/h4&gt;&lt;p&gt;本文的作者提出了一种可扩展的隐私保护方案，该方案能够容忍任何参与者在任何时候的掉线。他们在提案中使用了同态加密伪随机生成器（HPRG）和沙密尔秘密共享方案。该提议的解决方案通过避免从客户端构造用于生成掩码的种子，减少了通信开销。此外，在客户端掉线的情况下，也无需像他们在比较中使用的基于SecAgg的方案那样向服务器发送额外的沙密尔共享数据。他们声称，提出的方案在抗掉线恢复能力上比其他解决方案更强。&lt;/p&gt;
&lt;p&gt;在该方案的安全性分析中，提到了四个理论及其证明。其中两个理论讨论了系统在半诚实方的保护级别。在他们的定义中，半诚实参与者“不会偏离协议，但会试图推测诚实方的信息。” 第一个定理涉及排除服务器端的半诚实参与者。对于所有的 U, t, k，其中 |C| &amp;lt; t, $x_U$, U1, U2, U3 和 C，其中 C ⊆ U 且 U3 ⊆ U2 ⊆ U1，存在一个概率多项式时间（PPT）模拟器 SIM，使得：&lt;br&gt;
&lt;/p&gt;
$$
SIM^{U,t,k}_C (x_U, U_1, U_2, U_3) ≡ REAL^{U,t,k}_C (x_U, U_1, U_2, U_3)
$$&lt;p&gt;
第二个定理则关注包括服务器在内的半诚实参与者的安全性。对于所有的 U, t, k，其中 |C{S}| &amp;lt; t, $x_U$, U1, U2, U3 和 C，其中 C ⊆ U ∪ S 且 U3 ⊆ U2 ⊆ U1 ⊆ U，存在一个概率多项式时间（PPT）模拟器 SIM，使得：
&lt;/p&gt;
$$
SIM^{U,t,k}_C (x_U, U_1, U_2, U_3) ≡ REAL^{U,t,k}_C (x_U, U_1, U_2, U_3)
$$&lt;p&gt;
另外两个理论讨论了防御活跃恶意客户端的安全性。为提供所需的安全性，提出了一种基于 HPRG 的安全聚合协议。第三个定理关注恶意客户端和诚实服务器。对于所有的 U, t, k，其中 |C| &amp;lt; t, $x_{U/C}$ 和 C，使用算法 MC，其中 C ⊆ U，所提出的协议是一个计算 FHSecAgg 的安全协议。最后一个定理涉及包括服务器在内的恶意客户端的安全性。&lt;/p&gt;
&lt;p&gt;为了理解这些理论，我们需要了解其中术语的含义。$U$ 表示一组具有本地训练模型 $x_U$ 的客户端。$S$ 表示模型的服务器端。这两种理论中的等式表示两边分布上的恒等关系。关于所提出协议的概述，私有输入可以总结为每个客户端的本地训练模型或梯度，它们可以表示为向量、用于初始化秘密密钥的密钥、以及签名的秘密密钥。在服务器端，存储了用于验证秘密通道和签名的密钥。另一方面，协议的公共输入包括客户端的数量、丢失客户端的阈值和签名过程的公钥。本地训练模型的聚合结果可以视为协议的输出。图 2 显示了协议的步骤，并对每个步骤进行了简要描述。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105101503786.png&#34;
	width=&#34;1068&#34;
	height=&#34;666&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105101503786_hu12456287249950315205.png 480w, https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105101503786_hu10669958393845248634.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241105101503786&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;文章中进行了若干测试，证明了他们所提到的假设。&lt;/p&gt;
&lt;h4 id=&#34;gupta--singh-2022&#34;&gt;🧉Gupta &amp;amp; Singh, 2022
&lt;/h4&gt;&lt;p&gt;在本文中，作者提出了一种基于云的PPML模型，结合了差分隐私和机器学习模型。该模型明确了不可信方之间的通信协议。进行的实验包括朴素贝叶斯（NB）分类器，并应用于多个数据集，最终达到了95%的准确率，超过了已有文献中的结果。图3展示了所提模型，该模型被称为基于数据和分类服务的差分隐私保护机器学习模型（DA-PMLM）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102127794.png&#34;
	width=&#34;781&#34;
	height=&#34;713&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102127794_hu3910818257146447941.png 480w, https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102127794_hu6649347863904803145.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241105102127794&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;109&#34;
		data-flex-basis=&#34;262px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;从图3中，可以看到该架构中有四个主要参与者：数据所有者（DOid），即云客户端，是数据的创建者，并愿意将其共享到云端；分类器所有者（CO），是负责向云服务提供商（CSP）提供分类服务的一方，CSP 进一步向DOid提供服务；最后是请求用户（RUid），即从CSP请求所有者数据的实体。数据所有者在共享其私人数据之前，通过差分隐私向数据中注入噪声。&lt;/p&gt;
&lt;p&gt;他们证明了以下理论：“在所提出的模型中，分类模型的隐私保护机制满足ε-差分隐私的并行组合。”&lt;/p&gt;
&lt;p&gt;图4展示了所提模型的分类和数据流。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102353362.png&#34;
	width=&#34;953&#34;
	height=&#34;461&#34;
	srcset=&#34;https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102353362_hu16699203993426064.png 480w, https://JiangZhiyu-1024.github.io/p/%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9A%90%E7%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E8%BF%B0/image-20241105102353362_hu6502330861095835153.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241105102353362&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;206&#34;
		data-flex-basis=&#34;496px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在接收到带有噪声的数据 $D_{N1}^1, D_{N2}^2, \dots, D_{Nn}^n$ 来自相应的数据所有者（DOs）后，云服务提供商（CSP）通过应用标准化函数（使用Z-score标准化公式）对其进行预处理。得到的值可以用符号 $D_{\hat{N_i}}^i$ 表示。与任何机器学习过程类似，给定的数据集被划分为训练部分和测试部分。之后，进行分类模型（CM）的训练和测试，在实验中使用了朴素贝叶斯（NB）分类器，以进行评估测量。&lt;/p&gt;
&lt;h4 id=&#34;lee-et-al-2022&#34;&gt;🧊Lee, et al., 2022
&lt;/h4&gt;&lt;p&gt;本文的作者声称，当前的PPML解决方案仅限于非标准的机器学习模型。这些解决方案在实际数据集上未能证明取得良好的结果。此外，所使用的激活函数从算术角度来看较为简单，并且替换了非算术激活函数。它们还避免使用自助法（bootstrapping）。当前的解决方案中也无法实现大量层数，他们的提议包括使用标准的ResNet-20模型与RNS-CKKS全同态加密（FHE）结合自助法进行实现。为了验证他们的模型，他们将其应用于一个标准化的数据集——CIFAR-10。所得结果与使用原始ResNet-20模型且未加密时的结果非常接近，最终的准确率约为92.43%。&lt;/p&gt;
&lt;h2 id=&#34;ppml中的挑战&#34;&gt;🍻PPML中的挑战
&lt;/h2&gt;&lt;p&gt;PPML面临着若干挑战，这些挑战阻碍了其在实际应用中的广泛采用。主要的挑战之一是隐私与效用之间的权衡。像差分隐私和安全多方计算这样的PPML技术通常会引入噪声或通信开销，这可能显著影响机器学习模型的准确性和效率。另一个挑战是不同PPML技术之间缺乏标准化和互操作性，使得比较和结合不同方法的结果变得困难。此外，PPML需要专门的专业知识和资源，如加密学和安全计算的知识，这些可能并不容易为所有组织所拥有。最后，PPML还涉及法律和伦理问题，如合规性和透明度，这些问题必须仔细处理，以确保敏感数据的负责任和可信赖的使用。&lt;/p&gt;
&lt;h2 id=&#34;结论与未来工作&#34;&gt;🥂结论与未来工作
&lt;/h2&gt;&lt;p&gt;敏感数据的隐私必须得到保护，而隐私保护机器学习（PPML）这一研究领域正迅速发展。随着机器学习在医疗、银行和社交媒体等行业的广泛应用，隐私保护系统的重要性愈加突出。为了确保人们的敏感信息不被泄露，能够在尊重隐私的前提下支持机器学习的隐私保护算法和协议至关重要。该领域的研究进展涉及信息理论、机器学习和密码学等计算机科学多个分支，依赖于跨学科的合作。PPML是一个具有挑战性且复杂的问题，但它是实现机器学习带来益处的前提，而不牺牲个人隐私。在本次综述中，我们概述了PPML的主要难题和未来的研究方向，并回顾了PPML中使用的主要方法和技术。为了克服该领域中的重大问题，我们希望本综述能够成为对PPML感兴趣的研究人员和实践者的重要参考资源。未来的研究工作将包括在真实数据上的PPML研究，并与传统机器学习模型的表现进行比较。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
